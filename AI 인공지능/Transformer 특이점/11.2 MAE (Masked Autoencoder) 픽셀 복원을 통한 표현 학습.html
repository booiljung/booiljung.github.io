<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:11.2 MAE (Masked Autoencoder) 픽셀 복원을 통한 표현 학습</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>11.2 MAE (Masked Autoencoder) 픽셀 복원을 통한 표현 학습</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>11.2 MAE (Masked Autoencoder) 픽셀 복원을 통한 표현 학습</span></nav>
                </div>
            </header>
            <article>
                <h1>11.2 MAE (Masked Autoencoder) 픽셀 복원을 통한 표현 학습</h1>
<p>2025-12-22, G30DR</p>
<p>2020년대 초반, 컴퓨터 비전(Computer Vision) 분야는 비전 트랜스포머(Vision Transformer, ViT)의 등장으로 거대한 지각 변동을 겪었다. 이미지를 패치(Patch) 단위의 시퀀스로 취급하는 이 새로운 패러다임은 자연어 처리(NLP)에서의 성공 방정식을 비전 영역으로 이식하려는 시도였다. 그러나 진정한 의미의 ’트랜스포머 싱귤래리티’는 단순히 아키텍처를 차용한 것을 넘어, 학습 방법론의 근본적인 통합에서 비롯되었다. 그 중심에는 카이밍 허(Kaiming He) 연구팀이 제안한 **마스크드 오토인코더(Masked Autoencoder, MAE)**가 존재한다. MAE는 “보이지 않는 것을 통해 보이는 것을 이해한다“는 철학적 명제를 수학적으로 구현함으로써, 대조 학습(Contrastive Learning)이 지배하던 자기지도 학습(Self-Supervised Learning)의 흐름을 생성적(Generative) 모델링으로 회귀시켰다.1 본 절에서는 MAE가 어떻게 단순한 픽셀 복원 과제를 통해 고차원의 시각적 추론 능력을 획득하는지, 그 기저에 깔린 정보 이론적 배경과 아키텍처의 비대칭성, 그리고 확장성(Scalability)에 대해 심층적으로 논의한다.</p>
<h2>1.  시각적 정보의 중복성과 마스킹의 본질</h2>
<p>MAE의 설계 원리를 이해하기 위해서는 먼저 ’이미지’라는 데이터가 가진 고유한 정보 밀도(Information Density)의 특성을 파악해야 한다. 자연어 처리 분야의 기념비적인 모델인 BERT는 입력 문장의 약 15%를 마스킹하고 이를 예측하는 방식으로 언어의 문맥을 학습했다. 언어는 인간이 인위적으로 생성한 고도로 압축된 상징 체계(Symbolic System)이므로, 단어 하나하나가 내포하는 정보량이 매우 많다. 따라서 15%라는 적은 비율만 가려도 문장의 의미를 파악하고 누락된 단어를 추론하는 것은 상당히 난이도 높은 과제가 된다.3</p>
<p>반면, 이미지는 자연 신호(Natural Signal)로서 극도로 높은 공간적 중복성(Spatial Redundancy)을 가진다. 인접한 픽셀들은 서로 매우 유사한 색상과 텍스처를 공유하며, 국소적인 영역(Patch) 하나가 사라지더라도 주변 픽셀들을 통해 단순 보간(Interpolation)하는 것이 어렵지 않다. 만약 이미지에 대해 BERT와 동일하게 15%의 마스킹 비율을 적용한다면 어떤 현상이 발생할까? 모델은 굳이 객체의 형태나 장면의 의미를 이해하려 노력하지 않고, 주변 픽셀의 통계적 평균이나 텍스처 패턴을 복사하는 저차원적인 ‘지름길(Shortcut)’ 학습에만 몰두하게 된다.3</p>
<p>이러한 통찰에 기반하여 MAE는 기존의 상식을 깨는 파격적인 전략을 취한다. 바로 입력 이미지 패치의 **75%**를 무작위로 제거하는 것이다. 전체 정보의 4분의 3을 삭제하고 오직 25%의 파편화된 조각만을 남겨두었을 때, 단순한 보간이나 외삽(Extrapolation)은 더 이상 불가능해진다. 모델이 누락된 정보를 복원하기 위해서는 남겨진 조각들이 ’강아지의 귀’인지, ’자전거의 바퀴’인지를 식별하고, 이들이 전체 공간에서 어떻게 배치되어야 하는지 논리적으로 추론해야 한다. 즉, 극단적인 마스킹 비율은 모델로 하여금 국소적인 텍스처 매칭이 아닌, 전체적인 게슈탈트(Gestalt) 형성을 강제하는 핵심 기제가 된다.2</p>
<p>이러한 접근은 오토인코더(Autoencoder)의 고전적인 정의, 즉 입력을 압축된 잠재 표현(Latent Representation)으로 인코딩했다가 다시 원본으로 디코딩하는 과정을 현대적인 트랜스포머 아키텍처와 결합한 것이다. 과거의 디노이징 오토인코더(Denoising Autoencoder)가 잡음을 제거하며 신호를 복원했다면, MAE는 ’삭제된 공간’을 상상력으로 채우며 시각적 개념을 학습한다. 이는 컴퓨터 비전 모델이 단순한 패턴 인식기를 넘어, 장면의 인과관계와 구조를 이해하는 추론기로 진화하는 결정적인 계기를 마련했다.6</p>
<h2>2.  비대칭적 인코더-디코더 아키텍처 (Asymmetric Encoder-Decoder)</h2>
<p>MAE의 가장 혁신적인 기술적 기여는 인코더와 디코더를 비대칭적(Asymmetric)으로 설계하여 학습 효율성과 표현 능력이라는 두 마리 토끼를 동시에 잡았다는 점이다. 기존의 트랜스포머 기반 마스킹 모델들(예: BEiT)이나 BERT는 마스킹된 부분에는 특수한 `` 토큰을 대체해 넣고, 전체 시퀀스를 인코더에 통과시키는 방식을 사용했다. 그러나 MAE는 이를 철저히 분리한다.1</p>
<h3>2.1  가시적 패치만 처리하는 인코더 (Encoder on Visible Patches Only)</h3>
<p>MAE의 인코더는 표준적인 ViT(Vision Transformer) 구조를 따르지만, 입력 처리 방식에서 결정적인 차이가 있다. 입력 이미지를 패치로 분할한 후, 마스킹된 75%의 패치는 인코더 입력 단계에서 완전히 배제된다. 오직 <strong>보이는(Visible) 25%의 패치</strong>만이 선형 투영(Linear Projection)과 위치 임베딩(Positional Embedding)을 거쳐 인코더로 유입된다.1</p>
<p>이러한 설계는 연산 효율성 측면에서 엄청난 이득을 제공한다. 트랜스포머의 핵심 연산인 셀프 어텐션(Self-Attention)의 복잡도는 입력 토큰 수 <span class="math math-inline">N</span>에 대해 <span class="math math-inline">O(N^2)</span>로 증가한다. 입력 토큰을 4분의 1로 줄이면(<span class="math math-inline">N \rightarrow N/4</span>), 어텐션 연산량은 이론적으로 <span class="math math-inline">1/16</span>로 급감한다. 이는 ViT-Large나 ViT-Huge와 같은 거대 모델을 훈련할 때 메모리 병목을 해소하고 훈련 속도를 획기적으로 가속화한다. 실제로 MAE는 전체 토큰을 처리하는 방식 대비 3배 이상의 훈련 속도 향상을 보여주었다.9 또한, 인코더가 `` 토큰이라는 인공적인 신호를 전혀 보지 않게 됨으로써, 학습 단계와 추론 단계(전체 이미지가 입력됨) 사이의 분포 차이(Distribution Shift)를 원천적으로 제거하는 효과도 얻는다.</p>
<h3>2.2  픽셀 복원을 위한 경량 디코더 (Lightweight Decoder)</h3>
<p>디코더는 인코더가 압축한 잠재 표현을 다시 원본 이미지 픽셀로 복원하는 역할을 수행한다. MAE의 디코더 입력은 두 부분으로 구성된다.</p>
<ol>
<li><strong>인코딩된 가시적 토큰 (Encoded Visible Tokens):</strong> 인코더를 통과하여 고차원 특징으로 변환된 25%의 패치들.</li>
<li><strong>마스크 토큰 (Mask Token):</strong> 마스킹된 위치를 나타내는 학습 가능한 벡터. 이 토큰들은 모두 동일한 벡터를 공유하지만, 서로 다른 위치 임베딩이 더해져 각자가 복원해야 할 이미지 상의 위치 정보를 갖게 된다.1</li>
</ol>
<p>MAE의 디코더는 인코더에 비해 매우 작고 가볍게 설계된다. 기본 설정에서 디코더는 인코더 대비 10% 미만의 연산량만을 사용하며, 일반적으로 8개의 트랜스포머 블록과 512차원의 너비를 가진다.8 실험 결과, 디코더를 단 하나의 블록으로 극단적으로 축소해도 미세 조정(Fine-tuning) 성능에는 큰 저하가 없었다. 이는 인코더가 이미 이미지의 핵심적인 의미론적 정보를 충분히 추출했음을 시사하며, 디코더는 단지 이 잠재 표현을 픽셀 공간으로 사영(Projection)하는 역할에 그친다는 것을 보여준다.11 결정적으로, 이 디코더는 사전 학습(Pre-training)이 끝나면 폐기되며, 다운스트림 태스크(Downstream Task)에는 오직 인코더만이 사용된다.</p>
<h3>2.3  구조적 비대칭성의 의의</h3>
<p>이러한 비대칭 구조는 컴퓨터 비전에서의 오토인코더 적용을 가로막던 연산 비용 문제를 해결했다. 언어 모델과 달리 이미지는 픽셀 수가 많아 시퀀스 길이가 매우 긴데, 전체를 복원하는 무거운 디코더를 사용할 경우 학습 비용이 감당할 수 없을 만큼 커진다. MAE는 “인코더는 무겁게(추상화 담당), 디코더는 가볍게(복원 담당)“라는 원칙을 세우고, 입력 데이터 자체를 희소하게(Sparse) 만듦으로써 대규모 비전 모델의 학습을 민주화(Democratization)하는 데 기여했다.</p>
<h2>3.  학습 목표 및 최적화 전략</h2>
<p>MAE의 학습 과정은 단순명료하지만, 그 내부의 하이퍼파라미터 결정에는 깊은 실험적 통찰이 담겨 있다.</p>
<h3>3.1  마스킹 비율(Masking Ratio)의 최적점</h3>
<p>앞서 언급했듯 MAE는 75%라는 높은 마스킹 비율을 채택한다. 연구진은 마스킹 비율을 10%에서 90%까지 변화시키며 선형 프로빙(Linear Probing)과 미세 조정(Fine-tuning) 성능을 분석했다. 그 결과, 미세 조정 성능은 40%에서 80% 사이의 넓은 구간에서 강건(Robust)하게 유지되었으나, 선형 프로빙 성능은 75% 부근에서 정점을 찍고 급격히 하락하는 양상을 보였다.2</p>
<p>이는 마스킹 비율이 높아질수록 모델이 학습하는 표현(Representation)이 더 추상적이고 비선형적인 성격을 띠게 됨을 의미한다. 낮은 마스킹 비율(예: 20%)에서는 모델이 픽셀의 국소적 상관관계만을 학습하여 선형적으로 분리가 쉬운 특징을 추출하지만, 높은 마스킹 비율에서는 빈 공간을 채우기 위해 고차원적인 추론을 수행해야 하므로 특징 공간이 더 복잡하고 깊어진다. 75%는 이러한 난이도와 정보량 사이의 균형점(Sweet Spot)으로 작용한다.2</p>
<h3>3.2  픽셀 정규화(Pixel Normalization)와 재구성 타겟</h3>
<p>MAE는 복잡한 토크나이저(Tokenizer) 없이 픽셀 값(RGB)을 직접 예측하는 회귀(Regression) 문제를 푼다. 초기 연구들에서는 픽셀 값을 직접 예측하는 것이 고주파(High-frequency) 잡음이나 조명 변화 등에 민감하여 표현 학습에 불리하다고 여겨졌다. 이를 극복하기 위해 BEiT는 dVAE(Discrete Variational Autoencoder)를 사전 학습하여 이미지를 이산 토큰으로 변환하는 우회로를 택했다.2</p>
<p>그러나 MAE는 **패치 단위 정규화(Per-patch Normalization)**라는 단순한 기법으로 이 문제를 해결했다. 손실 함수를 계산하기 전에, 각 패치 내의 픽셀 값들에 대해 평균과 분산을 구하여 정규화(Normalize)를 수행하고, 모델이 이 정규화된 값을 예측하도록 한 것이다.<br />
<span class="math math-display">
y_{normalized} = \frac{y - \mu_{patch}}{\sigma_{patch}}
</span><br />
이 과정은 패치마다 달라질 수 있는 조명이나 대비(Contrast)와 같은 저수준 통계 정보를 제거하고, 오직 구조적인 형태(Structure)와 패턴에만 집중하게 만든다. 실험 결과, 정규화된 픽셀을 예측하는 것이 dVAE 토큰을 예측하는 것보다 성능이 우수하거나 대등했으며, 복잡한 추가 모델 없이도 안정적인 학습이 가능함을 증명했다.2</p>
<h3>3.3  손실 함수(Loss Function)</h3>
<p>손실 함수로는 평균 제곱 오차(Mean Squared Error, MSE)를 사용한다. 여기서 중요한 점은, 전체 이미지가 아닌 마스킹된 패치에 대해서만 손실을 계산한다는 것이다.<br />
<span class="math math-display">
Loss = \frac{1}{N_{masked}} \sum_{i \in Masked} (x_i - \hat{x}_i)^2
</span><br />
보이는 패치는 이미 인코더에 입력으로 주어졌으므로, 이를 다시 복원하는 것은 ‘정답을 보고 베끼는’ 것과 같아 학습에 아무런 도움이 되지 않는다. 오직 가려진 부분에 대한 예측 오차만을 역전파(Backpropagation)함으로써, 모델은 미지의 정보를 추론하는 능력만을 선택적으로 강화하게 된다. 이는 BERT가 마스킹된 단어에 대해서만 Cross-Entropy Loss를 계산하는 것과 동일한 원리다.1</p>
<h3>3.4  데이터 증강(Data Augmentation)의 최소화</h3>
<p>대조 학습(Contrastive Learning) 기반의 방법론들(SimCLR, MoCo, DINO)은 데이터 증강에 극도로 의존한다. 색상 왜곡(Color Jittering), 블러(Blur), 회전(Rotation) 등을 통해 뷰(View)를 다양화하지 않으면 모델이 붕괴(Collapse)하거나 학습이 제대로 이루어지지 않는다. 반면, MAE는 오직 **임의 자르기(Random Resized Crop)**와 수평 뒤집기(Horizontal Flip) 정도의 최소한의 증강만으로도 최고 성능을 달성한다.2</p>
<p>이는 마스킹 자체가 매우 강력한 데이터 증강 역할을 수행하기 때문이다. 동일한 이미지라도 에폭(Epoch)마다 서로 다른 75%의 영역이 가려지므로, 모델은 매번 새로운 퍼즐을 푸는 것과 같은 효과를 얻는다. 이는 모델이 색상 분포나 인공적인 변형에 과적합되지 않고, 이미지 본연의 구조적 특징을 학습하는 데 집중하게 돕는다.2</p>
<h2>4.  확장성(Scalability)과 성능 분석</h2>
<p>MAE의 진가는 모델의 크기를 키웠을 때 드러나는 확장성에서 확인된다. 이는 “트랜스포머 싱귤래리티“의 핵심 징후 중 하나로, 데이터와 모델 파라미터가 증가함에 따라 성능이 포화되지 않고 지속적으로 향상되는 현상이다.</p>
<h3>4.1  ImageNet-1K 벤치마크</h3>
<p>MAE는 ImageNet-1K 데이터셋만을 사용하여 ViT-Huge(파라미터 6억 3천만 개) 모델을 훈련시켰을 때, 미세 조정(Fine-tuning) 후 **87.8%**의 Top-1 정확도를 기록했다.2 이는 당시 JFT-300M(3억 장)과 같은 거대 비공개 데이터셋을 사용하지 않고 ImageNet-1K만으로 달성한 최고 기록이었다.</p>
<p>기존의 지도 학습(Supervised Learning) 방식은 데이터 양이 제한된 상태에서 모델 크기만 키우면 심각한 과적합(Overfitting) 문제가 발생했다. 그러나 MAE의 복원 과제(Reconstruction Task)는 지도 학습의 레이블보다 훨씬 더 풍부한 감독 신호(Supervisory Signal)를 제공하며, 75% 마스킹 자체가 강력한 정칙화(Regularization) 효과를 주어 과적합을 방지한다.</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>파라미터 수</strong></th><th><strong>사전 학습 데이터</strong></th><th><strong>Top-1 정확도 (%)</strong></th></tr></thead><tbody>
<tr><td>ViT-Large (Supervised)</td><td>304M</td><td>ImageNet-1K</td><td>82.5 (Strong Reg.)</td></tr>
<tr><td>ViT-Large (MAE)</td><td>304M</td><td>ImageNet-1K</td><td><strong>85.9</strong></td></tr>
<tr><td>ViT-Huge (MAE)</td><td>632M</td><td>ImageNet-1K</td><td><strong>87.8</strong></td></tr>
</tbody></table>
<p>위 표에서 볼 수 있듯, 동일한 아키텍처(ViT-Large) 하에서도 MAE 사전 학습은 처음부터 지도 학습을 수행한 경우(Scratch)보다 월등히 높은 성능을 보인다.2 이는 MAE가 초기화(Initialization) 단계에서 훨씬 더 일반화된 특징 공간을 학습해 놓음을 의미한다.</p>
<h3>4.2  훈련 효율성 비교</h3>
<p>효율성 측면에서도 MAE는 탁월하다. ViT-Huge 모델을 기준으로 MAE는 BEiT 대비 에폭당 훈련 시간이 <strong>3.5배</strong> 빠르며, 메모리 사용량도 현저히 적다.9 이는 입력 토큰을 25%만 사용하는 인코더 구조 덕분이며, 이를 통해 연구자들은 제한된 GPU 자원으로도 거대 모델을 실험할 수 있게 되었다. 예를 들어, 1600 에폭이라는 매우 긴 시간 동안 학습을 수행해도 전체 훈련 비용은 감당 가능한 수준으로 유지된다.9</p>
<h2>5.  다운스트림 태스크로의 전이 학습 (Transfer Learning)</h2>
<p>MAE로 학습된 표현의 범용성을 검증하기 위해, 단순 분류를 넘어 객체 탐지(Object Detection), 시멘틱 분할(Semantic Segmentation) 등 다양한 다운스트림 태스크에서의 성능을 분석해보자.</p>
<h3>5.1  COCO 객체 탐지 및 인스턴스 분할</h3>
<p>Mask R-CNN의 백본으로 MAE 사전 학습된 ViT를 적용했을 때, MAE는 기존의 지도 학습 기반 모델을 압도한다.</p>
<ul>
<li>
<p><strong>ViT-Base:</strong> 지도 학습 대비 AP(Average Precision) <strong>+2.4</strong> 포인트 상승 (50.3 vs 47.9).</p>
</li>
<li>
<p>ViT-Large: 지도 학습 대비 AP +4.0 포인트 상승 (53.3 vs 49.3).</p>
</li>
</ul>
<p>이 결과는 특히 주목할 만하다. 객체 탐지는 “무엇이(What)“뿐만 아니라 “어디에(Where)” 있는지를 알아야 하는 과제다. MAE는 픽셀 복원 과정에서 위치 정보와 객체의 경계, 부분과 전체의 관계를 정밀하게 학습했기 때문에, 위치 추정에 매우 강한 면모를 보인다.2</p>
<h3>5.2  ADE20K 시멘틱 분할</h3>
<p>UperNet을 이용한 시멘틱 분할 실험에서도 유사한 경향이 나타난다. MAE 기반 ViT-Large 모델은 지도 학습 모델보다 mIoU(mean Intersection over Union)가 <strong>3.7</strong> 포인트 높았다.2 이는 픽셀 단위의 조밀한 예측(Dense Prediction) 태스크에서도 MAE의 표현력이 유효함을 증명한다.</p>
<h3>5.3  타 도메인으로의 확장</h3>
<p>MAE의 성공은 이미지에만 국한되지 않는다. 오디오 스펙트로그램을 이미지처럼 취급하여 마스킹 복원을 수행하는 AudioMAE, 비디오의 시공간 패치를 마스킹하는 VideoMAE, 심지어 의료 영상(Medical Imaging)이나 화학 분자 구조 예측 등 다양한 도메인으로 확장되고 있다. MAE의 단순함(Simplicity)은 도메인 특화 지식(Domain-specific Knowledge) 없이도 해당 데이터의 통계적 구조를 학습할 수 있는 강력한 베이스라인을 제공한다.20</p>
<h2>6.  MAE의 특성 심층 분석: 선형성과 비선형성</h2>
<p>MAE 학습 결과에 대한 흥미로운 관찰 중 하나는 선형 프로빙(Linear Probing)과 미세 조정(Fine-tuning) 간의 성능 괴리다.</p>
<p>대조 학습 모델인 DINO나 MoCo는 선형 프로빙에서 매우 높은 성능을 보인다. 이는 대조 학습이 데이터 포인트들을 구의 표면(Hypersphere) 상에 고르게 분포시키며 선형적으로 잘 분리되도록 강제하기 때문이다. 반면, MAE의 선형 프로빙 성능은 이들보다 다소 낮다.14</p>
<p>그러나 전체 네트워크를 학습시키는 미세 조정에서는 MAE가 대조 학습 모델들을 앞선다. 이는 MAE가 학습한 특징이 비선형적이고 복잡한 구조를 띠고 있음을 시사한다. 픽셀 복원이라는 과제는 이미지의 심층적인 구조를 이해해야만 풀 수 있기에, 그 표현 공간 역시 단순한 선형 결합으로는 설명되지 않는 고차원적 정보를 담고 있는 것이다. 따라서 MAE를 사용할 때는 인코더의 가중치를 고정하지 않고 전체를 미세 조정하는 것이 잠재력을 100% 끌어내는 방법이다.13</p>
<h2>7.  비교 분석: MAE vs. MIM 경쟁자들</h2>
<p>MAE의 위치를 명확히 하기 위해 주요 경쟁 방법론들과의 비교를 요약하면 다음과 같다.</p>
<table><thead><tr><th><strong>특징</strong></th><th><strong>MAE (He et al.)</strong></th><th><strong>BEiT (Bao et al.)</strong></th><th><strong>DINO (Caron et al.)</strong></th></tr></thead><tbody>
<tr><td><strong>패러다임</strong></td><td>생성적 (Generative)</td><td>생성적 (Generative)</td><td>판별적 (Discriminative)</td></tr>
<tr><td><strong>목표 과제</strong></td><td>픽셀 복원 (Regression)</td><td>토큰 예측 (Classification)</td><td>특징 유사성 학습</td></tr>
<tr><td><strong>타겟 (Target)</strong></td><td>정규화된 픽셀값</td><td>dVAE 이산 토큰</td><td>교사 네트워크의 출력</td></tr>
<tr><td><strong>인코더 입력</strong></td><td><strong>가시적 패치 (25%)</strong></td><td>전체 패치 (마스크 포함)</td><td>전체 이미지 (Augmented)</td></tr>
<tr><td><strong>효율성</strong></td><td><strong>최상 (입력 토큰 1/4)</strong></td><td>보통 (전체 토큰 + dVAE)</td><td>보통 (Multi-crop 연산 과다)</td></tr>
<tr><td><strong>데이터 증강</strong></td><td>최소 (Random Crop)</td><td>보통</td><td><strong>필수 (Color Jitter 등)</strong></td></tr>
<tr><td><strong>주요 강점</strong></td><td>확장성, 미세 조정 성능, 속도</td><td>MIM의 선구자</td><td>선형 프로빙, 비지도 분할</td></tr>
</tbody></table>
<p>MAE는 BEiT가 필요로 했던 dVAE라는 사전 지식을 제거하고, DINO가 의존했던 복잡한 데이터 증강과 대조 손실 함수를 걷어냄으로써, “Less is More(단순한 것이 더 낫다)“라는 철학을 증명했다. 특히 픽셀 그 자체를 복원 타겟으로 삼아도 충분하다는 발견은, 우리가 데이터의 본질적인 정보를 과소평가하고 있었음을 일깨워준다.14</p>
<h2>8.  결론: 생성적 사전 학습의 부활</h2>
<p>MAE는 비전 트랜스포머의 잠재력을 완전히 해방시킨 연구이자, 컴퓨터 비전의 학습 패러다임을 대조 학습에서 다시 생성적 복원 학습으로 되돌린 변곡점이다. 11.2절에서 살펴본 MAE의 성공 요인은 세 가지로 요약된다.</p>
<ol>
<li><strong>높은 마스킹 비율 (75%):</strong> 이미지의 중복성을 상쇄하고 게슈탈트 추론을 강제함.</li>
<li><strong>비대칭 아키텍처:</strong> 인코더의 효율성과 디코더의 복원 능력을 분리하여 확장성을 확보함.</li>
<li><strong>단순함 (Simplicity):</strong> 픽셀 정규화와 최소한의 증강만으로도 최고의 성능을 달성함.</li>
</ol>
<p>이러한 MAE의 방법론은 ’트랜스포머 싱귤래리티’를 향한 여정에서 중요한 의미를 갖는다. 텍스트, 이미지, 오디오 등 서로 다른 모달리티가 이제는 “일부를 가리고 나머지를 복원한다“는 하나의 통일된 원리(Unified Principle) 아래에서 학습될 수 있게 되었다. MAE는 인공지능이 단순히 데이터를 분류하는 것을 넘어, 데이터의 생성 원리를 이해하고 누락된 정보를 상상할 수 있는 단계로 진화했음을 알리는 신호탄이라 할 수 있다. 이제 우리는 이미지를 토큰으로 처리하는 것을 넘어, 그 토큰들 사이의 빈 공간을 지능으로 채우는 시대를 맞이하였다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>[PDF] Masked Autoencoders Are Scalable Vision Learners - Semantic Scholar, https://www.semanticscholar.org/paper/Masked-Autoencoders-Are-Scalable-Vision-Learners-He-Chen/6351ebb4a3287f5f3e1273464b3b91e5df5a16d7</li>
<li>Masked Autoencoders Are Scalable Vision Learners - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf</li>
<li>TI-MAE: SELF-SUPERVISED MASKED TIME SERIES AUTOENCODERS - OpenReview, https://openreview.net/pdf?id=9AuIMiZhkL2</li>
<li>[2111.06377] Masked Autoencoders Are Scalable Vision Learners - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2111.06377</li>
<li>2: Masked Autoencoders Are Scalable Vision Learners - ML-Neuro - BayernCollab, https://collab.dvb.bayern/spaces/TUMmlneuro/pages/651271074/2+Masked+Autoencoders+Are+Scalable+Vision+Learners</li>
<li>[D] Surprisingly Simple SOTA Self-Supervised Pretraining - Masked Autoencoders Are Scalable Vision Learners by Kaiming He et al. explained (5-minute summary by Casual GAN Papers) : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/qvmkjg/d_surprisingly_simple_sota_selfsupervised/</li>
<li>Understanding Masked Autoencoders From a Local Contrastive Perspective - arXiv, https://arxiv.org/html/2310.01994v1</li>
<li>Masked Autoencoder (MAE) Architecture - Emergent Mind, https://www.emergentmind.com/topics/masked-autoencoder-mae-architecture</li>
<li>Efficient MAE Towards Large-Scale Vision Transformers - CVF Open Access, https://openaccess.thecvf.com/content/WACV2024/papers/Han_Efficient_MAE_Towards_Large-Scale_Vision_Transformers_WACV_2024_paper.pdf</li>
<li>SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners - arXiv, https://arxiv.org/html/2205.14540v3</li>
<li>Masked Autoencoders (MAE): The Art of Seeing More by Masking Most &amp; PyTorch Implementation | by Övül Arslan | Medium, https://medium.com/@ovularslan/masked-autoencoders-mae-the-art-of-seeing-more-by-masking-most-pytorch-implementation-4566e08c66a6</li>
<li>Paper Review: Masked Autoencoders Are Scalable Vision Learners | by Cenk Bircanoglu, https://cenk-bircanoglu.medium.com/paper-review-masked-autoencoders-are-scalable-vision-learners-9c98fc50ff1a</li>
<li>Paper Summary: Masked Autoencoders Are Scalable Vision Learners | by Reza Yazdanfar, https://rezayazdanfar.medium.com/paper-summary-masked-autoencoders-are-scalable-vision-learners-2dea8cdb1884</li>
<li>Masked Autoencoder: Scalable Self-Supervised Vision Representation Learning via … - Medium, https://medium.com/@kdk199604/masked-autoencoder-scalable-self-supervised-vision-representation-learning-via-autoencoder-e9d96fd65ac2</li>
<li>The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning - arXiv, https://arxiv.org/html/2412.17566v1</li>
<li>Masked autoencoder (MAE) for visual representation learning. Form the author of ResNet., https://mchromiak.github.io/articles/2021/Nov/14/Masked-Autoencoders-Are-Scalable-Vision-Learners/</li>
<li>Loss Functions in Machine Learning Explained - DataCamp, https://www.datacamp.com/tutorial/loss-function-in-machine-learning</li>
<li>[D] (Paper Overview) MAE: Masked Autoencoders Are Scalable Vision Learners - Reddit, https://www.reddit.com/r/MachineLearning/comments/qt4y6g/d_paper_overview_mae_masked_autoencoders_are/</li>
<li>facebookresearch/mae: PyTorch implementation of MAE https//arxiv.org/abs/2111.06377 - GitHub, https://github.com/facebookresearch/mae</li>
<li>Comparison of fine-tuning strategies for transfer learning in medical image classification, https://arxiv.org/html/2406.10050v1</li>
<li>Revisiting MAE Pre-training for 3D Medical Image Segmentation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2025/papers/Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation_CVPR_2025_paper.pdf</li>
<li>Comparison between Masked Autoencoder (MAE) and DINO-based pre-training… | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Comparison-between-Masked-Autoencoder-MAE-and-DINO-based-pre-training-for-fine-grained_tbl2_384598759</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>