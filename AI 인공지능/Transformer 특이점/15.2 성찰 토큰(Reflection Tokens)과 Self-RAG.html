<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:15.2 성찰 토큰(Reflection Tokens)과 Self-RAG</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>15.2 성찰 토큰(Reflection Tokens)과 Self-RAG</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>15.2 성찰 토큰(Reflection Tokens)과 Self-RAG</span></nav>
                </div>
            </header>
            <article>
                <h1>15.2 성찰 토큰(Reflection Tokens)과 Self-RAG</h1>
<h2>1.  서론: 확률적 앵무새에서 메타인지적 추론기로</h2>
<p>2024년까지의 거대 언어 모델(LLM) 발전사가 파라미터의 규모와 데이터의 양을 늘려 ’다음 토큰 예측(Next Token Prediction)’의 정확도를 극한으로 끌어올리는 과정이었다면, 2025년은 모델이 자신의 출력물을 스스로 검증하고 제어하는 ’시스템 2(System 2)’적 사고 능력을 내재화하는 원년이라 정의할 수 있다. 우리는 앞선 장들에서 트랜스포머가 어텐션 메커니즘을 통해 문맥을 파악하고 정보를 압축하는 방식을 살펴보았다. 그러나 전통적인 트랜스포머 구조는 근본적인 한계를 지니고 있다. 그것은 모델이 자신이 무엇을 모르는지 알지 못하며(Unawareness of Ignorance), 자신이 생성한 문장이 사실에 부합하는지 실시간으로 판단할 메커니즘이 부재하다는 점이다.</p>
<p>이러한 한계는 검색 증강 생성(RAG, Retrieval-Augmented Generation)의 도입으로 일부 완화되었으나, 표준 RAG 역시 ’무차별적 검색(Indiscriminate Retrieval)’이라는 새로운 문제를 야기했다. 사용자의 질문이 창의적인 작문을 요구하거나 이미 모델이 알고 있는 상식적인 내용일 때조차 기계적으로 외부 문서를 검색함으로써 연산 자원을 낭비하고, 때로는 검색된 부정확한 정보(Noisy Context)가 모델의 환각(Hallucination)을 유도하는 역설적인 상황이 발생했다.1</p>
<p>이 지점에서 **성찰 토큰(Reflection Tokens)**과 <strong>Self-RAG(Self-Reflective Retrieval-Augmented Generation)</strong> 프레임워크가 등장한다. 2023년 Asai et al.에 의해 제안되고 2025년 DeepSeek-R1 등의 추론 모델로 계승 발전된 이 개념은, 모델이 텍스트 생성 과정 중에 특수한 제어 토큰을 스스로 생성함으로써 ‘검색이 필요한지’, ‘가져온 정보가 유용한지’, ’내 답변이 증거에 기반하는지’를 성찰하게 만든다.1 본 절에서는 생성형 AI가 단순한 생성(Generation)을 넘어 사고(Thinking)의 영역으로 진입하게 만든 핵심 기제인 성찰 토큰의 작동 원리와 이를 구현한 Self-RAG 아키텍처, 그리고 2025년 최신 추론 모델들과의 연결성을 심층적으로 분석한다.</p>
<h2>2.  표준 RAG의 한계와 Self-RAG의 등장 배경</h2>
<h3>2.1  정보 병목과 무차별적 검색의 딜레마</h3>
<p>표준 RAG 파이프라인은 ’검색(Retrieve)-읽기(Read)-생성(Generate)’의 선형적 구조를 따른다. 입력 쿼리가 들어오면 고정된 <span class="math math-inline">k</span>개의 문서를 검색하고, 이를 프롬프트에 포함시켜 답변을 생성한다. 이는 모델의 파라메트릭 지식(Parametric Knowledge)의 한계를 외부 데이터로 보완하는 강력한 수단이지만, 모델의 주체적 판단이 배제되어 있다.</p>
<p>연구 결과에 따르면, 모델이 이미 답을 알고 있는 경우에도 불필요한 검색 문맥을 주입하면, 모델이 자신의 내부 지식보다 검색된 정보(설령 그것이 오정보라 할지라도)에 과도하게 의존하여 정답률이 하락하는 현상이 관찰된다.1 또한, 검색된 문서가 질문과 관련성이 낮음에도 불구하고 모델은 이를 억지로 문맥에 통합하려 시도하며 논리적 비약이나 환각을 만들어낸다. 이는 1.2절에서 다루었던 Seq2Seq 모델의 정보 병목 현상이, 외부 지식을 다루는 RAG 시스템에서도 ’문맥 오염’이라는 형태로 재현되고 있음을 시사한다.</p>
<h3>2.2  Self-RAG: 적응형 검색과 자기 비평의 통합</h3>
<p>Self-RAG는 이러한 수동적 파이프라인을 능동적 의사결정 과정으로 전환한다. 핵심 아이디어는 거대 언어 모델을 학습시킬 때, 일반적인 텍스트뿐만 아니라 자신의 상태와 판단을 나타내는 ’성찰 토큰’을 함께 학습시키는 것이다.4 이를 통해 단일 모델이 검색기(Retriever)를 언제 호출할지 결정하고, 생성된 텍스트의 품질을 스스로 평가하는 비평가(Critic)의 역할을 동시에 수행하게 된다.</p>
<p>이는 인지과학에서 말하는 ’메타인지’를 공학적으로 구현한 시도로 해석할 수 있다. 모델은 단순히 <span class="math math-inline">P(y|x)</span>를 최대화하는 것이 아니라, 자신의 답변 <span class="math math-inline">y</span>가 외부 증거 <span class="math math-inline">d</span>에 의해 지지되는지(<span class="math math-inline">P(\text{supported}|y, d)</span>), 그리고 이 답변이 유용한지(<span class="math math-inline">P(\text{useful}|y, x)</span>)를 다각도로 평가하며 최적의 경로를 탐색한다.</p>
<h2>3.  성찰 토큰(Reflection Tokens)의 분류와 메커니즘</h2>
<p>Self-RAG 프레임워크의 근간을 이루는 성찰 토큰은 모델의 추론 과정을 제어하고 검증하는 명시적인 신호(Signal)다. 이들은 일반적인 자연어 토큰과 구분되는 특수 토큰(Special Tokens)으로 처리되며, 기능에 따라 크게 검색 제어 토큰과 비평 토큰으로 분류된다.4</p>
<h3>3.1  검색 제어 토큰: <code>Retrieve</code></h3>
<p>이 토큰은 모델이 외부 지식을 필요로 하는지 판단하는 문지기(Gatekeeper) 역할을 수행한다. 추론 시 모델은 텍스트 생성을 시작하기 전, 또는 세그먼트(문장 등)를 생성하는 중간에 <code>Retrieve</code> 토큰을 예측한다.</p>
<table><thead><tr><th><strong>토큰 값 (Value)</strong></th><th><strong>의미 및 기능</strong></th><th><strong>작동 예시</strong></th></tr></thead><tbody>
<tr><td><strong>Yes</strong></td><td>현재 문맥에서 사실적 근거가 부족하여 검색이 필수적임.</td><td>“2024년 노벨 물리학상 수상자는 누구인가?” → ``</td></tr>
<tr><td><strong>No</strong></td><td>모델의 내부 지식만으로 충분하거나, 검색이 불필요한 창의적/주관적 작업임.</td><td>“아름다운 이별에 대한 시를 써줘.” → ``</td></tr>
<tr><td><strong>Continue</strong></td><td>이전에 검색된 문맥을 계속 활용하여 답변을 이어갈 수 있음.</td><td>(이전 문장에 이어) “그들의 주요 업적은…” → ``</td></tr>
</tbody></table>
<p><code>Retrieve</code> 토큰의 도입으로 모델은 모든 쿼리에 대해 검색을 수행하는 비효율을 제거하고, 필요한 순간에만 리소스를 투입하는 ’적응형 검색(Adaptive Retrieval)’을 실현한다.6</p>
<h3>3.2  비평 토큰 (Critique Tokens): 품질 관리의 3중 방어선</h3>
<p>검색이 수행된 후, 또는 텍스트가 생성되는 동안 모델은 다음 세 가지 유형의 비평 토큰을 통해 정보의 품질을 다층적으로 검증한다.4</p>
<h4>3.2.1  관련성 평가: <code>IsRel</code> (Is Relevant)</h4>
<p>검색된 문서 <span class="math math-inline">d</span>가 입력 쿼리 <span class="math math-inline">x</span>와 실제로 관련이 있는지 평가한다.</p>
<ul>
<li><strong>Relevant:</strong> 문서가 질문에 대한 답을 포함하거나 유용한 문맥을 제공함.</li>
<li><strong>Irrelevant:</strong> 문서가 질문과 무관함.</li>
</ul>
<p>기존 RAG는 검색된 상위 <span class="math math-inline">k</span>개의 문서를 맹목적으로 신뢰하지만, Self-RAG는 <code>IsRel</code> 토큰을 통해 노이즈 문서를 필터링한다. 만약 모델이 ``를 출력하면, 해당 문서는 이후 생성 과정에서 무시되거나 낮은 가중치를 갖게 된다.5</p>
<h4>3.2.2  사실적 지지 여부: <code>IsSup</code> (Is Supported)</h4>
<p>이 토큰은 자연어 추론(NLI)의 함의(Entailment) 개념을 차용한 것으로, Self-RAG의 환각 방지 메커니즘의 핵심이다. 생성된 답변 <span class="math math-inline">y</span>가 검색된 문서 <span class="math math-inline">d</span>의 내용에 의해 사실적으로 뒷받침되는지를 검증한다.</p>
<ul>
<li><strong>Fully supported:</strong> 생성된 모든 정보가 문서 내의 증거와 일치함.</li>
<li><strong>Partially supported:</strong> 일부 정보는 지지되나, 일부는 문서에 없거나 불확실함.</li>
<li><strong>No support / Contradictory:</strong> 답변이 문서 내용과 무관하거나 모순됨.</li>
</ul>
<p>모델이 ``를 예측한다는 것은 스스로 “내가 지금 지어낸 이야기를 하고 있다“고 고백하는 것과 같다. 추론 알고리즘은 이러한 경로를 배제함으로써 사실성을 담보한다.4</p>
<h4>3.2.3  유용성 평가: <code>IsUse</code> (Is Useful)</h4>
<p>답변이 사실적이라도 질문의 의도와 맞지 않을 수 있다. <code>IsUse</code> 토큰은 답변의 전반적인 완성도와 유용성을 5점 척도(1~5)로 평가한다. 이는 RLHF(인간 피드백 강화학습) 과정에서 보상 모델이 수행하는 역할을 토큰 생성 과정에 내재화한 것으로 볼 수 있다.8</p>
<h2>4.  Self-RAG의 학습 파이프라인과 추론 알고리즘</h2>
<p>Self-RAG의 강력함은 이 복잡한 성찰 과정이 별도의 외부 모듈 없이, 단일 언어 모델의 생성 과정 속에 통합되어 있다는 점에서 나온다. 이를 구현하기 위해 2단계의 정교한 학습 파이프라인과 성찰적 디코딩 알고리즘이 사용된다.</p>
<h3>4.1  학습 방법론: 비평가(Critic)와 생성자(Generator)</h3>
<p>수작업으로 성찰 토큰이 포함된 데이터를 구축하는 것은 비용이 매우 많이 든다. 따라서 Self-RAG는 고성능 모델(예: GPT-4)을 ’비평가’로 활용하여 데이터를 증강하고, 이를 통해 ‘생성자’ 모델을 학습시키는 지식 증류(Knowledge Distillation) 전략을 취한다.9</p>
<ol>
<li>비평가 모델 학습 (Critic Model Training):</li>
</ol>
<p>먼저 소량의 데이터에 대해 GPT-4를 프롬프팅하여 성찰 토큰(Retrieve, IsRel, IsSup 등)을 생성하게 한다. 이렇게 생성된 데이터를 정답지(Ground Truth)로 삼아, 상대적으로 작은 모델(예: Llama-2-7B 기반)을 미세 조정하여 전용 비평가 모델 <span class="math math-inline">C</span>를 만든다. 이 모델은 입력 텍스트에 대해 적절한 성찰 토큰을 예측하는 데 특화된다.</p>
<ol start="2">
<li>데이터 증강 (Generator Data Augmentation):</li>
</ol>
<p>원본 입력-출력 쌍 <span class="math math-inline">(x, y)</span>로 구성된 대규모 코퍼스를 비평가 모델 <span class="math math-inline">C</span>에 통과시킨다. 비평가 모델은 텍스트의 각 세그먼트마다 검색 필요 여부를 판단하고, 검색된 문서를 삽입하며, 이에 대한 평가 토큰을 덧붙인다.</p>
<ul>
<li>
<p>변환 전: <code>Query: x -&gt; Answer: y</code></p>
</li>
<li>
<p>변환 후: Query: x -&gt; <Document d> Answer segment y1[IsUse=5]…</p>
</li>
</ul>
<p>이 과정에서 검색된 문서가 도움이 되지 않거나(IsRel=Irrelevant), 답변이 지지받지 못하는(IsSup=No support) 부정적인 예시들도 포함되는데, 이는 모델이 ’하지 말아야 할 것’을 학습하는 데 기여한다.6</p>
<ol start="3">
<li>생성자 모델 학습 (Generator Model Training):</li>
</ol>
<p>최종적으로 사용자에게 배포될 생성자 모델 <span class="math math-inline">M</span>은 증강된 코퍼스를 사용하여 표준적인 다음 토큰 예측 손실(Next Token Prediction Loss)을 최소화하는 방식으로 학습된다. 이 과정에서 <span class="math math-inline">M</span>은 문맥에 따라 적절한 텍스트를 생성하는 능력뿐만 아니라, 스스로 성찰 토큰을 생성하여 자신의 사고 과정을 제어하는 능력을 동시에 습득하게 된다. 학습이 완료되면 생성자 모델은 더 이상 비평가 모델 없이도 독자적으로 성찰 토큰을 생성할 수 있다.11</p>
<h3>4.2  성찰적 디코딩(Reflective Decoding)과 세그먼트 빔 서치</h3>
<p>Self-RAG는 추론(Inference) 단계에서 단순히 확률이 가장 높은 단어를 선택하는 탐욕적(Greedy) 방식을 사용하지 않는다. 대신 성찰 토큰의 확률을 가중치로 활용하는 <strong>세그먼트 단위 빔 서치(Segment-level Beam Search)</strong> 알고리즘을 적용한다.1</p>
<p>알고리즘의 상세 과정은 다음과 같다 5:</p>
<ol>
<li>
<p><strong>예측 및 분기:</strong> 현재 시점 <span class="math math-inline">t</span>에서 모델이 ``를 예측하면, 검색기를 호출하여 <span class="math math-inline">K</span>개의 관련 문서를 확보한다.</p>
</li>
<li>
<p><strong>병렬 생성:</strong> 확보된 각 문서 <span class="math math-inline">d_k</span>를 문맥으로 하여 각각의 답변 세그먼트 후보를 병렬적으로 생성한다. 이때 검색을 하지 않는 경로(<code>Retrieve=No</code>)도 함께 생성하여 경쟁시킨다.</p>
</li>
<li>
<p>비평 및 점수 산출: 모델은 각 후보 세그먼트에 대해 스스로 IsRel, IsSup, IsUse 토큰을 생성한다. 각 세그먼트의 최종 점수 <span class="math math-inline">S</span>는 텍스트 생성 확률과 성찰 토큰 확률의 가중 합으로 계산된다.<br />
<span class="math math-display">
S(y_t) = P(y_t|x, d) + \alpha P(\text{IsRel}) + \beta P(\text{IsSup}) + \gamma P(\text{IsUse})
</span><br />
여기서 <span class="math math-inline">\alpha, \beta, \gamma</span>는 하이퍼파라미터로, 사용자가 원하는 답변의 특성(정확성 중시 vs 창의성 중시)에 따라 추론 시점에 조절할 수 있다. 예를 들어, 엄격한 팩트 체크가 필요한 경우 <span class="math math-inline">\beta</span>(IsSup) 값을 높여 지지되지 않는 답변을 강력하게 페널티 처리한다.8</p>
</li>
<li>
<p><strong>순위 선정 및 갱신:</strong> 계산된 점수를 바탕으로 상위 <span class="math math-inline">B</span>개의 세그먼트만을 남기고(Beam Search), 다음 세그먼트 생성을 이어간다.</p>
</li>
</ol>
<p>이러한 방식은 모델이 생성 도중에 “잠깐, 이 정보가 확실한가?“라고 자문하고, 여러 가능성 중 가장 신뢰할 수 있는 경로를 선택하는 시스템 2의 숙고(Deliberation) 과정을 알고리즘적으로 구현한 것이다.</p>
<h2>5.  2025년의 진화: 암묵적 성찰과 시스템 2 에이전트</h2>
<p>Self-RAG가 제시한 ‘토큰을 통한 자기 제어’ 패러다임은 2025년에 이르러 더욱 정교한 형태의 추론 모델과 에이전트 시스템으로 진화하였다.</p>
<h3>5.1  DeepSeek-R1과 암묵적 사고의 사슬</h3>
<p>2025년 초 공개된 DeepSeek-R1은 Self-RAG의 성찰 메커니즘을 **암묵적 사고의 사슬(Implicit Chain of Thought)**로 확장시켰다. Self-RAG가 <code>IsSup</code>과 같은 이산적인(Discrete) 토큰으로 성찰을 수행했다면, DeepSeek-R1은 <code>&lt;think&gt;</code>와 <code>&lt;/think&gt;</code> 태그 사이에서 자연어 형태의 사고 과정을 전개한다.3</p>
<p>주목할 점은 DeepSeek-R1의 학습 과정에서 나타난 ’아하 모멘트(Aha Moment)’다. 강화학습(RL)을 통해 정답을 맞히도록 유도된 모델은, 명시적인 지시 없이도 스스로 “Wait, let me double check(잠깐, 다시 확인해보자)“라며 자신의 논리를 중간에 수정하는 패턴을 창발적으로 학습했다. 이는 Self-RAG의 <code>IsSup</code> 토큰이 자연어라는 더 풍부한 표현력을 가진 형태로 진화한 것으로 해석할 수 있다. DeepSeek-R1은 성찰 토큰을 통해 단순한 사실 검증을 넘어, 문제 해결 전략을 수정하고 논리적 오류를 스스로 교정하는 고차원적인 메타인지를 보여준다.13</p>
<h3>5.2  Reflection Tuning과 자가 수정 데이터</h3>
<p>’Reflection Tuning’은 모델이 자신의 출력을 비판하고 개선하는 데이터를 반복적으로 학습하는 기법으로, 2025년 LLM 학습의 표준으로 자리 잡았다.14 초기 Self-RAG가 비평가 모델을 통해 데이터를 증강했다면, Reflection Tuning은 모델 자신이 생성한 초안(Draft)에 대해 스스로 비평(Critique)하고 수정(Revise)한 데이터를 학습 데이터로 재사용한다.</p>
<p>이는 “최고의 선생님은 바로 자신“이라는 원리에 기반하며, 모델이 생성할 수 있는 텍스트 분포 내에서 오류 수정 능력을 극대화한다. 특히 System 2 Thinking(느리고 논리적인 사고)을 유도하기 위해, 추론 과정에서 의도적으로 성찰 토큰을 생성하며 사고 시간을 늘리는 전략이 사용된다. 연구 결과에 따르면, 이러한 성찰 과정은 추론 시 더 많은 토큰을 생성하게 하여 레이턴시를 증가시키지만, 복잡한 추론 문제에서의 정답률을 비약적으로 향상시킨다.16</p>
<h3>5.3  Self-RAG vs. Agentic RAG</h3>
<p>2025년의 AI 생태계에서는 모델 내부의 성찰을 강조하는 Self-RAG와, 모델 외부의 도구 및 워크플로우 제어를 강조하는 Agentic RAG가 상호 보완적으로 발전하고 있다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Self-RAG</strong></th><th><strong>Agentic RAG</strong></th></tr></thead><tbody>
<tr><td><strong>제어 주체</strong></td><td>모델 내부의 성찰 토큰 (<code>IsRel</code>, <code>IsSup</code>)</td><td>LangGraph, LlamaIndex 등 외부 오케스트레이터</td></tr>
<tr><td><strong>의사결정 방식</strong></td><td>내재화된 확률 분포에 기반한 즉각적 판단</td><td>명시적인 계획(Planning) 및 다단계 도구 호출</td></tr>
<tr><td><strong>유연성</strong></td><td>텍스트 생성 및 검색 제어에 특화</td><td>API 호출, 코드 실행 등 다양한 행동(Action) 가능</td></tr>
<tr><td><strong>구현 복잡도</strong></td><td>학습 비용 높음 (데이터 증강 및 미세 조정 필요)</td><td>설계 복잡도 높음 (프롬프트 엔지니어링 및 그래프 설계 필요)</td></tr>
<tr><td><strong>주요 사용처</strong></td><td>고품질 텍스트 생성, 사실성 검증, 지연 시간이 중요한 추론</td><td>복잡한 워크플로우 자동화, 기업용 의사결정 시스템</td></tr>
</tbody></table>
<p><strong>Agentic RAG</strong>는 여러 에이전트가 협업하거나, 명시적인 루프를 통해 검색 결과를 검증하고 쿼리를 재작성하는 과정을 거친다.18 반면 <strong>Self-RAG</strong>는 이러한 에이전트의 사고 과정을 모델 하나의 가중치 안에 압축해 넣은 형태다. 따라서 Self-RAG 모델은 Agentic RAG 시스템 내에서 매우 신뢰할 수 있는 단일 노드(Node)로서 기능할 때 시너지를 발휘한다. 예를 들어, Agentic 워크플로우의 ‘검증자(Verifier)’ 에이전트로 Self-RAG 모델을 사용하면, 외부 프롬프트에 의존하는 것보다 훨씬 빠르고 정확하게 문서의 관련성을 판단할 수 있다.20</p>
<h2>6.  결론: 트랜스포머의 자의식을 향한 여정</h2>
<p>제15.2장에서 살펴본 성찰 토큰과 Self-RAG는 트랜스포머가 단순한 통계적 앵무새를 넘어, 자신의 발화에 책임을 지는 지성체로 진화하는 중요한 변곡점을 보여준다. <code>Retrieve</code> 토큰으로 자신의 무지를 인정하고, <code>IsSup</code> 토큰으로 자신의 논리를 검증하며, <code>IsUse</code> 토큰으로 유용성을 고민하는 과정은 인간의 사고 과정과 놀랍도록 닮아 있다.</p>
<p>2025년 현재, 이러한 성찰 기제는 DeepSeek-R1과 같은 최신 모델에서 자연어 기반의 암묵적 사고(Implicit Thought)로 통합되며 더욱 유려해지고 있다. 이제 모델은 토큰 단위의 미시적 성찰을 넘어, 사고의 흐름 전체를 관장하는 거시적 메타인지 능력을 갖추기 시작했다. 이는 환각 문제를 해결하는 것을 넘어, 인공지능이 인간의 개입 없이도 스스로 지식을 확장하고 오류를 수정하는 **자율적 지능(Autonomous Intelligence)**으로 나아가는 핵심 동력이 될 것이다.</p>
<p>다음 장인 “15.3 DPO, KTO 인간 선호 정렬의 최신 기법“에서는, 이러한 모델의 자율적 성찰 능력을 인간의 가치관과 윤리 기준에 부합하도록 정렬(Alignment)하는 기술적 방법론들에 대해 논의할 것이다. Self-RAG가 ’사실(Fact)’에 대한 성찰이었다면, DPO와 KTO는 ’가치(Value)’에 대한 성찰을 모델에 주입하는 과정이라 할 수 있다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Self-RAG: Learning to Retrieve, Generate and Critique through Self-Reflection, https://selfrag.github.io/</li>
<li>The Self-RAG Shortcut Every AI Expert Wishes They Knew - ProjectPro, https://www.projectpro.io/article/self-rag/1176</li>
<li>IN-CONTEXT LEARNING ENHANCES REASONING LARGE LANGUAGE MODELS WITH LESS OVERTHINKING - OpenReview, https://openreview.net/pdf/305756d3bd1baafbb083affad0b79e33aa38b19f.pdf</li>
<li>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection - arXiv, https://arxiv.org/abs/2310.11511</li>
<li>SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION - OpenReview, https://openreview.net/pdf?id=hSyW5go0v8</li>
<li>SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION - NSF Public Access Repository, https://par.nsf.gov/servlets/purl/10539591</li>
<li>SELF-RAG (Self-Reflective Retrieval-Augmented Generation): The Game-Changer in Factual AI… - Medium, https://medium.com/@sahin.samia/self-rag-self-reflective-retrieval-augmented-generation-the-game-changer-in-factual-ai-dd32e59e3ff9</li>
<li>This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. - GitHub, https://github.com/AkariAsai/self-rag</li>
<li>[Paper Reading] Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection - Clay-Technology World, https://clay-atlas.com/us/blog/2024/01/22/self-rag-learning-to-retrieve-generate-critique/</li>
<li>Paper Review: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection | by Andrew Lukyanenko, https://artgor.medium.com/paper-review-self-rag-learning-to-retrieve-generate-and-critique-through-self-reflection-3ae3ccac3c4e</li>
<li>Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection, https://openreview.net/forum?id=hSyW5go0v8</li>
<li>Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills - ACL Anthology, https://aclanthology.org/2025.emnlp-main.220.pdf</li>
<li>From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models - arXiv, https://arxiv.org/html/2509.23676v1</li>
<li>Reflection Tuning Explained: Self-Improving LLMs 101 - Galileo AI, https://galileo.ai/blog/reflection-tuning-llms</li>
<li>Breaking Down Reflection Tuning: Enhancing LLM Performance with Self-Learning, https://arize.com/blog/breaking-down-reflection-tuning-enhancing-llm-performance-with-self-learning/</li>
<li>First Try Matters: Revisiting the Role of Reflection in Reasoning Models - arXiv, https://arxiv.org/html/2510.08308v1</li>
<li>Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking - arXiv, https://arxiv.org/html/2502.12470v1</li>
<li>RAG vs. Self-RAG vs. Agentic RAG: Which One Is Right for You? | by Bhavik Jikadara | AI Agent Insider | Medium, https://medium.com/ai-agent-insider/rag-vs-self-rag-vs-agentic-rag-which-one-is-right-for-you-3d233ef42cac</li>
<li>RAG vs Agentic RAG in 2025: Key Differences and Why They Matter - Kanerika, https://kanerika.com/blogs/rag-vs-agentic-rag/</li>
<li>Agentic RAG Systems: Integration of Retrieval and Generation in AI Architectures - Galileo AI, https://galileo.ai/blog/agentic-rag-integration-ai-architecture</li>
<li>A tutorial on building local agent using LangGraph, LLaMA3 and Elasticsearch vector store from scratch, https://www.elastic.co/search-labs/blog/local-rag-agent-elasticsearch-langgraph-llama3</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>