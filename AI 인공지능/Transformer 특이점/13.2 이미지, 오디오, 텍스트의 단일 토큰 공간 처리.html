<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:13.2 이미지, 오디오, 텍스트의 단일 토큰 공간 처리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>13.2 이미지, 오디오, 텍스트의 단일 토큰 공간 처리</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>13.2 이미지, 오디오, 텍스트의 단일 토큰 공간 처리</span></nav>
                </div>
            </header>
            <article>
                <h1>13.2 이미지, 오디오, 텍스트의 단일 토큰 공간 처리</h1>
<p>2025-12-23, G30DR</p>
<h2>1.  서론: 모달리티의 장벽을 넘어서는 단일 토큰 공간의 도래</h2>
<p>인공지능 연구의 역사에서 ’특이점(Singularity)’이라는 용어는 다양한 맥락에서 사용되어 왔으나, 트랜스포머(Transformer) 아키텍처의 관점에서 논의되는 싱귤래리티는 데이터의 형태적 이질성이 소멸하고 모든 정보가 **‘토큰(Token)’**이라는 단일한 원자적 단위로 통합되는 지점을 의미한다. 과거의 멀티모달 학습은 텍스트, 이미지, 오디오를 각기 다른 신경망(CNN, RNN 등)이나 별도의 인코더를 통해 처리한 후, 고차원 벡터 공간에서 느슨하게 연결하는 ‘늦은 융합(Late Fusion)’ 방식을 취했다. 그러나 2024년과 2025년을 기점으로 등장한 최신 모델들은 이러한 패러다임을 근본적으로 전복시키고 있다.1</p>
<p>본 보고서는 서적 ’트랜스포머 싱귤래리티’의 13.2장 “이미지, 오디오, 텍스트의 단일 토큰 공간 처리“를 심층적으로 확장하여 기술한다. 이 장의 핵심 주제는 서로 다른 물리적 차원과 엔트로피 특성을 가진 데이터를 어떻게 하나의 <strong>‘통합 어휘(Unified Vocabulary)’</strong> 공간으로 매핑하고, 이를 단일 트랜스포머 백본(Backbone)으로 처리할 수 있는가에 대한 방법론적 고찰이다. Meta의 Chameleon, Google의 Gemini 1.5, Allen AI의 Unified-IO 2, 그리고 OpenAI의 GPT-4o와 같은 최전선 모델들은 이제 텍스트 입출력을 넘어, 이미지 생성, 음성 합성, 비디오 이해를 모두 ’다음 토큰 예측(Next Token Prediction)’이라는 하나의 목표 함수(Objective Function)로 환원시키고 있다.3</p>
<p>단일 토큰 공간 처리는 단순한 데이터의 결합이 아니다. 이는 연속적인(Continuous) 아날로그 신호를 이산적인(Discrete) 디지털 기호로 변환하는 양자화(Quantization) 기술의 정교함, 이질적인 데이터가 혼재된 시퀀스를 학습할 때 발생하는 최적화 불안정성을 제어하는 아키텍처 엔지니어링, 그리고 이러한 통합 과정에서 발현되는 새로운 창발적 능력(Emergent Capabilities)을 포함하는 거대한 기술적 체계이다.5 본고에서는 이러한 기술적 요소들을 분해하고 재조립하여, 진정한 의미의 ‘Any-to-Any’ 모델이 어떻게 구현되는지 그 기저 원리를 낱낱이 파헤친다.</p>
<h2>2.  통합의 전제 조건: 연속적 신호의 이산화와 통합 어휘 구축</h2>
<p>텍스트는 본질적으로 이산적인 데이터이다. 언어는 유한한 문자나 단어의 집합으로 구성되며, 이는 자연스럽게 정수 인덱스로 치환될 수 있다. 반면, 이미지(픽셀의 연속)와 오디오(파형의 진폭)는 연속적인 값을 가지며, 이를 트랜스포머가 처리할 수 있는 형태인 ’토큰’으로 변환하기 위해서는 **벡터 양자화(Vector Quantization, VQ)**라는 수학적 변환 과정이 필수적이다.9</p>
<h3>2.1  통합 어휘(Unified Vocabulary)의 구조와 설계</h3>
<p>단일 토큰 공간 처리의 핵심은 모델이 참조하는 코드북(Codebook)을 확장하여, 텍스트 토큰뿐만 아니라 이미지 토큰, 오디오 토큰 등을 모두 포함하는 거대한 단일 어휘 집합을 구성하는 것이다.<br />
<span class="math math-display">
V_{total} = V_{text} \cup V_{image} \cup V_{audio} \cup V_{special}
</span><br />
이러한 통합 어휘 구조는 모델로 하여금 모달리티를 구분하지 않고, 현재 시점 <span class="math math-inline">t</span>에서의 입력 토큰 <span class="math math-inline">x_t</span>를 바탕으로 다음 토큰 <span class="math math-inline">x_{t+1}</span>의 확률 분포 <span class="math math-inline">P(x_{t+1}|x_{&lt;t})</span>를 예측하게 한다. 이 과정에서 모델은 “고양이“라는 텍스트 토큰과 실제 고양이 이미지를 나타내는 시각적 토큰 사이의 의미론적 관계를, 마치 단어와 단어 사이의 관계를 학습하듯이 내부적으로 정렬(Align)하게 된다.3</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>전체 어휘 크기 (Vocabulary Size)</strong></th><th><strong>구성 요소 (추정 및 명시)</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td><strong>Meta Chameleon</strong></td><td>65,536</td><td>BPE 텍스트 토큰 + 8,192 이미지 토큰</td><td>이미지 토큰이 전체 어휘의 약 12.5%를 차지하며, SentencePiece 라이브러리를 통해 통합 관리됨.3</td></tr>
<tr><td><strong>Unified-IO 2</strong></td><td>49,536</td><td>32k 텍스트, 16k 비전, 1k 오디오/행동</td><td>비전, 오디오, 행동(Action), 바운딩 박스 등 다양한 모달리티를 명시적인 구간으로 나누어 할당함.5</td></tr>
<tr><td><strong>VALL-E</strong></td><td>가변적 (Hierarchical)</td><td>텍스트 음소 + 1024 오디오 코드북 x 8 계층</td><td>EnCodec의 계층적 코드북 구조를 사용하며, 텍스트와 오디오 토큰이 시퀀스 내에서 혼합됨.13</td></tr>
</tbody></table>
<p>이러한 통합 어휘의 설계는 단순히 크기를 늘리는 문제가 아니다. 텍스트 토큰은 의미적 압축률이 높은 반면, 이미지나 오디오 토큰은 패턴 인식에 가깝기 때문에, 두 토큰 집합 간의 <strong>정보 밀도(Information Density)</strong> 차이를 어떻게 극복하느냐가 학습의 성패를 좌우한다.3</p>
<h3>2.2  이미지의 토큰화: 픽셀에서 의미론적 패치로</h3>
<p>이미지를 토큰화한다는 것은 고해상도 이미지를 저차원의 이산적 벡터 시퀀스로 압축한다는 것을 의미한다. 이를 위해 <strong>VQ-GAN(Vector Quantized Generative Adversarial Network)</strong> 아키텍처가 널리 사용된다.9</p>
<h4>2.2.1  VQ-GAN의 메커니즘과 코드북 학습</h4>
<p>VQ-GAN은 CNN(Convolutional Neural Network) 기반의 인코더를 사용하여 이미지의 지역적(Local) 특징을 추출하고, 이를 코드북의 가장 유사한 벡터로 치환(Quantization)한다.</p>
<ol>
<li>
<p><strong>인코딩:</strong> 입력 이미지 <span class="math math-inline">x \in \mathbb{R}^{H \times W \times 3}</span>는 인코더를 통해 잠재 맵 <span class="math math-inline">z = E(x) \in \mathbb{R}^{h \times w \times d}</span>로 변환된다. 여기서 <span class="math math-inline">h, w</span>는 원본 해상도보다 훨씬 작다(예: 1/16, 1/32).</p>
</li>
<li>
<p>벡터 양자화: 잠재 맵의 각 벡터 <span class="math math-inline">z_{ij}</span>는 코드북 <span class="math math-inline">\mathcal{C} = \{e_k\}_{k=1}^{K}</span> 내의 벡터 중 유클리드 거리가 가장 가까운 벡터 <span class="math math-inline">e_{k}</span>로 대체된다.<br />
<span class="math math-display">
z_q = \text{argmin}_{e_k \in \mathcal{C}} || z_{ij} - e_k ||_2
</span></p>
</li>
</ol>
<p>이때 선택된 인덱스 <span class="math math-inline">k</span>가 바로 ’이미지 토큰’이 된다.</p>
<ol start="3">
<li>디코딩 및 학습: 양자화된 벡터들은 디코더를 통해 원본 이미지로 복원되며, 이 과정에서 재구성 손실(Reconstruction Loss), 코드북 손실(Codebook Loss), 그리고 적대적 손실(Adversarial Loss)을 최소화하도록 학습된다.9</li>
</ol>
<h4>2.2.2  최신 모델의 적용 사례와 한계</h4>
<p>Meta의 <strong>Chameleon</strong> 모델은 512×512 해상도의 이미지를 1024개의 토큰(32×32 격자)으로 변환하는 새로운 이미지 토크나이저를 개발했다.3 이 토크나이저는 8192개의 코드북 크기를 가지며, 특히 사람의 얼굴과 같은 민감한 객체의 품질을 높이기 위해 학습 데이터에서 얼굴 이미지의 비율을 2배로 늘리는 전략을 취했다.3 그러나 이러한 이산적 토큰화 방식은 텍스트가 많이 포함된 이미지나 미세한 디테일이 중요한 이미지에서 정보를 소실하는 경향이 있어, OCR(광학 문자 인식) 관련 작업에서 성능의 상한선(Upper Bound)으로 작용한다는 한계가 지적된다.3</p>
<h3>2.3  오디오의 토큰화: 시간적 해상도와 잔차 양자화(RVQ)</h3>
<p>오디오 데이터는 1차원 시계열 데이터이면서 동시에 매우 높은 시간적 해상도(샘플링 레이트, 예: 24kHz, 44.1kHz)를 가진다. 이를 단일 코드북으로 압축하면 음질이 심각하게 저하되거나, 코드북의 크기가 기하급수적으로 커져야 하는 딜레마에 빠진다. 이를 해결하기 위해 **잔차 벡터 양자화(Residual Vector Quantization, RVQ)**가 표준으로 자리 잡았다.11</p>
<h4>2.3.1  RVQ의 계층적 정보 압축</h4>
<p>RVQ는 하나의 벡터를 여러 개의 양자화기(Quantizer)가 순차적으로 근사하는 방식이다.</p>
<ul>
<li><strong>1단계 (Coarse):</strong> 첫 번째 양자화기 <span class="math math-inline">Q_1</span>이 입력 벡터 <span class="math math-inline">x</span>를 가장 잘 표현하는 코드 <span class="math math-inline">c_1</span>을 찾는다. 잔차 <span class="math math-inline">r_1 = x - Q_1(x)</span>가 남는다.</li>
<li><strong>2단계 (Fine):</strong> 두 번째 양자화기 <span class="math math-inline">Q_2</span>는 잔차 <span class="math math-inline">r_1</span>을 근사하여 코드 <span class="math math-inline">c_2</span>를 찾는다. 잔차 <span class="math math-inline">r_2 = r_1 - Q_2(r_1)</span>이 남는다.</li>
<li><strong>N단계:</strong> 이 과정을 <span class="math math-inline">N</span>번 반복하면, 하나의 오디오 프레임은 <span class="math math-inline">N</span>개의 토큰 집합 <span class="math math-inline">(c_1, c_2, \dots, c_N)</span>으로 표현된다. 상위 계층 토큰은 오디오의 전체적인 내용(의미, 화자 특성)을, 하위 계층 토큰은 미세한 음향적 디테일(잡음, 배음 등)을 담당하게 된다.14</li>
</ul>
<p><strong>SoundStream</strong>과 <strong>EnCodec</strong>은 이러한 RVQ 방식을 채택한 대표적인 뉴럴 오디오 코덱이다. 예를 들어, EnCodec은 24kHz 오디오를 320배 압축하여 프레임당 8~32개의 코드를 생성한다.11</p>
<h4>2.3.2  2차원 토큰의 1차원 시퀀스화 전략</h4>
<p>RVQ는 시간 축(<span class="math math-inline">T</span>)과 깊이 축(<span class="math math-inline">N_q</span>)을 가진 2차원 행렬 <span class="math math-inline">(T, N_q)</span>를 생성한다. 트랜스포머는 1차원 시퀀스만을 입력으로 받으므로, 이를 어떻게 1차원으로 펼치느냐(Flattening)가 모델의 성능과 효율성을 결정한다.18</p>
<table><thead><tr><th><strong>전략</strong></th><th><strong>설명</strong></th><th><strong>장점</strong></th><th><strong>단점</strong></th><th><strong>대표 모델</strong></th></tr></thead><tbody>
<tr><td><strong>평탄화 (Flattening)</strong></td><td><span class="math math-inline">T</span> 시점의 모든 <span class="math math-inline">N_q</span> 토큰을 순서대로 나열한 후, <span class="math math-inline">T+1</span> 시점으로 넘어감. (Row-major)</td><td>구현이 직관적임. 모든 정보를 순차적으로 처리.</td><td>시퀀스 길이가 <span class="math math-inline">N_q</span>배로 폭증하여 연산 비용 급증 (<span class="math math-inline">O(L^2)</span>).</td><td>AudioLM 18</td></tr>
<tr><td><strong>지연 패턴 (Delay Pattern)</strong></td><td>각 양자화 계층을 한 스텝씩 지연시켜 인터리빙함. <span class="math math-inline">c_{t, 1}, c_{t-1, 2}, \dots</span></td><td>병렬적 예측이 가능하면서도 상위 계층 정보 활용 용이.</td><td>구현 복잡도가 높고, 여전히 시퀀스 길이가 긺.</td><td>MusicGen 19</td></tr>
<tr><td><strong>AR + NAR 하이브리드</strong></td><td>첫 번째 계층(<span class="math math-inline">Q_1</span>)만 자기회귀(AR)로 예측하고, 나머지는 비자기회귀(NAR)로 병렬 생성.</td><td>추론 속도가 매우 빠름. AR 모델의 부담 경감.</td><td>하위 계층 간의 의존성 모델링이 약할 수 있음.</td><td>VALL-E 13</td></tr>
</tbody></table>
<p>MusicGen의 경우, ’지연 패턴’을 사용하여 코드북 간의 의존성을 유지하면서도 병렬 처리가 가능한 구조를 채택했다. 반면, VALL-E는 첫 번째 퀀타이저가 전체적인 음성 내용을 결정하고, 나머지 퀀타이저들은 이를 바탕으로 음질을 개선하는 역할을 하도록 역할을 분담시켰다. 이는 단일 토큰 공간 내에서도 오디오의 특성을 고려한 다양한 시퀀스 모델링 전략이 존재함을 시사한다.</p>
<h2>3.  통합 아키텍처 및 학습 역학: 경쟁과 균형</h2>
<p>단일 토큰 공간 모델링의 가장 큰 기술적 난관은 서로 다른 모달리티가 하나의 네트워크 내에서 **‘경쟁(Competition)’**한다는 점이다. 텍스트, 이미지, 오디오는 각기 다른 엔트로피(Entropy)와 손실(Loss) 스케일을 가지며, 이를 단순히 합쳐서 학습시키면 특정 모달리티가 학습을 지배하거나, 반대로 학습이 발산하는 현상이 발생한다.3</p>
<h3>3.1  아키텍처 비교: Encoder-Decoder vs. Decoder-only</h3>
<p>통합 모델을 구현하는 아키텍처는 크게 두 가지 흐름으로 나뉜다.</p>
<h4>3.1.1  Unified-IO 2: 범용성을 위한 인코더-디코더</h4>
<p>Unified-IO 2는 T5 스타일의 인코더-디코더(Encoder-Decoder) 구조를 채택했다.5</p>
<ul>
<li><strong>동작 원리:</strong> 입력된 텍스트, 이미지, 오디오는 모두 임베딩 시퀀스로 변환되어 인코더에 입력된다. 디코더는 이 통합된 표현을 바탕으로 타겟 모달리티의 토큰을 생성한다.</li>
<li><strong>장점:</strong> 입력과 출력의 분리가 명확하여, “이미지를 보고(입력) 텍스트를 생성(출력)“하거나 “텍스트를 보고(입력) 오디오를 생성(출력)“하는 다양한 태스크를 유연하게 처리할 수 있다. 특히 양방향(Bidirectional) 어텐션을 사용하는 인코더 덕분에 입력 데이터에 대한 이해도가 높다.</li>
<li><strong>특징:</strong> 약 29억 개의 파라미터를 가지며, GRIT 벤치마크와 같은 다중 작업 평가에서 탁월한 성능을 보였다.12</li>
</ul>
<h4>3.1.2  Meta Chameleon: 생성에 최적화된 디코더 온리</h4>
<p>Chameleon은 GPT나 LLaMA와 같은 디코더 전용(Decoder-only) 아키텍처를 따른다.3</p>
<ul>
<li><strong>동작 원리:</strong> 텍스트와 이미지 토큰이 하나의 긴 스트림으로 결합되어 인과적(Causal) 마스킹을 통해 순차적으로 처리된다. 이는 “텍스트-이미지-텍스트“와 같이 인터리브(Interleave)된 문서를 생성하는 데 매우 적합하다.</li>
<li><strong>의의:</strong> 별도의 인코더 없이 모든 모달리티를 <strong>‘이른 융합(Early Fusion)’</strong> 단계에서 처리한다는 점에서 진정한 의미의 ’싱귤래리티’에 가깝다. Flamingo와 같은 기존 모델들이 사전 학습된 비전 인코더를 고정하고 크로스 어텐션으로 연결했던 것과 달리, Chameleon은 처음부터 끝까지 하나의 모델이 모든 모달리티를 학습한다.22</li>
</ul>
<h3>3.2  학습 불안정성: 모달리티 경쟁과 로짓 드리프트</h3>
<p>Meta와 Allen AI의 연구진은 멀티모달 통합 학습 시 심각한 학습 불안정성을 보고했다. 그 주된 원인은 <strong>‘규범 경쟁(Norm Competition)’</strong> 및 <strong>‘로짓 드리프트(Logit Drift)’</strong> 현상이다.3</p>
<ul>
<li><strong>원인:</strong> 텍스트 토큰과 이미지 토큰은 서로 다른 엔트로피를 가진다. Softmax 함수는 입력값의 절대적인 크기보다는 차이에 민감(Translation Invariance)하기 때문에, 모델은 손실을 줄이기 위해 특정 모달리티의 로짓 값을 비정상적으로 키우는 경향을 보인다.</li>
<li><strong>증상:</strong> 학습 중반 이후 그라디언트 노름(Gradient Norm)이 폭발하거나, 손실이 갑자기 발산하여 학습이 실패한다. 특히 이미지 생성을 포함할 때 이 현상이 두드러진다.</li>
</ul>
<h3>3.3  안정화를 위한 핵심 기술적 해결책</h3>
<p>이러한 문제를 해결하기 위해 13.2장 및 관련 연구들은 다음과 같은 정교한 정규화 기법들을 필수적으로 도입하고 있다.</p>
<table><thead><tr><th><strong>기술 (Technique)</strong></th><th><strong>메커니즘 (Mechanism)</strong></th><th><strong>효과 및 적용 사례</strong></th></tr></thead><tbody>
<tr><td><strong>QK-Norm (Query-Key Normalization)</strong></td><td>어텐션 메커니즘의 Query와 Key 벡터에 LayerNorm을 적용함.</td><td>어텐션 스코어의 크기를 제한하여 로짓이 폭발하는 것을 방지. <strong>Unified-IO 2, Chameleon, ViT-22B</strong> 등 대규모 멀티모달 모델의 필수 요소로 자리 잡음.5</td></tr>
<tr><td><strong>Scaled Cosine Attention</strong></td><td><span class="math math-inline">Q \cdot K^T</span> 내적 대신 코사인 유사도를 사용하고 스케일링을 적용.</td><td>어텐션 값의 범위를 <span class="math math-inline">[-1, 1]</span>로 강제하여 수치적 안정성을 극대화함. Unified-IO 2에서 사용됨.5</td></tr>
<tr><td><strong>Z-Loss (Router Z-loss)</strong></td><td><span class="math math-inline">\log(\sum \exp(\text{logits}))^2</span> 항을 손실 함수에 추가.</td><td>로짓의 합(Partition Function)이 0에서 멀어지는 것을 억제하여 라우터나 분류기의 안정성을 높임. MoE 및 멀티모달 학습에서 중요.5</td></tr>
<tr><td><strong>2D Rotary Embeddings (RoPE)</strong></td><td>위치 임베딩을 2차원으로 확장.</td><td>이미지의 공간적 구조 정보를 1차원 토큰 시퀀스 내에서도 보존하여, 이미지 생성 및 이해 성능을 향상시킴.5</td></tr>
<tr><td><strong>Dropout 변형</strong></td><td>모달리티별로 서로 다른 드롭아웃 비율 적용.</td><td>각 모달리티의 과적합 및 학습 속도 차이를 조절. Unified-IO 2에서 이미지와 오디오의 학습 균형을 위해 사용됨.</td></tr>
</tbody></table>
<p>특히 QK-Norm은 멀티모달 트랜스포머 학습의 ’Game Changer’로 평가받는다. Chameleon 논문의 절제 실험(Ablation Study)에 따르면, QK-Norm 없이는 대규모 모델의 학습이 불가능할 정도로 발산하는 것으로 나타났다.3</p>
<h2>4.  최신 모델 사례 연구: 13.2장의 실현</h2>
<h3>4.1  Meta Chameleon: 인터리브드 생성의 혁신</h3>
<p>Chameleon은 텍스트와 이미지가 혼합된 문서를 자유자재로 생성할 수 있는 능력을 보여준다. 이는 모델이 텍스트의 논리적 흐름을 따라가면서 필요할 때 적절한 이미지를 생성하고, 다시 텍스트로 복귀하는 제어 능력을 갖췄음을 의미한다. 70억(7B) 및 340억(34B) 파라미터 모델로 공개되었으며, 인간 평가에서 GPT-4V나 Gemini Pro와 대등하거나 능가하는 성능을 보였다.25 이는 단일 토큰 공간 접근법이 단순히 가능한 것을 넘어, 고성능을 달성할 수 있음을 증명한다.</p>
<h3>4.2  Unified-IO 2: 진정한 Any-to-Any</h3>
<p>Unified-IO 2는 텍스트, 이미지, 오디오뿐만 아니라 로봇의 행동(Action)까지 토큰화하여 처리한다. 이는 물리적 세계와의 상호작용까지 단일 모델로 통합하려는 시도이다. 특히 오디오 생성 시, 텍스트나 이미지 입력을 받아 오디오 토큰을 출력하는 과정이 자연스럽게 이루어지며, 이는 멀티모달 명령어 추종(Instruction Following) 능력으로 이어진다.21</p>
<h3>4.3  Google Gemini 1.5 &amp; GPT-4o: 네이티브 멀티모달의 확장</h3>
<p>Gemini 1.5는 최대 1,000만 토큰이라는 컨텍스트 윈도우를 통해 비디오와 오디오를 장시간 처리한다.28 이는 토큰화된 비디오 프레임 수천, 수만 장을 한 번에 메모리에 올리고 연산할 수 있다는 뜻이며, 이를 위해 Ring Attention과 같은 효율적인 어텐션 메커니즘과 MoE(Mixture-of-Experts) 구조가 결합되었다.29 GPT-4o는 음성 인식-텍스트 생성-음성 합성이라는 기존의 파이프라인(Pipeline)을 깨고, 오디오 입력을 받아 오디오를 직접 출력하는 ‘Omni’ 모델을 구현했다. 이는 응답 지연을 300ms대로 단축시키고, 음성에 담긴 감정과 억양까지 보존하는 결과를 낳았다.6 일부 분석에서는 GPT-4o가 시각 정보 처리에 있어 이산 토큰 대신 연속적인 표현(Continuous Token)을 사용할 가능성도 제기하고 있으나 31, 근본적으로는 단일 모델 내에서의 통합 처리라는 방향성을 공유한다.</p>
<h2>5.  창발적 능력과 향후 과제</h2>
<p>단일 토큰 공간 처리는 모델에게 새로운 **창발적 능력(Emergent Capabilities)**을 부여한다.</p>
<ol>
<li><strong>지식 전이(Transfer Learning):</strong> 텍스트에서 배운 논리적 추론 능력이 이미지나 오디오 생성에 전이된다. 예를 들어, 복잡한 공간적 추론이 필요한 이미지 생성이나, 문맥에 맞는 적절한 배경음악 생성이 가능해진다.</li>
<li><strong>인컨텍스트 러닝(In-context Learning):</strong> 퓨샷(Few-shot) 예시를 통해 새로운 모달리티 조합 작업을 수행할 수 있다. Gemini 1.5가 낯선 언어의 문법서를 읽고 번역하는 능력은 이러한 맥락에서 이해될 수 있다.32</li>
</ol>
<p>하지만 여전히 해결해야 할 과제들이 존재한다.</p>
<ul>
<li><strong>토큰 효율성:</strong> 이미지와 오디오는 텍스트에 비해 토큰 수가 압도적으로 많다. 1초의 오디오나 작은 이미지 패치도 수십, 수백 개의 토큰으로 변환되므로, 시퀀스 길이가 폭발적으로 증가한다. 이는 추론 비용 증가로 직결된다.</li>
<li><strong>이산화 손실:</strong> VQ 과정에서 필연적으로 정보 손실이 발생한다. 미세한 텍스트가 포함된 이미지나 고음질 오디오의 디테일이 뭉개지는 현상이 발생할 수 있다.3</li>
<li><strong>디코딩 전략:</strong> 텍스트, 이미지, 오디오 토큰이 섞여 있을 때, 빔 서치(Beam Search)나 샘플링(Sampling) 전략을 어떻게 최적화할 것인가는 여전히 어려운 문제다. 예를 들어, 텍스트 생성 중에는 높은 온도를, 이미지 생성 중에는 낮은 온도를 적용해야 할 수도 있다.</li>
</ul>
<h2>6.  결론</h2>
<p>서적 ’트랜스포머 싱귤래리티’의 13.2장에서 다루는 “이미지, 오디오, 텍스트의 단일 토큰 공간 처리“는 인공지능이 인간과 유사한 방식으로 정보를 통합하고 처리하는 **범용 인공지능(AGI)**으로 나아가는 결정적인 기술적 이정표이다. VQ-GAN과 RVQ를 통한 신호의 이산화, 통합 어휘의 구축, 그리고 QK-Norm과 같은 학습 안정화 기술의 발전은 서로 다른 세계에 존재하던 데이터들을 하나의 ’언어’로 통일시켰다.</p>
<p>2024년과 2025년의 연구들은 이 통합된 공간에서 모델이 단순히 데이터를 재생산하는 것을 넘어, 모달리티 간의 경계를 허물고 자유롭게 정보를 변환하고 생성할 수 있음을 증명하고 있다. 앞으로의 연구는 더욱 효율적인 토큰화 방식, 연속적 표현과의 하이브리드 결합, 그리고 폭발적으로 늘어나는 컨텍스트를 제어할 수 있는 새로운 아키텍처의 탐구로 이어질 것이다. 단일 토큰 공간은 이제 선택이 아닌, 멀티모달 지능의 필수적인 전제 조건이 되었다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>AIDC-AI/Awesome-Unified-Multimodal-Models - GitHub, https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models</li>
<li>Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists? - ACL Anthology, https://aclanthology.org/2025.acl-long.130.pdf</li>
<li>Chameleon: Mixed-Modal Early-Fusion Foundation Models - arXiv, https://arxiv.org/html/2405.09818v1</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - arXiv, https://arxiv.org/abs/2403.05530</li>
<li>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.pdf</li>
<li>Introduction to GPT-4o Image Generation - A Complete Guide - Learn OpenCV, https://learnopencv.com/gpt-4o-image-generation/</li>
<li>Representation Collapsing Problems in Vector Quantization - OpenReview, https://openreview.net/forum?id=2aOEiTfcZ4</li>
<li>Scaling Laws for Native Multimodal Models - arXiv, https://arxiv.org/pdf/2504.07951</li>
<li>Taming Transformers for High-Resolution Image Synthesis (aka #VQGAN) - GitHub Pages, https://compvis.github.io/taming-transformers/</li>
<li>12월 23, 2025에 액세스, [https://medium.com/@waleed.physics/the-future-of-time-series-modeling-tokenization-through-vector-quantization-77ed3bc745d8#:<sub>:text=The%20corresponding%20quantized%20vector%20is,the%20forecaster%20will%20later%20use.](https://medium.com/@waleed.physics/the-future-of-time-series-modeling-tokenization-through-vector-quantization-77ed3bc745d8#:</sub>:text=The corresponding quantized vector is, <a href="https://medium.com/@waleed.physics/the-future-of-time-series-modeling-tokenization-through-vector-quantization-77ed3bc745d8#:~:text=The%20corresponding%20quantized%20vector%20is,the%20forecaster%20will%20later%20use.">https://medium.com/@waleed.physics/the-future-of-time-series-modeling-tokenization-through-vector-quantization-77ed3bc745d8#:~:text=The%20corresponding%20quantized%20vector%20is,the%20forecaster%20will%20later%20use.</a></li>
<li>Waveform to Tokens | Archieb.in, https://www.archieb.in/rvq</li>
<li>Kosmos-1/2 &amp; UnifiedIO v1/2, https://faculty.cc.gatech.edu/~zk15/teaching/AY2025_cs8803vlm_fall/L11_KosmosUnifiedIO.pdf</li>
<li>Vall E - Microsoft, https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e/</li>
<li>VALL-E - The Future of Text to Speech? | Towards Data Science, https://towardsdatascience.com/vall-e-the-future-of-text-to-speech-d090b6ede07b/</li>
<li>Understanding Taming Transformers for High-Resolution Image Synthesis, https://www.analyticsvidhya.com/blog/2021/07/understanding-taming-transformers-for-high-resolution-image-synthesis-vqgan/</li>
<li>VQGAN: Taming Transformers for High-Resolution Image Synthesis [Paper Explained], https://www.youtube.com/watch?v=-wDSDtIAyWQ</li>
<li>Audio Tokenization: An Overview — The GenAI Guidebook - Ravin Kumar, https://ravinkumar.com/GenAiGuidebook/audio/audio_tokenization.html</li>
<li>AudioLM: a Language Modeling Approach to Audio Generation - David Grangier, https://david.grangier.info/papers/2023/audio-lm-generation.pdf</li>
<li>Simple and Controllable Music Generation - arXiv, https://arxiv.org/pdf/2306.05284</li>
<li>VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers - arXiv, https://arxiv.org/html/2406.05370v1</li>
<li>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action - arXiv, https://arxiv.org/html/2312.17172v1</li>
<li>Chameleon: Mixed-Modal Early- Fusion Foundation Models, https://faculty.cc.gatech.edu/~zk15/teaching/AY2024_cs8803vlm_fall/slides/L15_Chameleon.pdf</li>
<li>QK Norm and the Curious Case of Logit Drift - Ross Taylor, https://rossjtaylor.com/blog/qk-norm-and-the-curious-case-of-logit-drift/</li>
<li>How MoE Models Actually Learn: A Guide to Auxiliary Losses and Expert Balancing | by Chris Hughes | Dec, 2025 | Medium, https://medium.com/@chris.p.hughes10/how-moe-models-actually-learn-a-guide-to-auxiliary-losses-and-expert-balancing-293084e3f600</li>
<li>[2405.09818] Chameleon: Mixed-Modal Early-Fusion Foundation Models - arXiv, https://arxiv.org/abs/2405.09818</li>
<li>Meta AI’s Chameleon: A Revolutionary Leap in Mixed-Modal AI | by My Social - Medium, https://medium.com/aimonks/meta-ais-chameleon-a-revolutionary-leap-in-mixed-modal-ai-e4f4fc11d5ab</li>
<li>[2312.17172] Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action - arXiv, https://arxiv.org/abs/2312.17172</li>
<li>Our next-generation model: Gemini 1.5 - Google Blog, https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - arXiv, https://arxiv.org/pdf/2403.05530</li>
<li>Tokens Not Noise: How GPT-4o’s Approach Changes Everything About AI Art, https://gregrobison.medium.com/tokens-not-noise-how-gpt-4os-approach-changes-everything-about-ai-art-99ab8ef5195d</li>
<li>GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation - arXiv, https://arxiv.org/html/2504.02782v3</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - Kapler o AI, https://www.kapler.cz/wp-content/uploads/gemini_v1_5_report.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>