<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:4.3 LLaMA와 오픈 소스 모델의 대두</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>4.3 LLaMA와 오픈 소스 모델의 대두</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>4.3 LLaMA와 오픈 소스 모델의 대두</span></nav>
                </div>
            </header>
            <article>
                <h1>4.3 LLaMA와 오픈 소스 모델의 대두</h1>
<p>2025-12-19, G30DR</p>
<h2>1.  서론: 폐쇄형 AI의 독점과 그 균열</h2>
<p>2020년대 초반, 인공지능(AI) 분야는 OpenAI의 GPT-3(Generative Pre-trained Transformer 3)가 촉발한 거대 언어 모델(LLM)의 혁명적 성능에 매료되어 있었다. 그러나 당시의 기술적 진보는 아이러니하게도 철저한 ’폐쇄성’을 담보로 이루어졌다. 최첨단 모델들은 소수의 빅테크(Big Tech) 기업들이 보유한 비공개 데이터셋, 수천 개의 GPU로 구성된 독점적 클러스터, 그리고 API(Application Programming Interface)라는 제한된 접근 경로 뒤에 숨겨져 있었다. 학계와 외부 연구자들은 모델이 어떤 데이터로 학습되었는지, 가중치(Weights)가 어떻게 구성되어 있는지 검증할 수 없었으며, 이를 “블랙박스(Black Box)“라 불렀다.</p>
<p>이러한 중앙집중화된 AI 권력 구조는 2023년 2월 24일, Meta(메타)가 **LLaMA(Large Language Model Meta AI)**를 공개하면서 거대한 균열을 맞이하게 된다.1 LLaMA 프로젝트는 단순히 새로운 모델의 출시를 넘어, 고성능 AI 모델을 누구나 자신의 로컬 장비에서 구동하고, 수정하고, 연구할 수 있다는 가능성을 증명하며 ’AI의 민주화(Democratization of AI)’라는 거대한 흐름을 촉발했다. 이는 폐쇄형(Closed source) 모델이 주도하던 AI 개발 패러다임을 개방형(Open source) 생태계와의 경쟁 구도로 재편하는 결정적 전환점이 되었다.</p>
<p>본 장에서는 LLaMA 1에서 시작된 오픈 소스 모델의 기술적 진화 과정과 Mistral, Qwen, DeepSeek, Google Gemma 등 파생 및 경쟁 모델들의 아키텍처 혁신, 그리고 이를 둘러싼 라이선스 논쟁과 로컬 구동 생태계의 변화를 심도 있게 분석한다. 특히, 단순히 모델의 성능 나열에 그치지 않고, 각 모델이 채택한 기술적 선택(Engineering Choices)이 전체 생태계에 미친 파급 효과와 미래 AI 지형도에 던지는 시사점을 포괄적으로 고찰한다.</p>
<pre><code class="language-mermaid">graph TD
    A["2020s Early: Closed AI Era"] --&gt;|"Focus"| B["Generative Pre-trained Transformer 3 (GPT-3)"]
    A --&gt;|"Characteristics"| C["Closed Source / Proprietary Data / API Access Only"]
    C --&gt; D["Black Box Problem"]
    
    E["2023.02.24: Meta LLaMA Release"] --&gt;|"Impact"| F["Cracking the Monopoly"]
    F --&gt;|"Proof"| G["High Performance on Local Devices"]
    F --&gt;|"Result"| H["Democratization of AI"]
    
    H --&gt; I["New Paradigm"]
    I --&gt; J["Closed Source Models"]
    I --&gt; K["Open Source Ecosystem"]
    J &lt;--&gt;|"Competition"| K
</code></pre>
<h2>2.  Meta LLaMA 1: 패러다임의 전환과 캄브리아기 대폭발</h2>
<pre><code class="language-mermaid">graph LR
    subgraph "Old Trend: Bigger is Better"
    direction TB
    OT1["Goal: Maximize Parameters"] --&gt; OT2["GPT-3 (175B)"]
    OT2 --&gt; OT3["PaLM (540B)"]
    OT3 --&gt;|"Result"| OT4["High Inference Cost / Low Accessibility"]
    end

    subgraph "New Trend: Chinchilla &amp; LLaMA"
    direction TB
    NT1["Goal: Inference Budget Optimization"] --&gt; NT2["Hoffmann's Chinchilla Scaling Laws"]
    NT2 --&gt;|"Strategy"| NT3["Decrease Size, Increase Data (Tokens)"]
    NT3 --&gt; NT4["LLaMA (13B trained on 1T tokens)"]
    NT4 --&gt;|"Result"| NT5["Small but Powerful / Local Executable"]
    end
</code></pre>
<h3>2.1  친칠라 스케일링 법칙(Chinchilla Scaling Laws)의 재해석과 적용</h3>
<p>LLaMA 1의 등장이 학계와 산업계에 충격을 준 근본적인 이유는 모델 설계 철학의 급진적인 전환에 있었다. GPT-3의 1,750억(175B) 개 파라미터나 PaLM의 5,400억(540B) 개 파라미터 등, 당시 업계는 “거거익선(Bigger is Better)“이라는 믿음 아래 모델의 크기를 키우는 데 혈안이 되어 있었다. 그러나 이러한 거대 모델들은 학습뿐만 아니라 추론(Inference) 시에도 막대한 비용을 유발하여, 실질적인 서비스 적용이나 연구 접근성을 저해하는 요인이 되었다.</p>
<p>Meta의 연구진은 DeepMind가 제안한 **‘친칠라 스케일링 법칙(Chinchilla Scaling Laws)’**에 주목했다. 호프만(Hoffmann) 등이 제안한 이 법칙은 주어진 연산 예산(Compute Budget) 내에서 최적의 성능을 내기 위해서는, 모델의 파라미터 수를 무작정 늘리는 것보다 학습 데이터의 양(토큰 수)을 비례적으로 늘리는 것이 더 효율적임을 시사했다.1 LLaMA 연구팀은 이 법칙을 한 단계 더 발전시켜, “학습 예산 최적화“가 아닌 **“추론 예산 최적화”**를 목표로 삼았다. 즉, 학습 시간이 더 오래 걸리더라도 모델 크기를 줄이고 압도적인 양의 데이터로 학습시키면, 추론 시에는 훨씬 적은 자원으로도 동등하거나 더 우수한 성능을 낼 수 있다는 가설을 세운 것이다.1</p>
<table><thead><tr><th><strong>모델 (Model)</strong></th><th><strong>파라미터 수 (Parameters)</strong></th><th><strong>학습 토큰 수 (Training Tokens)</strong></th><th><strong>주요 특징</strong></th></tr></thead><tbody>
<tr><td><strong>GPT-3</strong></td><td>175B</td><td>300B (추정)</td><td>거대 모델 트렌드의 시초, 희소 데이터 학습</td></tr>
<tr><td><strong>Chinchilla</strong></td><td>70B</td><td>1.4T</td><td>파라미터와 토큰의 1:20 비율 최적화 제안</td></tr>
<tr><td><strong>LLaMA-13B</strong></td><td>13B</td><td>1.0T</td><td>GPT-3(175B)를 능가하는 소형 모델의 가능성 입증</td></tr>
<tr><td><strong>LLaMA-65B</strong></td><td>65B</td><td>1.4T</td><td>당시 SOTA 모델인 Chinchilla-70B, PaLM-540B와 경쟁</td></tr>
</tbody></table>
<p>LLaMA 1은 7B, 13B, 33B, 65B의 네 가지 크기로 출시되었으며, 특히 13B 모델은 GPT-3(175B)보다 10배 이상 작음에도 불구하고 대부분의 벤치마크에서 이를 능가하는 기염을 토했다.1 이는 모델 아키텍처나 크기보다 <strong>데이터의 양과 질</strong>이 성능의 핵심 변수임을 증명한 사례로, 이후 모든 오픈 소스 모델이 ’소형화(Small but Powerful)’와 ‘데이터 중심(Data-centric)’ 전략을 채택하는 이정표가 되었다.</p>
<h3>2.2  LLaMA 아키텍처: 오픈 소스 LLM의 표준 설계</h3>
<p>LLaMA 1은 트랜스포머(Transformer) 구조를 기반으로 하되, 학습의 안정성과 추론 효율성을 위해 몇 가지 중요한 아키텍처 수정을 가했다. 이러한 수정 사항들은 LLaMA의 성공 이후 등장하는 거의 모든 오픈형 LLM(Mistral, Gemma 등)의 ‘사실상 표준(De facto standard)’ 설계로 자리 잡았다.4</p>
<pre><code class="language-mermaid">mindmap
  root(("LLaMA Architecture"))
    ("Normalization")
      ("Pre-normalization")
        ("Input Stage Normalization")
        ("Improved Stability")
      ("RMSNorm")
        ("Root Mean Square Layer Normalization")
        ("Calculation Efficiency")
    ("Activation Function")
      ("SwiGLU")
        ("Swish Gated Linear Unit")
        ("Better Performance than ReLU/GELU")
        ("xW * Sigmoid(xW) * xV")
    ("Position Embedding")
      ("RoPE")
        ("Rotary Positional Embeddings")
        ("Relative Position Encoding")
        ("Better Extrapolation")
</code></pre>
<h4>2.2.1  Pre-normalization (RMSNorm)</h4>
<p>기존의 트랜스포머(Original Transformer)는 각 서브 레이어(Sub-layer)의 출력에 정규화(Normalization)를 적용하는 Post-normalization 방식을 주로 사용했으나, 이는 깊은 신경망 학습 시 기울기 소실이나 폭주 문제를 일으킬 가능성이 있었다. LLaMA는 입력 단계에서 정규화를 수행하는 <strong>Pre-normalization</strong> 방식을 채택하고, 구체적으로 **RMSNorm(Root Mean Square Layer Normalization)**을 도입했다. RMSNorm은 평균(Mean)을 계산하지 않고 분산(Variance)만을 사용하여 계산 효율성을 높이면서도, 학습의 안정성을 크게 개선했다.4</p>
<h4>2.2.2  SwiGLU 활성화 함수</h4>
<p>전통적인 ReLU(Rectified Linear Unit)나 GPT-3가 사용한 GELU(Gaussian Error Linear Unit) 대신, LLaMA는 SwiGLU(Swish Gated Linear Unit) 활성화 함수를 채택했다. SwiGLU는 게이팅 메커니즘(Gating Mechanism)을 포함하고 있어 정보의 흐름을 더 정교하게 제어할 수 있다.</p>
<p>수식으로 표현하면 다음과 같다:<br />
<span class="math math-display">
\text{SwiGLU}(x, W, V) = \text{Swish}_{\beta}(xW) \otimes (xV)
</span><br />
여기서 <span class="math math-inline">\otimes</span>는 요소별 곱(Element-wise product)을 의미한다. 비록 파라미터 수가 증가하는 단점이 있지만(가중치 행렬 <span class="math math-inline">W, V</span> 두 개가 필요), 실험적으로 동일 연산량 대비 더 높은 성능을 보임이 입증되었다.4</p>
<h4>2.2.3  RoPE (Rotary Positional Embeddings)</h4>
<p>위치 정보를 인코딩하는 방식에서도 혁신이 있었다. 절대적인 위치 임베딩(Absolute Positional Embedding)을 더하는 기존 방식은 학습한 길이보다 긴 시퀀스를 처리하는 외삽(Extrapolation) 능력이 떨어졌다. LLaMA는 **RoPE(Rotary Positional Embeddings)**를 도입하여, 쿼리(Query)와 키(Key) 벡터를 복소 평면 상에서 회전시킴으로써 상대적인 위치 정보를 인코딩했다.</p>
<p>RoPE의 핵심 아이디어는 두 토큰 간의 내적(Dot product)이 오직 두 토큰의 상대적 거리 <span class="math math-inline">(m-n)</span>에만 의존하도록 만드는 것이다. 이는 긴 문맥을 처리할 때도 성능 저하가 적고, 모델이 학습하지 않은 길이의 텍스트도 어느 정도 처리할 수 있는 유연성을 제공했다.4</p>
<h3>2.3  4chan 유출 사태와 ‘알파카(Alpaca) 모멘트’</h3>
<p>Meta는 당초 LLaMA 1을 비상업적 연구 목적으로 승인된 연구자들에게만 제한적으로 공개했다. 그러나 배포 일주일 만인 2023년 3월, 모델의 가중치(Weights) 파일이 이미지보드 사이트인 ’4chan’을 통해 토렌트(Torrent)로 유출되는 초유의 사태가 발생했다.8 이는 기업의 보안 사고였으나, AI 역사상 가장 중요한 ’우발적 혁신’의 순간으로 기록된다.</p>
<p>유출된 가중치는 전 세계의 해커, 개발자, 연구자들의 손에 들어갔고, 이는 곧 오픈 소스 LLM 생태계의 **‘캄브리아기 대폭발(Cambrian Explosion)’**로 이어졌다. 스탠포드 대학 연구진은 유출된 LLaMA 7B 모델을 기반으로, OpenAI의 <code>text-davinci-003</code>을 이용해 생성한 52,000개의 명령어 데이터셋(Alpaca dataset)으로 미세 조정(Fine-tuning)한 <strong>‘Alpaca’</strong> 모델을 선보였다.8 불과 600달러 미만의 비용으로 훈련된 Alpaca는 거대 자본 없이도 상용 모델과 유사한 지시 따르기(Instruction Following) 능력을 갖출 수 있음을 증명했다.</p>
<p>이어 UC 버클리의 <strong>Vicuna</strong>, <strong>Koala</strong> 등 LLaMA 기반의 파생 모델들이 우후죽순처럼 등장했다.9 이 시기는 “Google에는 해자가 없다(Google Has No Moat)“라는 유출된 내부 문건의 제목처럼, 오픈 소스 진영이 집단 지성을 통해 빅테크의 기술적 우위를 위협하기 시작한 시발점이었다.</p>
<h2>3.  LLaMA 2와 3: 생태계의 표준화와 기술적 초격차</h2>
<pre><code class="language-mermaid">timeline
    title Evolution of LLaMA Series
    section LLaMA 1 (Feb 2023)
        Research&lt;br&gt;Only : Non-commercial license
        Architecture&lt;br&gt;Setup : "Pre-norm, SwiGLU, RoPE"
        Paradigm&lt;br&gt;Shift : "Small model, Huge data"
    section "LLaMA 2 (July 2023)"
        Commercial&lt;br&gt;Use : "Allowed (with restrictions)"
        2T&lt;br&gt;Tokens : "Increased training data"
        Safety &amp;&lt;br&gt;RLHF : "Ghost Attention, PPO"
        GQA : "Adopted in 34B/70B"
    section "LLaMA 3 (Apr 2024)"
        Performance&lt;br&gt;Peak : "Matches GPT-4 level"
        15T&lt;br&gt;Tokens : "Massive Data Scale"
        Vocab&lt;br&gt;Expansion : "128k Tokenizer"
        Multimodal&lt;br&gt;(v3.2) : "Image Processing Support"
</code></pre>
<h3>3.1  LLaMA 2: 상업적 이용의 허용과 안전성(Safety) 강화</h3>
<p>LLaMA 1의 성공과 유출 사태를 목격한 Meta는 2023년 7월, 후속작인 <strong>LLaMA 2</strong>를 공개하며 전략을 전면 수정했다. 가장 큰 변화는 <strong>라이선스 정책</strong>이었다. LLaMA 2는 연구 목적뿐만 아니라 상업적 이용까지 허용하는 파격적인 결정을 내렸다.11 다만, 월간 활성 사용자(MAU)가 7억 명 이상인 초대형 기업(Google, Amazon, Apple, Microsoft 등)의 경우 별도의 라이선스 계약을 맺어야 한다는 조항을 두어, 사실상 경쟁 빅테크 기업들의 무임승차를 견제하고 오픈 소스 생태계를 Meta 중심으로 재편하려는 의도를 분명히 했다.11</p>
<p>LLaMA 2는 <strong>2조(2T) 토큰</strong>의 데이터로 학습되었으며, 문맥 길이(Context Length)가 4,096 토큰으로 두 배 늘어났다. 기술적으로 주목할 점은 34B와 70B 모델에 **GQA(Grouped-Query Attention)**를 도입한 것이다. GQA는 MHA(Multi-Head Attention)의 높은 메모리 사용량과 MQA(Multi-Query Attention)의 성능 저하 사이에서 균형을 맞춘 기술로, 추론 속도를 획기적으로 개선했다.13</p>
<p>또한, LLaMA 2는 인간 피드백을 통한 강화 학습(RLHF: Reinforcement Learning from Human Feedback)을 적극적으로 적용했다. 수집된 백만 개 이상의 인간 선호도 데이터를 바탕으로 보상 모델(Reward Model)을 학습시키고, PPO(Proximal Policy Optimization) 알고리즘을 통해 모델을 정렬(Alignment)했다. 이는 오픈 소스 모델이 단순히 텍스트를 생성하는 도구를 넘어, 안전하고 유용한 ‘챗봇(Chatbot)’ 애플리케이션의 기반이 될 수 있음을 시사했다. 특히 멀티턴 대화에서 문맥을 잃지 않도록 하는 <strong>고스트 어텐션(Ghost Attention, GAtt)</strong> 기법이 도입되었다.</p>
<h3>3.2  LLaMA 3: 압도적인 스케일과 성능의 정점</h3>
<p>2024년 4월, Meta는 <strong>LLaMA 3</strong>를 공개하며 다시 한번 오픈 소스 모델의 기준을 상향 조정했다. LLaMA 3의 가장 큰 특징은 <strong>15조(15T) 토큰</strong>이라는 천문학적인 양의 학습 데이터다. 이는 LLaMA 2 대비 7배 이상 증가한 수치로, ’데이터가 깡패’라는 AI 학습의 불문율을 다시 한번 확인시켜 주었다.14</p>
<p>LLaMA 3는 8B와 70B 모델로 먼저 공개되었으며, 이후 <strong>405B</strong>라는 초대형 모델까지 확장되었다. 벤치마크 결과, LLaMA 3 70B 모델은 MMLU(Massive Multitask Language Understanding), GSM8K(수학), HumanEval(코딩) 등 주요 지표에서 GPT-4와 대등하거나 이를 능가하는 성능을 보여주었다.14</p>
<table><thead><tr><th><strong>벤치마크 (Benchmark)</strong></th><th><strong>LLaMA 3 70B</strong></th><th><strong>GPT-4</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>MMLU</strong> (일반 지식)</td><td>79.5%</td><td>86.4%</td><td>GPT-4가 여전히 우세하나 격차 축소</td></tr>
<tr><td><strong>GSM8K</strong> (수학)</td><td>93.0%</td><td>92.0%</td><td>LLaMA 3가 소폭 우세 (CoT 적용 시)</td></tr>
<tr><td><strong>HumanEval</strong> (코딩)</td><td>81.7%</td><td>67.0% (0-shot)</td><td>LLaMA 3의 코딩 능력 비약적 향상</td></tr>
<tr><td><strong>GPQA</strong> (전문가 추론)</td><td>39.5%</td><td>35.7%</td><td>복잡한 추론 영역에서 경쟁력 확보</td></tr>
</tbody></table>
<p>주: 벤치마크 점수는 측정 방식(shot 수, CoT 여부)에 따라 달라질 수 있음. 14</p>
<p>기술적으로 LLaMA 3는 모든 모델 크기에 GQA를 적용하여 추론 효율성을 극대화했고, 어휘 사전(Vocabulary) 크기를 128,000개(TikToken 기반)로 늘려 텍스트 인코딩 효율을 높였다. 이는 동일한 문장을 표현하는 데 더 적은 토큰을 사용하게 하여, 실질적인 문맥 처리 능력을 향상시켰다.</p>
<p>이어 출시된 <strong>LLaMA 3.1</strong>과 <strong>3.2</strong>는 문맥 길이를 **128K(128,000 토큰)**까지 확장하고, 이미지 처리가 가능한 <strong>멀티모달(Multimodal)</strong> 기능을 탑재했다.17 특히 1B, 3B와 같은 초소형 모델(On-device AI 용)부터 405B의 초거대 모델까지 라인업을 다각화하여, 모바일 기기부터 데이터 센터까지 모든 컴퓨팅 환경을 아우르는 전략을 완성했다.</p>
<h2>4.  효율성의 혁명: Mistral과 아키텍처의 진화</h2>
<p>Meta가 압도적인 자본과 데이터로 밀어붙일 때, 유럽의 스타트업 <strong>Mistral AI</strong>는 아키텍처의 효율성에 집중하여 ’작은 고추가 맵다’는 것을 증명했다.</p>
<pre><code class="language-mermaid">graph TD
    Input["Input Token Sequence"] --&gt; Layer["Transformer Layer"]
    Layer --&gt; Router{"Router (Gating Network)"}
    
    Router --&gt;|"Select Top-2 Experts"| Exp1["Expert A (Active)"]
    Router --&gt;|"Select Top-2 Experts"| Exp2["Expert B (Active)"]
    Router -.-&gt;|"Ignore (Inactive)"| Exp3["Expert C"]
    Router -.-&gt;|"Ignore (Inactive)"| Exp4["Expert D...H"]
    
    Exp1 --&gt;|"Output A"| Sum["Weighted Sum"]
    Exp2 --&gt;|"Output B"| Sum
    
    Sum --&gt; Next["Next Layer / Output"]
    
    subgraph "Efficiency"
    Eff1["Total Params: 47B"]
    Eff2["Active Params: 13B"]
    end
</code></pre>
<h3>4.1  Mistral 7B와 슬라이딩 윈도우 어텐션(SWA)</h3>
<p>2023년 9월 공개된 <strong>Mistral 7B</strong>는 불과 73억 개의 파라미터로 LLaMA 2 13B를 능가하고 LLaMA 1 34B와 대등한 성능을 보여주며 충격을 주었다.19 Mistral 7B의 핵심 기술은 **슬라이딩 윈도우 어텐션(Sliding Window Attention, SWA)**이다.</p>
<p>기존의 어텐션 메커니즘은 시퀀스 길이 <span class="math math-inline">n</span>에 대해 <span class="math math-inline">O(n^2)</span>의 연산량과 메모리를 요구한다. 이는 문맥이 길어질수록 비용이 기하급수적으로 증가함을 의미한다. 반면 SWA는 각 토큰이 이전의 모든 토큰을 보는 것이 아니라, 고정된 크기 <span class="math math-inline">W</span>(예: 4,096 토큰)의 윈도우 내에 있는 토큰들만 참조하도록 제한한다.19<br />
<span class="math math-display">
\text{Attention}(Q, K, V)_t = \text{Softmax}\left(\frac{Q_t K_{}^T}{\sqrt{d_k}}\right) V_{}
</span><br />
이 방식을 통해 연산 복잡도는 선형 시간 <span class="math math-inline">O(n \times W)</span>으로 감소한다.</p>
<p>“그렇다면 윈도우 밖의 먼 과거 정보는 잃어버리는 것이 아닌가?“라는 의문이 제기될 수 있다. Mistral은 이를 트랜스포머 레이어의 적층(Stacking)을 통해 해결했다. CNN(Convolutional Neural Network)에서 레이어를 쌓을수록 **수용 영역(Receptive Field)**이 넓어지는 것과 마찬가지로, SWA를 <span class="math math-inline">k</span>개 층 쌓으면 상위 레이어의 토큰은 이론적으로 <span class="math math-inline">k \times W</span> 토큰(약 131K 토큰)에 달하는 정보를 간접적으로 참조할 수 있게 된다.21</p>
<p>또한, Mistral은 **롤링 버퍼 KV 캐시(Rolling Buffer KV Cache)**를 도입하여 메모리 사용량을 고정했다. 윈도우를 벗어난 오래된 키(Key)와 값(Value)은 버퍼에서 덮어쓰기 되므로, 긴 문맥을 처리할 때도 메모리 폭증을 방지한다.23</p>
<h3>4.2  Mixtral 8x7B와 전문가 혼합(MoE)의 대중화</h3>
<p>Mistral AI는 이어 **‘Mixtral 8x7B’**를 통해 <strong>전문가 혼합(Mixture of Experts, MoE)</strong> 아키텍처를 오픈 소스 진영에 대중화시켰다. MoE는 전체 모델을 여러 개의 작은 전문 신경망(Expert)으로 나누고, 각 토큰을 처리할 때 라우터(Router)가 적합한 전문가(보통 2개)만 선택하여 연산하는 방식이다.22</p>
<p>Mixtral 8x7B는 총 47B(470억) 개의 파라미터를 가지지만, 추론 시에는 토큰당 약 <strong>13B(130억)</strong> 개의 파라미터만 활성화된다. 이는 47B 모델의 지식 용량을 가지고 있으면서도 13B 모델 수준의 빠른 속도(FLOPs)로 추론할 수 있음을 의미한다. Mixtral은 이 구조를 통해 LLaMA 2 70B보다 훨씬 적은 자원으로 더 높은 성능을 달성하며, 효율적인 스케일링의 새로운 모범 답안을 제시했다.22</p>
<h2>5.  중화권의 기술적 도약: DeepSeek와 Qwen</h2>
<p>오픈 소스 AI 경쟁에서 중국 모델들의 약진은 괄목할 만하다. 특히 DeepSeek(딥시크)와 Alibaba의 Qwen(통의천문)은 단순한 모방을 넘어 아키텍처 혁신을 주도하고 있다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "Traditional MHA"
    MHA_K["Key Matrix (Heavy VRAM)"]
    MHA_V["Value Matrix (Heavy VRAM)"]
    end

    subgraph "DeepSeek MLA (Multi-head Latent Attention)"
    Input["Input x"] --&gt; Compress["Low-rank Compression"]
    Compress --&gt; Latent["Latent Vector c_KV (Compressed)"]
    
    Latent --&gt;|"Cache Only This"| Cache[("Compressed KV Cache")]
    
    Cache --&gt;|"Expand when needed"| DecodeK["W_UK (Key Expansion)"]
    Cache --&gt;|"Expand when needed"| DecodeV["W_UV (Value Expansion)"]
    
    DecodeK --&gt; RealK["Restored Key"]
    DecodeV --&gt; RealV["Restored Value"]
    end
    
    MHA_K -.-&gt;|"Comparison: 1/8 Memory Usage"| Cache
</code></pre>
<h3>5.1  DeepSeek V3: MLA와 KV 캐시 혁명</h3>
<p>2024년 말 등장한 <strong>DeepSeek V3</strong>는 671B라는 거대한 MoE 모델임에도 불구하고, 아키텍처 최적화를 통해 추론 비용을 획기적으로 낮췄다. 그 핵심에는 **멀티 헤드 잠재 어텐션(Multi-head Latent Attention, MLA)**이 있다.</p>
<p>LLM 추론 과정에서 가장 큰 병목 현상 중 하나는 ’KV 캐시(Key-Value Cache)’가 차지하는 막대한 GPU 메모리다. 문맥이 길어질수록 KV 캐시는 선형적으로 증가하여 VRAM을 잠식한다. DeepSeek는 키(Key)와 값(Value) 벡터를 직접 저장하는 대신, 이를 저순위(Low-rank) 행렬로 압축하여 <strong>‘잠재 벡터(Latent Vector)’</strong> 형태로 저장하는 MLA를 고안했다.26<br />
<span class="math math-display">
c_{KV} = x W_{DKV}
</span></p>
<p><span class="math math-display">
K = c_{KV} W_{UK}, \quad V = c_{KV} W_{UV}
</span></p>
<p>추론 시에는 압축된 잠재 벡터 <span class="math math-inline">c_{KV}</span>만 캐싱하고, 어텐션 연산이 필요할 때만 복원 행렬(<span class="math math-inline">W_{UK}, W_{UV}</span>)을 곱해 키와 값을 생성한다. 이를 통해 KV 캐시 메모리 사용량을 기존 MHA 대비 1/8 수준으로 줄이면서도, 성능 손실을 최소화했다.29</p>
<p>또한, DeepSeek는 <strong>Auxiliary-loss-free Load Balancing</strong> 전략을 통해 MoE 모델 학습 시 전문가 간의 부하 균형을 맞추기 위해 사용하던 보조 손실(Auxiliary Loss)을 제거했다. 보조 손실은 모델의 주된 학습 목표와 상충하여 성능을 저하시키는 요인이었는데, DeepSeek는 이를 제거하고도 균형을 맞추는 알고리즘을 개발하여 모델의 순수한 성능을 극대화했다.31</p>
<h3>5.2  Qwen 2.5: 전방위적 성능의 완성형</h3>
<p>Alibaba Cloud의 <strong>Qwen 2.5</strong> 시리즈는 LLaMA 3.1의 가장 강력한 경쟁자로 평가받는다. 0.5B부터 72B까지 다양한 크기로 출시된 Qwen 2.5는 <strong>18조(18T) 토큰</strong>이라는 방대한 데이터로 학습되었으며, 특히 코딩(Qwen-Coder)과 수학(Qwen-Math) 능력에서 특화된 성능을 보인다.32</p>
<p>Qwen 2.5는 <strong>YARN</strong>과 <strong>Dual Chunk Attention(DCA)</strong> 기술을 적용하여 최대 <strong>100만(1M) 토큰</strong>의 문맥을 처리할 수 있는 능력을 갖췄다.32 이는 긴 문서를 분석하거나 복잡한 코드를 생성하는 작업에서 탁월한 이점을 제공한다. 벤치마크 상으로 Qwen 2.5 72B는 LLaMA 3.1 70B를 상회하며, 심지어 일부 지표에서는 LLaMA 3.1 405B와 경쟁하는 수준의 효율성을 보여준다.33</p>
<h2>6.  빅테크의 대응: Google Gemma와 ‘오픈 웨이트’</h2>
<p>Google은 자사의 최상위 폐쇄형 모델인 Gemini(제미나이)를 보호하면서도, LLaMA가 주도하는 오픈 생태계에 대응하기 위해 경량화된 오픈 모델군인 **Gemma(젬마)**를 출시했다.</p>
<pre><code class="language-mermaid">graph TD
    Teacher["Teacher Model (Large/Gemini)"] --"Generates Probabilities"--&gt; Soft["Soft Targets (Distribution)"]
    Student["Student Model (Small/Gemma 2B, 9B)"] --"Predicts"--&gt; Hard["Hard Predictions"]
    
    Soft --&gt; Loss{"Loss Function"}
    Hard --&gt; Loss
    
    Loss --"Update Weights"--&gt; Student
    
    style Teacher fill:#f9f,stroke:#333,stroke-width:2px
    style Student fill:#bbf,stroke:#333,stroke-width:2px
</code></pre>
<h3>6.1  아키텍처 및 학습의 특징: GeGLU와 지식 증류</h3>
<p>Gemma는 Gemini의 기술적 유산을 계승했다. 활성화 함수로 일반적인 SwiGLU 대신 **GeGLU(Gated Linear Unit with GeLU)**를 사용하여 비선형성을 강화했고, RoPE를 통해 위치 정보를 처리했다.35 특히 주목할 점은 <strong>지식 증류(Knowledge Distillation)</strong> 기법의 적극적인 활용이다. Gemma 2(2B, 9B)는 처음부터 다음 토큰 예측(Next Token Prediction)만으로 학습된 것이 아니라, 더 거대한 모델(Teacher Model)이 생성한 확률 분포를 따라 하도록 학습되었다.37 이를 통해 작은 파라미터 크기에서도 거대 모델에 필적하는 추론 능력과 언어 이해력을 확보할 수 있었다.</p>
<h3>6.2  ’오픈 소스’가 아닌 ‘오픈 모델’</h3>
<p>Google은 Gemma를 공개하면서 “Open Source“라는 용어 대신 **“Open Models”**라는 표현을 의도적으로 사용했다.38 이는 OSI(Open Source Initiative)가 정의하는 오픈 소스의 엄격한 요건(학습 데이터 공개 등)을 충족하지 못하기 때문이다. Gemma는 가중치는 공개되었으나, 학습 데이터는 비공개이며 라이선스 조항에서도 ’책임감 있는 AI 라이선스(RAIL)’를 통해 특정 유해 용도 사용을 제한한다. 이는 현재 ’오픈 소스 AI’라고 불리는 대부분의 모델(LLaMA 포함)이 사실은 <strong>‘오픈 웨이트(Open Weights)’</strong> 모델에 가깝다는 현실을 보여주는 사례다.</p>
<h2>7.  로컬 구동과 최적화 기술: 하드웨어의 제약을 넘어서</h2>
<p>모델의 공개만큼이나 중요한 것은 그 모델을 일반 사용자가 실제로 구동할 수 있게 만든 소프트웨어 및 최적화 기술의 발전이다. 수천만 원을 호가하는 엔터프라이즈급 GPU가 없는 개인 사용자들도 고성능 LLM을 사용할 수 있게 된 데에는 <strong>양자화(Quantization)</strong> 기술과 <strong>llama.cpp</strong> 프로젝트의 공헌이 절대적이었다.</p>
<pre><code class="language-mermaid">graph TD
    Origin["Original Model (FP16/BF16)"] --&gt;|"High VRAM Req (e.g., 70B = 140GB)"| Server["Data Center GPU"]
    
    Origin --&gt;|"Conversion"| Quant["Quantization (GGUF Format)"]
    
    subgraph "Quantization Process"
    Quant --&gt;|"Compression"| Q4["4-bit / 8-bit Integer Mapping"]
    Q4 --&gt;|"Optimization"| KQuants["k-quants (Variable bit-rate per layer)"]
    end
    
    KQuants --&gt; Final["Quantized Model (e.g., Q4_K_M)"]
    
    Final --&gt;|"Run with"| Engine["llama.cpp Engine"]
    
    Engine --&gt;|"Hardware"| Apple["Apple Silicon (Unified Memory)"]
    Engine --&gt;|"Hardware"| Consumer["Consumer GPU (RTX 3090/4090)"]
    
    Apple --&gt;|"Result"| Local["On-Device AI Execution"]
</code></pre>
<h3>7.1  llama.cpp와 GGUF 포맷</h3>
<p>Georgi Gerganov가 개발한 <strong>llama.cpp</strong>는 무거운 Python 종속성이나 PyTorch 없이 C/C++로 구현된 경량화 추론 엔진이다. 이 프로젝트는 특히 Apple Silicon(M1, M2 등)의 통합 메모리(Unified Memory) 구조를 효율적으로 활용하여, 맥북 한 대로도 수백억 파라미터의 모델을 구동할 수 있는 길을 열었다.39</p>
<p>이와 함께 등장한 <strong>GGUF(GPT-Generated Unified Format)</strong> 파일 포맷은 모델 배포의 표준으로 자리 잡았다. GGUF는 모델의 가중치뿐만 아니라 토크나이저 정보, 하이퍼파라미터, 양자화 정보 등을 하나의 단일 파일에 패키징하여 관리와 사용 편의성을 극대화했다.40 이는 복잡한 설정 없이 파일 하나만 다운로드하면 즉시 모델을 실행할 수 있게 해주었다.</p>
<h3>7.2  양자화(Quantization)의 마법</h3>
<p>LLM의 가중치는 기본적으로 16비트(FP16, BF16) 부동소수점으로 저장된다. 이를 4비트나 8비트 정수로 변환하는 <strong>양자화</strong> 기술은 모델의 크기를 1/4 수준으로 줄이면서도 성능 저하를 최소화한다.39</p>
<ul>
<li><strong>FP16 (16-bit):</strong> 모델 파라미터당 2바이트 소모. 70B 모델 구동에 약 140GB VRAM 필요.</li>
<li><strong>Q4_K_M (4-bit GGUF):</strong> 파라미터당 약 0.5바이트+메타데이터 소모. 70B 모델 구동에 약 40GB VRAM 필요.</li>
</ul>
<p>특히 <code>k-quants</code>와 같은 정교한 양자화 기법은 모델의 중요한 레이어(어텐션 등)는 높은 비트(6-bit 등)로, 덜 중요한 레이어는 낮은 비트(3-4-bit)로 저장하여 압축 효율과 성능의 균형을 맞춘다. 이러한 기술적 진보는 연구실의 슈퍼컴퓨터에 갇혀 있던 AI를 개인의 PC(GeForce RTX 시리즈 등)와 엣지 디바이스로 해방시켰다.</p>
<h2>8.  오픈 소스 AI의 정의와 미래 전망</h2>
<p>오픈 소스 모델의 확산과 함께, 진정한 ’오픈 소스 AI’가 무엇인지에 대한 철학적, 법적 논쟁도 가열되고 있다.</p>
<pre><code class="language-mermaid">graph TD
    Start{"AI Model Release"} --&gt; Check1{"Are Weights Public?"}
    Check1 --"No"--&gt; Closed["Closed Source (GPT-4, Gemini)"]
    Check1 --"Yes"--&gt; Check2{"Is Training Data Open?"}
    
    Check2 --"No"--&gt; OpenWeight["Open Weights / Source Available (LLaMA, Gemma, Mistral)"]
    Check2 --"Yes"--&gt; Check3{"Is Code/Processing Logic Open?"}
    
    Check3 --"Yes"--&gt; OSI["OSI Definition Open Source AI (OLMo, etc.)"]
    Check3 --"No"--&gt; OpenWeight
    
    OpenWeight --&gt;|"Limitation"| License["Restricted Commercial Use / Safety Clauses"]
    OSI --&gt;|"Freedom"| Free["Freedom to Study, Modify, Share fully"]
</code></pre>
<h3>8.1  OSI의 새로운 정의와 논쟁</h3>
<p>오픈 소스 이니셔티브(OSI)는 2024년 10월, **“Open Source AI Definition (OSAID) v1.0”**을 발표했다.42 이 정의에 따르면, 진정한 오픈 소스 AI는 사용자가 시스템을 자유롭게 사용, 연구, 수정, 배포할 수 있어야 하며, 이를 위해 <strong>“데이터 정보(Data Information)”</strong>, <strong>“코드(Code)”</strong>, **“파라미터(Parameters)”**의 세 가지 요소를 공개해야 한다.</p>
<p>핵심 쟁점은 <strong>학습 데이터의 공개 범위</strong>였다. 자유 소프트웨어 재단(FSF) 등 급진적 진영은 학습 데이터 원본 전체를 공개해야 한다고 주장했으나, 저작권 및 프라이버시 문제로 인해 이는 현실적으로 불가능에 가까웠다. 결국 OSI는 “숙련된 사람이 실질적으로 동등한 시스템을 재생산할 수 있는 수준“의 상세한 데이터 정보 공개를 요구하는 선에서 타협했다.43</p>
<p>이 기준에 따르면, 현재 ’오픈 소스’를 표방하는 LLaMA나 Gemma는 엄밀한 의미의 오픈 소스가 아닐 가능성이 높다. LLaMA는 7억 명 이상의 MAU를 가진 기업에 라이선스 제약을 두고 있으며, Gemma 역시 특정 용도를 제한한다. 따라서 현재의 생태계는 ’오픈 소스’보다는 <strong>‘오픈 웨이트(Open Weights)’</strong> 또는 **‘소스 사용 가능(Source-available)’**이라는 용어로 분류하는 것이 더 정확하다.38</p>
<h3>8.2  결론: 민주화된 지능과 주권 AI(Sovereign AI)</h3>
<p>Meta의 LLaMA 1 공개로 촉발된 오픈 소스 AI의 대두는 AI 기술의 독점을 막고 기술 주권을 분산시키는 역사적인 사건이었다. LLaMA가 불을 지피고, Mistral이 효율성을 증명했으며, DeepSeek와 Qwen이 아키텍처를 혁신하고, llama.cpp가 접근성을 낮추는 과정은 거대한 집단 지성의 승리였다.</p>
<p>이제 오픈 소스 모델은 성능 면에서 GPT-4와 같은 최상위 폐쇄형 모델들과 어깨를 나란히 하고 있다. 이는 기업들이 자신의 민감한 데이터를 외부 API로 보내지 않고도 자체적인 고성능 AI(On-premise AI)를 구축할 수 있음을 의미하며, 이는 곧 **주권 AI(Sovereign AI)**의 실현 가능성을 높여주었다. 향후 오픈 소스 진영은 단순한 성능 경쟁을 넘어, 더욱 투명하고 윤리적이며, 효율적인 인공지능의 표준을 제시하는 방향으로 진화해 나갈 것이다. 가중치가 한번 공개된 이상 그 흐름을 되돌릴 수는 없으며, ’오픈 소스’는 AI 발전의 가장 강력한 엔진으로 남을 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>LLaMA: Open and Efficient Foundation Language Models - PARSA, https://parsa.epfl.ch/course-info/cs723/papers/llama.pdf</li>
<li>Training at Scale: Chinchilla Scaling Laws for Compute-Optimal Training of LLMs - Medium, https://medium.com/@zaiinn440/training-at-scale-chinchilla-scaling-laws-for-compute-optimal-training-of-llms-eca49f58c358</li>
<li>arXiv:2302.13971v1 [cs.CL] 27 Feb 2023, https://arxiv.org/abs/2302.13971</li>
<li>LLaMA: Meta’s Open Foundation Models That Democratized Language AI Research - Interactive | Michael Brenndoerfer, https://mbrenndoerfer.com/writing/llama-meta-open-foundation-models-democratized-language-ai-research</li>
<li>LLaMA: Concepts Explained (Summary) | by Anshu Kumar - Medium, https://akgeni.medium.com/llama-concepts-explained-summary-a87f0bd61964</li>
<li>Better Transformers | lifei.ai, https://lifei.ai/posts/2023-08-31-better-transformers/</li>
<li>SwiGLU: The Activation Function Powering Modern LLMs | by Saeed Mehrang - Medium, https://medium.com/@saeed.mehrang/swiglu-the-activation-function-powering-modern-llms-70ea5cfdeafe</li>
<li>How Meta’s LLaMA NLP Model Leaked - DeepLearning.AI, https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/</li>
<li>The LLama Effect: Leak Sparked a Series of Open Source Alternatives to ChatGPT | Hacker News, https://news.ycombinator.com/item?id=35504428</li>
<li>Meta’s powerful language models leak online, the U.S. Commerce Department opens up applications for CHIPS funds, and a PRC reorganization will impact R&amp;D and data | Center for Security and Emerging Technology - CSET, https://cset.georgetown.edu/newsletter/march-9-2023/</li>
<li>LLaMA 2 and Open Source - OSPOCO, https://ospo.co/blog/llama-2-and-open-source/</li>
<li>Meta announces Llama 2; “open sources” it for commercial use - LessWrong, https://www.lesswrong.com/posts/9rdpdDerangjYeQGW/meta-announces-llama-2-open-sources-it-for-commercial-use</li>
<li>Is Llama 3 better than GPT4? - Galileo AI, https://galileo.ai/blog/is-llama-3-better-than-gpt4</li>
<li>Llama 3 vs GPT 4: A Detailed Comparison | Which to Choose? - PromptLayer Blog, https://blog.promptlayer.com/llama-3-vs-gpt-4/</li>
<li>GPT-4 vs Llama 3.1 8B Instruct - LLM Stats, https://llm-stats.com/models/compare/gpt-4-0613-vs-llama-3.1-8b-instruct</li>
<li>Meta AI’s Llama 3 vs GPT 4 - TextCortex, https://textcortex.com/post/llama-3-vs-gpt-4</li>
<li>Llama 3.1 vs Llama 3.2 | Eden AI, https://www.edenai.co/post/llama-3-1-vs-llama-3-2</li>
<li>Exploring Llama 3 Models: A Deep Dive - Galileo AI, https://galileo.ai/blog/exploring-llama-3-models-a-deep-dive</li>
<li>Mistral 7B, https://mistral.ai/news/announcing-mistral-7b</li>
<li>Summary of Mistral 7B. Abstract | by Manoj Kumal - TAI Blog, https://blog.tai.com.np/summary-of-mistral-7b-1d5ca9a6c17c</li>
<li>Why Stacking Sliding Windows Can’t See Very Far - Guangxuan Xiao, https://guangxuanx.com/blog/stacking-swa.html</li>
<li>Mastering Mistral AI: From Sliding Window Attention to Efficient Inference | by Ebad Sayed, https://medium.com/@sayedebad.777/mastering-mistral-ai-from-sliding-window-attention-to-efficient-inference-22d944384788</li>
<li>Breaking Down Mistral 7B . Exploring Mistral’s Rotary positional… | by JAIGANESAN | Towards AI, https://pub.towardsai.net/breaking-down-mistral-7b-3176f119f2eb</li>
<li>Mistral 7B: A Revolutionary Breakthrough in LLMs - Data Science Dojo, https://datasciencedojo.com/blog/mistral-7b-emergence-in-llm/</li>
<li>Mistral / Mixtral Explained: Sliding Window Attention, Sparse Mixture of Experts, Rolling Buffer - YouTube, https://www.youtube.com/watch?v=UiX8K-xBUpE</li>
<li>deepseek-ai/DeepSeek-V3 - Hugging Face, https://huggingface.co/deepseek-ai/DeepSeek-V3</li>
<li>Understanding DeepSeek-V3 Architecture | by Dewang Sultania | My musings with LLMs, https://medium.com/my-musings-with-llms/understanding-the-deepseek-v3-architecture-aee01112b938</li>
<li>DeepSeek-V3 Explained: Optimizing Efficiency and Scale, https://adasci.org/deepseek-v3-explained-optimizing-efficiency-and-scale/</li>
<li>Understanding Multi-Head Latent Attention, https://planetbanatt.net/articles/mla.html</li>
<li>Multi-head Latent Attention (MLA): Secret behind the success of DeepSeek Large Language Models | by Nagur Shareef Shaik | Medium, https://medium.com/@shaiknagurshareef/multi-head-latent-attention-mla-secret-behind-the-success-of-deepseek-large-language-models-66612071d756</li>
<li>DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/html/2412.19437v1</li>
<li>Qwen2.5-1M Technical Report, https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-1M/Qwen2_5_1M_Technical_Report.pdf</li>
<li>[2412.15115] Qwen2.5 Technical Report - arXiv, https://arxiv.org/abs/2412.15115</li>
<li>arXiv:2412.15115v2 [cs.CL] 3 Jan 2025, https://arxiv.org/pdf/2412.15115</li>
<li>Gemma: Open Models Based on Gemini Research and Technology - arXiv, https://arxiv.org/html/2403.08295v1</li>
<li>Gemma explained: An overview of Gemma model family architectures - Google Developers Blog, https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/</li>
<li>Gemma 2: Improving Open Language Models at a Practical Size - arXiv, https://arxiv.org/html/2408.00118v1</li>
<li>AI: The Difference Between Open and Open Source - RedMonk, https://redmonk.com/sogrady/2024/02/26/ai-open-source/</li>
<li>Quantize Llama models with GGUF and llama.cpp - Towards Data Science, https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172/</li>
<li>What is GGUF? A Beginner’s Guide - Shep Bryan, https://www.shepbryan.com/blog/what-is-gguf</li>
<li>Overview of GGUF quantization methods : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/1ba55rj/overview_of_gguf_quantization_methods/</li>
<li>2024 end-of-year review: Open Source AI Definition v1.0, https://opensource.org/blog/2024-end-of-year-review-open-source-ai-definition-v1-0</li>
<li>The Open Source AI Definition – 1.0, https://opensource.org/ai/open-source-ai-definition</li>
<li>OSI Releases New Definition for Open Source AI, Setting Standards for Transparency and Accessibility - InfoQ, https://www.infoq.com/news/2024/11/open-source-ai-definition/</li>
<li>AI Models From Google, Meta, Others May Not Be Truly ‘Open Source’ | PCMag, https://www.pcmag.com/news/ai-models-from-google-meta-others-may-not-be-truly-open-source</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>