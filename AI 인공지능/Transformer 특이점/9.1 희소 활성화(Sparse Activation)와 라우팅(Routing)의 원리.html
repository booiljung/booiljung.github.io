<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:9.1 희소 활성화(Sparse Activation)와 라우팅(Routing)의 원리</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>9.1 희소 활성화(Sparse Activation)와 라우팅(Routing)의 원리</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>9.1 희소 활성화(Sparse Activation)와 라우팅(Routing)의 원리</span></nav>
                </div>
            </header>
            <article>
                <h1>9.1 희소 활성화(Sparse Activation)와 라우팅(Routing)의 원리</h1>
<p>2025-12-21, G30DR</p>
<p>인공지능 연구의 역사는 곧 연산 자원과 모델 용량(Capacity) 사이의 투쟁의 역사라 해도 과언이 아니다. 딥러닝, 특히 트랜스포머(Transformer) 아키텍처의 등장 이후, 모델의 크기를 키우는 ’스케일링(Scaling)’은 성능 향상을 보장하는 가장 확실한 수표로 통용되어 왔다. 그러나 모델의 파라미터 수가 수천억 개를 넘어 조(Trillion) 단위에 육박하는 ’거대 언어 모델(LLM)’의 시대에 이르러, 우리는 물리적인 한계에 봉착했다. 모든 입력 토큰에 대해 네트워크의 모든 파라미터를 연산에 참여시키는 밀집(Dense) 모델의 방식은, 그 거대한 덩치만큼이나 비효율적인 에너지 소모와 추론 지연(Latency)을 야기하기 때문이다.</p>
<p>이러한 배경에서 전문가 혼합 모델(Mixture of Experts, MoE)은 ’조건부 연산(Conditional Computation)’이라는 개념을 통해 이 딜레마를 타개할 핵심 아키텍처로 부상했다. 본 장에서는 MoE의 심장이라 할 수 있는 희소 활성화(Sparse Activation)와 라우팅(Routing)의 원리를 수학적, 시스템적, 그리고 역사적 맥락에서 심층적으로 분석한다. 우리는 1990년대의 초기 개념부터 시작하여, 구글의 Switch Transformer를 거쳐, 최근 DeepSeek-V3에서 제안된 보조 손실 없는(Auxiliary-Loss-Free) 라우팅 메커니즘에 이르기까지, 전문가를 선택하고 관리하는 기술의 진화를 추적할 것이다.</p>
<h2>1.  조건부 연산의 철학과 희소성의 미학</h2>
<p>전통적인 밀집 트랜스포머 모델에서 피드포워드 네트워크(FFN) 층은 입력되는 데이터의 성격과 무관하게 항상 동일한 가중치 행렬을 사용하여 연산을 수행한다. 이는 마치 대학의 모든 학생이 전공과 상관없이 캠퍼스 내의 모든 교수에게 강의를 들어야만 졸업할 수 있는 비효율적인 시스템과 유사하다. 반면, MoE 아키텍처는 거대한 신경망을 다수의 ’전문가(Expert)’라 불리는 하위 네트워크(Sub-network)로 분할하고, 각 입력 데이터(토큰)의 특성에 따라 적합한 소수의 전문가만을 선택적으로 활성화한다.1</p>
<pre><code class="language-mermaid">graph LR
    subgraph "Dense Transformer (Traditional)"
        D_Input["입력 토큰 (Input Token)"] --&gt; D_FFN["거대 FFN 층 (전체 파라미터 활성화)"]
        D_FFN --&gt; D_Output["출력 (Output)"]
        style D_FFN fill:#f9f,stroke:#333,stroke-width:2px
    end

    subgraph "Mixture of Experts (MoE)"
        M_Input["입력 토큰 (Input Token)"] --&gt; Router["라우터 (Gating Network)"]
        
        Router --"Routing Score"--&gt; Select{"Top-k 선택"}
        
        Select --"경로 1"--&gt; E1["전문가 1 (Expert 1)"]
        Select --"경로 2"--&gt; E2["전문가 2 (Expert 2)"]
        Select -. "선택 안됨" .- E3["전문가 3 (Inactive)"]
        Select -. "선택 안됨" .- E4["전문가 N (Inactive)"]
        
        E1 --&gt; Sum["가중 합 (Weighted Sum)"]
        E2 --&gt; Sum
        
        Sum --&gt; M_Output["출력 (Output)"]
        
        style Router fill:#ff9,stroke:#333,stroke-width:2px
        style E1 fill:#bbf,stroke:#333,stroke-width:2px
        style E2 fill:#bbf,stroke:#333,stroke-width:2px
        style E3 fill:#eee,stroke:#999,stroke-dasharray: 5 5
        style E4 fill:#eee,stroke:#999,stroke-dasharray: 5 5
    end
</code></pre>
<h3>1.1  생물학적 영감과 스케일링 법칙의 재해석</h3>
<p>이러한 접근은 생물학적 뇌의 작동 원리와 깊이 맞닿아 있다. 인간의 뇌는 약 860억 개의 뉴런을 가지고 있지만, 특정 인지 과제를 수행할 때 이들 모두가 동시에 발화하지 않는다. 시각 정보를 처리할 때는 후두엽의 시각 피질이, 언어를 생성할 때는 브로카 영역이 활성화되는 것처럼, 뇌는 기능적으로 분화된 영역들의 ’희소한 활성화’를 통해 막대한 에너지 효율을 달성한다.1</p>
<p>MoE는 이러한 원리를 인공신경망에 도입함으로써, 모델의 ’총 파라미터(Total Parameters)’와 ’활성 파라미터(Active Parameters)’를 분리한다. 예를 들어, DeepSeek-V3 모델은 총 6,710억 개(671B)의 파라미터를 보유하고 있지만, 단일 토큰을 처리할 때는 오직 370억 개(37B)의 파라미터만을 활성화한다.3 이는 모델이 671B 규모의 지식 용량을 가지면서도, 실제 연산 비용(FLOPs)은 37B 모델 수준으로 억제할 수 있음을 의미한다.</p>
<p>이러한 구조적 특성은 스케일링 법칙(Scaling Laws)에 새로운 차원을 더한다. 밀집 모델에서는 파라미터 수를 늘리면 연산량도 정비례하여 증가하지만, MoE에서는 전문가의 수 <span class="math math-inline">N</span>을 늘려 총 파라미터를 확장하더라도, 토큰당 선택되는 전문가의 수 <span class="math math-inline">k</span>를 고정하면 연산 비용을 거의 일정하게 유지할 수 있다.4 이는 무어의 법칙이 둔화된 하드웨어 환경에서 모델의 지능을 지속적으로 확장할 수 있는 유일한 탈출구로 여겨진다.</p>
<h3>1.2  전문가의 정의와 구조</h3>
<p>MoE 층에서 각 ’전문가’는 통상적으로 독립적인 FFN(Feed-Forward Network)으로 구현된다. 트랜스포머 블록 내에서 FFN은 전체 파라미터의 약 2/3를 차지하는 거대한 부분이며, 주로 지식(Knowledge)을 저장하는 메모리 역할을 수행하는 것으로 알려져 있다. MoE는 이 FFN을 <span class="math math-inline">N</span>개의 작은 FFN들 <span class="math math-inline">{E_1, E_2,..., E_N}</span>으로 쪼개고, 각 전문가가 입력 공간의 서로 다른 영역을 담당하도록 학습시킨다.</p>
<p>수학적으로, 입력 벡터 <span class="math math-inline">x</span>에 대한 MoE 층의 출력 <span class="math math-inline">y</span>는 게이팅 네트워크(Gating Network) <span class="math math-inline">G</span>와 전문가 네트워크 <span class="math math-inline">E_i</span>의 가중 합으로 표현된다:<br />
<span class="math math-display">
y = \sum_{i=1}^{N} G(x)_i E_i(x)
</span><br />
여기서 <span class="math math-inline">G(x)_i</span>는 입력 <span class="math math-inline">x</span>가 <span class="math math-inline">i</span>번째 전문가에게 할당될 확률 또는 가중치(Routing Score)를 의미한다. 희소성을 달성하기 위해 <span class="math math-inline">G(x)</span>는 벡터의 대부분의 원소가 0인 희소 벡터(Sparse Vector)가 되도록 설계된다. 즉, <span class="math math-inline">G(x)_i \neq 0</span>인 <span class="math math-inline">i</span>의 개수는 <span class="math math-inline">k</span>개(<span class="math math-inline">k \ll N</span>)로 제한된다.2</p>
<h3>1.3  게이팅 메커니즘(Gating Mechanism)의 진화와 수학적 원리</h3>
<p>어떤 전문가에게 어떤 토큰을 보낼지 결정하는 ’라우팅(Routing)’은 MoE의 성능을 결정짓는 가장 중요한 요소이다. 라우팅 알고리즘은 미분 불가능한 이산적 선택(Discrete Selection)을 포함하면서도, 전체 네트워크가 역전파(Backpropagation)를 통해 학습될 수 있도록 설계되어야 하는 난제를 안고 있다.</p>
<pre><code class="language-mermaid">graph TD
    Input["입력 벡터 x"] --&gt; DotProd["내적 연산 (x · Wg)"]
    Input --&gt; NoiseGen["노이즈 생성 (StandardNormal)"]
    
    subgraph "Noisy Gating Calculation"
        DotProd --&gt; Logits["기본 로짓 (Raw Logits)"]
        NoiseGen --&gt; NoiseScale["Softplus(x · W_noise)"]
        NoiseScale --&gt; Add["노이즈 주입 (+)"]
        Logits --&gt; Add
        Add --&gt; NoisyLogits["H(x): 노이즈가 섞인 로짓"]
    end
    
    NoisyLogits --&gt; TopK_Op["Top-k 연산"]
    TopK_Op --&gt; Mask["마스킹 (-∞ for non-top-k)"]
    Mask --&gt; Softmax["Softmax 함수"]
    
    Softmax --&gt; Route["최종 라우팅 가중치 G(x)"]
    
    Route --"Top-k Indices"--&gt; ExpertSelect["전문가 활성화"]
    Route --"Weights"--&gt; OutputScale["출력 스케일링"]
</code></pre>
<h3>1.4  초기 접근: 소프트맥스 게이팅과 그 한계</h3>
<p>초기의 혼합 모델 연구(Jacobs et al., 1991; Jordan &amp; Jacobs, 1994)에서는 전문가 선택을 확률적 과정으로 모델링하고 소프트맥스 함수를 사용했다.<br />
<span class="math math-display">
G_\sigma(x) = \text{Softmax}(x \cdot W_g)
</span><br />
여기서 <span class="math math-inline">W_g</span>는 라우터의 학습 가능한 가중치 행렬이다. 소프트맥스 함수는 출력의 합이 1이 되는 확률 분포를 생성하지만, 이론적으로 모든 전문가에 대해 0이 아닌 값을 출력한다. 이는 모든 전문가가 입력 <span class="math math-inline">x</span>에 대해 미세하게나마 연산에 참여해야 함을 의미하며, 따라서 연산량 절감 효과(희소성)를 얻을 수 없다.7</p>
<h3>1.5  Top-k 게이팅과 노이즈 주입 (Noisy Top-k Gating)</h3>
<p>진정한 희소 활성화를 구현하기 위해 Shazeer et al.(2017)은 Top-k 연산자를 도입했다. 이는 라우터가 계산한 점수 중 상위 <span class="math math-inline">k</span>개의 전문가만을 남기고 나머지는 강제로 0으로 만드는 방식이다.<br />
<span class="math math-display">
G(x) = \text{Softmax}(\text{TopK}(H(x), k))
</span><br />
그러나 단순한 Top-k 게이팅은 두 가지 심각한 문제를 야기한다. 첫째, <strong>전문가 붕괴(Expert Collapse)</strong> 현상이다. 학습 초기 가중치가 무작위일 때, 특정 전문가가 운 좋게 더 자주 선택되면 해당 전문가는 더 많은 데이터를 학습하게 되고, 성능이 좋아져 더욱 자주 선택되는 ’양성 피드백 루프(Positive Feedback Loop)’에 빠지게 된다. 반면 선택받지 못한 전문가는 학습 기회를 박탈당해 영원히 도태된다. 둘째, <strong>탐색(Exploration)의 부재</strong>이다. 라우터는 항상 현재 시점에서 가장 점수가 높은 전문가만 선택하려 하므로, 잠재적으로 더 적합할 수 있는 다른 전문가를 시도조차 하지 않게 된다.</p>
<p>이러한 문제를 해결하기 위해 Shazeer는 게이팅 로짓 <span class="math math-inline">H(x)</span>에 가우시안 노이즈를 주입하는 ’Noisy Top-k Gating’을 제안했다2:<br />
<span class="math math-display">
H(x)_i = (x \cdot W_g)_i + \text{StandardNormal}() \cdot \text{Softplus}((x \cdot W_{noise})_i)
</span><br />
여기서 <span class="math math-inline">W_{noise}</span>는 노이즈의 크기를 조절하는 또 다른 학습 가능한 파라미터이다. 이 노이즈는 학습 초기에는 전문가 선택에 무작위성을 부여하여 모든 전문가가 고르게 학습될 기회를 제공하고(Exploration), 학습이 진행됨에 따라 <span class="math math-inline">W_{noise}</span>가 조정되면서 모델이 확신을 가지고 전문가를 선택하도록(Exploitation) 유도한다.</p>
<h3>1.6  라우팅의 미분 불가능성과 역전파의 딜레마</h3>
<p>MoE 학습의 가장 큰 이론적 장벽은 Top-k 연산의 미분 불가능성이다. Top-k는 입력값의 미세한 변화가 선택되는 인덱스 집합을 불연속적으로 변화시키는 함수이다. 즉, 선택되지 않은 전문가(<span class="math math-inline">i \notin \text{Top-k}</span>)에 대해서는 <span class="math math-inline">G(x)_i = 0</span>이므로, 출력 <span class="math math-inline">y</span>에 대한 <span class="math math-inline">E_i(x)</span>의 기여도가 0이 되고, 따라서 연쇄 법칙(Chain Rule)에 의한 그래디언트 <span class="math math-inline">\frac{\partial L}{\partial W_{E_i}}</span> 또한 0이 된다. 이를 ‘그래디언트 블랙아웃(Gradient Blackout)’ 현상이라 한다.9</p>
<p>더욱이 라우터 자체의 학습(<span class="math math-inline">W_g</span> 업데이트)도 문제다. 라우터가 전문가 <span class="math math-inline">A</span> 대신 전문가 <span class="math math-inline">B</span>를 선택했어야 했다는 신호를 어떻게 받을 수 있는가? 전문가 <span class="math math-inline">B</span>는 선택되지 않았으므로 손실 함수 값에 기여하지 않았고, 따라서 라우터는 <span class="math math-inline">B</span>를 선택했을 때의 결과를 알 수 없다.</p>
<p>이 문제를 우회하기 위해 MoE는 선택된 전문가의 출력에 게이팅 확률값(Softmax 출력)을 가중치로 곱하는 방식을 취한다.<br />
<span class="math math-display">
y = \sum_{i \in \text{Top-k}} p_i \cdot E_i(x)
</span><br />
여기서 <span class="math math-inline">p_i</span>는 게이팅 네트워크의 출력값이다. 비록 전문가의 ‘선택’ 자체는 이산적(Discrete)이지만, 선택된 전문가에 부여되는 ’가중치(<span class="math math-inline">p_i</span>)’는 연속적(Continuous)이다. 따라서 역전파 시 모델은 “이 전문가를 선택했어야 했다“는 직접적인 정보 대신, “현재 선택된 전문가 중 누구의 가중치를 더 높여야 손실이 줄어드는가“에 대한 정보를 통해 간접적으로 라우터를 학습시킨다.9</p>
<p>하지만 이 방식은 여전히 선택받지 못한 전문가에 대한 그래디언트가 끊겨 있다는 한계를 가진다. 이를 보완하기 위해 딥러닝 프레임워크들은 때때로 Gumbel-Softmax와 같은 이산 확률 분포의 연속 완화(Continuous Relaxation) 기법이나, STE(Straight-Through Estimator)와 유사한 프록시 그래디언트를 사용하기도 하지만, 대규모 LLM에서는 주로 보조 손실(Auxiliary Loss)을 통해 전문가 활용의 균형을 맞추는 실용적인 접근법이 선호된다.</p>
<h2>2.  라우팅의 동역학: 전문가 붕괴와 로드 밸런싱</h2>
<p>MoE 모델이 성공적으로 학습되기 위해서는 ’전문가 붕괴’를 막고 모든 전문가가 고르게 활용되도록 하는 ’로드 밸런싱(Load Balancing)’이 필수적이다. 이는 단순히 자원 활용의 효율성뿐만 아니라, 모델이 다양한 패턴을 학습하고 일반화 능력을 갖추는 데 핵심적인 역할을 한다.</p>
<pre><code class="language-mermaid">graph TD
    Start(("시작 (초기화)")) --&gt; RandomBias["초기 가중치의 미세한 불균형"]
    
    subgraph "Positive Feedback Loop (악순환)"
        RandomBias --&gt; SelectMore["특정 전문가(E_best)가 더 자주 선택됨"]
        SelectMore --&gt; MoreData["E_best가 더 많은 데이터를 학습"]
        MoreData --&gt; BetterPerf["E_best의 성능 향상 (Loss 감소)"]
        BetterPerf --&gt; HigherLogit["라우터가 E_best에게 더 높은 점수 부여"]
        HigherLogit --&gt; SelectMore
    end
    
    SelectMore -.-&gt; DeadExpert["선택받지 못한 전문가는 도태 (Dead Expert)"]
    
    style SelectMore fill:#fcc,stroke:#333
    style DeadExpert fill:#bbb,stroke:#333
    style BetterPerf fill:#cfc,stroke:#333
</code></pre>
<h3>2.1  양성 피드백 루프와 붕괴의 메커니즘</h3>
<p>앞서 언급했듯, MoE 학습 과정은 본질적으로 불안정성을 내포하고 있다. 특정 전문가 <span class="math math-inline">E_{best}</span>가 초기화 과정에서 운 좋게 약간 더 나은 성능을 보이거나 라우터의 편향으로 인해 더 자주 선택되면, 다음과 같은 순환 구조가 형성된다11:</p>
<ol>
<li>라우터는 <span class="math math-inline">E_{best}</span>에게 더 많은 토큰을 보낸다.</li>
<li><span class="math math-inline">E_{best}</span>는 더 많은 역전파 신호를 받아 가중치를 빠르게 업데이트하고 성능이 향상된다.</li>
<li>성능이 좋아진 <span class="math math-inline">E_{best}</span>는 라우터에게 더 높은 신뢰(높은 로짓 값)를 얻는다.</li>
<li>라우터는 <span class="math math-inline">E_{best}</span>에게 더욱 많은 토큰을 보낸다.</li>
</ol>
<p>이 과정이 반복되면 결국 1~2개의 전문가가 모든 토큰을 독점하고, 나머지 <span class="math math-inline">N-2</span>개의 전문가는 ’죽은 전문가(Dead Expert)’가 되어 연산 자원과 메모리를 낭비하게 된다. 이 경우 MoE 모델은 사실상 크기만 큰 단일 밀집 모델보다 못한 성능을 보이게 된다.</p>
<h3>2.2  고전적 해결책: 보조 손실(Auxiliary Loss)의 도입</h3>
<p>Google의 Switch Transformer와 GShard 등 1세대 대규모 MoE 모델들은 이 문제를 해결하기 위해 주 목적 함수(Cross-Entropy Loss)에 ’로드 밸런싱 손실(Load Balancing Loss)’을 추가하는 방식을 채택했다.</p>
<p>Switch Transformer에서 제안한 보조 손실 <span class="math math-inline">\mathcal{L}_{balance}</span>는 다음과 같이 정의된다11:<br />
<span class="math math-display">
\mathcal{L}_{balance} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i
</span><br />
여기서 변수들의 정의는 다음과 같다:</p>
<ul>
<li><span class="math math-inline">N</span>: 전문가의 총 개수.</li>
<li><span class="math math-inline">f_i</span>: 배치(Batch) 내에서 실제로 전문가 <span class="math math-inline">i</span>에게 할당된 토큰의 비율(Fraction). 이는 Top-k 선택의 결과이므로 미분 불가능한 이산 값이다.</li>
<li><span class="math math-inline">P_i</span>: 배치 내의 모든 토큰에 대해 라우터가 전문가 <span class="math math-inline">i</span>에게 부여한 확률값(Softmax 출력)의 평균. 이는 미분 가능한 연속 값이다.</li>
<li><span class="math math-inline">\alpha</span>: 보조 손실의 강도를 조절하는 하이퍼파라미터(통상 <span class="math math-inline">10^{-2}</span> 수준).</li>
</ul>
<p>이 손실 함수는 벡터 <span class="math math-inline">f</span>와 <span class="math math-inline">P</span>의 내적을 최소화하는 형태를 띤다. 수학적으로, 합이 1인 두 벡터의 내적은 두 벡터가 균등 분포(<span class="math math-inline">1/N</span>)를 따를 때 최솟값을 갖는다. 따라서 모델은 <span class="math math-inline">P_i</span>를 조절하여 라우터가 모든 전문가에게 균등한 확률을 부여하도록, 그리고 <span class="math math-inline">f_i</span>가 균등해지도록(즉, 실제 배정이 고르게 되도록) 압박을 받는다.</p>
<pre><code class="language-mermaid">graph TD
    Input["입력 배치 (Batch)"] --&gt; MoE["MoE Layer"]
    
    MoE --&gt; Output["모델 출력"]
    Output --&gt; MainLoss["주 손실 함수 (Cross-Entropy)"]
    
    MoE --"라우터 출력 (Probabilities P)"--&gt; BalanceCalc
    MoE --"실제 배정 비율 (Fractions f)"--&gt; BalanceCalc
    
    subgraph "Load Balancing Mechanism"
        BalanceCalc["밸런싱 연산 (N · Σ f_i · P_i)"]
        BalanceCalc --&gt; AuxLoss["보조 손실 (Auxiliary Loss)"]
        Weight["가중치 α (Hyperparameter)"] --&gt; AuxLoss
    end
    
    MainLoss --&gt; TotalLoss["총 손실 (Total Loss)"]
    AuxLoss --&gt; TotalLoss
    
    TotalLoss --&gt; Backprop["역전파 (Backpropagation)"]
    Backprop --"Update"--&gt; RouterUpdate["라우터 및 전문가 가중치 업데이트"]
    
    style AuxLoss fill:#ffc,stroke:#333
    style MainLoss fill:#cff,stroke:#333
</code></pre>
<h3>2.3  보조 손실의 딜레마와 한계</h3>
<p>보조 손실은 전문가 붕괴를 막는 데 효과적이었지만, 근본적인 모순을 안고 있다. 라우터의 입장에서 ’성능 최적화’와 ’균등 분배’는 서로 상충하는 목표일 수 있기 때문이다.</p>
<p>예를 들어, 입력 데이터 배치가 모두 ’Python 코드’로 구성되어 있다고 가정해보자. 이상적으로는 ’코딩 전문 전문가’가 모든 토큰을 처리하는 것이 성능상 유리하다. 그러나 보조 손실은 이러한 전문화를 패널티로 인식하고, 관련 없는 ’문학 전문 전문가’나 ’수학 전문 전문가’에게 억지로 토큰을 분배하도록 강요한다.</p>
<p>이는 다음과 같은 문제점들을 야기한다11:</p>
<ol>
<li><strong>목적 함수의 간섭(Interference):</strong> 보조 손실을 줄이려는 그래디언트가 주 학습 목표(언어 모델링)의 그래디언트와 충돌하여 모델의 수렴을 방해하거나 최종 성능을 저하시킨다.</li>
<li><strong>하이퍼파라미터 민감성:</strong> <span class="math math-inline">\alpha</span> 값이 너무 크면 밸런싱은 잘 되지만 모델 성능이 망가지고, 너무 작으면 밸런싱에 실패하여 붕괴가 일어난다. 최적의 <span class="math math-inline">\alpha</span>를 찾는 것은 막대한 실험 비용을 요구한다.</li>
<li><strong>학습 불안정성:</strong> 서로 다른 두 목표 사이의 줄다리기는 학습 곡선의 진동이나 발산을 유발할 수 있다.</li>
</ol>
<p>이를 완화하기 위해 ST-MoE 연구팀은 ’Router Z-Loss’를 도입하기도 했다.<br />
<span class="math math-display">
\mathcal{L}_z = \frac{1}{T} \sum_{t=1}^{T} (\log \sum_{i=1}^{N} \exp(x_{t,i}))^2
</span><br />
Z-Loss는 라우터 로짓의 크기 자체가 지나치게 커지는 것을 억제하여, 소프트맥스 연산 시 발생할 수 있는 수치적 불안정성과 라운드오프 에러를 방지한다.2 이는 밸런싱 문제와는 별개로 대규모 MoE 학습의 필수적인 안정화 장치로 자리 잡았다.</p>
<h2>3.  현대적 라우팅 전략: 보조 손실 없는(Auxiliary-Loss-Free) 패러다임</h2>
<p>2024년 이후, MoE 연구의 최전선은 보조 손실이 가진 구조적 한계를 극복하고, 모델의 성능을 저해하지 않으면서 자연스러운 밸런싱을 유도하는 ‘보조 손실 없는(Auxiliary-Loss-Free)’ 라우팅 전략으로 이동하고 있다. 이 분야의 가장 혁신적인 진전은 DeepSeek-V3 아키텍처에서 구체화되었다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "DeepSeek-V3 Routing Logic"
        Token["입력 토큰 u_t"] --&gt; Affinity["친밀도 점수 계산 (Sigmoid(u_t · e_i))"]
        Affinity --&gt; OriginalScore["원본 점수 s_i,t"]
        
        Bias["전문가 편향 b_i (로드 기반 조절)"] --&gt; AddBias["편향 추가"]
        OriginalScore --&gt; AddBias
        
        AddBias --&gt; BiasedScore["보정된 점수 (s_i,t + b_i)"]
        
        BiasedScore --&gt; TopK["Top-k 선택 (Routing Decision)"]
        
        TopK --"Selected Indices"--&gt; ExpertExe["전문가 연산 수행"]
        OriginalScore --"Original Weights"--&gt; ExpertExe
        
        note["중요: 실제 연산 가중치는 편향이 제거된 원본 점수 사용"] -.-&gt; ExpertExe
    end
    
    subgraph "Dynamic Bias Update"
        Monitor["전문가 부하 모니터링"] --&gt; LoadDiff["목표 부하 - 현재 부하"]
        LoadDiff --&gt; UpdateB["편향 b_i 업데이트 (b_i ← b_i + γ · sign)"]
        UpdateB --&gt; Bias
    end
    
    ExpertExe --&gt; FinalOut["최종 출력"]
</code></pre>
<h3>3.1  DeepSeek-V3의 로드 밸런싱 혁신</h3>
<p>DeepSeek-V3는 기존의 보조 손실 기반 접근법을 폐기하고, <strong>전문가 편향(Expert Bias)</strong> 항을 동적으로 조절하는 새로운 메커니즘을 도입했다.3 이 전략의 핵심 통찰은 ’전문가 선택(Selection)’과 ‘전문가 가중치(Weighting)’ 과정을 분리하는 데 있다.</p>
<p>기존 MoE에서는 라우터의 출력 확률값(Affinity Score)이 전문가 선택의 기준이 됨과 동시에, 전문가 출력의 가중치로도 사용되었다. DeepSeek-V3는 이를 다음과 같이 재설계한다:</p>
<ol>
<li>
<p><strong>친밀도 점수(Affinity Score) 계산:</strong> 입력 토큰 <span class="math math-inline">u_t</span>와 전문가 <span class="math math-inline">i</span>의 중심 벡터 <span class="math math-inline">e_i</span> 간의 내적을 통해 기본 점수 <span class="math math-inline">s_{i,t}</span>를 계산한다. 이때 DeepSeek는 소프트맥스 대신 시그모이드(Sigmoid) 함수를 사용하여 각 전문가와의 독립적인 적합도를 측정한다.16<br />
<span class="math math-display">
s_{i,t} = \text{Sigmoid}(u_t^T e_i)
</span><br />
편향된 점수(Biased Score)를 통한 선택: 로드 밸런싱을 위해 동적으로 업데이트되는 편향 <span class="math math-inline">b_i</span>를 원본 점수에 더하여 Top-k 선택을 수행한다.<br />
<span class="math math-display">
g&#39;_{i,t} = \begin{cases} s_{i,t} &amp; \text{if } (s_{i,t} + b_i) \in \text{TopK}(\{s_{j,t} + b_j\}, K) \\ 0 &amp; \text{otherwise} \end{cases}
</span><br />
여기서 주목할 점은, Top-k의 진입 여부는 편향이 포함된 값(<span class="math math-inline">s_{i,t} + b_i</span>)으로 결정되지만, 실제 연산에 사용되는 가중치는 편향이 제거된 원본 값(<span class="math math-inline">s_{i,t}</span>)을 사용한다는 것이다.17</p>
</li>
<li>
<p>동적 편향 업데이트 (Dynamic Bias Update): 편향 <span class="math math-inline">b_i</span>는 그래디언트 하강법으로 학습되는 파라미터가 아니다. 대신 시스템은 매 스텝마다 각 전문가의 부하(Load)를 모니터링하고, 과부하된 전문가의 편향은 낮추고 과소부하된 전문가의 편향은 높이는 방식으로 <span class="math math-inline">b_i</span>를 직접 수정한다.<br />
<span class="math math-display">
b_i \leftarrow b_i + \gamma \cdot \text{sgn}(\text{TargetLoad} - \text{CurrentLoad})
</span><br />
여기서 <span class="math math-inline">\gamma</span>는 ’편향 업데이트 속도(Bias Update Speed)’라는 하이퍼파라미터이다.18</p>
</li>
</ol>
<h3>3.2  편향 조절의 효과와 의의</h3>
<p>이 방식이 혁신적인 이유는 주 학습 목표와 밸런싱 목표 간의 충돌을 원천적으로 차단했기 때문이다.</p>
<ul>
<li><strong>성능 보존:</strong> 전문가의 출력에 곱해지는 가중치 <span class="math math-inline">s_{i,t}</span>는 오직 토큰과 전문가 사이의 의미론적 적합도만을 반영한다. 편향 <span class="math math-inline">b_i</span>가 가중치 계산에서 배제되므로, 밸런싱을 위해 모델의 추론 논리가 왜곡되지 않는다.</li>
<li><strong>자연스러운 부하 분산:</strong> 시스템은 마치 현실 세계의 트래픽 제어 시스템처럼 작동한다. “이 전문가는 실력이 좋으니(높은 <span class="math math-inline">s_{i,t}</span>) 가중치를 높게 쳐주되, 지금 너무 바쁘니(낮아진 <span class="math math-inline">b_i</span>) 이번 토큰은 2순위 전문가에게 맡기자“는 논리가 성립된다.</li>
<li><strong>빠른 수렴:</strong> 보조 손실을 최소화하기 위한 불필요한 그래디언트 경쟁이 사라져 학습 수렴 속도가 빨라지고 최종 성능이 향상된다.3</li>
</ul>
<p>DeepSeek-V3는 여기에 더해 시퀀스(Sequence) 단위의 극단적인 불균형을 막기 위해 매우 작은 계수(<span class="math math-inline">\alpha = 0.0001</span>)의 보조 손실을 안전장치로 추가했지만, 주된 밸런싱 기제는 이 ‘Auxiliary-Loss-Free’ 편향 업데이트이다.3</p>
<h2>4.  시스템 관점에서의 라우팅: 토큰의 여정과 병목</h2>
<p>MoE의 라우팅 원리를 이해하기 위해서는 이것이 실제 하드웨어 상에서 어떻게 구현되는지, 즉 ’시스템 관점’에서의 고찰이 반드시 필요하다. 이론적으로 완벽한 라우팅이라 할지라도 물리적인 병목을 해결하지 못하면 무용지물이기 때문이다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "GPU 1"
        Input1["토큰 그룹 A"] --&gt; Router1["라우터 1"]
        E1["전문가 1"]
        E2["전문가 2"]
    end
    
    subgraph "GPU 2"
        Input2["토큰 그룹 B"] --&gt; Router2["라우터 2"]
        E3["전문가 3"]
        E4["전문가 4"]
    end
    
    Router1 == "All-to-All 통신 (대역폭 소모)" ==&gt; E3
    Router1 == "All-to-All 통신" ==&gt; E4
    Router1 --&gt; E1
    
    Router2 == "All-to-All 통신 (대역폭 소모)" ==&gt; E1
    Router2 == "All-to-All 통신" ==&gt; E2
    Router2 --&gt; E3
    
    subgraph "Capacity Constraint"
        E3 --&gt; BufferCheck{"버퍼 용량 확인"}
        BufferCheck --"여유 있음"--&gt; Process["연산 수행"]
        BufferCheck --"용량 초과"--&gt; Drop["토큰 드롭 (Token Drop)"]
    end
    
    style Drop fill:#f99,stroke:#333
</code></pre>
<h3>4.1  전문가 병렬화(Expert Parallelism)와 통신 비용</h3>
<p>초거대 모델에서 <span class="math math-inline">N</span>개의 전문가는 단일 GPU 메모리에 담길 수 없다. 따라서 전문가들은 여러 GPU(또는 노드)에 분산 배치된다. 이를 ’전문가 병렬화(Expert Parallelism, EP)’라 한다. 라우팅 결과에 따라 토큰들은 자신이 처리되어야 할 전문가가 위치한 GPU로 이동해야 한다. 이는 시스템 전체에 ‘All-to-All’ 통신 패턴을 유발한다.21</p>
<p>모든 GPU가 다른 모든 GPU에게 토큰을 보내고 받는 이 과정은 네트워크 대역폭을 막대하게 소모한다. 만약 라우팅이 불균형하여 특정 전문가(특정 GPU)에게 토큰이 몰리면, 해당 GPU는 연산 과부하뿐만 아니라 통신 병목(Congestion)의 진원지가 되어 전체 클러스터의 대기 시간을 증가시킨다. 이것이 바로 로드 밸런싱이 단순히 학습 품질뿐만 아니라 학습 속도(Throughput)에도 치명적인 영향을 미치는 이유이다.</p>
<h3>4.2  용량 계수(Capacity Factor)와 토큰 드롭</h3>
<p>구글의 GShard와 Switch Transformer는 시스템의 안정성을 보장하기 위해 ’전문가 용량(Expert Capacity)’을 강제로 제한하는 전략을 사용했다.<br />
<span class="math math-display">
C = \frac{\text{Total Tokens}}{N} \times \text{Capacity Factor}
</span><br />
여기서 용량 계수(Capacity Factor)는 보통 1.0~1.2 정도로 설정된다. 만약 특정 전문가에게 배정된 토큰 수가 이 용량 <span class="math math-inline">C</span>를 초과하면, 초과된 토큰들은 해당 전문가에게 처리되지 못하고 버려지거나(Dropped), 처리를 건너뛰고 다음 레이어로 잔차 연결(Residual Connection)만을 통해 전달된다.2</p>
<p>’토큰 드롭’은 하드웨어 효율성을 보장하지만, 정보의 손실을 의미하므로 모델 성능에 악영향을 미친다. 따라서 현대의 MoE 연구(DeepSeek, Mixtral 등)는 토큰 드롭을 최소화하거나 없애는 방향(Dropless MoE)으로 나아가고 있으며, 이를 위해서는 앞서 설명한 정교한 로드 밸런싱 기술이 전제되어야 한다.</p>
<h3>4.3  공유 전문가(Shared Expert)와 미세 입자 전문가(Fine-Grained Expert)</h3>
<p>라우팅의 효율을 높이기 위한 또 다른 구조적 혁신은 ’공유 전문가(Shared Expert)’의 도입이다. DeepSeekMoE와 같은 아키텍처는 일부 전문가를 ’공유 전문가’로 지정하여 모든 토큰이 무조건 거쳐가도록 하고, 나머지 ’라우팅 전문가(Routed Expert)’들 중에서 Top-k를 선택하게 한다.3</p>
<p>공유 전문가는 모든 토큰에 공통적으로 필요한 지식(일반 상식, 문법 등)을 처리하고, 라우팅 전문가는 희소한 특수 지식을 처리하도록 역할을 분담시킨다. 이는 라우터가 매번 공통 지식을 처리할 전문가를 찾아 헤매는 부담을 줄여주고, 전문가 간의 중복 학습을 방지하여 파라미터 효율성을 극대화한다.</p>
<pre><code class="language-mermaid">graph TD
    Input["입력 토큰"] --&gt; Split["경로 분기"]
    
    Split --&gt; SharedPath["공유 경로 (Common Knowledge)"]
    Split --&gt; RoutedPath["라우팅 경로 (Specialized Knowledge)"]
    
    subgraph "Shared Experts"
        SharedPath --&gt; SE["공유 전문가 (Shared Expert)"]
        SE --"항상 활성화"--&gt; SE_Out["공유 출력"]
    end
    
    subgraph "Routed Experts"
        RoutedPath --&gt; Router["라우터"]
        Router --&gt; Select{"Top-k 선택"}
        Select --&gt; RE1["라우팅 전문가 A"]
        Select --&gt; RE2["라우팅 전문가 B"]
        Select -.-&gt; RE_Others["..."]
        
        RE1 --&gt; RE_Sum["라우팅 출력 합"]
        RE2 --&gt; RE_Sum
    end
    
    SE_Out --&gt; FinalSum["최종 합산 (Add)"]
    RE_Sum --&gt; FinalSum
    FinalSum --&gt; FinalOutput["레이어 출력"]
</code></pre>
<h2>5.  결론</h2>
<p>9.1절에서는 전문가 혼합 모델(MoE)의 핵심 작동 원리인 희소 활성화와 라우팅 메커니즘을 심도 있게 분석했다. MoE는 단순한 모델 크기의 확장이 아니라, 컴퓨팅 자원의 효율적 배분을 통해 지능의 밀도를 높이는 패러다임의 전환이다.</p>
<p>우리는 소프트맥스 게이팅의 비효율성에서 시작하여, Top-k 라우팅의 이산성 문제를 해결하기 위한 수학적 기교들, 그리고 전문가 붕괴를 막기 위한 보조 손실의 도입과 그 한계를 추적했다. 특히 최신의 DeepSeek-V3 모델이 보여준 ’보조 손실 없는 로드 밸런싱’은 전문가 선택과 가중치 부여를 분리하고, 동적 편향 업데이트를 통해 시스템의 평형을 유지하는 우아한 해법을 제시했다.</p>
<p>라우팅 기술의 발전은 이제 단순히 ’어느 전문가에게 보낼 것인가’를 넘어, ’어떻게 시스템 전체의 통신과 연산 부하를 최적화할 것인가’라는 시스템 엔지니어링의 영역으로 확장되고 있다. 희소 활성화와 정교한 라우팅 알고리즘의 결합은, 무한히 커져가는 파라미터의 우주 속에서 우리가 잃어버리지 않고 효율적으로 지능을 항해할 수 있게 하는 나침반이 될 것이다.</p>
<h3>5.1 [표 9.1-1] 주요 라우팅 메커니즘 비교</h3>
<table><thead><tr><th><strong>라우팅 방식</strong></th><th><strong>특징</strong></th><th><strong>장점</strong></th><th><strong>단점 및 한계</strong></th><th><strong>대표 모델</strong></th></tr></thead><tbody>
<tr><td><strong>소프트맥스 게이팅</strong></td><td>모든 전문가에 확률 부여</td><td>미분 가능, 구현 용이</td><td>희소성 없음 (모든 전문가 활성화), 연산 비효율</td><td>초기 MoE 연구</td></tr>
<tr><td><strong>Top-k 게이팅</strong></td><td>상위 k개 전문가만 선택</td><td>연산 효율성 극대화, 희소성 확보</td><td>미분 불가능(Gradient Blackout), 전문가 붕괴 위험</td><td>Switch Transformer, GShard</td></tr>
<tr><td><strong>Noisy Top-k</strong></td><td>로짓에 노이즈 추가 후 Top-k</td><td>탐색(Exploration) 유도, 붕괴 완화</td><td>노이즈 튜닝 필요, 여전히 붕괴 가능성 존재</td><td>Shazeer et al. (2017)</td></tr>
<tr><td><strong>Auxiliary Loss 기반</strong></td><td>로드 밸런싱 손실 추가</td><td>명시적인 균등 분배 유도</td><td>주 목적 함수와 충돌, 성능 저하, 하이퍼파라미터 민감</td><td>Switch Transformer, Mixtral</td></tr>
<tr><td><strong>Expert Choice</strong></td><td>전문가가 토큰을 선택 (Top-k tokens)</td><td>완벽한 로드 밸런싱 보장</td><td>토큰별 처리 전문가 수 가변적, 구현 복잡</td><td>Zhou et al. (2022)</td></tr>
<tr><td><strong>Aux-Loss-Free (Bias Update)</strong></td><td>편향(<span class="math math-inline">b_i</span>) 동적 조절로 밸런싱</td><td>성능 저하 없음, 빠른 수렴, 주 목적 함수 보존</td><td>시스템 모니터링 필요, 추가적인 상태 변수 관리</td><td><strong>DeepSeek-V3</strong></td></tr>
</tbody></table>
<h3>5.2 [표 9.1-2] DeepSeek-V3의 파라미터 효율성 분석</h3>
<table><thead><tr><th><strong>항목</strong></th><th><strong>수치</strong></th><th><strong>의미 및 해석</strong></th></tr></thead><tbody>
<tr><td><strong>총 파라미터 (Total Params)</strong></td><td>671B (6,710억 개)</td><td>모델이 보유한 총 지식 용량 (GPT-4급 추정)</td></tr>
<tr><td><strong>활성 파라미터 (Active Params)</strong></td><td>37B (370억 개)</td><td>토큰당 연산 비용 (Llama-2 70B의 절반 수준)</td></tr>
<tr><td><strong>활성화 비율 (Sparsity)</strong></td><td>약 5.5%</td><td>94.5%의 파라미터는 휴면 상태 (메모리만 점유)</td></tr>
<tr><td><strong>전문가 구성</strong></td><td>1 Shared + 256 Routed Experts</td><td>미세 입자(Fine-grained) 전문가로 전문화 극대화</td></tr>
<tr><td><strong>토큰당 선택 전문가</strong></td><td>8개 (<span class="math math-inline">K_r=8</span>)</td><td>다양한 전문가의 지식을 조합하여 추론</td></tr>
</tbody></table>
<h2>6. 참고 자료</h2>
<ol>
<li>What is mixture of experts? | IBM, https://www.ibm.com/think/topics/mixture-of-experts</li>
<li>Mixture of Experts Explained - Hugging Face, https://huggingface.co/blog/moe</li>
<li>DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/pdf/2412.19437</li>
<li>Mixture of Experts: Sparse Activation for Scaling Language Models - Michael Brenndoerfer, https://mbrenndoerfer.com/writing/mixture-of-experts-sparse-activation</li>
<li>Applying Mixture of Experts in LLM Architectures | NVIDIA Technical Blog, https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/</li>
<li>Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert Utilization - arXiv, https://arxiv.org/html/2509.26520v1</li>
<li>Mixture of experts - Wikipedia, https://en.wikipedia.org/wiki/Mixture_of_experts</li>
<li>OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER - Department of Computer Science, University of Toronto, https://www.cs.toronto.edu/~hinton/absps/Outrageously.pdf?ref=blog.premai.io</li>
<li>The Stability Gap: Why Top-K Routing Breaks RL Optimization | Yingru Li, https://richardli.xyz/post/topk-routing-stability-gap/</li>
<li>[D] I don’t understand how backprop works on sparsely gated MoE - Reddit, https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</li>
<li>How MoE Models Actually Learn: A Guide to Auxiliary Losses and Expert Balancing | by Chris Hughes | Dec, 2025 | Medium, https://medium.com/@chris.p.hughes10/how-moe-models-actually-learn-a-guide-to-auxiliary-losses-and-expert-balancing-293084e3f600</li>
<li>Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts - arXiv, https://arxiv.org/html/2408.15664v1</li>
<li>Routing and balancing losses with Mixture of Experts - DEV Community, https://dev.to/lewis_won/routing-and-balancing-losses-with-mixture-of-experts-19be</li>
<li>Adaptive-expert-weight-based load balance scheme for dynamic routing of MoE - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC12558867/</li>
<li>DeepSeek-V3 Explained 3: Auxiliary-Loss-Free Load Balancing | by Shirley Li - AI Advances, https://ai.gopubby.com/deepseek-v3-explained-3-auxiliary-loss-free-load-balancing-4beeb734ab1f</li>
<li>Understanding DeepSeek-V3 Architecture | by Dewang Sultania | My musings with LLMs, https://medium.com/my-musings-with-llms/understanding-the-deepseek-v3-architecture-aee01112b938</li>
<li>FINETUNING MOE LLMS WITH CONDENSER EXPERTS - OpenReview, https://openreview.net/pdf/eff4bb1443da7f79fb47744b9d51582950bfd38f.pdf</li>
<li>DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/html/2412.19437v2</li>
<li>DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/html/2412.19437v1</li>
<li>Primers • DeepSeek V3 - aman.ai, https://aman.ai/primers/ai/deepseekV3/</li>
<li>Mixtral of experts - arXiv, https://arxiv.org/pdf/2401.04088</li>
<li>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, https://home.cse.ust.hk/~cktang/csit6000s/Password_Only/lec16-csit.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>