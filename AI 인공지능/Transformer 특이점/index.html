<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:트랜스포머 싱귤래리티 (The Transformer Singularity)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css?v=1772282414">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>트랜스포머 싱귤래리티 (The Transformer Singularity)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a></nav>
                </div>
            </header>
            <article>
                <h1>트랜스포머 싱귤래리티 (The Transformer Singularity)</h1>
<p><img src="index1.png" alt="" /></p>
<p>어텐션의 탄생부타 2025년 멀티모달 에이전트와 추론 모델까지</p>
<h2>제1부. 기원과 원리: 시퀀스 모델링의 패러다임 시프트 (Foundations)</h2>
<h3>제1장. 순차적 처리의 한계와 도전</h3>
<ul>
<li><a href="1.1%20RNN%EA%B3%BC%20LSTM%EC%9D%B4%20%EC%A7%80%EB%B0%B0%ED%95%98%EB%8D%98%20%EC%8B%9C%EB%8C%80.html">1.1 RNN과 LSTM이 지배하던 시대</a></li>
<li><a href="1.2%20Seq2Seq%20%EB%AA%A8%EB%8D%B8%EA%B3%BC%20%EC%A0%95%EB%B3%B4%20%EB%B3%91%EB%AA%A9(Information%20Bottleneck)%20%ED%98%84%EC%83%81.html">1.2 Seq2Seq 모델과 정보 병목(Information Bottleneck) 현상</a></li>
<li><a href="1.3%20%EC%96%B4%ED%85%90%EC%85%98(Attention)%EC%9D%98%20%EB%93%B1%EC%9E%A5%20%EB%B3%B4%EC%A1%B0%20%EC%88%98%EB%8B%A8%EC%97%90%EC%84%9C%20%ED%95%B5%EC%8B%AC%20%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EC%9C%BC%EB%A1%9C.html">1.3 어텐션(Attention)의 등장 - 보조 수단에서 핵심 메커니즘으로</a></li>
</ul>
<h3>제2장. 트랜스포머 아키텍처 해부 (Anatomy of Transformer)</h3>
<ul>
<li><a href="2.1%20Attention%20Is%20All%20You%20Need%20%EB%85%BC%EB%AC%B8%EC%9D%B4%20%EB%B0%94%EA%BE%BC%20%EB%AA%A8%EB%93%A0%20%EA%B2%83.html">2.1 Attention Is All You Need 논문이 바꾼 모든 것</a></li>
<li><a href="2.2%20%EC%9D%B8%EC%BD%94%EB%8D%94-%EB%94%94%EC%BD%94%EB%8D%94(Encoder-Decoder)%20%EA%B5%AC%EC%A1%B0%EC%9D%98%20%EC%99%84%EB%B2%BD%ED%95%9C%20%EC%9D%B4%ED%95%B4.html">2.2 인코더-디코더(Encoder-Decoder) 구조의 완벽한 이해</a></li>
<li><a href="2.3%20%EC%85%80%ED%94%84%20%EC%96%B4%ED%85%90%EC%85%98(Self-Attention)%EA%B3%BC%20Q,%20K,%20V%20%EC%97%B0%EC%82%B0%EC%9D%98%20%EA%B8%B0%ED%95%98%ED%95%99%EC%A0%81%20%EC%9D%98%EB%AF%B8.html">2.3 셀프 어텐션(Self-Attention)과 Q, K, V 연산의 기하학적 의미</a></li>
<li><a href="2.4%20%EB%A9%80%ED%8B%B0%20%ED%97%A4%EB%93%9C%20%EC%96%B4%ED%85%90%EC%85%98(Multi-Head%20Attention)%20%EB%8B%A4%EB%A9%B4%EC%A0%81%20%EB%AC%B8%EB%A7%A5%20%ED%95%99%EC%8A%B5.html">2.4 멀티 헤드 어텐션(Multi-Head Attention) 다면적 문맥 학습</a></li>
<li><a href="2.5%20%ED%94%BC%EB%93%9C%ED%8F%AC%EC%9B%8C%EB%93%9C%20%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC(FFN)%EC%99%80%20%EC%9E%94%EC%B0%A8%20%EC%97%B0%EA%B2%B0(Residual%20Connection).html">2.5 피드포워드 네트워크(FFN)와 잔차 연결(Residual Connection)</a></li>
</ul>
<h2>제2부. 언어 모델의 캄브리아기 폭발 (The LLM Explosion)</h2>
<h3>제3장. 인코더의 시대: 이해(Understanding)의 혁명</h3>
<ul>
<li><a href="3.1%20BERT%20%EC%96%91%EB%B0%A9%ED%96%A5%20%EB%AC%B8%EB%A7%A5%20%ED%95%99%EC%8A%B5%EA%B3%BC%20%EB%A7%88%EC%8A%A4%ED%81%AC%EB%93%9C%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8(MLM).html">3.1 BERT 양방향 문맥 학습과 마스크드 언어 모델(MLM)</a></li>
<li><a href="3.2%20RoBERTa%20%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%20%EC%B5%9C%EC%A0%81%ED%99%94%EC%9D%98%20%EC%A4%91%EC%9A%94%EC%84%B1.html">3.2 RoBERTa 하이퍼파라미터 최적화의 중요성</a></li>
<li><a href="3.3%20DeBERTa%20Disentangled%20Attention%EA%B3%BC%20%EC%83%81%EB%8C%80%EC%A0%81%20%EC%9C%84%EC%B9%98%20%EC%A0%95%EB%B3%B4%EC%9D%98%20%EA%B2%B0%ED%95%A9.html">3.3 DeBERTa - Disentangled Attention과 상대적 위치 정보의 결합</a></li>
<li><a href="3.4%20ELECTRA%20%ED%8C%90%EB%B3%84%EC%9E%90(Discriminator)%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%ED%9A%A8%EC%9C%A8%EC%A0%81%20%ED%95%99%EC%8A%B5.html">3.4 ELECTRA 판별자(Discriminator)를 활용한 효율적 학습</a></li>
</ul>
<h3>제4장. 디코더의 시대: 생성(Generation)과 스케일링 법칙</h3>
<ul>
<li><a href="4.1%20GPT%20%EC%8B%9C%EB%A6%AC%EC%A6%88%EC%9D%98%20%EC%A7%84%ED%99%94%20Zero-shot,%20One-shot,%20Few-shot.html">4.1 GPT 시리즈의 진화 - Zero-shot, One-shot, Few-shot</a></li>
<li><a href="4.2%20%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81%20%EB%B2%95%EC%B9%99(Scaling%20Laws)%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%99%80%20%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%EC%9D%98%20%EC%83%81%EA%B4%80%EA%B4%80%EA%B3%84.html">4.2 스케일링 법칙(Scaling Laws) 데이터와 파라미터의 상관관계</a></li>
<li><a href="4.3%20LLaMA%EC%99%80%20%EC%98%A4%ED%94%88%20%EC%86%8C%EC%8A%A4%20%EB%AA%A8%EB%8D%B8%EC%9D%98%20%EB%8C%80%EB%91%90.html">4.3 LLaMA와 오픈 소스 모델의 대두</a></li>
</ul>
<h3>제5장. 통합 아키텍처와 변형</h3>
<ul>
<li><a href="5.1%20T5%20%EB%AA%A8%EB%93%A0%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%AC%B8%EC%A0%9C%EB%8A%94%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%83%9D%EC%84%B1%20%EB%AC%B8%EC%A0%9C%EB%8B%A4.html">5.1 T5 - 모든 텍스트 문제는 텍스트 생성 문제다</a></li>
<li><a href="5.2%20BART%20%EC%9D%B8%EC%BD%94%EB%8D%94-%EB%94%94%EC%BD%94%EB%8D%94%EC%9D%98%20%EC%9E%AC%EA%B2%B0%ED%95%A9%EA%B3%BC%20Denoising.html">5.2 BART 인코더-디코더의 재결합과 Denoising</a></li>
</ul>
<h2>제3부. 아키텍처의 최적화와 심화 (Optimization &amp; Deep Dive)</h2>
<h3>제6장. 위치 인코딩의 진화 (Position Encoding)</h3>
<ul>
<li><a href="6.1%20%EC%A0%88%EB%8C%80%EC%A0%81%20%EC%9C%84%EC%B9%98%20%EC%9D%B8%EC%BD%94%EB%94%A9(APE)%EC%9D%98%20%ED%95%9C%EA%B3%84.html">6.1 절대적 위치 인코딩(APE)의 한계</a></li>
<li><a href="6.2%20RoPE%20(Rotary%20Positional%20Embedding)%20%ED%9A%8C%EC%A0%84%20%EB%B3%80%ED%99%98%EC%9D%84%20%ED%86%B5%ED%95%9C%20%EC%83%81%EB%8C%80%EC%A0%81%20%EC%9C%84%EC%B9%98%20%ED%95%99%EC%8A%B5.html">6.2 RoPE (Rotary Positional Embedding) - 회전 변환을 통한 상대적 위치 학습</a></li>
<li><a href="6.3%20ALiBi%20(Attention%20with%20Linear%20Biases)%20%EC%99%B8%EC%82%BD(Extrapolation)%20%EC%84%B1%EB%8A%A5%EC%9D%98%20%EA%B7%B9%EB%8C%80%ED%99%94.html">6.3 ALiBi (Attention with Linear Biases) 외삽(Extrapolation) 성능의 극대화</a></li>
</ul>
<h3>제7장. 학습 안정성과 정규화 (Normalization)</h3>
<ul>
<li><a href="7.1%20Pre-Norm%20vs%20Post-Norm%20%EA%B9%8A%EC%9D%80%20%EB%AA%A8%EB%8D%B8%20%ED%95%99%EC%8A%B5%EC%9D%98%20%EB%94%9C%EB%A0%88%EB%A7%88.html">7.1 Pre-Norm vs Post-Norm 깊은 모델 학습의 딜레마</a></li>
<li><a href="7.2%20RMSNorm%20%EA%B0%84%EC%86%8C%ED%99%94%EB%90%9C%20%EC%A0%95%EA%B7%9C%ED%99%94%EC%99%80%20%EC%84%B1%EB%8A%A5%20%ED%96%A5%EC%83%81.html">7.2 RMSNorm - 간소화된 정규화와 성능 향상</a></li>
</ul>
<h3>제8장. 연산 효율화: O(N2) 극복하기</h3>
<ul>
<li><a href="8.1%20FlashAttention-1%20-%20IO%20%EC%9D%B8%EC%A7%80%EB%A5%BC%20%ED%86%B5%ED%95%9C%20%EC%A0%95%ED%99%95%ED%95%9C%20%EC%96%B4%ED%85%90%EC%85%98%EC%9D%98%20%EA%B0%80%EC%86%8D.html">8.1 FlashAttention-1 - IO 인지를 통한 정확한 어텐션의 가속</a></li>
<li><a href="8.2%20FlashAttention-2%20-%20%EB%B3%91%EB%A0%AC%EC%84%B1%20%EC%B5%9C%EC%A0%81%ED%99%94%EC%99%80%20%EC%9E%91%EC%97%85%20%EB%B6%84%ED%95%A0%EC%9D%98%20%EC%9E%AC%EC%84%A4%EA%B3%84.html">8.2 FlashAttention-2 - 병렬성 최적화와 작업 분할의 재설계</a></li>
<li><a href="8.3%20FlashAttention-3.html">8.3 FlashAttention-3</a></li>
<li><a href="8.4%20Ring%20Attention%20%EC%88%98%EB%B0%B1%EB%A7%8C%20%ED%86%A0%ED%81%B0%20%EC%B2%98%EB%A6%AC%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EB%B6%84%EC%82%B0%20%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%B2%95.html">8.4 Ring Attention - 수백만 토큰 처리를 위한 분산 처리 기법</a></li>
</ul>
<h2>제4부. 확장의 기술: 효율성과 추론 (Scaling &amp; Inference)</h2>
<h3>제9장. 전문가 혼합 모델 (Mixture of Experts, MoE)</h3>
<ul>
<li><a href="9.1%20%ED%9D%AC%EC%86%8C%20%ED%99%9C%EC%84%B1%ED%99%94(Sparse%20Activation)%EC%99%80%20%EB%9D%BC%EC%9A%B0%ED%8C%85(Routing)%EC%9D%98%20%EC%9B%90%EB%A6%AC.html">9.1 희소 활성화(Sparse Activation)와 라우팅(Routing)의 원리</a></li>
<li><a href="9.2%20DeepSeek-V3%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%20%EB%B3%B4%EC%A1%B0%20%EC%86%90%EC%8B%A4%20%EC%97%86%EB%8A%94%20%EB%A1%9C%EB%93%9C%20%EB%B0%B8%EB%9F%B0%EC%8B%B1(Auxiliary-Loss-Free%20Balancing).html">9.2 DeepSeek-V3 아키텍처 보조 손실 없는 로드 밸런싱 (Auxiliary-Loss-Free Load Balancing)</a></li>
<li><a href="9.3%20%EB%8B%A4%EC%A4%91%20%ED%86%A0%ED%81%B0%20%EC%98%88%EC%B8%A1(Multi-Token%20Prediction,%20MTP)%20%ED%95%99%EC%8A%B5%20%EC%A0%84%EB%9E%B5.html">9.3 다중 토큰 예측(Multi-Token Prediction, MTP) 학습 전략</a></li>
</ul>
<h3>제10장. 추론 가속과 경량화</h3>
<ul>
<li><a href="10.1%20%ED%88%AC%EA%B8%B0%EC%A0%81%20%EB%94%94%EC%BD%94%EB%94%A9(Speculative%20Decoding)%20%EC%B4%88%EC%95%88%20%EB%AA%A8%EB%8D%B8%EA%B3%BC%20%EA%B2%80%EC%A6%9D%20%EB%AA%A8%EB%8D%B8%EC%9D%98%20%ED%98%91%EC%97%85.html">10.1 투기적 디코딩(Speculative Decoding) - 초안 모델과 검증 모델의 협업</a></li>
<li><a href="10.2%20%EC%96%91%EC%9E%90%ED%99%94(Quantization)%20FP8%20%ED%95%99%EC%8A%B5%EA%B3%BC%20%EC%B6%94%EB%A1%A0%EC%9D%98%20%EC%8B%9C%EB%8C%80.html">10.2 양자화(Quantization) - FP8 학습과 추론의 시대</a></li>
<li><a href="10.3%20PEFT%EC%99%80%20LoRA%20QLoRA%20%EA%B1%B0%EB%8C%80%20%EB%AA%A8%EB%8D%B8%EC%9D%98%20%ED%9A%A8%EC%9C%A8%EC%A0%81%20%EB%AF%B8%EC%84%B8%20%EC%A1%B0%EC%A0%95.html">10.3 PEFT와 LoRA/QLoRA - 거대 모델의 효율적 미세 조정</a></li>
</ul>
<h2>제5부. 모달리티의 확장 (Beyond Text)</h2>
<h3>제11장. 비전 트랜스포머 (ViT)와 이미지 처리</h3>
<ul>
<li><a href="11.1%20ViT%20%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%A5%BC%20%ED%86%A0%ED%81%B0%EC%9C%BC%EB%A1%9C%20%EC%B2%98%EB%A6%AC%ED%95%98%EB%8B%A4.html">11.1 ViT, 이미지를 토큰으로 처리하다</a></li>
<li><a href="11.2%20MAE%20(Masked%20Autoencoder)%20%ED%94%BD%EC%85%80%20%EB%B3%B5%EC%9B%90%EC%9D%84%20%ED%86%B5%ED%95%9C%20%ED%91%9C%ED%98%84%20%ED%95%99%EC%8A%B5.html">11.2 MAE (Masked Autoencoder) 픽셀 복원을 통한 표현 학습</a></li>
<li><a href="11.3%20DINOv2%20%EC%9E%90%EA%B8%B0%EC%A7%80%EB%8F%84%20%ED%95%99%EC%8A%B5%EA%B3%BC%20%EC%8B%9C%EA%B0%81%EC%A0%81%20%ED%8A%B9%EC%A7%95%20%EC%B6%94%EC%B6%9C.html">11.3 DINOv2 자기지도 학습과 시각적 특징 추출</a></li>
</ul>
<h3>제12장. 오디오와 시계열 데이터</h3>
<ul>
<li><a href="12.1%20Whisper%20%EC%95%BD%EC%A7%80%EB%8F%84%20%ED%95%99%EC%8A%B5%EC%9D%84%20%ED%86%B5%ED%95%9C%20%EC%9D%8C%EC%84%B1%20%EC%9D%B8%EC%8B%9D%EC%9D%98%20%EC%9D%BC%EB%B0%98%ED%99%94.html">12.1 Whisper 약지도 학습을 통한 음성 인식의 일반화</a></li>
<li><a href="12.2%20AudioLM%20%EC%98%A4%EB%94%94%EC%98%A4%EC%9D%98%20%EC%9D%98%EB%AF%B8%EC%A0%81%20%ED%86%A0%ED%81%B0%ED%99%94%EC%99%80%20%EC%83%9D%EC%84%B1.html">12.2 AudioLM - 오디오의 의미적 토큰화와 생성</a></li>
<li><a href="12.3%20PatchTST%EC%99%80%20iTransformer%20%EC%8B%9C%EA%B3%84%EC%97%B4%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98%20%ED%8C%A8%EC%B9%98%ED%99%94%20%EB%B0%8F%20%EC%97%AD%EB%B0%9C%EC%83%81%20%EA%B5%AC%EC%A1%B0.html">12.3 PatchTST와 iTransformer - 시계열 데이터의 패치화 및 역발상 구조</a></li>
</ul>
<h2>제6부. 2025 최신 트렌드: 하이브리드와 에이전트 (Frontiers)</h2>
<h3>제13장. 네이티브 멀티모달과 융합 (Native Multimodal)</h3>
<ul>
<li><a href="13.1%20%EC%97%B0%EA%B2%B0(Glue%20Layers)%EC%97%90%EC%84%9C%20%ED%86%B5%ED%95%A9(Native%20Fusion)%EC%9C%BC%EB%A1%9C%20Gemini%EC%99%80%20GPT-4o.html">13.1 연결(Glue Layers)에서 통합(Native Fusion)으로 - Gemini와 GPT-4o</a></li>
<li><a href="13.2%20%EC%9D%B4%EB%AF%B8%EC%A7%80,%20%EC%98%A4%EB%94%94%EC%98%A4,%20%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9D%98%20%EB%8B%A8%EC%9D%BC%20%ED%86%A0%ED%81%B0%20%EA%B3%B5%EA%B0%84%20%EC%B2%98%EB%A6%AC.html">13.2 이미지, 오디오, 텍스트의 단일 토큰 공간 처리</a></li>
<li><a href="13.3%20BLIP-2%EC%99%80%20Q-Former%20%EC%A7%88%EC%9D%98%20%EA%B8%B0%EB%B0%98%20%EC%8B%9C%EA%B0%81%20%EC%A0%95%EB%B3%B4%20%EC%B6%94%EC%B6%9C.html">13.3 BLIP-2와 Q-Former 질의 기반 시각 정보 추출</a></li>
</ul>
<h3>제14장. 구조적 혁신: 트랜스포머를 넘어서 (Beyond Transformer)</h3>
<ul>
<li><a href="14.1%20SSM(State%20Space%20Models)%EA%B3%BC%20Mamba%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%EC%9D%98%20%EB%B6%80%EC%83%81.html">14.1 SSM(State Space Models)과 Mamba 아키텍처의 부상</a></li>
<li><a href="14.2%20Jamba%20%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8%EC%99%80%20Mamba%EC%9D%98%20%ED%95%98%EC%9D%B4%EB%B8%8C%EB%A6%AC%EB%93%9C%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.html">14.2 Jamba 트랜스포머와 Mamba의 하이브리드 아키텍처</a></li>
<li><a href="14.3%20%EC%84%A0%ED%98%95%20%EC%96%B4%ED%85%90%EC%85%98(Linear%20Attention)%EA%B3%BC%20%EB%AC%B4%ED%95%9C%20%EB%AC%B8%EB%A7%A5%EC%9D%98%20%EA%B0%80%EB%8A%A5%EC%84%B1.html">14.3 선형 어텐션(Linear Attention)과 무한 문맥의 가능성</a></li>
</ul>
<h3>제15장. 추론(Reasoning) 모델과 강화학습</h3>
<ul>
<li><a href="15.1%20DeepSeek-R1%20SFT%20%EC%97%86%EB%8A%94%20%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5(RL)%EA%B3%BC%20%EC%82%AC%EA%B3%A0%EC%9D%98%20%EC%82%AC%EC%8A%AC(CoT)%20%EC%B0%BD%EB%B0%9C.html">15.1 DeepSeek-R1 - SFT 없는 강화학습(RL)과 사고의 사슬(CoT) 창발</a></li>
<li><a href="15.2%20%EC%84%B1%EC%B0%B0%20%ED%86%A0%ED%81%B0(Reflection%20Tokens)%EA%B3%BC%20Self-RAG.html">15.2 성찰 토큰(Reflection Tokens)과 Self-RAG</a></li>
<li><a href="15.3%20DPO,%20KTO%20%EC%9D%B8%EA%B0%84%20%EC%84%A0%ED%98%B8%20%EC%A0%95%EB%A0%AC(Alignment)%EC%9D%98%20%EC%B5%9C%EC%8B%A0%20%EA%B8%B0%EB%B2%95.html">15.3 DPO, KTO 인간 선호 정렬(Alignment)의 최신 기법</a></li>
</ul>
<h3>제16장. 에이전틱 AI (Agentic AI) 시대로</h3>
<ul>
<li><a href="16.1%20%EC%83%9D%EC%84%B1(Generative)%EC%97%90%EC%84%9C%20%ED%96%89%EB%8F%99(Action)%EC%9C%BC%EB%A1%9C%202025%20%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8%20%ED%8A%B8%EB%A0%8C%EB%93%9C.html">16.1 생성(Generative)에서 행동(Action)으로 - 2025 에이전트 트렌드</a></li>
<li><a href="16.2%20GraphRAG%20%EC%A7%80%EC%8B%9D%20%EA%B7%B8%EB%9E%98%ED%94%84%EB%A5%BC%20%ED%99%9C%EC%9A%A9%ED%95%9C%20%EA%B8%80%EB%A1%9C%EB%B2%8C%20%EC%BB%A8%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%9D%B4%ED%95%B4.html">16.2 GraphRAG 지식 그래프를 활용한 글로벌 컨텍스트 이해</a></li>
<li><a href="16.3%20%EC%9E%90%EC%9C%A8%EC%A0%81%20%EB%8F%84%EA%B5%AC%20%EC%82%AC%EC%9A%A9%EA%B3%BC%20%EA%B3%84%ED%9A%8D(Planning)%20%EB%8A%A5%EB%A0%A5%EC%9D%84%20%EA%B0%96%EC%B6%98%20%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8.html">16.3 자율적 도구 사용과 계획(Planning) 능력을 갖춘 트랜스포머</a></li>
</ul>
<p><img src="index2.png" alt="" /></p>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>