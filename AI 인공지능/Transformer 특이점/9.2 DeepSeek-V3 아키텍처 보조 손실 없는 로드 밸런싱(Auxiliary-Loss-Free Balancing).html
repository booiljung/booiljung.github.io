<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:9.2 DeepSeek-V3 아키텍처 보조 손실 없는 로드 밸런싱 (Auxiliary-Loss-Free Load Balancing)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>9.2 DeepSeek-V3 아키텍처 보조 손실 없는 로드 밸런싱 (Auxiliary-Loss-Free Load Balancing)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>9.2 DeepSeek-V3 아키텍처 보조 손실 없는 로드 밸런싱 (Auxiliary-Loss-Free Load Balancing)</span></nav>
                </div>
            </header>
            <article>
                <h1>9.2 DeepSeek-V3 아키텍처 보조 손실 없는 로드 밸런싱 (Auxiliary-Loss-Free Load Balancing)</h1>
<p>2025-12-21, G30DR</p>
<h2>1.  서론: 혼합 전문가 모델(MoE)의 역설과 로드 밸런싱의 난제</h2>
<p>대규모 언어 모델(Large Language Model, LLM)의 발전사는 연산 효율성과 모델 용량(Capacity) 사이의 끊임없는 투쟁의 기록이다. ’확장 법칙(Scaling Law)’이 지배하는 이 시대에, 모델의 파라미터 수를 늘리는 것은 지능을 향상시키는 가장 확실한 방법으로 여겨져 왔다. 그러나 단일 밀집(Dense) 모델의 크기를 무한정 확장하는 것은 학습 비용과 추론 지연(Latency)이라는 물리적 한계에 봉착하게 된다. 이러한 배경 속에서 등장한 혼합 전문가(Mixture-of-Experts, MoE) 아키텍처는 전체 파라미터 수는 극대화하되, 각 토큰을 처리할 때는 전체의 극히 일부인 ’활성 파라미터(Active Parameters)’만을 사용함으로써 연산 효율성의 정점을 찍었다. DeepSeek-V3는 총 6,710억(671B) 개의 파라미터를 보유하고 있으면서도, 토큰당 활성화되는 파라미터는 370억(37B) 개에 불과한 아키텍처로, 이 MoE 패러다임의 최전선에 서 있다.1</p>
<p>하지만 MoE 아키텍처는 태생적으로 ’로드 밸런싱(Load Balancing)’이라는 치명적인 난제를 안고 있다. 수백, 수천 개의 전문가(Expert) 네트워크가 존재하는 상황에서, 게이팅 네트워크(Gating Network)가 특정 소수의 전문가에게만 데이터를 몰아주는 ‘라우팅 붕괴(Routing Collapse)’ 현상이 발생하기 쉽기 때문이다. 라우팅 붕괴가 발생하면, 선택받지 못한 다수의 전문가는 학습 기회를 잃고 유휴 상태(Idle)로 전락하며, 반대로 과부하가 걸린 전문가는 병목 현상을 일으켜 전체 학습 속도를 저하시킨다.3 이는 수천 대의 GPU가 동원되는 분산 학습 환경에서 하드웨어 자원의 심각한 낭비를 초래한다.</p>
<p>이 문제를 해결하기 위해 학계와 산업계는 지난 수년간 ’보조 손실(Auxiliary Loss)’이라는 기법에 의존해 왔다. 이는 모델의 손실 함수(Loss Function)에 전문가들의 선택 빈도가 균등해지도록 강제하는 정규화 항(Regularization Term)을 추가하는 방식이다. GShard부터 Switch Transformer에 이르기까지, 이 방식은 MoE 학습의 표준으로 자리 잡았다.5 그러나 DeepSeek-V3 연구팀은 이 표준에 근본적인 의문을 제기했다. “과연 인위적인 균형 강제가 모델의 지능 발현을 방해하지 않는가?“라는 질문이다. 보조 손실은 모델이 데이터의 본질적인 특성에 따라 최적의 전문가를 선택하려는 학습 유인(Incentive)과 충돌하며, 필연적으로 주 임무(Language Modeling)의 성능 저하를 야기한다.6</p>
<p>본 절에서는 DeepSeek-V3가 제시한 혁신적인 해법인 ‘보조 손실 없는 로드 밸런싱(Auxiliary-Loss-Free Load Balancing)’ 전략을 심층적으로 분석한다. 이 전략은 손실 함수에 개입하여 그라디언트(Gradient)를 왜곡하는 대신, 라우팅 결정 메커니즘 자체에 편향(Bias)을 주입하고 이를 동적으로 제어하는 공학적 접근법을 취한다. 이는 모델의 성능을 훼손하지 않으면서도 완벽에 가까운 부하 분산을 달성하는, MoE 아키텍처의 새로운 지평을 여는 기술이다. 우리는 이 기술의 이론적 배경, 수학적 메커니즘, 구현상의 디테일, 그리고 이것이 전체 AI 시스템에 미치는 함의를 포괄적으로 다룰 것이다.</p>
<h2>2.  기존 로드 밸런싱의 한계: 보조 손실의 딜레마</h2>
<p>DeepSeek-V3의 혁신성을 온전히 이해하기 위해서는, 먼저 기존 MoE 모델들이 겪었던 구조적 모순과 그들이 선택했던 타협책인 ’보조 손실’의 한계를 명확히 짚고 넘어가야 한다.</p>
<h3>2.1  라우팅 붕괴(Routing Collapse)의 메커니즘</h3>
<p>MoE 레이어에서 라우터(Router)는 입력 토큰 <span class="math math-inline">x</span>와 각 전문가 <span class="math math-inline">i</span>의 대표 벡터(Centroid) <span class="math math-inline">e_i</span> 간의 내적(Dot Product)을 통해 유사도(Affinity Score)를 계산하고, 이를 바탕으로 상위 <span class="math math-inline">k</span>개의 전문가를 선택한다.<br />
<span class="math math-display">
s_{i} = x^T e_i
</span><br />
학습 초기 단계에서, 무작위 초기화(Random Initialization)의 영향으로 특정 전문가의 가중치가 입력 데이터 분포와 우연히 더 잘 맞거나, 라우터가 특정 전문가에게 편향된 점수를 부여할 수 있다. 이렇게 되면 해당 전문가는 더 많은 데이터를 처리하게 되고, 그 결과 그라디언트 업데이트를 더 많이 받아 성능이 빠르게 향상된다. 성능이 좋아진 전문가는 라우터로부터 더 높은 점수를 받게 되고, 이는 다시 더 많은 데이터를 끌어들이는 ’부익부 빈익빈(Rich-get-richer)’의 피드백 루프를 형성한다.3</p>
<p>결국 극소수의 전문가만이 모든 토큰을 독점하고, 나머지 대다수의 전문가는 훈련되지 않은 채로 남게 된다. 이는 모델의 유효 파라미터 수를 급격히 감소시켜, 거대 모델을 쓰는 의미를 퇴색시킨다. 또한, 데이터 병렬화(Data Parallelism)와 전문가 병렬화(Expert Parallelism)가 결합된 분산 학습 환경에서, 특정 GPU(과부하 전문가가 위치한)의 메모리가 넘치거나 연산이 지연되면, 전체 클러스터가 해당 GPU의 작업이 끝날 때까지 대기해야 하는 동기화 비용이 발생한다.4</p>
<p><strong>라우팅 붕괴 메커니즘</strong></p>
<pre><code class="language-mermaid">graph TD

    Start("학습 시작: 무작위 초기화") --&gt; Condition{"특정 전문가 i의&lt;br&gt;초기 가중치 유리함?"}
    Condition -- "YES" --&gt; HighScore("라우터가 전문가 i에게&lt;br&gt;높은 점수 부여")
    HighScore --&gt; MoreData("전문가 i에게&lt;br&gt;더 많은 토큰 배정")
    MoreData --&gt; BetterGrad("전문가 i만 집중적으로 학습&lt;br&gt;(Gradient Update)")
    BetterGrad --&gt; PerformanceUp("전문가 i의 성능 향상")
    PerformanceUp --&gt; HighScore
    
    Condition -- "NO" --&gt; LowScore("라우터가 낮은 점수 부여")
    LowScore --&gt; Idle("전문가 학습 기회 박탈&lt;br&gt;(Idle State)")
    Idle --&gt; Collapse("라우팅 붕괴&lt;br&gt;(Routing Collapse)")
    
    style Start fill:#f9f,stroke:#333,stroke-width:2px
    style Collapse fill:#f00,stroke:#333,stroke-width:2px,color:#fff
</code></pre>
<h3>2.2  보조 손실(Auxiliary Loss)의 도입과 부작용</h3>
<p>이를 방지하기 위해 구글의 GShard, Switch Transformer 등 선구적인 연구들은 손실 함수에 로드 밸런싱을 위한 보조 항을 추가했다.<br />
<span class="math math-display">
\mathcal{L}_{total} = \mathcal{L}_{task} + \alpha \cdot \mathcal{L}_{aux}
</span><br />
여기서 <span class="math math-inline">\mathcal{L}{task}</span>는 다음 토큰 예측과 같은 주 임무의 손실이며, <span class="math math-inline">\mathcal{L}{aux}</span>는 전문가 배정의 불균형도(분산 등)를 측정하는 항이다. <span class="math math-inline">\alpha</span>는 이 둘 사이의 균형을 조절하는 하이퍼파라미터로, 보통 <span class="math math-inline">10^{-2}</span> 수준에서 설정된다.5</p>
<p>이러한 접근법은 표면적으로는 전문가들에게 토큰을 강제로 배분하여 붕괴를 막는 데 성공했다. 그러나 깊은 수준에서는 심각한 부작용을 낳았다.</p>
<ol>
<li><strong>목적 함수의 충돌(Objective Conflict):</strong> 모델은 정답을 맞히기 위해 특정 전문가(예: ‘코딩 전문가’)를 선택해야 한다고 판단하지만, 보조 손실은 균형을 위해 엉뚱한 전문가(예: ‘요리 전문가’)를 선택하도록 그라디언트를 유도한다. 이 상충되는 신호는 모델의 학습 효율을 떨어뜨린다.</li>
<li><strong>전문성 희석:</strong> 보조 손실은 전문가들이 특정 도메인에 특화되는 것을 방해하고, 모든 전문가가 얕은 지식을 가진 ’범용가(Generalist)’가 되도록 압력을 가한다. 이는 MoE의 존재 의의인 ’전문화(Specialization)’와 정면으로 배치된다.</li>
<li><strong>하이퍼파라미터 민감성:</strong> <span class="math math-inline">\alpha</span> 값이 너무 크면 모델 성능이 망가지고, 너무 작으면 밸런싱이 깨진다. 이 최적점을 찾는 것은 막대한 컴퓨팅 자원을 소모하는 실험을 요구한다.7</li>
</ol>
<p>DeepSeek-V3 연구팀은 이러한 ’보조 손실의 딜레마’를 해결하지 않고서는 모델 성능의 다음 단계로 나아갈 수 없다고 판단했다. 그들은 손실 함수에 의존하지 않는, 보다 직접적이고 외과적인 수술 방식의 로드 밸런싱 전략을 모색했다.</p>
<p><strong>보조 손실의 딜레마</strong></p>
<pre><code class="language-mermaid">graph TD
    Input("입력 토큰 x") --&gt; Router("라우터 (Router)")
    
    Router --&gt; PathTask("주 임무 경로&lt;br&gt;(Task Objective)")
    Router --&gt; PathAux("보조 손실 경로&lt;br&gt;(Auxiliary Objective)")
    
    PathTask --&gt; ExpertA("전문가 A: 정답 도출에 유리&lt;br&gt;(코딩 전문가)")
    PathAux --&gt; ExpertB("전문가 B: 균형 맞추기용&lt;br&gt;(요리 전문가)")
    
    ExpertA --&gt; LossTask("Task Loss 최소화&lt;br&gt;(정확도 중시)")
    ExpertB --&gt; LossAux("Aux Loss 최소화&lt;br&gt;(균등 분배 중시)")
    
    LossTask -- "Gradient A" --&gt; Conflict("목적 함수의 충돌&lt;br&gt;(Gradient Conflict)")
    LossAux -- "Gradient B" --&gt; Conflict
    
    Conflict --&gt; Result("전문성 희석 및&lt;br&gt;학습 효율 저하")
    
    style Conflict fill:#ff9,stroke:#f66,stroke-width:4px,stroke-dasharray: 5 5
</code></pre>
<h2>3.  DeepSeek-V3의 핵심 혁신: 보조 손실 없는 로드 밸런싱</h2>
<p>DeepSeek-V3는 “학습 목표(Loss)를 건드리지 않고, 행동(Routing)을 교정한다“는 철학 아래, ‘보조 손실 없는 로드 밸런싱(Auxiliary-Loss-Free Load Balancing)’ 전략을 제안했다. 이 전략은 라우팅 과정에 명시적인 ’편향 항(Bias Term)’을 도입하고, 이를 별도의 제어 루프를 통해 동적으로 업데이트하는 방식을 채택했다.1</p>
<h3>3.1  라우팅과 게이팅의 분리 (Decoupling Routing and Gating)</h3>
<p>이 전략의 핵심 아이디어는 ’누가 토큰을 처리할 것인가(Routing)’를 결정하는 점수와, ’최종 출력에 얼마나 기여할 것인가(Gating/Weighting)’를 결정하는 점수를 분리하는 것이다.</p>
<p>기존 방식에서는 <span class="math math-inline">s_{i,t}</span> (입력 <span class="math math-inline">t</span>와 전문가 <span class="math math-inline">i</span>의 유사도)가 전문가 선택과 가중치 계산에 모두 쓰였다. 반면, DeepSeek-V3는 각 전문가 <span class="math math-inline">i</span>마다 편향 항 <span class="math math-inline">b_i</span>를 할당하고 다음과 같이 이원화된 프로세스를 적용한다 8:</p>
<ol>
<li>전문가 선택 (Top-K Routing):</li>
</ol>
<p>라우터는 편향 항이 더해진 점수를 기준으로 상위 <span class="math math-inline">K</span>개의 전문가를 선택한다.<br />
<span class="math math-display">
   g&#39;_{i,t} = \begin{cases} s_{i,t} &amp; \text{if } (s_{i,t} + b_i) \in \text{TopK}(\{s_{j,t} + b_j\}_{j=1}^{N_r}, K_r) \\ 0 &amp; \text{otherwise} \end{cases}
</span><br />
여기서 <span class="math math-inline">s_{i,t} = \text{Sigmoid}(u_t^T e_i)</span>이다. 즉, <span class="math math-inline">b_i</span>가 높은 전문가는 실제 유사도(<span class="math math-inline">s_{i,t}</span>)가 다소 낮더라도 선택될 확률이 높아진다.</p>
<ol start="2">
<li>게이팅 값 계산 (Gating Value):</li>
</ol>
<p>실제 전문가의 출력을 결합할 때 사용하는 가중치는 **편향 항 <span class="math math-inline">b_i</span>를 제외한 원본 유사도 <span class="math math-inline">s_{i,t}</span>**를 정규화하여 사용한다.<br />
<span class="math math-display">
   g_{i,t} = \frac{g&#39;_{i,t}}{\sum_{j=1}^{N_r} g&#39;_{j,t}}
</span><br />
(선택되지 않은 전문가의 <span class="math math-inline">g&#39;_{i,t}</span>는 0이므로, 선택된 전문가들 사이에서만 정규화가 일어난다.)</p>
<p><strong>통찰(Insight):</strong> 이 분리는 매우 정교한 공학적 장치다. <span class="math math-inline">b_i</span>를 통해 강제로 특정 전문가에게 일을 시키더라도(Load Balancing), 그 전문가가 내놓은 답을 최종 결과에 반영할 때는 원래의 유사도(Affinity)만을 사용한다. 즉, “일감은 강제로 배분하되, 그 결과값의 신뢰도는 실력대로 평가하겠다“는 원칙이다. 이를 통해 로드 밸런싱을 강제함으로 인해 발생하는 모델의 표현력 왜곡을 최소화한다.2</p>
<p><strong>라우팅과 게이팅의 분리 프로세스</strong></p>
<pre><code class="language-mermaid">graph TD
    Input("입력 토큰 u_t") --&gt; Affinity("유사도 계산: s = Sigmoid(u * e)")
    
    Affinity --&gt; BranchSelect("경로 1: 전문가 선택 (Routing)")
    Affinity --&gt; BranchWeight("경로 2: 가중치 계산 (Gating)")
    
    %% 경로 1: 선택
    BranchSelect --&gt; AddBias("편향 추가: s + b_i")
    AddBias --&gt; TopK("Top-K 선정")
    TopK --&gt; SelectedIdx("선택된 전문가 목록 I_t")
    
    %% 경로 2: 가중치
    SelectedIdx -.-&gt; Filter("선택된 전문가만 필터링")
    BranchWeight --&gt; Filter
    Filter --&gt; Norm("원본 점수 s 정규화 (Bias 제외)")
    Norm --&gt; Weights("최종 게이팅 값: g_i")
    
    %% 결합
    SelectedIdx --&gt; Computation("전문가 연산 수행")
    Weights --&gt; Computation
    Computation --&gt; Output("최종 출력 결합")
    
    style AddBias fill:#d4f1f4,stroke:#333
    style Norm fill:#d4f1f4,stroke:#333
</code></pre>
<h3>3.2  동적 편향 업데이트 알고리즘 (Dynamic Bias Update)</h3>
<p>그렇다면 편향 항 <span class="math math-inline">b_i</span>는 어떻게 결정되는가? DeepSeek-V3는 이를 그라디언트 하강법으로 학습시키는 대신, 시스템의 상태(부하)를 모니터링하여 실시간으로 조절하는 ‘피드백 제어(Feedback Control)’ 방식을 사용한다.10</p>
<p>각 학습 스텝(Step)이 끝날 때마다, 전체 배치(Batch)에서 각 전문가에게 할당된 토큰의 총량을 집계한다. 그리고 다음과 같은 단순한 규칙에 따라 <span class="math math-inline">b_i</span>를 갱신한다:</p>
<ul>
<li>과부하(Overloaded): 전문가 <span class="math math-inline">i</span>의 부하가 평균보다 높으면, <span class="math math-inline">b_i</span>를 <span class="math math-inline">\gamma</span>만큼 감소시킨다.<br />
<span class="math math-display">
b_i \leftarrow b_i - \gamma
</span><br />
저부하(Underloaded): 전문가 <span class="math math-inline">i</span>의 부하가 평균보다 낮으면, <span class="math math-inline">b_i</span>를 <span class="math math-inline">\gamma</span>만큼 증가시킨다.<br />
<span class="math math-display">
b_i \leftarrow b_i + \gamma
</span></li>
</ul>
<p>여기서 <span class="math math-inline">\gamma</span>는 **편향 업데이트 속도(Bias Update Speed)**라 불리는 하이퍼파라미터다.</p>
<p>이 알고리즘은 PID 제어기와 유사하게 작동한다.</p>
<ul>
<li>특정 전문가가 인기가 많아지면 <span class="math math-inline">b_i</span>가 점진적으로 낮아져 선택될 확률이 떨어진다.</li>
<li>인기가 없는 전문가는 <span class="math math-inline">b_i</span>가 계속 높아져 언젠가는 반드시 선택되게 된다.</li>
<li>이 과정이 반복되면서 시스템은 자연스럽게 평형 상태(Equilibrium)에 도달한다.</li>
</ul>
<p>이 방식의 가장 큰 장점은 <strong>비용이 거의 들지 않는다는 것</strong>이다. 별도의 손실 함수 계산이나 역전파 과정 없이, CPU 상에서 덧셈/뺄셈 연산만으로 수행되므로 학습 속도에 전혀 지장을 주지 않는다.12</p>
<p><strong>동적 편향 업데이트 로직</strong></p>
<pre><code class="language-mermaid">graph TD
    StepEnd("학습 스텝(Step) 종료") --&gt; Monitor("전문가별 부하(Load) 집계")
    Monitor --&gt; CalcAvg("평균 부하 계산")
    
    CalcAvg --&gt; Check{"전문가 i의 부하 &gt; 평균?"}
    
    Check -- "YES (과부하)" --&gt; DecBias("편향 감소: b_i = b_i - gamma")
    DecBias --&gt; Result1("선택 확률 감소")
    
    Check -- "NO (저부하)" --&gt; IncBias("편향 증가: b_i = b_i + gamma")
    IncBias --&gt; Result2("선택 확률 증가")
    
    Result1 --&gt; Equilibrium("시스템 평형 도달")
    Result2 --&gt; Equilibrium
    
    style DecBias fill:#ffcccc,stroke:#333
    style IncBias fill:#ccffcc,stroke:#333
</code></pre>
<h3>3.3  하이퍼파라미터 <span class="math math-inline">\gamma</span>의 튜닝과 전략</h3>
<p><span class="math math-inline">\gamma</span> 값은 시스템의 반응 속도를 결정한다. DeepSeek-V3의 기술 보고서는 이 값의 설정에 대한 구체적인 가이드를 제공한다.8</p>
<ul>
<li>초기 단계 (학습 시작 ~ 14.3T 토큰): <span class="math math-inline">\gamma = 0.001</span>.</li>
</ul>
<p>학습 초기에는 전문가들의 역할이 정해지지 않았고 라우팅 붕괴 위험이 크므로, 비교적 큰 값을 사용하여 적극적으로 개입한다.</p>
<ul>
<li>후기 단계 (나머지 500B 토큰): <span class="math math-inline">\gamma = 0.0</span>.</li>
</ul>
<p>학습이 충분히 진행되어 전문가들의 전문화가 이루어진 후에는, 인위적인 개입을 중단하고 모델이 미세한 최적화에 집중하도록 한다. 편향을 고정(Freeze)시킴으로써 학습의 수렴성을 보장한다.</p>
<p>이러한 단계적 접근은 로드 밸런싱이 학습 초기에는 ‘가이드’ 역할을 하다가, 나중에는 모델의 자율성을 존중하는 방향으로 진화함을 보여준다.</p>
<p><strong>감마 튜닝 타임라인</strong></p>
<pre><code class="language-mermaid">graph TD
    Phase1("초기 단계&lt;br&gt;(0 ~ 14.3T 토큰)") --&gt; Setting1("설정: gamma = 0.001")
    Setting1 --&gt; Action1("적극적 개입:&lt;br&gt;전문가 불균형 강제 교정")
    
    Action1 --&gt; Phase2("후기 단계&lt;br&gt;(나머지 500B 토큰)")
    Phase2 --&gt; Setting2("설정: gamma = 0.0")
    Setting2 --&gt; Action2("개입 중단 (Freeze):&lt;br&gt;모델 자율성 및 수렴성 보장")
    
    Action2 --&gt; Inference("추론 단계&lt;br&gt;(Inference)")
    Inference --&gt; Benefit("고정된 편향으로&lt;br&gt;최적화된 라우팅 유지")
    
    style Setting1 fill:#ffd700,stroke:#333
    style Setting2 fill:#c0c0c0,stroke:#333   
</code></pre>
<h2>4.  DeepSeekMoE 아키텍처: 로드 밸런싱을 위한 구조적 토대</h2>
<p>DeepSeek-V3의 로드 밸런싱 전략은 허공에 떠 있는 것이 아니라, ’DeepSeekMoE’라는 독창적인 아키텍처 위에서 작동한다. 이 아키텍처의 구조적 특성은 로드 밸런싱의 난이도를 낮추고 효율성을 극대화하는 데 기여한다.</p>
<p><strong>DeepSeekMoE 아키텍처 구조</strong></p>
<pre><code class="language-mermaid">graph TD
    Token("입력 토큰") --&gt; Split("경로 분기")
    
    Split --&gt; PathShared("공유 전문가 경로&lt;br&gt;(Shared Expert)")
    Split --&gt; PathRouted("라우팅 전문가 경로&lt;br&gt;(Routed Experts)")
    
    PathShared --&gt; SharedExp("공유 전문가&lt;br&gt;(항상 활성)")
    SharedExp --&gt; Role1("역할: 공통 지식,&lt;br&gt;빈출 토큰 처리")
    
    PathRouted --&gt; Router("라우터")
    Router --&gt; Select("Top-K 선택 (N_r 중 K개)")
    Select --&gt; Exp1("세분화 전문가 1")
    Select --&gt; Exp2("세분화 전문가 2")
    Select --&gt; ExpK("세분화 전문가 K")
    
    Role1 --&gt; Sum("출력 합산")
    Exp1 --&gt; Sum
    Exp2 --&gt; Sum
    ExpK --&gt; Sum
    
    Sum --&gt; Final("레이어 출력 h_t")
    
    style SharedExp fill:#ff9999,stroke:#333
    style Router fill:#99ccff,stroke:#333
</code></pre>
<h3>4.1  세분화된 전문가(Fine-Grained Experts) 전략</h3>
<p>DeepSeek-V3는 전문가를 매우 잘게 쪼개는 전략을 취했다. 기존 모델들이 소수의 거대한 전문가를 사용했다면, DeepSeekMoE는 다수의 작은 전문가를 운용한다.</p>
<ul>
<li><strong>총 전문가 수 (<span class="math math-inline">N</span>):</strong> 256개 (라우팅 전문가) + 1개 (공유 전문가).2</li>
<li><strong>전문가당 파라미터:</strong> 각 전문가는 상대적으로 작은 크기를 가지며, 이는 지식의 분해(Knowledge Decomposition)를 촉진한다.</li>
</ul>
<p>전문가가 세분화될수록 특정 전문가에게 부하가 집중될 확률이 통계적으로 분산된다. 또한, 더 다양한 전문가 조합이 가능해져 모델의 표현력이 풍부해진다.10</p>
<h3>4.2  공유 전문가(Shared Experts)의 역할</h3>
<p>DeepSeekMoE의 가장 결정적인 특징은 **공유 전문가(Shared Expert)**의 존재다. 각 레이어마다 1개의 전문가는 라우팅 과정 없이 모든 토큰을 처리하도록 고정되어 있다.2</p>
<p>로드 밸런싱 관점에서의 중요성:</p>
<p>언어 데이터에는 ’Zipf의 법칙’이 적용된다. ‘the’, ‘is’, ’a’와 같은 기능어나 일반적인 문법 구조는 모든 문장에 매우 빈번하게 등장한다. 만약 공유 전문가가 없다면, 이러한 빈출 토큰들을 처리하는 특정 전문가에게 과도한 부하가 걸릴 수밖에 없다.</p>
<p>DeepSeek-V3에서는 공유 전문가가 이러한 **“공통 지식(Common Knowledge)”**과 **“높은 빈도의 부하(Base Load)”**를 전담하여 흡수한다.16 덕분에 나머지 256개의 라우팅 전문가들은 진정으로 특화된 지식(예: 양자 역학, 고대 역사 등)에만 집중할 수 있게 된다. 이는 라우팅 전문가들 사이의 부하 불균형을 구조적으로 완화하는 효과를 낳는다. 공유 전문가가 일종의 ‘충격 흡수 장치(Shock Absorber)’ 역할을 하는 셈이다.</p>
<h3>4.3  노드 제한 라우팅 (Node-Limited Routing)</h3>
<p>671B 파라미터 모델을 학습시키기 위해서는 수천 대의 GPU가 필요하다. 이때 전문가들이 물리적으로 서로 다른 GPU 노드(Node)에 흩어져 있으면, 토큰을 전송하는 과정에서 막대한 네트워크 대역폭이 소모된다.</p>
<p>DeepSeek-V3는 이를 해결하기 위해 노드 제한 라우팅을 도입했다. 각 토큰은 최대 <span class="math math-inline">M=4</span>개의 노드에 위치한 전문가들만 선택할 수 있도록 제약을 둔다.14</p>
<p>이 제약은 로드 밸런싱 알고리즘과 결합되어 강력한 시너지를 낸다. 편향 항 <span class="math math-inline">b_i</span>를 조절할 때, 단순히 연산 부하뿐만 아니라 네트워크 부하까지 고려할 수 있게 된다. 결과적으로 DeepSeek-V3는 연산량(Compute)과 통신량(Communication)의 균형을 동시에 달성하며, 데이터 센터 수준의 효율성을 확보한다.18</p>
<h2>5.  구현의 디테일과 수학적 정식화</h2>
<p>DeepSeek-V3의 기술적 성취를 재현하거나 깊이 이해하기 위해서는 구체적인 구현 로직과 수식을 들여다볼 필요가 있다. 다음은 기술 보고서와 공개된 코드를 바탕으로 재구성한 구현 상세이다.</p>
<h3>5.1  주요 파라미터 정의</h3>
<p>다음은 DeepSeek-V3의 로드 밸런싱과 관련된 주요 변수 및 설정값이다.</p>
<table><thead><tr><th><strong>표기</strong></th><th><strong>의미</strong></th><th><strong>값/설정</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><span class="math math-inline">N_r</span></td><td>라우팅 전문가 수</td><td>256</td><td>레이어당</td></tr>
<tr><td><span class="math math-inline">N_{shared}</span></td><td>공유 전문가 수</td><td>1</td><td>항상 활성화</td></tr>
<tr><td><span class="math math-inline">K_r</span></td><td>토큰당 선택 전문가 수</td><td>8</td><td>Top-K 선택</td></tr>
<tr><td><span class="math math-inline">M</span></td><td>최대 선택 가능 노드 수</td><td>4</td><td>통신 비용 제한</td></tr>
<tr><td><span class="math math-inline">\gamma</span></td><td>편향 업데이트 속도</td><td>0.001 <span class="math math-inline">\to</span> 0.0</td><td>동적 조절</td></tr>
<tr><td><span class="math math-inline">\alpha</span></td><td>보조 손실 계수</td><td>0.0001</td><td>시퀀스 단위 밸런싱용</td></tr>
<tr><td><span class="math math-inline">d_h</span></td><td>히든 차원</td><td>7168</td><td>14</td></tr>
</tbody></table>
<p>1</p>
<h3>5.2  알고리즘 흐름 (Algorithm Flow)</h3>
<p>전체 프로세스는 크게 **순전파(Forward Pass)**와 <strong>편향 업데이트(Bias Update)</strong> 두 단계로 나뉜다.</p>
<p><strong>[단계 1: 순전파 (Forward Pass)]</strong></p>
<ol>
<li>
<p>유사도 계산: 입력 <span class="math math-inline">u_t</span>와 전문가 중심 <span class="math math-inline">e_i</span> 간의 내적 후 Sigmoid 활성화 함수를 적용한다.<br />
<span class="math math-display">
s_{i,t} = \text{Sigmoid}(u_t^T e_i)
</span><br />
(참고: Softmax 대신 Sigmoid를 사용하는 것은 각 전문가의 적합성을 독립적인 확률로 다루기 위함이다.8)</p>
</li>
<li>
<p>편향 적용 및 선택: 편향 <span class="math math-inline">b_i</span>를 더한 점수로 상위 <span class="math math-inline">K_r</span>개를 선택한다.<br />
<span class="math math-display">
\mathcal{I}_t = \text{TopK}(\{s_{i,t} + b_i\}_{i=1}^{N_r}, K_r)
</span></p>
</li>
<li>
<p>최종 가중치 정규화: 선택된 전문가들에 한해, **원본 점수 <span class="math math-inline">s_{i,t}</span>**를 사용하여 가중치를 정규화한다.<br />
<span class="math math-display">
g_{i,t} = \frac{s_{i,t}}{\sum_{j \in \mathcal{I}_t} s_{j,t}} \quad (\forall i \in \mathcal{I}_t)
</span></p>
</li>
<li>
<p>출력 결합:<br />
<span class="math math-display">
h_t = u_t + \sum_{i \in \mathcal{I}_t} g_{i,t} E_i(u_t) + E_{shared}(u_t)
</span><br />
여기서 <span class="math math-inline">E_i</span>는 <span class="math math-inline">i</span>번째 전문가 네트워크, <span class="math math-inline">E_{shared}</span>는 공유 전문가 네트워크다.</p>
</li>
</ol>
<p>학습 스텝이 끝난 직후, 별도의 프로세스에서 수행된다.</p>
<ol>
<li>
<p>배치 내의 모든 토큰에 대해 각 전문가의 선택 횟수(Load)를 집계한다: <span class="math math-inline">C_i = \sum_{t \in \text{Batch}} \mathbb{1}(i \in \mathcal{I}_t)</span>.</p>
</li>
<li>
<p>평균 부하 <span class="math math-inline">\bar{C} = \frac{1}{N_r} \sum_{i} C_i</span> 를 계산한다.</p>
</li>
<li>
<p>각 전문가에 대해 편향을 갱신한다:<br />
<span class="math math-display">
b_i \leftarrow b_i + \gamma \cdot \text{sgn}(\bar{C} - C_i)
</span><br />
(부하가 평균보다 작으면 <span class="math math-inline">b_i</span> 증가, 크면 감소)</p>
</li>
</ol>
<pre><code class="language-mermaid">sequenceDiagram
    autonumber
    participant T as "Trainer (학습기)"
    participant R as "Router (CPU/GPU)"
    participant E as "Experts (GPU Cluster)"
    participant B as "Bias Controller"
   
    Note over T, B: [Step 1: 순전파 (Forward Pass)]
    T-&gt;&gt;R: "입력 토큰 전달"
    R-&gt;&gt;B: "현재 편향(b_i) 요청"
    B--&gt;&gt;R: "편향 값 반환"
    R-&gt;&gt;R: "유사도 계산 &amp; Top-K 선택 (with Bias)"
    R-&gt;&gt;E: "토큰 라우팅 (Selected Experts)"
    E-&gt;&gt;E: "전문가 연산 수행"
    E--&gt;&gt;T: "결과 반환 (Weighted Sum without Bias)"
    
    Note over T, B: [Step 2: 역전파 (Backward Pass)]
    T-&gt;&gt;E: "Gradient 계산 (Loss_task)"
    E--&gt;&gt;T: "파라미터 업데이트"
    Note right of T: "보조 손실(Aux Loss) 없음/미미함"

    Note over T, B: [Step 3: 편향 업데이트 (Post-Step)]
    T-&gt;&gt;B: "배치 내 전문가별 부하 통계 전달"
    B-&gt;&gt;B: "평균 부하 계산"
    B-&gt;&gt;B: "편향(b_i) 갱신 (CPU 연산)"
    Note right of B: "다음 스텝 라우팅에 즉시 반영"
</code></pre>
<h3>5.3  보완적 시퀀스 단위 밸런스 손실 (Complementary Sequence-Wise Balance Loss)</h3>
<p>DeepSeek-V3는 ‘보조 손실 없는’ 전략을 표방하지만, 기술적인 안전장치로 매우 미미한 수준의 보조 손실을 사용한다.<br />
<span class="math math-display">
\mathcal{L}_{bal} = \alpha \sum_{i=1}^{N_r} f_i P_i
</span><br />
여기서 <span class="math math-inline">\alpha = 0.0001</span>이다.8</p>
<p>왜 필요한가? 편향 업데이트는 배치(Batch) 전체의 통계를 기반으로 작동한다. 하지만 개별 시퀀스(Sequence) 내에서 특정 전문가에게 토큰이 몰리는 국소적인 불균형은 배치 통계에 묻혀 감지되지 않을 수 있다. 이를 방지하기 위해 1만 분의 1 수준의 매우 작은 가중치를 가진 손실 함수를 추가하여, 시퀀스 레벨의 극단적인 쏠림을 방지한다. 이는 주 학습에 거의 영향을 주지 않으면서 시스템의 안정성을 높이는 실용적인 타협이다.5</p>
<h2>6.  실증적 검증: 절제 실험(Ablation Study)과 성능 분석</h2>
<p>DeepSeek-V3의 전략이 단순한 이론적 유희가 아님은 실제 학습 결과와 절제 실험(Ablation Study)을 통해 명확히 증명된다. 기술 보고서의 ’Table 5’는 이 전략의 우수성을 보여주는 결정적인 데이터를 담고 있다.8</p>
<h3>6.1  보조 손실 유무에 따른 성능 비교</h3>
<p>연구팀은 동일한 모델 구조(DeepSeekMoE) 하에서, 기존의 보조 손실 방식과 DeepSeek-V3의 편향 조절 방식을 비교 실험했다.</p>
<p><strong>표 2: 보조 손실 방식 vs. 보조 손실 없는 방식 (DeepSeek-V3) 성능 비교</strong></p>
<table><thead><tr><th><strong>평가 지표</strong></th><th><strong>보조 손실 방식 (Aux Loss)</strong></th><th><strong>DeepSeek-V3 (Aux-Free)</strong></th><th><strong>분석 및 함의</strong></th></tr></thead><tbody>
<tr><td><strong>Perplexity (PPL)</strong></td><td>상대적으로 높음 (나쁨)</td><td><strong>더 낮음 (좋음)</strong></td><td>보조 손실이 학습을 방해하지 않아 언어 모델링 성능이 향상됨.8</td></tr>
<tr><td><strong>Load Balance (MaxVio)</strong></td><td>양호</td><td><strong>매우 양호</strong></td><td>편향 조절이 보조 손실보다 더 직접적이고 강력하게 부하를 통제함.7</td></tr>
<tr><td><strong>MMLU (지식)</strong></td><td>기준점</td><td><strong>상승</strong></td><td>전문가들이 불필요한 간섭 없이 각자의 도메인에 더 잘 특화됨.</td></tr>
<tr><td><strong>GSM8K (수학)</strong></td><td>기준점</td><td><strong>상승</strong></td><td>복잡한 추론 문제에서도 라우팅 효율성이 성능 향상으로 이어짐.</td></tr>
<tr><td><strong>학습 안정성</strong></td><td>스파이크 발생 가능</td><td><strong>매우 안정적</strong></td><td>그라디언트 충돌이 없어 손실 곡선이 매끄럽게 수렴함.8</td></tr>
</tbody></table>
<p>7</p>
<p>이 결과는 “로드 밸런싱을 위해 성능을 희생해야 한다“는 기존의 통념을 뒤집는다. DeepSeek-V3는 균형과 성능이라는 두 마리 토끼를 모두 잡았다. 특히 Perplexity의 개선은 모델이 더 자연스럽고 정확한 텍스트를 생성하게 되었음을 의미하며, 이는 <span class="math math-inline">b_i</span>를 통한 라우팅 제어가 모델의 내재적 지식 학습을 방해하지 않았음을 방증한다.</p>
<h3>6.2  학습 비용과 효율성</h3>
<p>DeepSeek-V3의 전체 학습에는 약 278.8만 시간의 H800 GPU가 사용되었다. 이는 유사한 성능을 내는 Llama 3.1 405B 등과 비교했을 때 10배 가까이 적은 비용이다.3 이러한 압도적인 효율성의 배경에는 로드 밸런싱이 있다. 모든 전문가(GPU)가 쉴 새 없이 균등하게 연산에 참여함으로써, 하드웨어의 유휴 시간(Idle Time)을 최소화하고 MFU(Model FLOPs Utilization)를 극대화한 것이다.</p>
<h2>7.  시스템적 함의와 미래 전망</h2>
<p>DeepSeek-V3의 ’보조 손실 없는 로드 밸런싱’은 단순한 알고리즘의 개선을 넘어, 차세대 AI 시스템 설계에 중요한 함의를 던진다.</p>
<h3>7.1  하드웨어-소프트웨어 공진화 (Co-design)</h3>
<p>이 기술은 소프트웨어(알고리즘)가 하드웨어(GPU 클러스터)의 특성을 고려하여 설계될 때 얼마나 큰 효율성을 낼 수 있는지를 보여준다. 노드 제한 라우팅과 편향 조절의 결합은 물리적 통신 병목을 알고리즘 레벨에서 해소한 사례다. 이는 향후 수만 대의 GPU를 연결하는 엑사스케일(Exascale) AI 시스템에서 필수적인 설계 원칙이 될 것이다.21</p>
<h3>7.2  추론 가속화와 MTP (Multi-Token Prediction)</h3>
<p>학습된 편향 <span class="math math-inline">b_i</span>는 추론 시에도 유용하게 사용된다. 학습 후반부에 <span class="math math-inline">\gamma=0.0</span>으로 고정된 편향은, 추론 시에도 전문가들 사이의 부하를 적절히 분산시켜 응답 지연(Latency)을 줄인다. 또한, DeepSeek-V3가 채택한 MTP(다중 토큰 예측) 전략과 결합하여, 투기적 디코딩(Speculative Decoding) 시에도 여러 전문가를 효율적으로 활용할 수 있게 한다.6</p>
<h3>7.3  ’제약 없는 학습’으로의 패러다임 전환</h3>
<p>DeepSeek-V3는 AI 모델에게 “이것을 하지 마라(Loss)“고 강요하는 대신, “이쪽으로 가는 것이 좋다(Bias)“고 환경을 조성해 주는 것이 더 효과적임을 입증했다. 이는 강화학습이나 다른 딥러닝 분야에도 적용될 수 있는 철학적 전환이다. 앞으로 등장할 모델들은 내부의 학습 목표와 외부의 시스템적 제약을 분리하는 이원화된 제어 구조를 더 적극적으로 채택할 것으로 전망된다.</p>
<h2>8.  결론</h2>
<p>서적 ’트랜스포머 싱귤래리티’의 제9장 2절에서 살펴본 DeepSeek-V3의 <strong>보조 손실 없는 로드 밸런싱</strong>은 MoE 아키텍처의 오랜 딜레마를 해결한 우아하고 강력한 해법이다.</p>
<ol>
<li><strong>혁신적 접근:</strong> 라우팅 점수(Bias 포함)와 가중치 점수(Bias 제외)를 분리함으로써, 로드 밸런싱을 강제하면서도 모델의 표현력을 보존했다.</li>
<li><strong>공학적 완성:</strong> 동적 편향 업데이트 알고리즘을 통해 그라디언트 계산 비용 없이 실시간으로 시스템의 평형을 유지했다.</li>
<li><strong>구조적 통합:</strong> DeepSeekMoE의 공유 전문가 및 노드 제한 라우팅과 결합하여, 연산 효율성과 통신 효율성을 동시에 달성했다.</li>
<li><strong>실증적 성과:</strong> 보조 손실 방식 대비 우월한 Perplexity와 벤치마크 성능을 입증하며, 671B 파라미터 거대 모델의 안정적인 학습을 이끌어냈다.</li>
</ol>
<p>결국 DeepSeek-V3는 “복잡성을 제어하는 것이 아니라, 복잡성 그 자체를 효율성의 원천으로 삼는” MoE의 진정한 잠재력을 해방시켰다. 이는 단순히 하나의 모델을 넘어, 인공지능이 더 거대하고 효율적인 구조로 진화하는 과정에서 반드시 거쳐야 할 ’특이점(Singularity)’의 중요한 조각이라 할 수 있다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>[2412.19437] DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/abs/2412.19437</li>
<li>DeepSeek v3 and R1 Model Architecture: Why it’s powerful and economical - Fireworks AI, https://fireworks.ai/blog/deepseek-model-architecture</li>
<li>How has DeepSeek improved the Transformer architecture? - Epoch AI, https://epoch.ai/gradient-updates/how-has-deepseek-improved-the-transformer-architecture</li>
<li>A Review on the Evolvement of Load Balancing Strategy in MoE LLMs: Pitfalls and Lessons, https://normaluhr.github.io/2025/01/15/moe-load-balancing/</li>
<li>How MoE Models Actually Learn: A Guide to Auxiliary Losses and Expert Balancing, https://medium.com/@chris.p.hughes10/how-moe-models-actually-learn-a-guide-to-auxiliary-losses-and-expert-balancing-293084e3f600</li>
<li>deepseek-ai/DeepSeek-V3 - Hugging Face, https://huggingface.co/deepseek-ai/DeepSeek-V3</li>
<li>DeepSeek-V3 Explained 3: Auxiliary-Loss-Free Load Balancing | by Shirley Li - AI Advances, https://ai.gopubby.com/deepseek-v3-explained-3-auxiliary-loss-free-load-balancing-4beeb734ab1f</li>
<li>DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/pdf/2412.19437</li>
<li>DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training | by Yugen.ai - Medium, https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c</li>
<li>DeepSeek-V3 Technical Report - The VITALab website, https://vitalab.github.io/article/2025/02/11/DeepSeekV3.html</li>
<li>DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/html/2412.19437v2</li>
<li>DeepSeek-V3 Technical Report - arXiv, https://arxiv.org/html/2412.19437v1</li>
<li>FINETUNING MOE LLMS WITH CONDENSER EXPERTS - OpenReview, https://openreview.net/pdf/eff4bb1443da7f79fb47744b9d51582950bfd38f.pdf</li>
<li>Primers • DeepSeek V3 - aman.ai, https://aman.ai/primers/ai/deepseekV3/</li>
<li>DeepSeek and the Power of Mixture of Experts (MoE) - DEV Community, https://dev.to/sayed_ali_alkamel/deepseek-and-the-power-of-mixture-of-experts-moe-ham</li>
<li>DeepSeek-V3.2: Open Source AI Matches GPT-5 and Gemini 3 at 10× Lower Cost - Introl, https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage</li>
<li>DeepSeek-V3 Blog 2: The Smartest Use of Mixture of Experts (MoE) in AI Yet - Medium, https://medium.com/@prashantsahdev/deepseek-v3-blog-2-the-smartest-use-of-mixture-of-experts-moe-in-ai-yet-f72227d3a0e3</li>
<li>deepseek-ai/EPLB: Expert Parallelism Load Balancer - GitHub, https://github.com/deepseek-ai/EPLB</li>
<li>Day 6 - One More Thing: DeepSeek-V3/R1 Inference System Overview - GitHub, https://github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/day_6_one_more_thing_deepseekV3R1_inference_system_overview.md</li>
<li>Aman’s AI Journal • Primers • DeepSeek-R1, https://aman.ai/primers/ai/deepseek-R1/</li>
<li>Liyue Zhang’s research works - ResearchGate, https://www.researchgate.net/scientific-contributions/Liyue-Zhang-2278739018</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>