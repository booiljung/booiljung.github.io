<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:12.2 AudioLM - 오디오의 의미적 토큰화와 생성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>12.2 AudioLM - 오디오의 의미적 토큰화와 생성</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>12.2 AudioLM - 오디오의 의미적 토큰화와 생성</span></nav>
                </div>
            </header>
            <article>
                <h1>12.2 AudioLM - 오디오의 의미적 토큰화와 생성</h1>
<p>2025-12-23, G30DR</p>
<p>오디오 생성 분야, 특히 음성 및 음악 합성 기술은 지난 수년 동안 비약적인 발전을 거듭해왔다. 초기 파라메트릭(parametric) 방식의 통계적 모델링에서부터 WaveNet과 같은 신경망 기반의 파형 생성 모델에 이르기까지, 연구자들은 오디오 신호의 복잡성을 포착하기 위해 부단히 노력해왔다. 그러나 이러한 기존 접근법들은 대부분 국소적인(local) 음향 디테일을 재현하는 데에는 성공적이었으나, 문장의 구문 구조나 음악의 멜로디 전개와 같은 장기적인(long-term) 일관성을 유지하는 데에는 한계를 보여왔다. 이러한 배경에서 등장한 AudioLM은 오디오 생성 문제를 일종의 언어 모델링(Language Modeling) 과제로 재정의함으로써 새로운 패러다임을 제시한다. 본 절에서는 AudioLM이 어떻게 연속적인 오디오 파형을 이산적인(discrete) 토큰으로 변환하고, 이를 통해 의미적(semantic) 정보와 음향적(acoustic) 정보를 계층적으로 결합하여 고품질의 오디오를 생성하는지, 그 기술적 메커니즘을 심층적으로 분석한다.</p>
<h2>1.  오디오 언어 모델링의 이론적 배경과 난제</h2>
<p>자연어 처리(NLP) 분야에서 트랜스포머(Transformer) 기반의 대규모 언어 모델(LLM)이 보여준 혁신적인 성과는 오디오 연구자들에게 중요한 영감을 주었다. GPT-3와 같은 모델은 텍스트를 이산적인 토큰 시퀀스로 처리하며, 이를 통해 문맥을 이해하고 자연스러운 문장을 생성해낸다. AudioLM은 이러한 접근 방식을 오디오 도메인에 적용하고자 하는 시도이다. 즉, 오디오를 텍스트와 유사한 이산 토큰의 시퀀스로 변환할 수 있다면, 검증된 트랜스포머 아키텍처를 활용하여 장기적인 의존성(long-term dependency)을 효과적으로 학습할 수 있을 것이라는 가설에서 출발한다.1</p>
<h3>1.1 연속 신호의 이산화와 정보 밀도의 딜레마</h3>
<p>그러나 오디오 데이터를 언어 모델링 프레임워크에 직접 적용하는 것은 결코 단순한 문제가 아니다. 텍스트 데이터와 오디오 데이터 사이에는 정보 밀도와 시간 해상도 측면에서 근본적인 차이가 존재하기 때문이다.</p>
<p>첫째, **데이터 속도(Data Rate)**의 문제다. 텍스트의 경우, 하나의 문장은 수십 개의 문자나 단어 토큰으로 표현될 수 있다. 반면, 오디오는 일반적으로 16kHz 이상의 샘플링 레이트를 가지며, 이는 1초 분량의 오디오조차 16,000개 이상의 실수값(floating-point values)으로 구성됨을 의미한다. 이러한 고차원 데이터를 트랜스포머 모델에 직접 입력하는 것은 계산 복잡도 측면에서 불가능에 가까우며, 모델이 수만 단계 이전에 발생한 정보(예: 문장의 주어)를 기억하여 현재 시점(예: 서술어)에 반영하는 것을 어렵게 만든다.2</p>
<p>둘째, <strong>정보의 다층성(Multi-layered Information)</strong> 문제다. 텍스트는 주로 의미적 정보(semantic content)를 전달하는 데 집중되어 있는 반면, 오디오 신호는 언어적 의미 외에도 화자의 신원(speaker identity), 감정 상태, 운율(prosody), 배경 잡음, 녹음 환경 등 다양한 층위의 정보가 하나의 파형 안에 얽혀 있다. 단순한 양자화(quantization)를 통해 오디오를 토큰화할 경우, 이러한 미묘한 정보들이 손실되거나 혹은 반대로 불필요한 노이즈까지 과도하게 모델링하게 되어 생성 품질을 저하시키는 원인이 된다.</p>
<p>AudioLM은 이러한 난제를 해결하기 위해 <strong>하이브리드 토큰화(Hybrid Tokenization)</strong> 전략을 채택한다. 이는 오디오 신호를 단일 종류의 토큰으로 표현하는 대신, ’의미’를 담당하는 토큰과 ’음향’을 담당하는 토큰으로 분리하여 처리하는 방식이다.</p>
<h2>2.  의미적 토큰화 (Semantic Tokenization): 구조와 내용의 추상화</h2>
<p>AudioLM 아키텍처의 첫 번째 핵심 기둥은 **의미적 토큰(Semantic Tokens)**의 추출이다. 이 과정의 목표는 오디오 신호에서 화자의 목소리 톤이나 배경 잡음과 같은 표면적인 음향 디테일을 제거하고, 언어적 내용이나 음악적 구조와 같은 고수준의 추상적 정보를 정제해내는 것이다. 이를 통해 모델은 매우 낮은 비트레이트로 오디오의 ’내용’을 표현할 수 있게 되며, 결과적으로 트랜스포머 모델이 학습해야 할 시퀀스의 길이를 획기적으로 단축시킨다.3</p>
<h3>2.1 w2v-BERT: 자기지도 학습을 통한 특징 추출</h3>
<p>의미적 토큰을 추출하기 위해 AudioLM은 <strong>w2v-BERT</strong>라는 대규모 자기지도 학습(Self-Supervised Learning) 모델을 활용한다. w2v-BERT는 라벨링된 텍스트 데이터 없이 방대한 양의 오디오 데이터만으로 학습된 모델로, 오디오 신호 내의 잠재적인 구조를 파악하는 데 탁월한 성능을 보인다.</p>
<p>w2v-BERT는 두 가지 주요 학습 목표를 결합하여 훈련된다:</p>
<ol>
<li><strong>마스크된 언어 모델링(Masked Language Modeling, MLM)</strong>: 입력 오디오의 일부 구간을 마스킹하고, 주변 문맥을 통해 마스크된 부분의 특징을 예측하도록 한다. 이는 모델이 양방향 문맥(bidirectional context)을 이해하도록 돕는다.</li>
<li><strong>대조 학습(Contrastive Learning)</strong>: 올바른 양자화된 표현과 잘못된 표현을 구별하도록 하여, 모델이 불변하는 특징(invariant features)을 학습하게 한다.</li>
</ol>
<h3>2.2 레이어 선택과 정규화의 중요성</h3>
<p>AudioLM 연구진은 w2v-BERT의 최종 출력이 아닌 **중간 레이어(Intermediate Layer)**의 활성화 값(activation)이 오디오 생성에 필요한 의미적 정보를 가장 잘 보존하고 있음을 발견했다. 구체적으로는 w2v-BERT XL 모델의 MLM 모듈 내 <strong>7번째 레이어</strong>가 사용된다. 하위 레이어는 지나치게 음향적인 특징에 치우쳐 있고, 상위 레이어는 사전 학습 과제에 과도하게 특화되어 정보 손실이 발생할 수 있기 때문이다. 7번째 레이어는 음소 판별력(phonetic discriminability)과 재구성 품질 사이의 균형이 가장 이상적인 지점으로 확인되었다.3</p>
<p>추출된 연속적인 벡터값들은 그대로 토큰으로 사용할 수 없다. 따라서 이산화를 수행하기 전에 <strong>정규화(Normalization)</strong> 과정이 선행된다. 각 차원이 평균 0, 분산 1을 갖도록 정규화함으로써, 이후 수행될 클러스터링 과정에서 특정 차원이 지배적인 영향을 미치는 것을 방지하고, 임베딩 공간 내에서 음소 간의 거리 관계를 명확히 한다. 연구 결과에 따르면, 이 정규화 과정은 의미적 토큰의 음소 판별력을 비약적으로 향상시키는 핵심 요소이다.3</p>
<h3>2.3 K-평균 클러스터링을 통한 이산화</h3>
<p>정규화된 임베딩 벡터들은 <strong>K-평균 클러스터링(K-means Clustering)</strong> 알고리즘을 통해 이산 토큰으로 변환된다. AudioLM은 클러스터의 개수 <span class="math math-inline">K</span>를 <strong>1024</strong>로 설정한다. 즉, 오디오의 의미적 상태 공간을 1024개의 대표적인 프로토타입(centroid)으로 근사하는 것이다.</p>
<p>이 과정에서 시간 해상도의 축소(downsampling)가 함께 일어난다. w2v-BERT는 입력 오디오를 처리하면서 시간 축으로 압축을 수행하므로, 최종적으로 생성되는 의미적 토큰은 약 <strong>25Hz</strong>의 속도를 갖는다. 이는 40ms마다 하나의 토큰이 생성됨을 의미하며, 원본 오디오(16kHz) 대비 약 640배의 압축률을 달성한다. 이렇게 생성된 의미적 토큰 시퀀스 <span class="math math-inline">z = (z_1, z_2, \dots, z_{T_S})</span>는 오디오의 전역적인 구조와 내용을 담고 있지만, 이를 통해 오디오를 다시 복원하면 마치 로봇이 말하는 것과 같은 매우 낮은 품질의 소리만 얻을 수 있다. 따라서 고품질 오디오 생성을 위해서는 음향적 디테일을 담당할 별도의 토큰이 필요하다.3</p>
<h2>3.  음향적 토큰화 (Acoustic Tokenization): 고충실도 재구성을 위한 전략</h2>
<p>의미적 토큰이 ‘무엇을(What)’ 말하는지에 대한 뼈대라면, **음향적 토큰(Acoustic Tokens)**은 그 뼈대 위에 살을 입히고 ‘어떻게(How)’ 들리는지를 결정하는 피부와 근육에 해당한다. AudioLM은 고품질의 음향 합성을 위해 신경망 오디오 코덱인 <strong>SoundStream</strong>을 채택하여 음향적 토큰을 생성한다.5</p>
<h3>3.1 SoundStream과 잔차 벡터 양자화 (RVQ)</h3>
<p>SoundStream은 인코더(Encoder), 디코더(Decoder), 그리고 잔차 벡터 양자화기(Residual Vector Quantizer, RVQ)로 구성된 오토인코더 기반의 신경망 코덱이다. 이 모델은 입력 파형을 압축된 잠재 표현(latent representation)으로 변환하고, 이를 다시 고품질의 파형으로 복원하도록 종단간(end-to-end) 학습된다. 여기서 핵심은 <strong>잔차 벡터 양자화(RVQ)</strong> 기술이다.</p>
<p>일반적인 벡터 양자화(VQ)가 단일 코드북을 사용하여 벡터를 근사하는 것과 달리, RVQ는 여러 개의 양자화기(Quantizer)를 계층적으로 배치하여 오차를 점진적으로 줄여나가는 방식을 취한다.</p>
<ol>
<li><strong>계층적 정제 과정</strong>:</li>
</ol>
<ul>
<li>첫 번째 양자화기(<span class="math math-inline">q=1</span>)는 입력 벡터를 가장 거칠게 근사하여 기본 정보를 캡처한다.</li>
<li>두 번째 양자화기(<span class="math math-inline">q=2</span>)는 원본 벡터와 첫 번째 근사값 간의 차이(residual)를 입력으로 받아 이를 양자화한다.</li>
<li>이 과정을 <span class="math math-inline">Q</span>개의 양자화기에 대해 반복 수행한다.</li>
<li>최종적으로 디코더는 모든 양자화기에서 출력된 벡터들의 합을 사용하여 오디오를 재구성한다.</li>
</ul>
<p>이러한 구조는 각 시간 단계(time step)마다 <span class="math math-inline">Q</span>개의 토큰이 생성됨을 의미한다. SoundStream은 약 <strong>50Hz</strong>의 시간 해상도(20ms마다 1 프레임)로 토큰을 생성하는데, 이는 의미적 토큰(25Hz)보다 2배 높은 해상도이다. 이는 음향적 디테일이 의미적 정보보다 더 빠른 시간적 변화를 담고 있음을 반영한다.4</p>
<h3>3.2 거친 음향 토큰과 미세 음향 토큰의 분리</h3>
<p>AudioLM은 RVQ의 계층적 특성을 모델링 전략에 적극적으로 활용한다. RVQ의 앞쪽 레이어(초기 양자화기들)는 오디오의 에너지, 기본적인 스펙트럼 포락선(spectral envelope), 화자의 전반적인 음색 등 거시적인 음향 정보를 담고 있다. 반면, 뒤쪽 레이어(후기 양자화기들)는 고주파 성분, 미세한 떨림, 양자화 잡음 보정 등 청각적 품질을 결정짓는 미세 정보를 담당한다.</p>
<p>이를 바탕으로 AudioLM은 음향적 토큰을 다시 두 그룹으로 나눈다:</p>
<ol>
<li><strong>거친 음향 토큰 (Coarse Acoustic Tokens)</strong>: RVQ의 첫 번째 레이어부터 <span class="math math-inline">Q&#39;</span>번째 레이어까지의 토큰들. (<span class="math math-inline">1 \le q \le Q&#39;</span>)</li>
<li><strong>미세 음향 토큰 (Fine Acoustic Tokens)</strong>: 나머지 <span class="math math-inline">Q&#39;+1</span>번째 레이어부터 <span class="math math-inline">Q</span>번째 레이어까지의 토큰들. (<span class="math math-inline">Q&#39; &lt; q \le Q</span>)</li>
</ol>
<p>일반적으로 <span class="math math-inline">Q&#39;=3</span> 또는 <span class="math math-inline">Q&#39;=4</span> 정도로 설정되며, 전체 양자화기 수 <span class="math math-inline">Q</span>는 8 또는 12로 설정된다. 이 분리 전략은 생성 모델이 한 번에 학습해야 할 정보량을 조절하고, ’의미 <span class="math math-inline">\rightarrow</span> 거친 음향 <span class="math math-inline">\rightarrow</span> 미세 음향’으로 이어지는 자연스러운 생성 흐름을 만들어낸다.3</p>
<table><thead><tr><th><strong>토큰 유형</strong></th><th><strong>소스 모델</strong></th><th><strong>시간 해상도</strong></th><th><strong>주요 역할</strong></th><th><strong>AudioLM 단계</strong></th></tr></thead><tbody>
<tr><td><strong>의미적 토큰</strong> (<span class="math math-inline">z</span>)</td><td>w2v-BERT</td><td>25 Hz</td><td>내용, 구문, 전역적 구조</td><td>1단계 (Semantic Modeling)</td></tr>
<tr><td><strong>거친 음향 토큰</strong> (<span class="math math-inline">y_{coarse}</span>)</td><td>SoundStream (Layer <span class="math math-inline">1 \sim Q&#39;</span>)</td><td>50 Hz</td><td>화자 특성, 운율, 환경음</td><td>2단계 (Coarse Acoustic Modeling)</td></tr>
<tr><td><strong>미세 음향 토큰</strong> (<span class="math math-inline">y_{fine}</span>)</td><td>SoundStream (Layer <span class="math math-inline">Q&#39;+1 \sim Q</span>)</td><td>50 Hz</td><td>고음질 디테일, 파형 정제</td><td>3단계 (Fine Acoustic Modeling)</td></tr>
</tbody></table>
<h2>4.  계층적 트랜스포머 아키텍처와 생성 파이프라인</h2>
<p>AudioLM은 단일 모델이 모든 토큰을 한 번에 예측하는 방식을 지양하고, <strong>3단계 계층적 모델링(Three-Stage Hierarchical Modeling)</strong> 접근법을 채택한다. 각 단계는 독립적인 디코더 전용(Decoder-only) 트랜스포머로 구성되며, 자기회귀적(Autoregressive) 방식으로 다음 토큰을 예측하도록 학습된다. 이러한 분할 정복(Divide-and-Conquer) 방식은 긴 시퀀스를 효과적으로 다루고, 각 단계에서 필요한 정보의 추상화 수준을 최적화한다.3</p>
<h3>4.1 단계: 의미적 모델링 (Semantic Modeling)</h3>
<p>첫 번째 단계는 순수하게 의미적 토큰 <span class="math math-inline">z</span>의 시퀀스를 생성하는 과정이다. 이 단계의 트랜스포머는 이전 시점까지의 의미적 토큰 <span class="math math-inline">z_{&lt;t}</span>를 입력으로 받아, 현재 시점의 토큰 <span class="math math-inline">z_t</span>의 확률 분포 <span class="math math-inline">p(z_t | z_{&lt;t})</span>를 예측한다.</p>
<p>이 과정은 텍스트 기반의 언어 모델(GPT 등)과 매우 유사하다. 오디오의 ’음향적 껍데기’가 제거된 상태에서 순수한 ’내용적 구조’만을 학습하기 때문에, 모델은 매우 긴 시간 범위(수십 초 이상)에 걸친 문맥을 유지할 수 있다. 예를 들어, 문장의 주어와 술어 사이의 호응이나, 음악에서의 멜로디 반복 구조 등이 이 단계에서 결정된다. 이 모델은 화자가 누구인지, 음질이 어떠한지에 대해서는 전혀 고려하지 않는다.2</p>
<h3>4.2 단계: 거친 음향 모델링 (Coarse Acoustic Modeling)</h3>
<p>두 번째 단계는 추상적인 의미적 토큰을 구체적인 소리의 형태로 변환하는 가교 역할을 한다. 이 단계의 트랜스포머는 의미적 토큰 시퀀스 전체(<span class="math math-inline">z</span>)를 조건(conditioning)으로 받아, 거친 음향 토큰 시퀀스 <span class="math math-inline">y_{coarse}</span>를 자기회귀적으로 생성한다.</p>
<p>수식적으로는 <span class="math math-inline">p(y_t^q | z, y_{&lt;t}^{\le Q&#39;}, y_{t}^{&lt;q})</span>를 모델링한다. 여기서 주목할 점은, 트랜스포머가 의미적 토큰 시퀀스와 거친 음향 토큰 시퀀스를 **연결(concatenation)**하여 입력으로 사용한다는 것이다. 즉, 입력 시퀀스는 [의미적 토큰들… 거친 음향 토큰들]의 형태를 띤다.</p>
<p>이 단계에서 비로소 화자의 아이덴티티(목소리 톤), 말하기 속도, 감정 실린 억양(prosody), 녹음 환경의 특성 등이 결정된다. 모델은 주어진 의미적 내용(<span class="math math-inline">z</span>)을 유지하면서, 이를 특정 화자의 목소리와 스타일로 표현하는 방법을 학습한다. 생성된 거친 음향 토큰만으로도 내용을 알아들을 수 있고 화자 구분이 가능하지만, 약간의 기계적인 잡음이 섞여 있거나 소리가 다소 거칠게 들릴 수 있다.5</p>
<h3>4.3 단계: 미세 음향 모델링 (Fine Acoustic Modeling)</h3>
<p>마지막 단계는 오디오의 품질을 스튜디오급으로 끌어올리는 정제 과정이다. 이 단계에서는 2단계에서 생성된 거친 음향 토큰 <span class="math math-inline">y_{coarse}</span>를 조건으로 하여, 나머지 미세 음향 토큰 <span class="math math-inline">y_{fine}</span>을 예측한다.</p>
<p>목표 확률 분포는 <span class="math math-inline">p(y_t^q | y^{\le Q&#39;}, y_{&lt;t}^{&gt;Q&#39;}, y_{t}^{&lt;q})</span>이다. 흥미로운 점은, 이 단계에서는 의미적 토큰 <span class="math math-inline">z</span>를 더 이상 조건으로 사용하지 않는다는 것이다. 거친 음향 토큰 <span class="math math-inline">y_{coarse}</span> 안에 이미 언어적 내용과 화자 정보, 운율 등이 충분히 포함되어 있기 때문에, 미세 모델링 단계에서는 오로지 음향적 디테일을 채우는 데에만 집중하면 된다. 이 단계는 스펙트럼의 고주파 대역을 복원하고, 양자화로 인해 발생한 아티팩트(artifact)를 제거하여 매끄럽고 자연스러운 파형을 완성한다.3</p>
<h3>4.4 시퀀스 평탄화 (Sequence Flattening)와 모델링 기법</h3>
<p>SoundStream의 출력은 시간 축(<span class="math math-inline">T</span>)과 양자화기 깊이 축(<span class="math math-inline">Q</span>)을 가진 2차원 구조이다. 이를 1차원 시퀀스를 처리하는 트랜스포머에 입력하기 위해, AudioLM은 행 우선 평탄화(Row-Major Flattening) 방식을 사용한다. 즉, 시간 순서(<span class="math math-inline">t</span>)를 주축으로 하고, 같은 시간 내에서는 양자화기 순서(<span class="math math-inline">q</span>)대로 토큰을 나열한다:</p>
<p><span class="math math-inline">(y_1^1, y_1^2, \dots, y_1^{Q&#39;}, y_2^1, y_2^2, \dots)</span></p>
<p>이러한 순서는 모델이 시간적 흐름을 따르면서도, 각 프레임 내에서 상위 비트(거친 정보)가 하위 비트(미세 정보)를 예측하는 데 도움을 주는 인과적(Causal) 관계를 자연스럽게 학습하도록 유도한다.</p>
<h2>5.  학습 및 추론 전략</h2>
<p>AudioLM의 학습은 각 단계별로 독립적으로 수행된다. 이는 거대 모델을 한 번에 학습시키는 데 따르는 메모리 부담을 줄이고, 각 단계의 최적화를 용이하게 한다.</p>
<h3>5.1 교사 강요 (Teacher Forcing) 기반 학습</h3>
<p>모든 단계의 트랜스포머는 정답 토큰(Ground Truth)이 주어졌을 때 다음 토큰을 예측하는 교사 강요 방식으로 학습된다.</p>
<ol>
<li><strong>데이터 준비</strong>: 학습 데이터셋의 오디오 파형으로부터 w2v-BERT를 통해 정답 의미적 토큰 <span class="math math-inline">z</span>를 추출하고, SoundStream을 통해 정답 음향적 토큰 <span class="math math-inline">y</span>를 추출한다.</li>
<li><strong>손실 함수</strong>: 각 트랜스포머는 예측된 토큰 분포와 실제 토큰 사이의 크로스 엔트로피(Cross-Entropy) 손실을 최소화하도록 최적화된다.</li>
</ol>
<h3>5.2 추론 시의 샘플링 (Sampling at Inference)</h3>
<p>학습이 완료된 후 새로운 오디오를 생성할 때는 3단계 모델을 순차적으로 실행한다.</p>
<ol>
<li><strong>의미적 생성</strong>: 먼저 1단계 모델을 사용하여 원하는 길이만큼 의미적 토큰 시퀀스 <span class="math math-inline">\hat{z}</span>를 생성한다. 이때 **온도 스케일링(Temperature Scaling)**을 적용하여 생성의 다양성을 조절한다. 온도가 너무 낮으면 반복적인 패턴이 나올 수 있고, 너무 높으면 횡설수설하는 내용이 나올 수 있으므로 적절한 값의 설정이 중요하다.</li>
<li><strong>거친 음향 생성</strong>: 생성된 <span class="math math-inline">\hat{z}</span>를 조건으로 2단계 모델을 구동하여 거친 음향 토큰 <span class="math math-inline">\hat{y}_{coarse}</span>를 샘플링한다. 이 과정에서 화자의 스타일이 결정된다.</li>
<li><strong>미세 음향 생성</strong>: 마지막으로 <span class="math math-inline">\hat{y}*{coarse}</span>를 조건으로 3단계 모델을 통해 미세 음향 토큰 <span class="math math-inline">\hat{y}*{fine}</span>을 생성한다.</li>
<li><strong>파형 복원</strong>: 최종적으로 결합된 음향 토큰 시퀀스 <span class="math math-inline">\hat{y}</span>를 SoundStream 디코더에 입력하여 최종 오디오 파형을 합성한다.</li>
</ol>
<h2>6.  실험적 검증: 성능과 품질 분석</h2>
<p>AudioLM의 성능은 주로 음성 연속 생성(Speech Continuation)과 피아노 음악 생성 과제를 통해 검증되었다. 연구진은 객관적 지표와 주관적 청취 테스트를 모두 활용하여 모델의 우수성을 입증했다.</p>
<h3>6.1 음성 연속 생성 (Speech Continuation)</h3>
<p>짧은 음성 프롬프트(예: 3초)가 주어졌을 때, AudioLM은 의미적으로 일관성 있고 문법적으로 완벽한 문장을 생성하며 자연스럽게 말을 이어간다. 특히 주목할 점은 화자 복제(Speaker Cloning) 능력이다. 학습 과정에서 본 적 없는 화자의 목소리로 프롬프트가 주어져도, 모델은 그 화자의 음색, 억양, 말하기 속도, 심지어 숨소리까지 모방하여 일관되게 유지한다.</p>
<p>이는 2단계(거친 음향 모델링)에서 프롬프트의 음향적 특성을 강력하게 조건화했기 때문이다. 실험 결과, AudioLM이 생성한 음성은 실제 사람의 음성과 구별하기 어려울 정도로 높은 자연스러움을 보였으며, 이는 기존의 텍스트 기반 TTS(Text-to-Speech) 시스템이 텍스트 입력에 의존해야 했던 한계를 뛰어넘는 성과이다.1</p>
<h3>6.2 객관적 평가 지표</h3>
<p>연구진은 토큰화 방식의 유효성을 검증하기 위해 다음과 같은 지표들을 사용했다:</p>
<ul>
<li><strong>ABX 에러율</strong>: 음소 판별력을 측정하는 지표로, 낮을수록 좋다. w2v-BERT 기반의 의미적 토큰은 매우 낮은 ABX 에러율을 보여, 언어적 내용을 명확히 구별할 수 있음을 증명했다.</li>
<li><strong>sWUGGY &amp; sBLIMP</strong>: 생성된 오디오의 언어적, 구문적 적절성을 평가하는 지표이다. 의미적 토큰을 사용한 AudioLM은 이 지표들에서 높은 점수를 기록하여, 텍스트 없이 오디오만으로도 언어 모델링이 가능함을 입증했다.3</li>
</ul>
<h3>6.3 피아노 음악 생성으로의 확장</h3>
<p>AudioLM의 아키텍처는 음성에만 국한되지 않는다. 연구진은 동일한 구조를 피아노 음악 데이터셋에 적용하여, 프롬프트로 주어진 멜로디와 화성을 자연스럽게 이어가는 음악 생성에도 성공했다. 이는 w2v-BERT가 추출하는 의미적 토큰이 언어의 문법뿐만 아니라, 음악의 리듬이나 화성 진행과 같은 구조적 정보도 포착할 수 있음을 시사한다. MIDI와 같은 기호적(symbolic) 표현 없이, 순수하게 오디오 파형만으로 음악의 구조를 학습하고 생성했다는 점은 생성형 AI 분야에서 큰 의미를 갖는다.5</p>
<h2>7.  AudioLM의 유산과 파급 효과</h2>
<p>AudioLM은 오디오 생성 분야에 ‘언어 모델링’ 패러다임을 성공적으로 이식한 이정표적인 모델이다. 복잡한 오디오 신호를 **의미(Semantic)**와 **음향(Acoustic)**이라는 두 가지 층위로 분리하고, 이를 계층적 트랜스포머 아키텍처를 통해 통합함으로써, 기존 모델들이 겪었던 ’구조적 일관성’과 ‘음향적 충실도’ 간의 트레이드오프(trade-off)를 해결했다.</p>
<p>이 모델이 남긴 기술적 유산은 이후의 연구들로 이어진다:</p>
<ol>
<li><strong>MusicLM</strong>: AudioLM의 계층적 토큰화 전략은 텍스트를 입력받아 음악을 생성하는 MusicLM 모델의 기초가 되었다. MusicLM은 의미적 토큰 생성 단계를 텍스트-오디오 임베딩(MuLan)을 활용한 조건부 생성으로 대체하여, 텍스트 설명에 부합하는 고품질 음악을 생성한다.9</li>
<li><strong>AudioPaLM</strong>: AudioLM의 구조를 더욱 확장하여, 텍스트 토큰과 오디오 토큰을 하나의 어휘 사전으로 통합한 다중모달(Multimodal) 모델인 AudioPaLM이 등장했다. 이는 음성 인식(ASR), 텍스트 번역, 음성 합성(TTS), 음성-음성 번역(S2ST)을 단일 모델 내에서 수행할 수 있게 하였으며, 특히 텍스트 기반 LLM의 사전 학습된 지식을 음성 작업으로 전이시키는 데 성공했다.10</li>
<li><strong>SoundStorm</strong>: AudioLM의 유일한 단점이었던 느린 추론 속도(자기회귀적 생성으로 인한 순차적 처리)를 개선하기 위해, 병렬 디코딩이 가능한 SoundStorm 모델이 제안되었다. SoundStorm은 AudioLM의 의미적 토큰 생성 단계는 유지하되, 음향적 토큰 생성 단계를 비자기회귀적(Non-autoregressive) 방식으로 대체하여 생성 속도를 약 100배 가속화했다.11</li>
</ol>
<p>결론적으로, 12.2절에서 다룬 AudioLM의 방법론은 단순한 모델 아키텍처를 넘어, 연속적인 오디오 데이터를 어떻게 추상화하고 구조화해야 하는지에 대한 근본적인 통찰을 제공한다. 이는 오디오 지능(Audio Intelligence)이 단순히 소리를 듣고 분석하는 것을 넘어, 소리의 구조를 이해하고 창의적으로 생성하는 단계로 진화했음을 알리는 신호탄이라 할 수 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>AudioLM: a Language Modeling Approach to Audio Generation | Request PDF - ResearchGate, https://www.researchgate.net/publication/363363172_AudioLM_a_Language_Modeling_Approach_to_Audio_Generation</li>
<li>AudioLM: a Language Modeling Approach to Audio Generation - Google Research, https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/</li>
<li>AudioLM: a Language Modeling Approach to Audio Generation - David Grangier, https://david.grangier.info/papers/2023/audio-lm-generation.pdf</li>
<li>AudioLM: a Language Modeling Approach to Audio Generation, https://home.cse.ust.hk/~cktang/csit6000s/Password_Only/lec23-csit.pdf</li>
<li>AudioLM: a Language Modeling Approach to … - David Grangier, https://arxiv.org/abs/2209.03143</li>
<li>SoundStream and Transformer num units · Issue #110 · lucidrains/audiolm-pytorch - GitHub, https://github.com/lucidrains/audiolm-pytorch/issues/110</li>
<li>AudioLM, https://google-research.github.io/seanet/audiolm/examples/</li>
<li>Sounding The Secrets Of AudioLM | Shaped Blog, https://www.shaped.ai/blog/sounding-the-secrets-of-audiolm</li>
<li>Music LM | AITU, https://www.aitu.group/blog/music-lm</li>
<li>AudioPaLM: A Large Language Model That Can Speak and Listen - arXiv, https://arxiv.org/pdf/2306.12925</li>
<li>SoundStorm: Efficient parallel audio generation - Google Research, https://research.google/blog/soundstorm-efficient-parallel-audio-generation/</li>
<li>SoundStorm: Efficient Parallel Audio Generation - arXiv, https://arxiv.org/pdf/2305.09636</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>