<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:10.3 PEFT와 LoRA/QLoRA - 거대 모델의 효율적 미세 조정</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>10.3 PEFT와 LoRA/QLoRA - 거대 모델의 효율적 미세 조정</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>10.3 PEFT와 LoRA/QLoRA - 거대 모델의 효율적 미세 조정</span></nav>
                </div>
            </header>
            <article>
                <h1>10.3 PEFT와 LoRA/QLoRA - 거대 모델의 효율적 미세 조정</h1>
<p>2025-12-22, G30DR</p>
<p>인공지능 연구의 흐름은 트랜스포머(Transformer) 아키텍처의 등장 이후, 모델의 크기를 기하급수적으로 키우는 ’거대화(Scaling)’의 시대로 진입했다. 수십억(Billions)을 넘어 수천억 개의 파라미터를 가진 거대 언어 모델(Large Language Model, LLM)들은 일반적인 자연어 이해와 생성 능력에서 타의 추종을 불허하는 성능을 보여주며, 이른바 ’트랜스포머 특이점(Transformer Singularity)’을 견인하고 있다. 그러나 이러한 확장은 필연적으로 모델의 최적화, 즉 미세 조정(Fine-Tuning) 단계에서 치명적인 병목 현상을 야기한다. 본 장에서는 이러한 병목을 해결하기 위해 등장한 파라미터 효율적 미세 조정(Parameter-Efficient Fine-Tuning, PEFT)의 기술적 원리와, 그중 사실상의 표준(De facto Standard)으로 자리 잡은 LoRA(Low-Rank Adaptation) 및 이를 양자화(Quantization)와 결합하여 극한의 효율성을 달성한 QLoRA의 메커니즘을 심층적으로 분석한다.</p>
<h2>1.  거대 모델 미세 조정의 난제와 PEFT의 부상</h2>
<h3>1.1  전체 미세 조정(Full Fine-Tuning)의 계산적 한계</h3>
<p>전통적인 전이 학습(Transfer Learning) 패러다임에서 가장 널리 사용되던 방식은 전체 미세 조정(Full Fine-Tuning, FFT)이었다. 이는 사전 학습된 모델의 모든 가중치 파라미터를 초기화 상태로 두고, 다운스트림 태스크(Downstream Task)의 데이터셋에 대해 역전파(Backpropagation)를 수행하여 모든 파라미터를 업데이트하는 방식이다. 그러나 모델의 크기가 GPT-3(175B) 수준에 도달하면서 FFT는 현실적으로 지속 불가능한 방식이 되었다.</p>
<p>FFT가 요구하는 계산 자원과 메모리는 단순히 모델 가중치를 저장하는 공간을 넘어선다. 훈련 과정에서는 가중치(Weights)뿐만 아니라, 각 가중치에 대한 기울기(Gradients)와 옵티마이저 상태(Optimizer States)를 메모리에 상주시켜야 한다.1 예를 들어, 175B 파라미터 모델을 Adam 옵티마이저로 훈련할 경우, 파라미터 자체는 16-bit 정밀도에서 약 350GB를 차지하지만, 옵티마이저 상태(모멘텀, 분산 등)는 파라미터당 추가적인 8~12바이트를 요구하여 총 1.2TB 이상의 VRAM을 필요로 하게 된다. 이는 최고 사양의 데이터센터 GPU인 NVIDIA A100(80GB) 수십 대를 연결해야만 가능한 수준으로, 대다수의 연구자와 기업에게는 접근 불가능한 장벽으로 작용한다.1</p>
<h3>1.2  저장 공간 및 배포의 비효율성 (The Deployment Bottleneck)</h3>
<p>메모리 문제 외에도, FFT는 배포 단계에서 심각한 비효율성을 초래한다. 만약 하나의 기업이 요약, 번역, 감성 분석, 코드 생성 등 100개의 서로 다른 태스크를 수행해야 한다면, FFT 방식으로는 175B 크기의 거대 모델 사본 100개를 별도로 저장하고 관리해야 한다. 이는 스토리지 비용을 폭발적으로 증가시킬 뿐만 아니라, 서비스 운영 시 모델 간의 전환(Switching)을 어렵게 만든다.1 각 태스크를 위해 수백 기가바이트의 모델을 메모리에 로드하고 언로드하는 과정은 실시간 서비스에서 허용될 수 없는 레이턴시(Latency)를 유발한다.</p>
<h3>1.3  PEFT의 핵심 철학: 동결과 적응</h3>
<p>이러한 배경에서 등장한 PEFT는 “거대 모델의 모든 파라미터를 수정할 필요가 있는가?“라는 근본적인 의문에서 출발한다. PEFT의 핵심 원리는 사전 학습된 거대 모델의 대부분의 파라미터를 <strong>동결(Freeze)</strong> 시키고, 소수의 추가적인 파라미터만을 학습하거나 기존 가중치의 극히 일부만을 업데이트하는 것이다.3</p>
<p>PEFT는 다음과 같은 패러다임 전환을 가져왔다:</p>
<ol>
<li><strong>메모리 효율성:</strong> 훈련 가능한 파라미터 수를 전체의 1% 미만, 때로는 0.01% 수준으로 줄임으로써, 옵티마이저 상태가 차지하는 메모리를 획기적으로 절감한다.1</li>
<li><strong>모듈형 아키텍처:</strong> 수 기가바이트에 달하는 전체 모델 대신, 수 메가바이트(MB) 수준의 어댑터 가중치만 저장하면 된다. 이는 하나의 거대 베이스 모델(Base Model)을 메모리에 상주시킨 채, 입력되는 태스크의 종류에 따라 가벼운 어댑터만 교체하며 추론하는 효율적인 서빙(Serving) 구조를 가능하게 한다.6</li>
<li><strong>성능 보존:</strong> 연구 결과에 따르면, 적절히 설계된 PEFT 방법론은 FFT와 대등하거나, 과적합(Overfitting)을 방지함으로써 오히려 더 우수한 일반화 성능을 보이기도 한다.1</li>
</ol>
<h2>2.  LoRA: 저랭크 적응(Low-Rank Adaptation)의 수학적 원리</h2>
<p>PEFT의 초기 접근법인 어댑터(Adapter) 방식이나 프롬프트 튜닝(Prompt Tuning) 등은 추론 시 레이턴시가 증가하거나(Sequence Length 제한), 최적화가 어렵다는 단점이 있었다. 이에 마이크로소프트 연구진이 제안한 **LoRA(Low-Rank Adaptation)**는 이러한 단점들을 극복하고 현재 가장 널리 사용되는 미세 조정 기법으로 자리 잡았다.1</p>
<h3>2.1  내재적 랭크 가설 (Intrinsic Rank Hypothesis)</h3>
<p>LoRA의 이론적 토대는 Aghajanyan 등의 연구(2020)에서 제시된 “내재적 랭크(Intrinsic Rank)” 가설에 있다. 딥러닝 모델, 특히 거대 언어 모델은 과잉 파라미터화(Over-parameterized)되어 있어, 실제 데이터의 특징을 학습하는 데 필요한 차원은 전체 파라미터 공간(Full Rank)보다 훨씬 낮은 차원의 부분 공간(Subspace)에 존재한다는 것이다.8</p>
<p>이 가설을 미세 조정에 적용하면, 사전 학습된 가중치 행렬 <span class="math math-inline">W_0</span>가 특정 태스크에 적응하기 위해 변화해야 하는 가중치 업데이트 양 <span class="math math-inline">\Delta W</span> 역시 <strong>저랭크(Low-Rank)</strong> 특성을 가질 것이라고 가정할 수 있다. 즉, <span class="math math-inline">W_0</span>가 <span class="math math-inline">d \times k</span> 차원을 가질 때, 업데이트 행렬 <span class="math math-inline">\Delta W</span>를 표현하기 위해 <span class="math math-inline">d \times k</span>의 자유도가 모두 필요한 것이 아니라, 훨씬 작은 랭크 <span class="math math-inline">r</span>로도 충분히 근사할 수 있다는 것이다.1</p>
<h3>2.2  행렬 분해와 재매개변수화 (Matrix Decomposition)</h3>
<p>LoRA는 가중치 업데이트 <span class="math math-inline">\Delta W</span>를 두 개의 작은 행렬의 곱 <span class="math math-inline">BA</span>로 분해하여 표현한다. 이를 통해 학습해야 할 파라미터 수를 획기적으로 줄인다.<br />
<span class="math math-display">
W = W_0 + \Delta W = W_0 + \frac{\alpha}{r} (BA)
</span><br />
이 수식의 구성 요소는 다음과 같이 정의된다:</p>
<ul>
<li><span class="math math-inline">W_0 \in \mathbb{R}^{d \times k}</span>: 사전 학습된 가중치 행렬로, 훈련 과정에서 <strong>동결(Frozen)</strong> 되어 기울기가 계산되지 않는다.</li>
<li><span class="math math-inline">B \in \mathbb{R}^{d \times r}</span>: 학습 가능한 저랭크 행렬.</li>
<li><span class="math math-inline">A \in \mathbb{R}^{r \times k}</span>: 학습 가능한 저랭크 행렬.</li>
<li><span class="math math-inline">r</span>: LoRA의 랭크(Rank). 일반적으로 <span class="math math-inline">r \ll \min(d, k)</span>이며, 1, 2, 4, 8, 64 등의 작은 정수값이 사용된다. GPT-3의 경우 <span class="math math-inline">r=1</span>이나 <span class="math math-inline">r=2</span>만으로도 충분한 성능을 낼 수 있음이 입증되었다.1</li>
<li><span class="math math-inline">\alpha</span>: 스케일링 팩터(Scaling Factor). 학습률과 유사하게 업데이트의 크기를 조절하는 상수로, 보통 <span class="math math-inline">r</span>과 같거나 <span class="math math-inline">2r</span> 정도로 설정하여 학습 안정성을 도모한다.9</li>
</ul>
<h3>2.3  초기화 전략과 학습 안정성</h3>
<p>LoRA의 성공적인 학습을 위해서는 행렬 <span class="math math-inline">A</span>와 <span class="math math-inline">B</span>의 초기화가 매우 중요하다.</p>
<ul>
<li><strong>행렬 A:</strong> 무작위 가우시안 분포(Random Gaussian Distribution) <span class="math math-inline">\mathcal{N}(0, \sigma^2)</span>로 초기화한다. 이는 모델이 다양한 방향으로 학습을 시작할 수 있는 다양성을 제공한다.1</li>
<li><strong>행렬 B:</strong> **0(Zero)**으로 초기화한다.</li>
</ul>
<p>이러한 초기화 전략(<span class="math math-inline">B=0</span>)은 훈련이 시작되는 시점(<span class="math math-inline">t=0</span>)에서 <span class="math math-inline">\Delta W = BA = 0</span>이 되도록 보장한다. 즉, 학습 초기에는 모델의 출력이 사전 학습된 베이스 모델의 출력과 완벽하게 동일하다. 이는 무작위 초기화로 인해 발생할 수 있는 초기 학습의 불안정성을 제거하고, 모델이 기존의 지식 위에서 점진적으로 태스크에 적응하도록 유도한다.1</p>
<h3>2.4  제로 레이턴시 추론 (Zero Latency Inference)</h3>
<p>기존의 어댑터 레이어 방식은 트랜스포머 블록 사이에 새로운 연산 층을 삽입하는 방식이었다. 이는 순전파(Forward Pass) 과정에서 연산 단계를 추가하므로, 필연적으로 추론 지연(Inference Latency)을 발생시킨다. 반면, LoRA는 선형 대수의 분배 법칙을 활용하여 추론 시 추가 비용을 완전히 제거할 수 있다.</p>
<p>입력 벡터 <span class="math math-inline">x</span>에 대해, LoRA가 적용된 층의 출력 <span class="math math-inline">h</span>는 다음과 같다:<br />
<span class="math math-display">
h = W_0x + BAx = (W_0 + BA)x
</span><br />
배포(Deployment) 단계에서는 학습된 <span class="math math-inline">BA</span>를 계산하여 원래의 가중치 <span class="math math-inline">W_0</span>에 더해버린 새로운 가중치 <span class="math math-inline">W&#39; = W_0 + BA</span>를 저장하고 사용한다. 이 <span class="math math-inline">W&#39;</span>는 원래 모델의 <span class="math math-inline">W_0</span>와 차원이 동일하므로, 모델 아키텍처의 변경 없이 그대로 사용할 수 있다. 따라서 LoRA를 적용한 모델은 베이스 모델과 비교하여 <strong>추론 속도의 저하가 전혀 없다(No Additional Inference Latency)</strong>.8</p>
<p>만약 다른 태스크로 전환해야 한다면, 현재의 <span class="math math-inline">BA</span>를 빼서 <span class="math math-inline">W_0</span>를 복원한 뒤, 새로운 태스크의 <span class="math math-inline">B&#39;A&#39;</span>를 더하는 연산만으로 즉각적인 태스크 스위칭이 가능하다. 이는 서비스 운영 관점에서 엄청난 유연성을 제공한다.8</p>
<h2>3.  QLoRA: 양자화와 결합된 극한의 효율성</h2>
<p>LoRA가 파라미터 수(연산량)를 줄이는 혁신이었다면, 2023년 워싱턴 대학교의 Tim Dettmers 등이 제안한 **QLoRA(Quantized LoRA)**는 메모리 사용량을 물리적 한계까지 압축하여 소비자용 GPU에서도 거대 모델의 미세 조정을 가능하게 한 기술적 도약이다.11 QLoRA는 LoRA의 구조 위에 4-bit 양자화 기술을 결합하여, 65B 파라미터 모델을 단일 48GB GPU에서 훈련시키는 성과를 달성했다.</p>
<p>QLoRA는 단순한 양자화 적용을 넘어, 성능 저하 없는 고압축 훈련을 위해 세 가지 핵심 기술을 도입했다: <strong>4-bit NormalFloat (NF4)</strong>, <strong>이중 양자화 (Double Quantization)</strong>, <strong>페이지드 옵티마이저 (Paged Optimizers)</strong>.</p>
<h3>3.1  4-bit NormalFloat (NF4): 정보 이론적 최적화</h3>
<p>일반적인 4비트 정수형(Int4) 양자화나 부동소수점(FP4) 양자화는 데이터가 균등 분포(Uniform Distribution)를 따른다고 가정하거나, 특정 구간에 데이터가 쏠리는 현상을 효과적으로 반영하지 못한다. 그러나 딥러닝 모델의 사전 학습된 가중치는 대부분 0을 중심으로 종 모양의 **정규 분포(Normal Distribution)**를 따르는 경향이 강하다.12 이는 중심 극한 정리(Central Limit Theorem)에 의해, 수많은 가중치 업데이트가 누적된 결과로 해석된다.</p>
<p>QLoRA 연구진은 이러한 가중치의 분포 특성에 착안하여, **NF4(4-bit NormalFloat)**라는 새로운 데이터 타입을 설계했다.</p>
<ul>
<li><strong>원리:</strong> NF4는 정규 분포 <span class="math math-inline">\mathcal{N}(0, 1)</span>의 누적 분포 함수(CDF)를 기반으로 하여, 4비트로 표현 가능한 16개의 값(<span class="math math-inline">2^4</span>)을 데이터의 확률 밀도에 비례하여 배치한다. 즉, 가중치가 밀집된 0 근처 영역에는 더 많은 표현 값을 할당하고, 빈도가 낮은 양 끝단(Outlier)에는 적은 값을 할당한다.11</li>
<li><strong>분위수 양자화(Quantile Quantization):</strong> 각 양자화 구간(Bin)에 동일한 수의 가중치 값이 들어가도록 구간을 설정하는 방식이다. 이는 정보 이론적으로 정규 분포 데이터를 4비트로 표현할 때 정보 손실을 최소화하는 최적의(Optimal) 방식이다.</li>
<li><strong>구현:</strong> QLoRA는 베이스 모델의 가중치를 NF4로 양자화하여 동결(Freeze)하고, LoRA 어댑터의 가중치만 고정밀도(BrainFloat16 등)로 유지하여 역전파 계산을 수행한다. 이 과정에서 발생하는 양자화 오차는 학습 가능한 LoRA 파라미터가 이를 보정하는 방향으로 학습되면서 상쇄된다.11</li>
</ul>
<h3>3.2  이중 양자화 (Double Quantization)</h3>
<p>양자화 기법은 가중치의 범위를 맞추기 위해 ’양자화 상수(Quantization Constant, 또는 Scale Parameter)’를 필요로 한다. 정밀도를 유지하기 위해 보통 64개의 파라미터 블록마다 하나의 32비트(FP32) 상수를 두는 블록 단위 양자화(Block-wise Quantization)를 사용한다. 그러나 수백억 개의 파라미터를 가진 거대 모델에서는 이 상수들이 차지하는 메모리조차 무시할 수 없는 수준이 된다.</p>
<ul>
<li><strong>메모리 병목:</strong> 블록 크기가 64일 때, 파라미터당 <span class="math math-inline">32/64 = 0.5</span> 비트의 추가 메모리가 양자화 상수를 저장하는 데 소요된다. 이는 4비트 양자화 모델에서 전체 메모리의 상당 부분을 차지하는 오버헤드이다.</li>
<li><strong>해결책:</strong> QLoRA는 **이중 양자화(DQ)**를 도입하여, 1차 양자화 상수들(<span class="math math-inline">c_1^{FP32}</span>)을 다시 한번 8비트로 양자화(<span class="math math-inline">c_2^{FP8}</span>)한다. 2차 양자화는 256개 블록 단위로 수행된다.</li>
<li><strong>효과:</strong> 이를 통해 파라미터당 메모리 오버헤드를 0.5비트에서 약 0.127비트(<span class="math math-inline">8/64 + 32/(64 \times 256)</span>)로 줄인다. 모델 전체로 보면 파라미터당 0.373비트의 절감 효과가 있으며, 65B 모델 기준으로는 약 3GB의 VRAM을 추가로 확보할 수 있게 해준다.11 이 작은 차이가 GPU 메모리 한계선(예: 24GB, 48GB) 근처에서는 모델을 로드할 수 있느냐 없느냐를 결정짓는 중요한 요소가 된다.</li>
</ul>
<h3>3.3  페이지드 옵티마이저 (Paged Optimizers)</h3>
<p>거대 모델 훈련 중 가장 빈번하게 발생하는 실패 원인은 GPU 메모리 부족(OOM, Out-Of-Memory) 오류이다. 특히 학습 중 긴 시퀀스(Long Sequence)가 입력되거나 배치 크기가 클 때, 활성화(Activation) 메모리가 일시적으로 급증하는 스파이크(Spike) 현상이 발생한다.</p>
<p>QLoRA는 NVIDIA의 통합 메모리(Unified Memory) 기능을 활용한 <strong>페이지드 옵티마이저</strong>를 도입하여 이 문제를 해결했다.</p>
<ul>
<li><strong>메커니즘:</strong> 운영체제의 가상 메모리 페이징 기법과 유사하게, GPU 메모리가 부족할 때 옵티마이저 상태(Optimizer States)를 자동으로 CPU RAM으로 방출(Eviction)한다. 이후 해당 파라미터의 업데이트가 필요할 때 다시 GPU로 로드(Prefetch)한다.11</li>
<li><strong>이점:</strong> 이를 통해 OOM 오류로 인한 훈련 중단을 방지하고, GPU 메모리 용량을 초과하는 큰 배치 사이즈나 긴 문맥 처리를 가능하게 한다. 비록 CPU-GPU 간 데이터 전송으로 인한 약간의 속도 저하가 있을 수 있으나, 훈련의 안정성을 보장한다는 측면에서 필수적인 기능이다.</li>
</ul>
<h3>3.4  Guanaco 모델의 성과</h3>
<p>QLoRA 논문 저자들은 이러한 기술들을 통합하여 ’Guanaco’라는 모델군을 훈련시켰다. 이들은 Llama 모델을 기반으로 OASST1 데이터셋을 사용하여 미세 조정하였는데, 65B 파라미터 모델을 단일 48GB GPU에서 단 24시간 만에 훈련하는 데 성공했다. 결과적으로 Guanaco는 Vicuna 벤치마크에서 ChatGPT 성능의 99.3%에 도달하는 놀라운 효율성을 입증했다.11 이는 하드웨어 제약으로 인해 소수의 거대 테크 기업이나 연구소만 수행할 수 있었던 거대 모델 훈련을 일반 연구자와 대중도 수행할 수 있게 만든, ’AI 민주화(Democratization)’의 결정적인 사건으로 평가받는다.18</p>
<h2>4.  심층 분석: LoRA vs. 전체 미세 조정 (FFT)</h2>
<p>LoRA와 QLoRA가 효율성 측면에서 혁신적인 것은 분명하지만, 학계와 현장에서는 “과연 PEFT가 전체 미세 조정(Full Fine-Tuning, FFT)과 질적으로 동일한 학습 결과를 만들어내는가?“에 대한 논의가 지속되어 왔다. 최신 연구들은 두 방식 사이에 구조적이고 스펙트럼적인 차이가 존재함을 밝혀내고 있다.</p>
<h3>4.1  스펙트럼 분석과 “침입 차원(Intruder Dimensions)”</h3>
<p>최근 NeurIPS 등의 주요 학회에서 발표된 연구들 19에 따르면, LoRA로 학습된 가중치 행렬과 FFT로 학습된 가중치 행렬은 <strong>스펙트럼(Spectrum)</strong> 관점에서 뚜렷한 차이를 보인다.</p>
<ul>
<li><strong>FFT의 학습 역학:</strong> 전체 미세 조정은 사전 학습된 가중치의 주요 특징(Singular Vectors)을 유지하면서, 기존의 특징 공간 내에서 미세하게 방향을 수정하거나 크기를 조정하는 경향이 있다. 즉, 모델의 가중치 공간에서 급격한 방향 전환이 일어나지 않으며, 사전 학습된 지식의 구조를 보존하려는 관성(Inertia)을 가진다.19</li>
<li><strong>LoRA의 침입 차원:</strong> 반면, LoRA는 저랭크 행렬 <span class="math math-inline">B</span>와 <span class="math math-inline">A</span>의 곱을 통해 업데이트를 생성하므로, 사전 학습된 가중치 행렬의 특이 벡터(Singular Vectors)와는 직교(Orthogonal)하거나 전혀 다른 방향의 새로운 강력한 벡터들을 도입한다. 연구자들은 이를 **“침입 차원(Intruder Dimensions)”**이라 명명했다.21</li>
<li><strong>정의:</strong> 미세 조정된 가중치 행렬(<span class="math math-inline">W_{tuned}</span>)의 특이 벡터 중, 사전 학습된 가중치 행렬(<span class="math math-inline">W_0</span>)의 특이 벡터들과 코사인 유사도가 매우 낮은(즉, 새로운 방향의), 그러나 큰 특이값(Singular Value)을 가지는 벡터들.</li>
<li><strong>의미:</strong> LoRA는 기존의 특징을 수정하기보다는, 새로운 특징(Feature)을 강제로 주입하는 방식으로 학습이 이루어진다는 것을 의미한다. 이는 LoRA가 왜 적은 파라미터로도 빠르게 태스크에 적응하는지를 설명해 주는 동시에, 그 한계를 시사하기도 한다.</li>
</ul>
<h3>4.2  침입 차원의 영향: 망각과 OOD 성능</h3>
<p>이러한 침입 차원은 양날의 검으로 작용한다.</p>
<ol>
<li><strong>지식의 망각(Forgetting)에 대한 재해석:</strong> 초기에는 LoRA가 대부분의 파라미터를 동결하므로 FFT보다 사전 학습된 지식을 덜 망각할 것이라고 여겨졌다. 그러나 최신 연구 19는 침입 차원이 사전 학습된 분포와 상충되는 새로운 방향성을 가지기 때문에, 오히려 특정 조건(특히 연속 학습, Continual Learning)에서는 FFT보다 사전 학습된 지식을 더 심각하게 훼손하거나 망각을 가속화할 수 있음을 보여준다.</li>
<li><strong>분포 외(OOD) 성능:</strong> LoRA 모델은 학습한 타겟 태스크(In-Distribution)에서는 FFT와 대등한 성능을 보이지만, 훈련 데이터 분포에서 벗어난 데이터(Out-Of-Distribution)나 일반적인 언어 능력 테스트에서는 FFT보다 성능이 떨어지거나 덜 강건(Robust)한 모습을 보일 수 있다. FFT가 모델의 전반적인 매니폴드(Manifold)를 부드럽게 이동시키는 반면, LoRA는 특정 방향으로 모델을 ‘찌그러뜨리는’ 형태의 변형을 가하기 때문이다.23</li>
</ol>
<h3>4.3  성능 트레이드오프: 추론 능력 vs. 지식 주입</h3>
<p>Anyscale의 비교 실험 연구 24에 따르면, 텍스트 요약, 스타일 변환, 단순 질의응답과 같은 태스크에서는 LoRA가 FFT와 거의 대등하거나 구분할 수 없는 성능을 보인다. 그러나 복잡한 **수학적 추론(Mathematical Reasoning)**이나 코딩과 같이 모델의 깊은 사고 능력을 재구성해야 하는 태스크에서는 LoRA가 FFT보다 성능이 유의미하게 떨어지는 경향이 관찰되었다.</p>
<p>이는 저랭크(<span class="math math-inline">r</span>) 업데이트만으로는 복잡한 추론 패턴을 완전히 학습하기에 표현력(Expressiveness)이 부족할 수 있음을 시사한다. 특히 QLoRA의 경우, 4비트 양자화로 인한 미세한 정보 손실이 더해지므로, 극한의 정확도가 요구되는 상황에서는 16비트 FFT가 여전히 ’골드 스탠다드(Gold Standard)’로 남아 있다.2 연구자들은 이러한 격차를 줄이기 위해 LoRA의 랭크를 높이거나, 모든 선형 층(All Linear Layers)에 LoRA를 적용하는 전략을 권장한다.9</p>
<h2>5.  하드웨어 및 비용 효율성 분석</h2>
<p>PEFT 기술의 가장 큰 공헌은 AI 개발의 진입 장벽을 획기적으로 낮춘 것이다. 이는 단순한 기술적 개선을 넘어, AI 생태계의 경제학을 변화시켰다.</p>
<h3>5.1  모델 크기별 VRAM 요구량 비교</h3>
<p>다음은 Llama 2 모델을 기준으로 전체 미세 조정(Full), LoRA, QLoRA 적용 시 필요한 VRAM 용량을 비교한 것이다. 이 데이터는 왜 QLoRA가 “게임 체인저“인지 명확히 보여준다.25</p>
<table><thead><tr><th><strong>모델 크기 (파라미터)</strong></th><th><strong>전체 미세 조정 (16-bit FFT)</strong></th><th><strong>LoRA (16-bit)</strong></th><th><strong>QLoRA (4-bit)</strong></th><th><strong>필요 하드웨어 예시 (최소 사양)</strong></th></tr></thead><tbody>
<tr><td><strong>7B</strong></td><td>~160 GB</td><td>~16 GB</td><td><strong>~6 GB</strong></td><td>RTX 3060, RTX 4060 (게이밍 노트북 가능)</td></tr>
<tr><td><strong>13B</strong></td><td>~320 GB</td><td>~32 GB</td><td><strong>~12 GB</strong></td><td>RTX 3080/4070 (고사양 소비자용 GPU)</td></tr>
<tr><td><strong>33B / 30B</strong></td><td>~600 GB</td><td>~80 GB</td><td><strong>~24 GB</strong></td><td>RTX 3090/4090 (하이엔드 소비자용 GPU)</td></tr>
<tr><td><strong>65B / 70B</strong></td><td>~1200 GB (GPU 클러스터 필수)</td><td>~160 GB (A100 x 2~3)</td><td><strong>~48 GB</strong></td><td>RTX A6000 x 1 또는 A100 x 1 (워크스테이션)</td></tr>
</tbody></table>
<ul>
<li><strong>7B 모델:</strong> QLoRA를 사용하면 6GB VRAM을 가진 일반 게이밍 노트북이나 저가형 클라우드 인스턴스에서도 최신 LLM의 미세 조정이 가능하다. 이는 AI 연구가 더 이상 거대 자본의 전유물이 아님을 의미한다.</li>
<li><strong>70B 모델:</strong> 기업급 성능을 보여주는 70B 모델의 경우, 전체 미세 조정을 하려면 수천만 원에 달하는 80GB A100 GPU 여러 장으로 구성된 복잡한 클러스터가 필요하다. 그러나 QLoRA를 사용하면 48GB VRAM을 가진 전문가용 GPU(예: NVIDIA RTX A6000) 한 장이면 훈련이 가능하다.24</li>
</ul>
<h3>5.2  비용 절감 효과</h3>
<p>클라우드 GPU 임대 비용 측면에서도 QLoRA의 경제성은 압도적이다. 예를 들어, 65B 모델을 미세 조정하기 위해 A100 8장을 사용하는 인스턴스는 시간당 수십 달러의 비용이 들지만, QLoRA를 통해 단일 GPU 인스턴스(예: A6000 또는 A100 1장)를 사용하면 시간당 비용을 1/8 수준으로 절감할 수 있다. RunPod과 같은 저가형 클라우드 서비스를 이용할 경우, 수십 달러(약 2~3만 원) 수준의 비용으로 7B~13B 모델의 커스텀 파인튜닝을 완료할 수 있다.25</p>
<h2>6.  생태계와 구현: AI의 민주화</h2>
<p>PEFT와 QLoRA의 확산은 강력한 오픈소스 생태계의 지원 덕분에 가능했다. Hugging Face는 이러한 기술들을 누구나 쉽게 사용할 수 있도록 라이브러리화하여 배포하고 있다.</p>
<h3>6.1  Hugging Face PEFT 라이브러리</h3>
<p>Hugging Face의 <code>peft</code> 라이브러리는 트랜스포머(<code>transformers</code>) 및 <code>accelerate</code> 라이브러리와 긴밀하게 통합되어 있다. 개발자는 복잡한 수학적 구현을 직접 할 필요 없이, <code>LoraConfig</code>를 정의하고 <code>get_peft_model</code> 함수를 호출하는 것만으로 베이스 모델을 LoRA 모델로 래핑(Wrapping)할 수 있다.30</p>
<pre><code class="language-Python">from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(base_model, config)
</code></pre>
<p>이러한 추상화는 연구자들이 하위 수준의 구현보다는 모델링과 데이터 자체에 집중할 수 있게 해준다.</p>
<h3>6.2  bitsandbytes와 4-bit 양자화 통합</h3>
<p>QLoRA의 구현체인 <code>bitsandbytes</code> 라이브러리는 CUDA 커널 레벨에서 4-bit NormalFloat 연산을 최적화하여 PyTorch와의 연결을 담당한다. Hugging Face 생태계는 <code>bitsandbytes</code>를 통합하여, <code>load_in_4bit=True</code>라는 간단한 옵션만으로 모델을 4비트로 로드하고 QLoRA 훈련을 시작할 수 있게 지원한다.17 이는 하드웨어 전문 지식이 없는 데이터 과학자들도 최첨단 양자화 기술을 활용할 수 있게 만든 혁신이다.</p>
<h2>7.  결론 및 미래 전망</h2>
<p>PEFT와 LoRA, 그리고 QLoRA는 단순한 최적화 기법을 넘어 거대 언어 모델의 패러다임을 ’중앙 집중식 훈련’에서 ’분산형 적응’으로 전환시켰다. LoRA는 수학적으로 우아한 행렬 분해를 통해 추론 레이턴시 없는 효율적인 적응을 가능하게 했으며, QLoRA는 정보 이론적 양자화를 통해 하드웨어의 물리적 제약을 뛰어넘었다.</p>
<p>물론 LoRA가 전체 미세 조정에 비해 침입 차원의 생성으로 인한 OOD 성능 저하나 복잡한 추론 태스크에서의 미세한 성능 열위와 같은 한계 20를 가지고 있는 것은 사실이다. 그러나 비용 대비 효용 측면에서, 그리고 모델의 배포 유연성 측면에서 PEFT가 제공하는 가치는 압도적이다.</p>
<p>향후 연구는 **DoRA (Weight-Decomposed Low-Rank Adaptation)**와 같이 가중치의 크기(Magnitude)와 방향(Direction)을 분리하여 학습함으로써 LoRA의 침입 차원 문제를 완화하고 FFT와의 성능 격차를 더욱 좁히는 방향으로 나아갈 것이다.9 또한, 4비트를 넘어 2비트, 3비트 양자화 훈련에 대한 연구(예: LoftQ)들도 가속화되고 있다. 현시점에서 자원이 제한된 환경에서 특화된 LLM을 구축하고자 하는 엔지니어와 연구자에게 QLoRA 기반의 미세 조정은 가장 합리적이고 강력한, 그리고 유일한 현실적 대안이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>LORA: LOW-RANK ADAPTATION OF LARGE LAN- GUAGE MODELS - OpenReview, https://openreview.net/pdf?id=nZeVKeeFYf9</li>
<li>PEFT vs Full Fine-Tuning: Key Limitations Compared - Artech Digital, https://www.artech-digital.com/blog/peft-vs-full-fine-tuning-key-limitations-compared</li>
<li>What is LoRA (Low-Rank Adaption)? - IBM, https://www.ibm.com/think/topics/lora</li>
<li>Parameter Efficient Fine Tuning: A Comprehensive Analysis Across Applications - arXiv, https://arxiv.org/html/2404.13506v1</li>
<li>What is parameter-efficient fine-tuning (PEFT)? - IBM, https://www.ibm.com/think/topics/parameter-efficient-fine-tuning</li>
<li>Parameter-Efficient Fine-Tuning using PEFT - Hugging Face, https://huggingface.co/blog/peft</li>
<li>Parameter-efficient fine-tuning (PEFT) and how it’s different from fine-tuning. - Medium, https://medium.com/@harshnpathak/parameter-efficient-fine-tuning-peft-and-how-its-different-from-fine-tuning-3f6b95c73bac</li>
<li>Lora: Low-rank adaptation of large lan - arXiv, https://arxiv.org/pdf/2106.09685</li>
<li>LoRA is inferior to Full Fine-Tuning / DreamBooth Training - A research paper just published : LoRA vs Full Fine-tuning: An Illusion of Equivalence : r/StableDiffusion - Reddit, https://www.reddit.com/r/StableDiffusion/comments/1gmwlfs/lora_is_inferior_to_full_finetuning_dreambooth/</li>
<li>Decoding LoRA: A Comprehensive Summary on Low-Rank Adaptation - Medium, https://medium.com/@egpivo/decoding-lora-a-comprehensive-summary-on-low-rank-adaptation-af432ab557c2</li>
<li>QLORA: Efficient Finetuning of Quantized LLMs - arXiv, https://arxiv.org/pdf/2305.14314</li>
<li>Breaking Down the “Things” Behind QLoRA | by Shubham - Medium, https://medium.com/@smishraonline16/breaking-down-the-things-behind-qlora-436c7fc2a49b</li>
<li>[D] Distribution of weights of trained Neural Network : r/MachineLearning - Reddit, https://www.reddit.com/r/MachineLearning/comments/5ufh0m/d_distribution_of_weights_of_trained_neural/</li>
<li>QLoRA — Train your LLMs on a Single GPU - AI Bites, https://www.ai-bites.net/qlora-train-your-llms-on-a-single-gpu/</li>
<li>Interactive Study with Conversation1st.ai — QLoRA: Efficient Finetuning of Quantized LLMs | by Tony Tong | Medium, https://medium.com/@tonytong.ai/interactive-study-with-conversation1st-ai-qlora-efficient-finetuning-of-quantized-llms-7b50cb31ea19</li>
<li>QLoRA: Fine-Tuning Large Language Models (LLM’s) - Medium, https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766</li>
<li>Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA, https://huggingface.co/blog/4bit-transformers-bitsandbytes</li>
<li>QLoRA - everyday series, https://newsletter.everydayseries.com/qlora/</li>
<li>LoRA vs Full Fine-tuning: An Illusion of Equivalence - arXiv, https://arxiv.org/html/2410.21228v3</li>
<li>LoRA vs Full Fine-tuning: An Illusion of Equivalence - NeurIPS 2025, https://neurips.cc/virtual/2025/poster/115207</li>
<li>12월 22, 2025에 액세스, [https://arxiv.org/html/2410.21228v3#:<sub>:text=Intruder%20dimensions%20are%20singular%20vectors,do%20not%20contain%20intruder%20dimensions.](https://arxiv.org/html/2410.21228v3#:</sub>:text=Intruder dimensions are singular vectors, <a href="https://arxiv.org/html/2410.21228v3#:~:text=Intruder%20dimensions%20are%20singular%20vectors,do%20not%20contain%20intruder%20dimensions.">https://arxiv.org/html/2410.21228v3#:~:text=Intruder%20dimensions%20are%20singular%20vectors,do%20not%20contain%20intruder%20dimensions.</a></li>
<li>LoRA vs Full Fine-Tuning: A Beautiful Illusion of Equivalence? | by Fernando Velasco Lozano | GoPenAI, https://blog.gopenai.com/lora-vs-full-fine-tuning-a-beautiful-illusion-of-equivalence-3394520d1967</li>
<li>LoRA vs Full Fine-tuning: An Illusion of Equivalence - arXiv, https://arxiv.org/html/2410.21228v1</li>
<li>Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2 - Anyscale, https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2</li>
<li>How can I fine-tune large language models on a budget using LoRA and QLoRA on cloud GPUs? - Runpod, https://www.runpod.io/articles/guides/how-to-fine-tune-large-language-models-on-a-budget</li>
<li>Helpful VRAM requirement table for qlora, lora, and full finetuning. : r/LocalLLaMA - Reddit, https://www.reddit.com/r/LocalLLaMA/comments/18o5u0k/helpful_vram_requirement_table_for_qlora_lora_and/</li>
<li>How much VRAM do I need for LLM model fine-tuning? - Modal, https://modal.com/blog/how-much-vram-need-fine-tuning</li>
<li>LoRA vs QLoRA: Best AI Model Fine-Tuning Platforms &amp; Tools 2025 | Index.dev, https://www.index.dev/blog/top-ai-fine-tuning-tools-lora-vs-qlora-vs-full</li>
<li>LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B - arXiv, https://arxiv.org/html/2310.20624v2</li>
<li>PEFT - Hugging Face, https://huggingface.co/docs/peft/en/index</li>
<li>Using PEFT at Hugging Face, https://huggingface.co/docs/hub/peft</li>
<li>Open-Source Text Generation &amp; LLM Ecosystem at Hugging Face, https://huggingface.co/blog/os-llms</li>
<li>Mastering QLoRa : A Deep Dive into 4-Bit Quantization and LoRa Parameter Efficient Fine-Tuning | Manal El Aidouni, https://manalelaidouni.github.io/4Bit-Quantization-Models-QLoRa.html</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>