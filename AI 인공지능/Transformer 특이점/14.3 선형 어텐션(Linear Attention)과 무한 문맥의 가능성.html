<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:14.3 선형 어텐션(Linear Attention)과 무한 문맥의 가능성</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>14.3 선형 어텐션(Linear Attention)과 무한 문맥의 가능성</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>14.3 선형 어텐션(Linear Attention)과 무한 문맥의 가능성</span></nav>
                </div>
            </header>
            <article>
                <h1>14.3 선형 어텐션(Linear Attention)과 무한 문맥의 가능성</h1>
<h2>1.  서론: 2차 복잡도의 장벽과 패러다임의 전환</h2>
<p>현대 자연어 처리(NLP)와 거대 언어 모델(LLM)의 역사는 곧 ’문맥(Context)’과의 투쟁사라 해도 과언이 아니다. 트랜스포머(Transformer) 아키텍처가 등장한 이래, 모델이 한 번에 “보고, 기억하고, 처리할 수 있는” 정보의 양은 지능의 척도로 자리 잡았다. 2017년 당시 512 토큰에 불과했던 문맥 윈도우는 GPT-4, Claude 등의 최신 모델에 이르러 128k, 심지어 100만(1M) 토큰 단위까지 확장되었다. 그러나 이러한 양적 팽창의 이면에는 강력한 물리적, 수학적 장벽이 존재한다. 바로 표준 소프트맥스 어텐션(Softmax Attention) 메커니즘이 가진 <span class="math math-inline">O(N^2)</span>의 시공간 복잡도이다.1</p>
<p>입력 시퀀스의 길이 <span class="math math-inline">N</span>이 증가함에 따라, 표준 어텐션이 요구하는 연산량과 메모리는 제곱으로 폭증한다. 예를 들어, 문맥 길이를 2배 늘리면 연산 비용은 4배가 되며, 10배 늘리면 100배의 비용이 소요된다. 이는 수만 페이지의 문서를 분석하거나, 긴 대화 로그를 유지해야 하는 에이전트 시스템, 혹은 유전체 서열 분석과 같은 초장문(Long-sequence) 작업에서 치명적인 병목으로 작용한다. 이러한 ’2차 복잡도의 장벽(Quadratic Barrier)’은 모델이 유한한 문맥 윈도우라는 감옥에 갇혀 있음을 의미하며, 진정한 의미의 ‘무한 문맥(Infinite Context)’—즉, 스트리밍 되는 데이터를 끊임없이 받아들이고 학습하며 추론하는 능력—을 구현하는 데 있어 가장 큰 걸림돌이 되어 왔다.3</p>
<p>선형 어텐션(Linear Attention)은 이 난제를 해결하기 위한 가장 근본적이고 수학적인 접근법이다. 이 기술의 핵심은 어텐션 연산의 순서를 재정립하여 복잡도를 <span class="math math-inline">O(N^2)</span>에서 <span class="math math-inline">O(N)</span>으로 획기적으로 낮추는 데 있다. 더 나아가, 선형 어텐션은 트랜스포머를 병렬 처리가 가능한 순환 신경망(RNN)의 형태로 변환시킴으로써, 이론상 고정된 크기의 메모리만으로 무한한 길이의 시퀀스를 처리할 수 있는 가능성을 제시한다.1</p>
<p>본 장에서는 선형 어텐션의 수학적 유도 과정부터 시작하여, 초기 선형 트랜스포머의 실패와 이를 극복하기 위해 등장한 2024~2025년의 최신 아키텍처들—RetNet, RWKV-7, Kimi Linear, Based 등—을 심도 있게 분석한다. 또한, 선형 어텐션이 필연적으로 마주하는 ’압축’과 ‘회상(Recall)’ 사이의 정밀도 트레이드오프 문제를 고찰하고, 이를 해결하기 위한 하이브리드 전략과 최신 추론 가속 기법(LoLA 등)을 통해 과연 우리가 진정한 무한 문맥의 시대로 진입하고 있는지 비평적으로 논의한다.</p>
<h2>2.  선형 어텐션의 수학적 원리와 순환적 이중성 (The Duality)</h2>
<h3>2.1  표준 어텐션의 병목과 커널 트릭의 도입</h3>
<p>선형 어텐션을 이해하기 위해서는 먼저 표준 어텐션이 왜 비효율적인지, 그리고 수학적으로 어느 지점을 공략해야 하는지를 명확히 해야 한다. 표준 어텐션(Scaled Dot-Product Attention)은 쿼리(<span class="math math-inline">Q</span>), 키(<span class="math math-inline">K</span>), 밸류(<span class="math math-inline">V</span>) 행렬을 통해 다음과 같이 정의된다.<br />
<span class="math math-display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
</span><br />
여기서 <span class="math math-inline">Q, K, V \in \mathbb{R}^{N \times d}</span>라고 할 때, <span class="math math-inline">QK^T</span> 연산은 <span class="math math-inline">N \times N</span> 크기의 어텐션 스코어 행렬(Attention Matrix)을 생성한다. 이 행렬은 시퀀스 내의 모든 토큰 쌍(Pair-wise) 간의 관계를 저장하므로, 시퀀스 길이 <span class="math math-inline">N</span>에 대해 <span class="math math-inline">O(N^2)</span>의 메모리와 연산을 요구한다. 이 <span class="math math-inline">N \times N</span> 행렬이 바로 병목의 원흉이다.</p>
<p>선형 어텐션의 핵심 아이디어는 **“소프트맥스 함수가 행렬 곱셈의 결합법칙(Associativity)을 방해한다”**는 통찰에서 출발한다. 소프트맥스는 비선형 함수이므로, <span class="math math-inline">\text{softmax}(QK^T)V</span>를 <span class="math math-inline">Q(\text{softmax}(K^T)V)</span>와 같은 형태로 분해할 수 없다. 만약 소프트맥스를 제거하거나 다른 선형적인 함수로 대체할 수 있다면, 우리는 행렬 곱셈의 순서를 바꿀 수 있게 된다.</p>
<p>이를 위해 **커널 트릭(Kernel Trick)**이 도입된다. 두 벡터의 유사도를 측정하는 커널 함수 <span class="math math-inline">K(q, k) = \text{softmax}(qk^T)</span>를 두 개의 특징 맵(Feature Map) <span class="math math-inline">\phi(\cdot)</span>의 내적 <span class="math math-inline">\phi(q)\phi(k)^T</span>로 근사하거나 대체하는 것이다.1 이를 수식으로 표현하면 다음과 같다.<br />
<span class="math math-display">
\text{LinearAttention}(Q, K, V) = \left( \phi(Q) \phi(K)^T \right) V
</span><br />
이제 결합법칙을 적용하여 연산 순서를 재조정할 수 있다.<br />
<span class="math math-display">
\left( \phi(Q) \phi(K)^T \right) V = \phi(Q) \left( \phi(K)^T V \right)
</span><br />
이 수식의 변화는 계산 복잡도에 있어 극적인 차이를 만들어낸다.</p>
<ol>
<li><strong>우변의 괄호 안 (<span class="math math-inline">\phi(K)^T V</span>):</strong> <span class="math math-inline">K</span>와 <span class="math math-inline">V</span>를 먼저 곱한다. <span class="math math-inline">\phi(K)^T \in \mathbb{R}^{d \times N}</span>이고 <span class="math math-inline">V \in \mathbb{R}^{N \times d}</span>이므로, 결과물은 <span class="math math-inline">d \times d</span> 크기의 행렬이 된다. 이 행렬의 크기는 시퀀스 길이 <span class="math math-inline">N</span>과 무관하다. 연산 비용은 <span class="math math-inline">O(N d^2)</span>이다.</li>
<li><strong>최종 곱셈 (<span class="math math-inline">\phi(Q) \times \dots</span>):</strong> 앞서 계산된 <span class="math math-inline">d \times d</span> 행렬에 <span class="math math-inline">Q</span>를 곱한다. 이 역시 <span class="math math-inline">O(N d^2)</span>의 비용이 든다.</li>
</ol>
<p>결과적으로 전체 복잡도는 <span class="math math-inline">O(N^2)</span>에서 <span class="math math-inline">O(N)</span>으로 감소한다. 여기서 <span class="math math-inline">d</span>는 모델의 은닉 차원(Hidden Dimension)으로, 일반적으로 <span class="math math-inline">N</span>보다 훨씬 작으므로 긴 시퀀스 처리에서 압도적인 효율성을 제공한다.1</p>
<h3>2.2  병렬 모드와 순환 모드의 이중성 (The duality)</h3>
<p>선형 어텐션의 가장 매력적인 특징은 트랜스포머의 **병렬성(Parallelism)**과 RNN의 **순환성(Recurrence)**을 동시에 갖는 ’이중성(Duality)’을 지닌다는 점이다. 이는 상황에 따라 유리한 모드로 전환하여 사용할 수 있음을 의미한다.</p>
<h4>2.2.1 병렬 모드 (Parallel Mode): 고속 학습</h4>
<p>학습(Training) 단계에서는 전체 시퀀스(Ground Truth)가 이미 주어져 있다. 따라서 위에서 유도한 <span class="math math-inline">\phi(Q) (\phi(K)^T V)</span> 형태를 사용하여, 루프(Loop) 없이 행렬 곱만으로 한 번에 그래디언트를 계산할 수 있다. 이는 GPU/TPU와 같은 하드웨어 가속기의 병렬 처리 능력을 극대화하여, 기존 RNN(LSTM, GRU 등)이 겪었던 느린 학습 속도 문제를 완벽하게 해결한다.6</p>
<h4>2.2.2 순환 모드 (Recurrent Mode): 저비용 추론 및 무한 문맥</h4>
<p>추론(Inference) 단계, 특히 텍스트 생성과 같이 토큰을 하나씩 생성하는 자기회귀(Auto-regressive) 상황에서는 순환 형식을 취한다. <span class="math math-inline">i</span>번째 토큰 시점에서의 은닉 상태(Hidden State) <span class="math math-inline">S_i</span>를 정의하면 다음과 같은 RNN 형태의 갱신 규칙(Update Rule)을 얻는다.<br />
<span class="math math-display">
S_i = S_{i-1} + \phi(K_i)^T V_i
</span></p>
<p><span class="math math-display">
O_i = \phi(Q_i) S_i
</span></p>
<p>여기서 상태 <span class="math math-inline">S_i</span>는 <span class="math math-inline">\sum_{j=1}^i \phi(K_j)^T V_j</span>로, 과거의 모든 키-밸류 정보를 누적한 <span class="math math-inline">d \times d</span> 크기의 행렬이다.</p>
<p>표준 트랜스포머는 추론 시 ’KV 캐시(KV Cache)’라는 이름으로 과거의 모든 <span class="math math-inline">K, V</span> 벡터를 저장해야 하므로, 문맥이 길어질수록 메모리 사용량이 선형적으로 증가한다(<span class="math math-inline">O(N)</span>). 반면, 선형 어텐션의 순환 모드는 시퀀스 길이가 100이든 100만이든 상관없이 항상 고정된 크기(<span class="math math-inline">d \times d</span>)의 상태 행렬 <span class="math math-inline">S</span>만 유지하면 된다.8 이것이 바로 선형 어텐션이 이론적으로 ’무한 문맥’을 처리할 수 있다고 주장하는 근거이다. 메모리가 폭발하지 않으므로, 하드웨어가 허용하는 한 영원히 대화를 이어나갈 수 있는 것이다.10</p>
<h3>2.3  특징 맵(Feature Map)의 선택과 딜레마</h3>
<p>커널 트릭을 성공적으로 구현하기 위해서는 적절한 특징 맵 <span class="math math-inline">\phi(\cdot)</span>을 선택하는 것이 중요하다. 이는 단순히 수학적 근사를 넘어, 모델의 학습 안정성과 표현력을 결정짓는 핵심 요소이다.</p>
<ul>
<li><strong>초기 접근 (Linear Transformer):</strong> Katharopoulos et al. (2020)은 <span class="math math-inline">\phi(x) = \text{elu}(x) + 1</span>이라는 단순한 함수를 제안했다. 이는 양수(positive) 조건을 만족시키면서 계산이 매우 빠르다. 그러나 소프트맥스 특유의 ‘집중(Focusing)’ 효과를 충분히 살리지 못해 성능 저하가 발생했다.11</li>
<li><strong>랜덤 피처 (Performer):</strong> 구글의 Performer는 FAVOR+ 알고리즘을 통해 <span class="math math-inline">\exp(qk^T)</span> 커널을 직교 무작위 특징(Orthogonal Random Features)으로 근사했다. 이론적으로는 소프트맥스에 수렴하지만, 실제로는 근사 오차와 노이즈로 인해 학습이 불안정하고, 특히 정확한 ’회상’이 필요한 작업에서 한계를 보였다.4</li>
<li><strong>현대적 접근 (2023-2025):</strong> 최근의 모델들(RetNet, RWKV 등)은 소프트맥스를 억지로 근사하려는 시도를 버렸다. 대신, <strong>감쇠(Decay)</strong> 메커니즘이나 <strong>게이팅(Gating)</strong> 구조를 특징 맵의 일부로 내재화하여, 모델 스스로 정보의 중요도를 학습하고 불필요한 과거 정보를 망각하도록 유도하는 방향으로 진화했다. 이는 뒤이어 설명할 RWKV와 Kimi Linear의 핵심 철학이 된다.</li>
</ul>
<h2>3.  무한 문맥의 역설: 압축과 회상(Recall)의 트레이드오프</h2>
<p>선형 어텐션이 제공하는 <span class="math math-inline">O(1)</span> 메모리(Inference State Size)는 무한 문맥을 위한 필요조건이지만 충분조건은 아니다. 여기서 우리는 근본적인 역설에 직면한다. <strong>“무한한 정보를 유한한 그릇에 담을 때, 필연적으로 발생하는 손실을 어떻게 관리할 것인가?”</strong></p>
<h3>3.1  고정 상태 크기의 한계와 정보 소실</h3>
<p>표준 트랜스포머의 KV 캐시는 ‘비손실(Lossless)’ 메모리이다. 과거의 모든 토큰 정보가 그대로 보존되므로, 필요하다면 언제든 100% 정확도로 과거의 정보를 참조할 수 있다. 반면, 선형 어텐션의 상태 <span class="math math-inline">S</span>는 ‘손실(Lossy)’ 압축 메모리이다. 새로운 정보 <span class="math math-inline">\phi(K_t)^T V_t</span>가 들어올 때마다 기존 상태 <span class="math math-inline">S_{t-1}</span>에 더해지거나(<span class="math math-inline">+</span>), 감쇠(<span class="math math-inline">\gamma</span>) 후 더해진다.<br />
<span class="math math-display">
S_t = \gamma S_{t-1} + K_t^T V_t
</span><br />
이 과정에서 필연적으로 ’정보의 덮어쓰기(Overwriting)’나 ’희석(Dilution)’이 발생한다.2 수만 토큰 전에 등장했던 중요한 정보(예: 비밀번호, 특정 고유명사)가 이후 등장한 수많은 잡음(Noise) 정보들에 의해 덮여버려, 나중에 이를 복원(Recall)하려 할 때 실패하는 현상이 발생한다. 이를 **“연상 회상(Associative Recall)의 실패”**라고 부른다.14</p>
<h3>3.2  Based 아키텍처와 정밀도(Precision) 이론</h3>
<p>스탠포드 Hazy Research 팀은 이 현상을 이론적으로 분석한 ‘Based’ 아키텍처 연구(2024)를 발표했다. 그들은 선형 어텐션이 연상 회상에 취약한 이유를 **“정밀도(Precision) 부족”**으로 진단했다. 선형 어텐션의 상태 갱신은 본질적으로 덧셈과 곱셈의 누적이며, 이는 정보를 ‘뭉개는’ 경향이 있다. 반면, 표준 어텐션의 소프트맥스는 특정 토큰에 확률을 1.0에 가깝게 몰아주는 ‘날카로운(Sharp)’ 선택이 가능하여 높은 정밀도를 보장한다.15</p>
<p>이 연구는 “건초더미에서 바늘 찾기(Needle in a Haystack)“와 같은 작업에서 선형 모델들이 맥을 못 추는 이유를 명쾌하게 설명한다. 선형 모델은 전체적인 문맥 흐름(Global Context)을 파악하는 데는 능숙하지만, 국소적이고 구체적인 정보를 정확히 짚어내는 데는 구조적인 한계를 가진다.</p>
<p>이를 극복하기 위해 Based 모델은 **“슬라이딩 윈도우 어텐션(Sliding Window Attention) + 선형 어텐션”**의 하이브리드 전략을 제시했다. 아주 짧은 윈도우(예: 64 토큰)만이라도 표준 어텐션(Exact Attention)을 사용하여 국소적인 정밀도를 확보하고, 나머지 긴 문맥은 선형 어텐션으로 처리하는 것이다. 놀랍게도, 불과 64 토큰의 윈도우를 추가한 것만으로도 선형 모델의 회상 능력이 표준 트랜스포머의 90.8% 수준까지 회복됨이 입증되었다.15 이는 무한 문맥을 구현하기 위해서는 무조건적인 선형화가 아니라, **“근거리의 정밀함(Precision)과 원거리의 요약(Summary)”**을 조화시키는 전략이 필수적임을 시사한다.</p>
<h2>4.  현대적 선형 어텐션 아키텍처의 진화 (2023-2025)</h2>
<p>2023년을 기점으로, 선형 어텐션 연구는 단순히 소프트맥스를 근사하는 단계를 넘어섰다. RNN의 오랜 지혜를 현대적인 트랜스포머 공학으로 재해석한 독창적인 아키텍처들이 쏟아져 나오며, 성능과 효율성의 새로운 지평을 열었다.</p>
<h3>4.1  RetNet (Retentive Networks): “불가능한 삼각형“의 해결</h3>
<p>마이크로소프트가 2023년 제안한 RetNet은 **‘Retention’**이라는 새로운 메커니즘을 통해 학습 병렬성, 저비용 추론, 고성능이라는 “불가능한 삼각형“을 동시에 달성하고자 했다. RetNet의 가장 큰 기여는 상태 갱신 규칙에 명시적인 <strong>감쇠 인자(Decay Factor)</strong> <span class="math math-inline">\gamma</span>를 도입하고, 이를 복소수 공간에서의 회전 변환으로 해석한 것이다.<br />
<span class="math math-display">
S_t = \gamma S_{t-1} + K_t^T V_t
</span></p>
<p><span class="math math-display">
\text{Retention}(X) = (QK^T \odot D)V
</span></p>
<p>여기서 <span class="math math-inline">D</span>는 거리 기반의 감쇠 행렬이다. 이 <span class="math math-inline">\gamma</span>는 과거의 정보가 현재에 미치는 영향을 지수적으로 감소(<span class="math math-inline">\gamma^{t-i}</span>)시킴으로써, 별도의 위치 임베딩(Positional Embedding) 없이도 토큰 간의 상대적 거리를 자연스럽게 모델링한다.17 특히, RetNet은 시퀀스를 청크(Chunk)로 나누어 처리하는 알고리즘을 최적화하여, 학습 시에는 트랜스포머처럼 병렬로, 추론 시에는 RNN처럼 순차적으로 작동하는 효율성을 극대화했다. 이는 선형 어텐션이 트랜스포머의 강력한 대안(Successor)이 될 수 있음을 증명한 상징적인 사건이었다.19</p>
<h3>4.2  RWKV 시리즈: RNN의 르네상스</h3>
<p>RWKV(Receptance Weighted Key Value)는 오픈소스 커뮤니티가 주도하여 발전시킨 모델로, 완전한 RNN 구조를 띠면서도 트랜스포머 수준의 학습 가용성을 확보한 프로젝트다. RWKV는 버전이 거듭될수록 ‘Time-mixing’(시간 혼합) 레이어의 수식을 정교하게 발전시켜 왔다.</p>
<p>아래 표는 RWKV 아키텍처의 진화를 요약한 것이다.20</p>
<table><thead><tr><th><strong>모델 버전</strong></th><th><strong>별칭</strong></th><th><strong>시간 단계 공식 (Time Step Formula) 특징</strong></th><th><strong>상태 갱신 메커니즘</strong></th></tr></thead><tbody>
<tr><td><strong>RWKV-5</strong></td><td>Eagle</td><td><span class="math math-inline">S_t = \text{diag}(w) S_{t-1}</span></td><td><strong>학습 가능한 상태 감쇠 (Trainable Decay)</strong>: 채널별로 고정된 감쇠율을 학습.</td></tr>
<tr><td><strong>RWKV-6</strong></td><td>Finch</td><td><span class="math math-inline">S_t = \text{diag}(w_t) S_{t-1}</span></td><td><strong>동적 상태 감쇠 (Dynamic Decay)</strong>: 입력 데이터에 따라 감쇠율 <span class="math math-inline">w_t</span>가 매 시점 변함 (Data-dependent).</td></tr>
<tr><td><strong>RWKV-7</strong></td><td>Goose</td><td><span class="math math-inline">S_t = W_t S_{t-1}</span> (Matrix mixing)</td><td><strong>동적 상태 진화 (Dynamic State Evolution)</strong>: 단순 감쇠를 넘어 상태 행렬 자체를 복잡하게 변환.</td></tr>
</tbody></table>
<ul>
<li><strong>RWKV-5/6:</strong> Mamba와 같은 상태 공간 모델(SSM)의 ‘선택적(Selective)’ 메커니즘을 도입했다. 즉, 입력 토큰의 내용에 따라 “이 정보는 중요하니 오래 기억하고(Decay <span class="math math-inline">\approx 1</span>), 저 정보는 잡음이니 즉시 잊어라(Decay <span class="math math-inline">\approx 0</span>)“를 동적으로 결정한다.21</li>
<li><strong>RWKV-7 (2025):</strong> 최신 버전인 RWKV-7은 “Off-by-one” 문제를 해결하고, 상태 갱신을 단순한 스칼라 곱이 아닌 행렬 연산으로 확장하여 표현력을 극대화했다. 이는 고정된 메모리 용량 안에서 정보의 압축 효율을 극한으로 끌어올리려는 시도로, 특히 장기 기억 능력에서 비약적인 성능 향상을 보여준다.7</li>
</ul>
<h3>4.3  Kimi Linear: 하이브리드와 정밀 게이팅의 승리</h3>
<p>2025년 Moonshot AI가 공개한 <strong>Kimi Linear</strong>는 선형 어텐션 기술의 결정체라 할 수 있다. Kimi Linear는 순수 선형 모델을 고집하는 대신, 실용적인 <strong>하이브리드 아키텍처</strong>를 채택하여 성능과 효율의 균형을 완벽하게 맞췄다.</p>
<h4>4.3.1 Kimi Delta Attention (KDA) 메커니즘</h4>
<p>Kimi Linear의 핵심은 KDA 모듈이다. 이는 기존의 ’Gated DeltaNet’을 개량한 것으로, **델타 규칙(Delta Rule)**과 **채널별 정밀 게이팅(Fine-grained Channel-wise Gating)**을 결합했다.<br />
<span class="math math-display">
S_t = S_{t-1} + \alpha_t (V_t - \text{Estimated}_t) K_t^T
</span><br />
여기서 <span class="math math-inline">\text{Estimated}_t</span>는 현재 상태 <span class="math math-inline">S_{t-1}</span>을 기반으로 예측한 값이다. 즉, KDA는 단순히 정보를 더하는 것이 아니라, “예측과 실제의 차이(Delta)“만큼만 메모리를 갱신한다. 이는 중복 정보의 저장을 막고 메모리 효율을 높인다. 특히, KDA는 기존 모델들이 헤드(Head) 단위로 게이팅을 적용하던 것과 달리, 각 채널(차원)마다 독립적인 게이트 <span class="math math-inline">\alpha_t</span>를 두어 훨씬 정교한 망각과 기억 제어를 가능하게 했다.23</p>
<h4>4.3.2 하이브리드 구성과 성과</h4>
<p>Kimi Linear는 전체 층을 선형 어텐션으로 구성하지 않고, KDA 층과 표준 어텐션(MLA) 층을 약 3:1 비율로 혼합했다. 이는 선형 어텐션의 빠른 속도와 표준 어텐션의 강력한 회상 능력을 모두 취하기 위함이다.</p>
<p>그 결과, 1.4조 토큰으로 학습된 Kimi Linear는 동급의 표준 트랜스포머(MLA)보다 전반적인 성능이 우수하면서도, 100만(1M) 토큰 문맥에서 6배 빠른 추론 속도와 75% 감소된 메모리 사용량을 달성했다.25 이는 선형 어텐션이 더 이상 실험적인 기술이 아니라, GPT-4 급의 대규모 서비스에 즉시 투입 가능한(Drop-in replacement) 수준에 도달했음을 입증한다.</p>
<h2>5.  무한 문맥의 실체와 구현: 추론 가속과 LoLA</h2>
<p>선형 어텐션이 ’무한’을 처리할 수 있다는 것은 이론적인 가능성일 뿐, 실제 애플리케이션에서 이를 구현하기 위해서는 ‘회상 실패’ 문제를 해결해야 한다. 2025년의 연구들은 모델 재학습 없이도 추론 단계에서 이를 보정하는 기법들에 주목하고 있다.</p>
<h3>5.1  LoLA (Low-Rank Linear Attention): 추론 시의 기억 증강</h3>
<p>LoLA는 선형 어텐션 모델의 부족한 연상 회상 능력을 보완하기 위해 제안된 <strong>학습 불필요(Training-free)</strong> 추론 증강 기법이다. LoLA는 과거의 모든 정보를 단일한 선형 상태 <span class="math math-inline">S</span>에 욱여넣는 대신, 정보의 중요도에 따라 세 가지 계층으로 나누어 저장한다.27</p>
<ol>
<li>
<p><strong>로컬 슬라이딩 윈도우 (Local Sliding Window):</strong> 가장 최근의 토큰들은 <span class="math math-inline">O(1)</span> 정밀도가 필요하므로, 짧은 구간(예: 256~512 토큰)은 KV 캐시를 그대로 유지한다.</p>
</li>
<li>
<p>전역 희소 캐시 (Sparse Global Cache): 이것이 LoLA의 핵심이다. 선형 어텐션이 기억하기 어려워하는 ’난제 토큰’들을 선별하여 별도로 저장한다. 이를 위해 **자기-회상 오차(Self-Recall Error, SRE)**라는 지표를 정의한다.<br />
<span class="math math-display">
\text{SRE}(k, v | H, s) = || \hat{v} - v ||_2
</span></p>
</li>
</ol>
<p>즉, 현재의 선형 상태 <span class="math math-inline">H, s</span>를 이용해 과거의 <span class="math math-inline">v</span>를 복원(Reconstruct)해보고, 오차가 큰(잘 기억하지 못하는) 토큰들만 골라내어 희소 캐시에 저장하는 것이다.28</p>
<ol start="3">
<li>선형 상태 (Linear Recurrent State): 나머지 일반적이고 전역적인 정보는 압축된 상태 행렬 <span class="math math-inline">S</span>에 저장하여 처리한다.</li>
</ol>
<p>이러한 3단계 전략을 통해 LoLA는 “중요한 정보는 원본으로, 덜 중요한 정보는 압축해서” 관리한다. 실험 결과, LoLA를 적용한 선형 모델은 ‘Needle in a Haystack’ 테스트에서 정확도를 0.6%에서 **97.4%**로 끌어올리는 기염을 토했다.28 이는 선형 어텐션이 가진 구조적 한계를 외부 메모리 전략으로 훌륭하게 보완할 수 있음을 보여준다.</p>
<h3>5.2  하드웨어 최적화와 IO-Awareness</h3>
<p>선형 어텐션의 이론적 이점인 <span class="math math-inline">O(N)</span> 복잡도를 실제 속도 향상으로 연결하기 위해서는 하드웨어 레벨의 최적화가 필수적이다.</p>
<ul>
<li><strong>청크 단위 병렬 처리 (Chunkwise Parallelism):</strong> 순수 RNN 모드는 GPU의 병렬성을 활용하지 못해 학습이 느리다. 이를 해결하기 위해 시퀀스를 청크로 나누어, 청크 내부는 병렬 어텐션으로, 청크 간 전이는 순환적으로 처리하는 기법이 표준이 되었다. Kimi Linear는 이를 위해 <strong>DPLR(Diagonal-Plus-Low-Rank)</strong> 전이 행렬의 특수한 변형을 사용하여, 텐서 코어(Tensor Core)의 활용률을 극대화했다.23</li>
<li><strong>Flash Linear Attention:</strong> FlashAttention이 표준 트랜스포머의 속도를 혁신했듯, 선형 어텐션 전용의 IO-Aware 커널들이 개발되고 있다. 이들은 HBM(High Bandwidth Memory) 접근을 최소화하고 SRAM 내에서 연산을 처리하여, 이론적인 FLOPs 감소를 실제 Wall-clock time 단축으로 실현한다.29</li>
</ul>
<h2>6.  결론 및 미래 전망</h2>
<p>우리는 14.3절을 통해 선형 어텐션이 단순한 근사 알고리즘에서 출발하여, 이제는 트랜스포머의 한계를 넘어서는 새로운 패러다임으로 진화했음을 확인했다.</p>
<ol>
<li><strong>순수주의의 종말과 하이브리드의 시대:</strong> <span class="math math-inline">O(N^2)</span>을 완전히 제거하려는 초기 시도(Performer 등)는 성능 격차로 인해 실패했다. 대신, Kimi Linear와 Based가 보여주듯, 선형 어텐션의 압축 능력과 표준 어텐션의 정밀함을 전략적으로 배합하는 <strong>하이브리드 아키텍처</strong>가 ’무한 문맥’과 ’고성능’을 동시에 달성하는 정답으로 자리 잡았다.</li>
<li><strong>동적 상태 관리의 중요성:</strong> RWKV-7과 KDA의 사례는 고정된 메모리에 무한한 정보를 담기 위해서는 “무엇을 지우고 무엇을 남길지“를 결정하는 게이팅 메커니즘이 모델의 지능을 좌우함을 시사한다. 단순한 감쇠를 넘어, 상태 자체가 입력에 따라 진화하는 **동적 시스템(Dynamic System)**으로 발전하고 있다.</li>
<li><strong>LMM (Large Memory Model)의 도래:</strong> 선형 어텐션은 LLM을 단순히 텍스트를 생성하는 기계에서, 방대한 지식을 실시간으로 습득하고 유지하는 **‘대용량 메모리 모델(LMM)’**로 진화시키고 있다. 100만 토큰을 넘어 10억 토큰, 나아가 평생의 데이터를 처리할 수 있는 에이전트의 등장이 머지않았다.</li>
</ol>
<p>2차 복잡도의 장벽은 무너졌다. 이제 남은 과제는 이 광활한 선형의 고속도로 위에서, 모델이 얼마나 정교하게 기억의 조각들을 엮어낼 수 있는가 하는 ’기억의 질’에 달려 있다. 무한 문맥의 시대는 이미 시작되었다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Linear Attention Fundamentals | Hailey Schoelkopf, https://haileyschoelkopf.github.io/blog/2024/linear-attn/</li>
<li>The Devil in Linear Transformer - ACL Anthology, https://aclanthology.org/2022.emnlp-main.473.pdf</li>
<li>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention - arXiv, https://arxiv.org/html/2404.07143v1</li>
<li>Linear Attention and Mamba: New Power to Old Ideas - Sergey Nikolenko, https://blog.sergeynikolenko.ru/linear-attention-and-mamba-new-power-to-old-ideas/</li>
<li>🎯 Building Attention Mechanisms from Scratch: A Complete Guide to Understanding Transformers - DEV Community, https://dev.to/gruhesh_kurra_6eb933146da/building-attention-mechanisms-from-scratch-a-complete-guide-to-understanding-transformers-5b3k</li>
<li>Linearizing Attention. Breaking the Quadratic Barrier: Modern… | by Shitanshu Bhushan | TDS Archive | Medium, https://medium.com/data-science/linearizing-attention-204d3b86cc1e</li>
<li>Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Understanding | OpenReview, https://openreview.net/forum?id=GoaWSQWtOE</li>
<li>DeltaNet Explained (Part I) - Songlin Yang, https://sustcsonglin.github.io/blog/2024/deltanet-1/</li>
<li>Based: Simple linear attention language models balance the recall-throughput tradeoff, https://hazyresearch.stanford.edu/blog/2024-03-03-based</li>
<li>Efficient Alternatives to Transformer Self-Attention: An Analysis of Modern Sequence Modeling Architectures | by Tom Eck | Medium, https://medium.com/@dr.teck/efficient-alternatives-to-transformer-self-attention-397851f324ab</li>
<li>Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey - arXiv, https://arxiv.org/html/2311.12351v2</li>
<li>Sampling Foundational Transformer: A Theoretical Perspective - arXiv, https://arxiv.org/html/2408.05822v2</li>
<li>On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers - arXiv, https://arxiv.org/html/2404.03380v1</li>
<li>Simple linear attention language models balance the recall-throughput tradeoff - arXiv, https://arxiv.org/abs/2402.18668</li>
<li>Simple linear attention language models balance the recall-throughput tradeoff - arXiv, https://arxiv.org/html/2402.18668v1</li>
<li>BASED: Simple linear attention language models balance the recall-throughput tradeoff, https://www.together.ai/blog/based</li>
<li>Revolutionizing Neural Networks: The Rise of Retentive Networks Over Transformers | by azhar - Medium, https://medium.com/ai-insights-cobet/revolutionizing-neural-networks-the-rise-of-retentive-networks-over-transformers-a24be8263f44</li>
<li>A Survey of Retentive Network - arXiv, https://arxiv.org/html/2506.06708v1</li>
<li>Retentive Networks (RetNet) Explained: The much-awaited Transformers-killer is here | by Shantanu Chandra | AI FUSION LABS | Medium, https://medium.com/ai-fusion-labs/retentive-networks-retnet-explained-the-much-awaited-transformers-killer-is-here-6c17e3e8add8</li>
<li>RWKV Architecture History, https://wiki.rwkv.com/basic/architecture.html</li>
<li>A Survey of RWKV - arXiv, https://arxiv.org/html/2412.14847v1</li>
<li>RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale - arXiv, https://arxiv.org/html/2505.03005v1</li>
<li>Designing Hardware-Aware Algorithms with Kimi Linear: Kimi Delta Attention - DigitalOcean, https://www.digitalocean.com/community/tutorials/kimi-linear-moonshot-ai</li>
<li>Kimi Linear: A Revolutionary Attention Mechanism for AI Models - Medium, https://medium.com/@cenghanbayram35/kimi-linear-a-revolutionary-attention-mechanism-for-ai-models-03f4de12047c</li>
<li>Kimi Linear: An Expressive, Efficient Attention Architecture - arXiv, https://arxiv.org/pdf/2510.26692</li>
<li>(PDF) Kimi Linear: An Expressive, Efficient Attention Architecture - ResearchGate, https://www.researchgate.net/publication/397088634_Kimi_Linear_An_Expressive_Efficient_Attention_Architecture</li>
<li>LoLA: Low-Rank Linear Attention With Sparse Caching - OpenReview, https://openreview.net/forum?id=dlLiBQ8tZD</li>
<li>LoLA: Low-Rank Linear Attention with Sparse Caching - arXiv, https://arxiv.org/html/2505.23666v2</li>
<li>What’s Next for Mamba? Towards More Expressive Recurrent Update Rules - Songlin Yang, https://sustcsonglin.github.io/assets/pdf/talk_250117.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>