<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:1.3 어텐션(Attention)의 등장 - 보조 수단에서 핵심 메커니즘으로</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>1.3 어텐션(Attention)의 등장 - 보조 수단에서 핵심 메커니즘으로</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>1.3 어텐션(Attention)의 등장 - 보조 수단에서 핵심 메커니즘으로</span></nav>
                </div>
            </header>
            <article>
                <h1>1.3 어텐션(Attention)의 등장 - 보조 수단에서 핵심 메커니즘으로</h1>
<p>2025-12-17, G30DR</p>
<h2>1.  서론: 인코더-디코더의 구조적 한계와 정보의 병목</h2>
<p>2010년대 중반, 인공지능 번역 기술은 통계적 기계 번역(SMT)의 시대를 지나 신경망 기계 번역(Neural Machine Translation, NMT)이라는 새로운 패러다임으로 진입하고 있었다. 이 변화의 중심에는 조경현 교수와 일리야 수츠케버(Ilya Sutskever) 등이 제안한 시퀀스-투-시퀀스(Sequence-to-Sequence, Seq2Seq) 모델이 있었다. 이 모델은 가변 길이의 입력 시퀀스를 받아 가변 길이의 출력 시퀀스를 생성할 수 있는 인코더-디코더(Encoder-Decoder) 구조를 채택함으로써, 기존의 복잡한 파이프라인을 단일 신경망으로 대체하는 데 성공했다.1</p>
<p>그러나 이 초기 NMT 모델은 곧 치명적인 구조적 결함에 직면하게 된다. 바로 ’고정 길이 벡터(Fixed-length Context Vector)’가 야기하는 정보 압축의 병목(Bottleneck) 현상이었다. 인코더 RNN(Recurrent Neural Network)은 입력 문장의 모든 정보를 마지막 은닉 상태(Last Hidden State)라는 단 하나의 고정된 크기의 벡터 <span class="math math-inline">c</span>에 압축해 넣어야 했다.1 이는 마치 수백 페이지에 달하는 소설을 단 한 장의 포스트잇에 요약한 뒤, 오직 그 요약본만을 보고 원문을 다른 언어로 완벽하게 복원해내라는 요구와 같았다. 이론적으로 LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit)는 장기 의존성(Long-term dependency) 문제를 해결할 수 있는 게이팅 메커니즘을 갖추고 있었으나, 실제로는 문장의 길이가 30단어를 넘어가면 번역 품질이 급격히 저하되는 현상이 관찰되었다.3</p>
<p>정보가 시퀀스의 타임스텝(Time-step)을 거쳐 전달되는 과정에서, 초기 입력 단어의 정보는 희석되거나 소실될 수밖에 없었다. 디코더가 문장의 첫 단어를 번역할 때 참조해야 할 정보가 인코더의 가장 먼 과거에 위치해 있기 때문에 발생하는 ‘배니싱 그래디언트(Vanishing Gradient)’ 문제 또한 여전히 해결되지 않은 난제였다.5 이러한 상황에서 학계는 더 큰 용량의 LSTM을 쌓거나 문장을 반대로 입력하는 등의 트릭을 시도했지만, 이는 근본적인 해결책이 될 수 없었다. 고정된 크기의 벡터라는 틀 자체가 문제였기 때문이다.</p>
<p>이 병목을 해결하기 위해 등장한 것이 바로 ‘어텐션(Attention)’ 메커니즘이다. 이는 인간이 번역을 수행할 때 문장 전체를 암기한 후 번역하는 것이 아니라, 번역하려는 타겟 단어에 해당하는 소스 문장의 특정 부분을 그때그때 찾아보는(Look up) 인지 과정에서 영감을 받았다.1 1.3절에서는 이 어텐션 메커니즘이 어떻게 단순한 RNN의 보조 기억 장치로 시작하여, 딥러닝 아키텍처의 핵심 엔진으로 진화했는지 그 기술적, 역사적 궤적을 심층적으로 추적한다.</p>
<pre><code class="language-mermaid">graph LR
    subgraph "1.3.1 인코더-디코더의 구조적 한계"
        Input("입력 문장 (가변 길이)") --&gt; Encoder["인코더 (RNN)"]
        Encoder -- "정보 압축" --&gt; Bottleneck(("고정 길이&lt;br&gt;문맥 벡터 (c)&lt;br&gt;(병목 구간)"))
        Bottleneck -- "정보 손실 발생&lt;br&gt;(Long-term Dependency)" --&gt; Decoder["디코더 (RNN)"]
        Decoder --&gt; Output("출력 문장&lt;br&gt;(번역 품질 저하)")
        
        style Bottleneck fill:#ff9999,stroke:#333,stroke-width:2px
    end
</code></pre>
<h2>2.  바다나우 어텐션(Bahdanau Attention): 정렬(Alignment)과 번역의 결합</h2>
<p>2014년, 요슈아 벤지오(Yoshua Bengio) 연구팀의 드미트리 바다나우(Dzmitry Bahdanau)는 기념비적인 논문 *“Neural Machine Translation by Jointly Learning to Align and Translate”*를 통해 고정 길이 벡터의 제약을 타파하는 새로운 아키텍처를 제안했다.1 이 연구는 기존의 인코더-디코더 모델이 가진 “전체 문장을 하나의 벡터로 압축한다“는 전제를 정면으로 부정하며, 대신 디코더가 출력을 생성하는 매 시점마다 인코더의 소스 문장 전체를 다시 ’조회(Soft-Search)’하는 방식을 도입했다.1</p>
<pre><code class="language-mermaid">graph TD
    %% 인코더 부분
    EncState1["인코더 은닉 상태 (h_1)"]
    EncState2["인코더 은닉 상태 (h_j)"]
    EncState3["인코더 은닉 상태 (h_Tx)"]

    %% 디코더 이전 상태
    DecPrevState["디코더 이전 상태 (s_t-1)"]

    %% 정렬 모델 (Alignment Model)
    subgraph "정렬 모델 (Feed-Forward Network)"
        Calc1("W_a * s_t-1")
        Calc2("U_a * h_j")
        Sum["덧셈 (Add)"]
        Tanh["활성화 함수 (Tanh)"]
        Va["벡터 v_a 내적"]

        DecPrevState --&gt; Calc1
        EncState2 --&gt; Calc2
        Calc1 --&gt; Sum
        Calc2 --&gt; Sum
        Sum --&gt; Tanh
        Tanh --&gt; Va
    end

    %% 어텐션 가중치 및 문맥 벡터
    Va --&gt; Score["정렬 점수 (e_tj)"]
    Score --&gt; Softmax["소프트맥스 (Softmax)"]
    Softmax --&gt; Weights["어텐션 가중치 (α_tj)"]

    Weights -- "가중합 (Weighted Sum)" --&gt; EncState1
    Weights -- "가중합 (Weighted Sum)" --&gt; EncState2
    Weights -- "가중합 (Weighted Sum)" --&gt; EncState3

    EncState1 &amp; EncState2 &amp; EncState3 --&gt; ContextVector["문맥 벡터 (c_t)"]
    ContextVector --&gt; DecCurrent["디코더 현재 상태 업데이트"]
</code></pre>
<h3>2.1  가산 어텐션(Additive Attention)의 메커니즘과 수식적 원리</h3>
<p>바다나우 모델의 핵심은 ’정렬(Alignment)’이라는 개념을 신경망 내부의 연산으로 구현한 데 있다. 전통적인 통계적 기계 번역(SMT)에서 정렬은 단어 간의 대응 관계를 찾는 별도의 전처리 과정이었으나, 바다나우는 이를 신경망이 학습 과정에서 스스로 깨우치도록(Jointly Learning) 설계했다.9</p>
<p>바다나우 어텐션은 양방향 RNN(Bi-directional RNN)을 인코더로 사용하여 각 단어의 순방향 및 역방향 문맥 정보를 모두 포함하는 주석(Annotation) 벡터 <span class="math math-inline">h_j</span>를 생성한다.11 디코더가 시점 <span class="math math-inline">t</span>에서 타겟 단어 <span class="math math-inline">y_t</span>를 예측하기 위해 필요한 문맥 벡터 <span class="math math-inline">c_t</span>는 더 이상 고정된 값이 아니라, 인코더의 모든 은닉 상태 <span class="math math-inline">h_1, \dots, h_{T_x}</span>의 가중합(Weighted Sum)으로 계산된다.1</p>
<p>이때 각 인코더의 은닉 상태 <span class="math math-inline">h_j</span>가 현재 시점의 디코더 상태 <span class="math math-inline">s_{t-1}</span>과 얼마나 연관되어 있는지를 결정하는 것이 바로 <strong>정렬 모델(Alignment Model)</strong>, 즉 어텐션 스코어 함수(Score Function)다. 바다나우는 이 유사도를 계산하기 위해 단일 은닉층을 가진 피드포워드 신경망(Feed-Forward Neural Network)을 사용했다.12<br />
<span class="math math-display">
\text{score}(s_{t-1}, h_j) = v_a^T \tanh(W_a s_{t-1} + U_a h_j)
</span><br />
여기서 <span class="math math-inline">W_a</span>는 디코더의 이전 은닉 상태 <span class="math math-inline">s_{t-1}</span>을 변환하는 가중치 행렬이고, <span class="math math-inline">U_a</span>는 인코더의 은닉 상태 <span class="math math-inline">h_j</span>를 변환하는 가중치 행렬이다. 두 벡터는 선형 변환 후 더해지며(Add), 비선형 활성화 함수인 <span class="math math-inline">\tanh</span>를 통과한 뒤, 벡터 <span class="math math-inline">v_a</span>와의 내적을 통해 최종적인 스칼라 값인 정렬 점수(Alignment Score) <span class="math math-inline">e_{tj}</span>가 산출된다. 이 과정에서 덧셈 연산이 사용되기에 바다나우 어텐션은 훗날 **가산 어텐션(Additive Attention)**이라 불리게 된다.12</p>
<p>계산된 정렬 점수 <span class="math math-inline">e_{tj}</span>들은 소프트맥스(Softmax) 함수를 통해 확률 분포 형태의 어텐션 가중치(Attention Weight) <span class="math math-inline">\alpha_{tj}</span>로 정규화된다.<br />
<span class="math math-display">
\alpha_{tj} = \frac{\exp(e_{tj})}{\sum_{k=1}^{T_x} \exp(e_{tk})}
</span><br />
최종적으로 문맥 벡터 <span class="math math-inline">c_t</span>는 이 가중치를 이용해 인코더 상태들을 결합하여 생성된다.<br />
<span class="math math-display">
c_t = \sum_{j=1}^{T_x} \alpha_{tj} h_j
</span><br />
이 메커니즘을 통해 모델은 긴 문장 내에서도 현재 번역해야 할 단어와 관련된 부분에만 집중(Attend)할 수 있게 되었다. 즉, 정보의 ’압축’이 아닌 정보의 ’선택적 인출’로 패러다임이 전환된 것이다.3</p>
<h3>2.2  정렬 행렬(Alignment Matrix)이 보여준 가능성</h3>
<p>바다나우 연구의 가장 흥미로운 결과 중 하나는 어텐션 가중치 <span class="math math-inline">\alpha_{tj}</span>를 시각화한 정렬 행렬(Alignment Matrix) 혹은 히트맵(Heatmap)이었다.16 영어-프랑스어 번역 실험에서, 모델은 명사와 형용사의 어순이 반대인 경우(예: “European Economic Area” <span class="math math-inline">\rightarrow</span> “la zone économique européenne”)를 정확하게 포착하여, 타겟 단어를 생성할 때 소스 문장의 대응 단어 위치로 어텐션을 이동시키는 모습을 보여주었다.16</p>
<p>이는 딥러닝 모델이 명시적인 언어학적 지식 주입 없이도 데이터만으로 언어 간의 복잡한 구조적 차이를 학습할 수 있음을 증명한 사례였다. 또한, 내부 동작을 알 수 없는 ’블랙박스(Black Box)’로 여겨지던 신경망에 **해석 가능성(Interpretability)**이라는 새로운 가치를 부여했다. 연구자들은 이제 모델이 번역 실패 시 ’어디를 잘못 보았는지’를 진단할 수 있게 되었다.17</p>
<h2>3.  루옹 어텐션(Luong Attention): 메커니즘의 일반화와 효율성 혁신</h2>
<p>바다나우의 연구가 어텐션의 개념을 정립했다면, 2015년 스탠포드 대학의 루옹(Minh-Thang Luong)과 크리스토퍼 매닝(Christopher Manning) 등이 발표한 *“Effective Approaches to Attention-based Neural Machine Translation”*은 이를 공학적으로 최적화하고 일반화한 연구였다.18 루옹 연구팀은 바다나우 모델의 복잡한 연산 비용과 구조적 제약을 개선하기 위해 다양한 실험을 수행했으며, 이를 통해 어텐션 메커니즘을 **전역(Global)**과 <strong>지역(Local)</strong>, 그리고 다양한 **스코어 함수(Score Function)**로 체계화했다.</p>
<pre><code class="language-mermaid">graph TD
    DecState["디코더 상태 (h_t)"] 
    EncState["인코더 상태 (h_s)"]

    ScoreFunc{"스코어 함수\n(Score Function)"}

    DecState &amp; EncState --&gt; ScoreFunc

    ScoreFunc -- "내적 (Dot)" --&gt; Dot["Dot Product\nscore = h_t^T * h_s"]
    ScoreFunc -- "일반 (General)" --&gt; General["Weight Matrix\nscore = h_t^T * W_a * h_s"]
    ScoreFunc -- "연결 (Concat)" --&gt; Concat["Concatenate &amp; Tanh\nscore = v_a^T * tanh(W[h_t; h_s])"]

    Dot --&gt; Result("빠른 연산 속도\n(트랜스포머의 기반)")
    General --&gt; Result2("표현력 강화")
    Concat --&gt; Result3("비선형성 포함")
</code></pre>
<h3>3.1  승산 어텐션(Multiplicative/Dot-Product Attention)의 제안</h3>
<p>루옹 어텐션의 가장 큰 기여는 정렬 점수를 계산하는 방식을 획기적으로 단순화한 것이다. 바다나우의 가산 어텐션은 <span class="math math-inline">\tanh</span> 활성화 함수와 거대 행렬 연산을 포함한 별도의 신경망 층을 거쳐야 했기에 학습 파라미터가 많고 연산 속도가 느렸다.13 루옹은 두 벡터 간의 유사도를 측정하는 가장 직관적인 방법인 **내적(Dot Product)**을 사용할 것을 제안했다.14<br />
<span class="math math-display">
\text{score}(h_t, \bar{h}_s) = h_t^T \bar{h}_s
</span><br />
여기서 <span class="math math-inline">h_t</span>는 디코더의 현재 은닉 상태, <span class="math math-inline">\bar{h}_s</span>는 인코더의 은닉 상태를 의미한다. 이 방식은 별도의 학습 파라미터가 필요 없으며, 고도로 최적화된 행렬 곱셈(Matrix Multiplication) 라이브러리를 활용할 수 있어 GPU 상에서의 연산 효율성이 극대화된다.20 이를 **승산 어텐션(Multiplicative Attention)**이라 부르며, 훗날 트랜스포머 모델의 근간이 되는 ’Scaled Dot-Product Attention’의 원형이 된다.</p>
<p>루옹은 내적 외에도 두 벡터 사이에 학습 가능한 가중치 행렬 <span class="math math-inline">W_a</span>를 배치하여 유연성을 높인 <strong>General</strong> 방식과, 두 벡터를 연결(Concatenate)하여 처리하는 <strong>Concat</strong> 방식(바다나우 방식의 변형)을 함께 제안하여 어텐션 계산의 범용성을 확보했다.21</p>
<table><thead><tr><th><strong>스코어 함수 명칭</strong></th><th><strong>수식</strong></th><th><strong>특징</strong></th></tr></thead><tbody>
<tr><td><strong>Dot (승산)</strong></td><td><span class="math math-inline">\text{score}(h_t, \bar{h}_s) = h_t^T \bar{h}_s</span></td><td>파라미터 없음, 연산 속도 매우 빠름</td></tr>
<tr><td><strong>General</strong></td><td><span class="math math-inline">\text{score}(h_t, \bar{h}_s) = h_t^T W_a \bar{h}_s</span></td><td>가중치 행렬 <span class="math math-inline">W_a</span> 도입, 표현력 강화</td></tr>
<tr><td><strong>Concat (가산 변형)</strong></td><td><span class="math math-inline">\text{score}(h_t, \bar{h}_s) = v_a^T \tanh(W_a [h_t; \bar{h}_s])</span></td><td>비선형성 포함, 바다나우 방식과 유사</td></tr>
</tbody></table>
<p>[표 1.3.1] 루옹 어텐션에서 제안된 다양한 스코어 함수 비교 21</p>
<h3>3.2  전역(Global) 어텐션과 지역(Local) 어텐션의 이원화</h3>
<p>루옹 연구팀은 어텐션이 소스 문장의 ‘어디를’ 볼 것인가에 대해서도 구조적인 확장을 시도했다. 바다나우 모델은 항상 소스 문장의 모든 단어를 참조하는 방식을 취했는데, 루옹은 이를 **전역 어텐션(Global Attention)**으로 명명했다.18 전역 어텐션은 문맥을 완벽하게 파악할 수 있다는 장점이 있지만, 문장의 길이 <span class="math math-inline">L</span>이 길어질수록 연산량이 <span class="math math-inline">O(L)</span>로 증가하여 매우 긴 문서나 시퀀스에서는 부담이 될 수 있었다.</p>
<p>이를 보완하기 위해 루옹은 **지역 어텐션(Local Attention)**을 제안했다. 이는 소스 문장 전체가 아닌, 현재 예측 위치를 중심으로 한 일정 윈도우(Window) 크기 내의 부분 집합(Subset)만을 참조하는 방식이다.18 지역 어텐션은 ’하드 어텐션(Hard Attention)’의 아이디어를 차용하되, 미분 불가능한 문제를 해결하기 위해 윈도우 내에서 가중치를 부드럽게 적용하는 방식을 택했다. 이는 긴 시퀀스를 처리할 때의 계산 복잡도를 줄이려는 시도였으나, 이후 하드웨어의 발전과 트랜스포머의 등장으로 인해 현대의 LLM에서는 주로 전역 어텐션(Self-Attention)이 표준으로 자리 잡게 되었다.24</p>
<h3>3.3  구조적 차이: 입력 피딩(Input-Feeding)과 정보 흐름</h3>
<p>바다나우와 루옹 모델은 정보가 흐르는 경로(Path)에서도 결정적인 차이를 보인다. 바다나우 모델은 디코더의 <strong>이전 시점(t-1)</strong> 은닉 상태 <span class="math math-inline">s_{t-1}</span>를 쿼리(Query)로 사용하여 어텐션 가중치를 계산하고, 이를 통해 얻은 문맥 벡터를 현재 시점의 입력과 결합하여 디코더 RNN에 주입한다.16 즉, 어텐션의 결과가 RNN의 입력으로 작용한다.</p>
<p>반면, 루옹 모델은 디코더 RNN의 <strong>현재 시점(t)</strong> 은닉 상태 <span class="math math-inline">h_t</span>를 먼저 계산한 후, 이를 쿼리로 사용하여 문맥 벡터 <span class="math math-inline">c_t</span>를 생성한다.25 그리고 이 문맥 벡터를 디코더의 은닉 상태 <span class="math math-inline">h_t</span>와 결합(Concatenate)하여 최종 예측을 위한 층(Softmax Layer)으로 보낸다. 루옹은 또한 이전 시점의 정렬 결정이 현재 시점에 영향을 미칠 수 있도록, 결합된 벡터를 다음 시점의 입력으로 다시 넣어주는 <strong>입력 피딩(Input-Feeding)</strong> 방식을 도입하여 모델이 정렬 이력(Alignment History)을 추적할 수 있게 했다.18 이러한 구조적 유연성은 어텐션 메커니즘이 다양한 신경망 아키텍처에 쉽게 이식될 수 있는 기반을 마련했다.</p>
<h2>4.  비교 분석: 가산 어텐션 대 승산 어텐션</h2>
<p>바다나우의 가산 어텐션과 루옹의 승산 어텐션은 NMT의 발전을 이끈 두 축이었으나, 그 특성과 효율성 면에서 뚜렷한 차이를 보였다.</p>
<pre><code class="language-mermaid">graph TD
    subgraph "바다나우 (Bahdanau)"
        B_Prev["이전 상태 (s_t-1)"] --&gt; B_Attn["어텐션 계산"]
        B_Source["소스 문장 (h)"] --&gt; B_Attn
        B_Attn --&gt; B_Context["문맥 벡터 (c_t)"]
        B_Context --&gt; B_RNN["디코더 RNN\n(입력으로 사용)"]
        B_RNN --&gt; B_Next["현재 상태 (s_t)"]
    end

    subgraph "루옹 (Luong)"
        L_Prev["이전 상태 (h_t-1)"] --&gt; L_RNN["디코더 RNN"]
        L_RNN --&gt; L_Curr["현재 상태 (h_t)"]
        L_Curr --&gt; L_Attn["어텐션 계산"]
        L_Source["소스 문장 (h)"] --&gt; L_Attn
        L_Attn --&gt; L_Context["문맥 벡터 (c_t)"]
        L_Curr &amp; L_Context --&gt; L_Concat["결합 (Concatenate)"]
        L_Concat --&gt; L_Pred["최종 예측 (Softmax)"]
    end
    
    style B_RNN fill:#e1f5fe
    style L_RNN fill:#e1f5fe
    style B_Attn fill:#fff9c4
    style L_Attn fill:#fff9c4
</code></pre>
<h3>4.1  계산 복잡도와 효율성</h3>
<p>가산 어텐션은 이론적으로 더 복잡한 관계를 학습할 수 있는 비선형 활성화 함수(<span class="math math-inline">\tanh</span>)를 포함하고 있어, 데이터의 양이 적거나 차원 <span class="math math-inline">d</span>가 작을 때는 승산 어텐션보다 더 높은 성능을 보이기도 했다.14 그러나 차원 <span class="math math-inline">d_k</span>가 커질수록 가산 어텐션의 연산 비용은 급격히 증가한다. 반면, 승산 어텐션은 최적화된 행렬 연산을 통해 훨씬 빠른 속도로 학습과 추론이 가능하며, 메모리 효율성 측면에서도 우위를 점했다.20</p>
<p>트랜스포머 논문의 저자들은 <span class="math math-inline">d_k</span>가 매우 큰 경우, 승산 어텐션의 내적 값이 너무 커져 소프트맥스 함수의 기울기가 0에 가까워지는(Vanishing Gradient) 문제를 지적했다.14 이로 인해 학습이 불안정해지는 현상이 발생하는데, 이를 해결하기 위해 내적 값을 <span class="math math-inline">\sqrt{d_k}</span>로 나누어주는 **스케일드 닷 프로덕트 어텐션(Scaled Dot-Product Attention)**이 고안되었다.24 이는 승산 어텐션의 효율성을 유지하면서도 가산 어텐션 못지않은 안정성을 확보하는 결정적인 계기가 되었다.</p>
<h3>4.2  정렬 성능과 실험 결과</h3>
<p>바다나우 어텐션은 WMT’14 영어-프랑스어 번역 작업에서 기존 RNNenc 모델(BLEU 17.82)을 크게 상회하는 BLEU 26.75를 기록하며 어텐션의 위력을 입증했다.3 루옹 어텐션은 여기서 한 걸음 더 나아가 WMT’15 영어-독일어 번역에서 전역 어텐션과 지역 어텐션을 앙상블한 모델로 당시 최고 수준인 25.9 BLEU를 달성, 기존 NMT 시스템 대비 5.0점 이상의 성능 향상을 이뤄냈다.18 두 모델 모두 문장의 길이가 길어질수록 성능 격차가 더 벌어지는 경향을 보여, 어텐션이 ’고정 길이 벡터의 병목’을 성공적으로 해소했음을 재확인시켜 주었다.</p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>바다나우 어텐션 (Bahdanau)</strong></th><th><strong>루옹 어텐션 (Luong)</strong></th></tr></thead><tbody>
<tr><td><strong>기반 메커니즘</strong></td><td>가산 (Additive)</td><td>승산 (Multiplicative)</td></tr>
<tr><td><strong>쿼리 소스</strong></td><td>디코더의 이전 상태 (<span class="math math-inline">s_{t-1}</span>)</td><td>디코더의 현재 상태 (<span class="math math-inline">h_t</span>)</td></tr>
<tr><td><strong>연산 방식</strong></td><td><span class="math math-inline">W_1 h + W_2 s</span> 후 <span class="math math-inline">\tanh</span></td><td><span class="math math-inline">h^T s</span> (내적)</td></tr>
<tr><td><strong>장점</strong></td><td>차원이 작을 때 높은 성능, 안정적</td><td>계산 속도 빠름, 메모리 효율적</td></tr>
<tr><td><strong>단점</strong></td><td>연산량 많음, 파라미터 수 증가</td><td>차원이 클 때 스케일링 필요 (<span class="math math-inline">\sqrt{d_k}</span>)</td></tr>
<tr><td><strong>트랜스포머 채택</strong></td><td>-</td><td><strong>채택 (Scaled Dot-Product)</strong></td></tr>
</tbody></table>
<p>[표 1.3.2] 바다나우 어텐션과 루옹 어텐션의 기술적 특성 심층 비교 13</p>
<h2>5.  어텐션의 확장: 텍스트를 넘어 멀티모달로</h2>
<p>어텐션 메커니즘의 성공은 NLP 분야에만 머무르지 않았다. “어디를 볼 것인가“라는 개념은 시각적 정보를 처리하는 컴퓨터 비전 분야에서도 즉각적인 반향을 일으켰다. 2015년 발표된 <em>“Show, Attend and Tell”</em> 논문은 이미지 캡셔닝(Image Captioning) 작업에 어텐션을 도입하여, 캡션을 생성하는 각 시점마다 이미지의 특정 영역(Region)에 집중하는 모델을 선보였다.26</p>
<p>이 연구에서 어텐션은 RNN이 이미지의 전체 특징 맵(Feature Map)을 한 번에 처리하는 대신, 마치 인간의 눈이 중요한 사물을 훑어보듯 이미지의 특정 부분을 순차적으로 ‘엿보는(Glimpse)’ 메커니즘으로 구현되었다.26 이는 바다나우 어텐션의 2차원적 확장판이라 할 수 있었으며, 어텐션이 데이터의 모달리티(Modality)와 무관하게 적용 가능한 범용적인 정보 선택 메커니즘임을 입증했다.</p>
<p>또한, 이 시기에 등장한 ‘글림스(Glimpse)’ 네트워크나 공간 어텐션(Spatial Attention) 모델들은 어텐션이 단순한 정보 조회를 넘어, 입력 데이터 내의 구조적 관계를 파악하고 중요한 특징을 능동적으로 추출하는 도구로 진화하고 있음을 보여주었다.28 이는 훗날 등장할 멀티모달(Multimodal) 트랜스포머와 비전 트랜스포머(ViT)의 사상적 토대가 되었다.</p>
<h2>6.  순환 신경망(RNN)의 퇴조와 어텐션 중심주의의 부상</h2>
<p>2016년까지 어텐션은 어디까지나 RNN의 성능을 보강하기 위한 ‘보조 수단(Auxiliary Tool)’ 혹은 ’엿보기 구멍(Peeping Hole)’으로 간주되었다.29 디코더는 여전히 순차적으로 데이터를 처리하는 RNN이었고, 어텐션은 과거의 인코더 정보를 참조하기 위한 인터페이스에 불과했다. 그러나 바다나우와 루옹의 연구를 거치며 연구자들은 점차 중요한 사실을 깨닫기 시작했다. <strong>“과연 RNN이 필수적인가?”</strong></p>
<pre><code class="language-mermaid">graph TD
    Phase1["1세대: RNN 인코더-디코더"]
    Phase2["2세대: RNN + 어텐션 (바다나우/루옹)"]
    Phase3["3세대: 트랜스포머 (Self-Attention Only)"]

    Phase1 -- "병목 현상 발생\n(고정 벡터)" --&gt; Limit1("긴 문장 번역 실패")
    Limit1 --&gt; Phase2

    Phase2 -- "여전한 문제점" --&gt; Limit2("순차 처리(RNN)로 인한\n병렬화 불가능 &amp; 느린 속도")
    Phase2 -- "핵심 발견" --&gt; Insight("어텐션만으로도\n정보 전달 가능")

    Limit2 &amp; Insight --&gt; Phase3

    Phase3 --&gt; Feature1("순환(Recurrence) 제거")
    Phase3 --&gt; Feature2("완전 병렬 처리 가능")
    Phase3 --&gt; Feature3("장거리 의존성 해결 (O(1))")
</code></pre>
<h3>6.1  RNN이 새로운 병목이 되다</h3>
<p>어텐션이 문맥 벡터의 병목을 해결하자, 이제는 RNN 자체의 구조적 한계가 새로운 병목으로 떠올랐다.</p>
<ol>
<li><strong>순차적 처리의 비효율성:</strong> RNN은 <span class="math math-inline">t</span> 시점의 계산을 위해 <span class="math math-inline">t-1</span> 시점의 결과가 필요하므로, GPU를 이용한 대규모 병렬 처리(Parallelization)가 불가능했다. 이는 데이터셋의 규모가 커질수록 학습 속도를 저하시키는 치명적인 단점이 되었다.24</li>
<li><strong>장거리 의존성 학습의 한계:</strong> 어텐션이 도입되었음에도 불구하고, RNN 기반의 디코더는 여전히 수십 스텝 전의 정보를 ’상태(State)’라는 형태로 릴레이하듯 전달해야 했다. 이는 정보의 경로(Path Length)를 길게 만들어 학습을 어렵게 했다.6</li>
</ol>
<h3>6.2  셀프 어텐션(Self-Attention)의 태동</h3>
<p>연구자들은 어텐션 메커니즘이 문장 내의 거리와 상관없이 <span class="math math-inline">O(1)</span>의 경로로 모든 단어 간의 관계를 직접 연결할 수 있다는 점에 주목했다.7 “만약 RNN 없이 오직 어텐션만으로 시퀀스를 처리한다면?” 이 과감한 질문은 구글의 연구진들로 하여금 <strong>셀프 어텐션(Self-Attention)</strong>, 즉 입력 시퀀스의 각 위치가 자기 자신을 포함한 시퀀스 내의 모든 다른 위치를 참조하여 표현을 갱신하는 방식을 고안하게 만들었다.31</p>
<p>바다나우가 ’인코더와 디코더 사이’를 연결하는 어텐션을 제안했다면, 이제는 ’인코더 내부’와 ’디코더 내부’에서도 어텐션을 사용하여 RNN의 순환 고리를 끊어내려는 시도가 시작된 것이다. 이는 2017년, *“Attention Is All You Need”*라는 도발적인 제목의 논문으로 구체화되며 딥러닝의 역사를 영원히 바꾸어 놓게 된다.24</p>
<h2>7.  결론: 특이점을 향한 마지막 퍼즐 조각</h2>
<p>1.3절에서 살펴본 어텐션의 등장은 단순한 기계 번역 성능의 개선을 넘어, 인공지능이 정보를 처리하는 방식의 근본적인 전환을 의미한다. 바다나우의 가산 어텐션은 “모든 것을 압축할 필요는 없다“는 깨달음을 통해 정보의 병목을 뚫었고, 루옹의 승산 어텐션은 이를 효율적인 행렬 연산으로 구현하여 거대 모델의 등장을 기술적으로 가능케 했다.</p>
<p>이 시기의 연구들은 어텐션을 단순한 ’기억 보조 장치’에서 신경망 아키텍처의 ’중추 신경’으로 격상시켰다. 해석 가능성(Interpretability)을 통해 블랙박스 내부를 비추는 등대가 되었으며, 멀티모달 확장성을 통해 텍스트를 넘어선 세계로 AI의 시야를 넓혔다. 무엇보다 중요한 것은, 이 과정에서 연구자들이 **“순차적(Sequential) 처리에서 관계적(Relational) 처리로”**의 패러다임 시프트를 준비하게 되었다는 점이다.</p>
<p>이제 무대는 마련되었다. RNN이라는 낡은 골조를 걷어내고, 오직 어텐션이라는 엔진만으로 구동되는 거대한 지능의 성(城), **트랜스포머(Transformer)**가 등장할 차례다. 바다나우와 루옹이 닦아놓은 길 위에서 시작될 이 ’트랜스포머 특이점’은, 2025년의 멀티모달 에이전트와 추론 모델에 이르기까지 끊임없이 확장될 AI 혁명의 서막이었다.</p>
<p><strong>(주요 통계 및 비교 요약)</strong></p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>RNN 인코더-디코더</strong></th><th><strong>바다나우 어텐션 (RNNsearch)</strong></th><th><strong>트랜스포머 (Self-Attention)</strong></th></tr></thead><tbody>
<tr><td><strong>정보 접근</strong></td><td>마지막 은닉 상태 (<span class="math math-inline">c</span>)</td><td>모든 은닉 상태의 가중합 (<span class="math math-inline">c_t</span>)</td><td>모든 토큰 간 직접 연결 (Attn)</td></tr>
<tr><td><strong>경로 길이</strong></td><td><span class="math math-inline">O(N)</span> (순차적)</td><td><span class="math math-inline">O(1)</span> (직접 접근)</td><td><span class="math math-inline">O(1)</span> (직접 접근)</td></tr>
<tr><td><strong>병렬화</strong></td><td>불가능 (Sequential)</td><td>부분적 불가능 (RNN 의존)</td><td><strong>완전 가능 (Parallelizable)</strong></td></tr>
<tr><td><strong>복잡도</strong></td><td><span class="math math-inline">O(N)</span></td><td><span class="math math-inline">O(N \times M)</span></td><td><span class="math math-inline">O(N^2)</span> (레이어 당)</td></tr>
<tr><td><strong>핵심 기여</strong></td><td>시퀀스 처리의 신경망화</td><td><strong>고정 벡터 병목 해결</strong></td><td>순환(Recurrence) 제거</td></tr>
</tbody></table>
<p>[표 1.3.3] 시퀀스 모델링 아키텍처의 진화와 어텐션의 역할 변화 24</p>
<h2>8. 참고 자료</h2>
<ol>
<li>[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate - arXiv, https://arxiv.org/abs/1409.0473</li>
<li>Neural machine translation by - arXiv, https://arxiv.org/pdf/1409.0473</li>
<li>Neural Machine Translation By Jointly Learning To Align And Translate, https://courses.engr.illinois.edu/cs546/sp2018/Slides/Mar15_Bahdanau.pdf</li>
<li>Universal Vector Neural Machine Translation with Effective Attention - SMU Scholar, https://scholar.smu.edu/cgi/viewcontent.cgi?article=1132&amp;context=datasciencereview</li>
<li>Pretrained RNN Attention Models - Emergent Mind, https://www.emergentmind.com/topics/pretrained-rnn-attention-models</li>
<li>The Evolution of the Attention Mechanism in Deep Learning: From RNNs to Transformers | by Nikhitha Joy | Medium, https://medium.com/@nikhitha.joy.official/the-evolution-of-the-attention-mechanism-in-deep-learning-from-rnns-to-transformers-317759fa11c5</li>
<li>Attention (machine learning) - Wikipedia, https://en.wikipedia.org/wiki/Attention_(machine_learning)</li>
<li>Neural Machine Translation by Jointly Learning to Align and Translate - Semantic Scholar, https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5</li>
<li>Neural Machine Translation by Jointly Learning to Align and Translate - ResearchGate, https://www.researchgate.net/publication/265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate</li>
<li>[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate - ar5iv, https://ar5iv.labs.arxiv.org/html/1409.0473</li>
<li>Intuitive Introduction to Neural Machine Translation with Bahdanau and Luong Attention, https://blog.paperspace.com/introduction-to-neural-machine-translation-with-bahdanaus-attention/</li>
<li>The Bahdanau Attention Mechanism - MachineLearningMastery.com, https://machinelearningmastery.com/the-bahdanau-attention-mechanism/</li>
<li>Differences Between Luong Attention and Bahdanau Attention | Baeldung on Computer Science, https://www.baeldung.com/cs/attention-luong-vs-bahdanau</li>
<li>What is an attention mechanism? | IBM, https://www.ibm.com/think/topics/attention-mechanism</li>
<li>11.4. The Bahdanau Attention Mechanism - Dive into Deep Learning, https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html</li>
<li>Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing, https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html</li>
<li>Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions - ACL Anthology, https://aclanthology.org/2021.findings-emnlp.39.pdf</li>
<li>[1508.04025] Effective Approaches to Attention-based Neural Machine Translation - arXiv, https://arxiv.org/abs/1508.04025</li>
<li>Evolution of Attention Mechanisms in NLP: From Additive to Self-Attention - Medium, https://medium.com/@akanyaani/evolution-of-attention-mechanisms-in-nlp-from-additive-to-self-attention-01e265925899</li>
<li>An intuitive guide to understanding Bahdanau and Luong attention computation - Medium, https://medium.com/@aajinkya1203/difference-between-bahdanau-and-luong-attention-computation-d0c8d7fe98b2</li>
<li>Define the luong attention mechanism - GitHub Gist, https://gist.github.com/edumunozsala/faa33c7abe2358b7944708f6cb31ec0f</li>
<li>Luong Style Attention Mechanism with Dot and General scoring functions in keras and tensorflow - Stack Overflow, https://stackoverflow.com/questions/62618222/luong-style-attention-mechanism-with-dot-and-general-scoring-functions-in-keras</li>
<li>The Luong Attention Mechanism - MachineLearningMastery.com, https://machinelearningmastery.com/the-luong-attention-mechanism/</li>
<li>Attention is All you Need - NIPS papers, https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf</li>
<li>What is the difference between Luong attention and Bahdanau attention? - Stack Overflow, https://stackoverflow.com/questions/44238154/what-is-the-difference-between-luong-attention-and-bahdanau-attention</li>
<li>Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC8459787/</li>
<li>93 Answer Questions with Right Image Regions: A Visual Attention Regularization Approach, <a href="https://jhyin12.github.io/Papers/TOMM22%20Answer%20Questions%20with%20Right%20Image%20Regions%20A%20Visual%20Attention%20Regularization%20Approach.pdf">https://jhyin12.github.io/Papers/TOMM22%20Answer%20Questions%20with%20Right%20Image%20Regions%20A%20Visual%20Attention%20Regularization%20Approach.pdf</a></li>
<li>Deep Video Representation Learning: a Survey, https://arxiv.org/html/2405.06574v1</li>
<li>seq2seq-Attention peeping into the encoder-states bypasses last encoder-hidden-state, https://stackoverflow.com/questions/37309086/seq2seq-attention-peeping-into-the-encoder-states-bypasses-last-encoder-hidden-s</li>
<li>Understanding RNNs: The Model That Paved the Way for Transformers and the AI Revolution | by Aditya Gupta | Oct, 2025 | Towards AI, https://pub.towardsai.net/understanding-rnns-the-model-that-paved-the-way-for-transformers-and-the-ai-revolution-0cf75099be2a</li>
<li>History of Attention Mechanism &amp; an Introduction to Self-Attention with Visuals &amp; code— Explained | by Shravan Kumar | Medium, https://medium.com/@shravankoninti/history-of-attention-mechanism-an-introduction-to-self-attention-with-visuals-code-explained-a1529c79923e</li>
<li>Semi-Autoregressive Neural Machine Translation - ACL Anthology, https://aclanthology.org/D18-1044.pdf</li>
<li>On The Computational Complexity of Self-Attention - Proceedings of Machine Learning Research, https://proceedings.mlr.press/v201/duman-keles23a/duman-keles23a.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>