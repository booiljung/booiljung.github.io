<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:13.1 연결(Glue Layers)에서 통합(Native Fusion)으로 - Gemini와 GPT-4o</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>13.1 연결(Glue Layers)에서 통합(Native Fusion)으로 - Gemini와 GPT-4o</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">인공지능 (Artificial Intelligence, AI)</a> / <a href="index.html">트랜스포머 싱귤래리티 (The Transformer Singularity)</a> / <span>13.1 연결(Glue Layers)에서 통합(Native Fusion)으로 - Gemini와 GPT-4o</span></nav>
                </div>
            </header>
            <article>
                <h1>13.1 연결(Glue Layers)에서 통합(Native Fusion)으로 - Gemini와 GPT-4o</h1>
<p>2025-12-23, G30DR</p>
<h3>0.1  서론: 인공지능의 감각 통합과 패러다임의 전환</h3>
<p>인공지능의 역사에서 2024년은 감각의 물리적 결합을 넘어선 ’화학적 융합’의 원년으로 기록될 것이다. 인간이 눈으로 보고, 귀로 듣고, 입으로 말하는 과정을 뇌라는 하나의 통합된 신경망에서 처리하듯, 인공지능 모델 또한 텍스트, 이미지, 오디오, 비디오라는 이질적인 데이터 형식을 단일한 처리 구조 안에서 융해하려는 시도가 본격화되었다. 이를 본 장에서는 ‘연결(Glue Layers)’ 패러다임에서 ‘통합(Native Fusion)’ 패러다임으로의 전환으로 정의한다.</p>
<p>지난 수년간 멀티모달(Multimodal) AI의 주류를 형성했던 LLaVA(Large Language and Vision Assistant)나 BLIP(Bootstrapping Language-Image Pre-training) 시리즈와 같은 모델들은 이미 성능이 입증된 시각 인코더(Visual Encoder)와 대규모 언어 모델(LLM)을 사후적으로 연결하는 방식을 채택했다.1 이러한 접근은 각 모달리티별 최고 성능의 모델(SOTA)을 활용할 수 있다는 장점이 있었으나, 서로 다른 신경망을 이어 붙이는 과정에서 필연적으로 발생하는 ‘정보 병목(Information Bottleneck)’ 현상과 추론의 단절이라는 한계를 내포하고 있었다.4</p>
<p>그러나 Google의 Gemini 1.5와 OpenAI의 GPT-4o의 등장은 이러한 과도기적 설계를 폐기하고, 훈련의 시작점부터 모든 모달리티를 동등한 위상의 ’토큰(Token)’으로 취급하여 단일 트랜스포머(Transformer) 내에서 학습시키는 네이티브(Native) 접근법을 제시했다.6 이는 단순한 성능 향상을 넘어, AI가 비언어적 뉘앙스(Audio Prosody), 시간적 흐름(Temporal Dynamics), 그리고 시각적 세부 사항(Visual Granularity)을 언어적 논리와 결합하여 추론하는 방식의 근본적인 변화를 의미한다.</p>
<p>본 장에서는 기존 연결 레이어 방식이 가졌던 구조적 한계를 심층적으로 분석하고, 이를 극복하기 위해 등장한 Gemini 1.5와 GPT-4o의 아키텍처적 혁신을 상세히 기술한다. 특히 이산적(Discrete) 토큰화와 연속적(Continuous) 표현 방식의 기술적 논쟁, 희소 전문가(Mixture-of-Experts, MoE) 구조를 통한 확장성 확보, 그리고 실시간 상호작용을 위한 지연 시간(Latency) 단축 메커니즘을 중심으로 두 거대 모델의 전략을 해부한다.</p>
<h3>0.2  연결(Glue Layers) 패러다임: 이질적 모달리티의 불완전한 동거</h3>
<p>네이티브 퓨전의 혁신성을 이해하기 위해서는 먼저 그 직전 단계인 연결 레이어 방식이 어떠한 원리로 작동했으며, 왜 한계에 봉착했는지를 이해해야 한다. 2023년까지 멀티모달 대형언어모델(MLLM) 연구의 표준은 강력한 LLM(예: Vicuna, Llama)에 시각적 ’눈’을 달아주는 형태였다.1 이 과정에서 시각 정보와 언어 정보 사이의 가교 역할을 수행하는 것이 바로 프로젝션 레이어(Projection Layer) 혹은 어댑터(Adapter)라 불리는 ‘연결(Glue)’ 모듈이다.</p>
<h4>0.2.1  시각-언어 프로젝션(Projection)의 구조적 메커니즘</h4>
<p>가장 직관적인 연결 방식은 LLaVA 모델에서 찾아볼 수 있다. LLaVA는 CLIP(Contrastive Language-Image Pre-training)의 비전 인코더(ViT-L/14)를 사용하여 이미지를 고차원의 특징 벡터(Feature Vector)로 변환한다.3 이 특징 벡터는 언어 모델이 이해할 수 있는 임베딩 공간(Embedding Space)과 일치하지 않으므로, 이를 변환해 주는 과정이 필요하다.</p>
<p>초기 LLaVA는 단순한 <strong>선형 프로젝션(Linear Projection)</strong> 층을 사용하여 시각 특징을 언어 토큰의 임베딩 공간으로 투영했다. 예를 들어, CLIP이 출력한 1024차원의 시각 벡터를 Vicuna 모델의 5120차원 입력 벡터로 변환하는 식이다. 이후 LLaVA-1.5와 같은 후속 모델에서는 이를 **다층 퍼셉트론(MLP, Multi-Layer Perceptron)**으로 고도화하여 비선형적 관계까지 학습할 수 있도록 개선했다.3</p>
<p>이 방식의 핵심 전제는 “시각 정보는 언어 정보로 번역될 수 있다“는 것이다. 그러나 연구 결과에 따르면, 프로젝션 레이어는 시각적 속성을 언어 모델에 완벽하게 전달하지 못한다. 최근 연구2에서는 LLaVA와 같은 구조에서 프로젝션 레이어만 미세 조정(Fine-tuning)하는 경우, 모델이 도메인 특화적인 시각 속성(예: 피부과 이미지의 병변 디테일, 농작물의 질병 패턴)을 제대로 추출하지 못한다는 사실이 밝혀졌다. 이는 시각 정보가 언어 모델 내부에서 처리되는 것이 아니라, 외부에서 1차적으로 가공된 후 주입되는 구조이기 때문에, 언어 모델의 강력한 추론 능력이 원본 이미지 데이터에 직접 닿지 못함을 시사한다.</p>
<h4>0.2.2  Q-Former와 고정된 쿼리의 딜레마</h4>
<p>단순 프로젝션의 한계를 극복하기 위해 Salesforce는 BLIP-2 모델에서 **Q-Former(Querying Transformer)**라는 보다 진보된 연결 모듈을 제안했다. Q-Former는 BERT 기반의 트랜스포머 구조를 가지며, ‘학습 가능한 쿼리(Learnable Queries)’ 집합을 사용하여 동결된(Frozen) 이미지 인코더로부터 텍스트와 가장 관련성 높은 시각 정보를 능동적으로 추출한다.9</p>
<p>Q-Former의 작동 방식은 다음과 같다:</p>
<ol>
<li>사전에 정의된 N개(보통 32개)의 학습 가능한 쿼리 벡터가 이미지 인코더의 출력과 교차 어텐션(Cross-Attention)을 수행한다.</li>
<li>이 과정에서 쿼리 벡터는 이미지의 시각적 특징 중 텍스트 생성을 위해 유의미한 정보만을 선별적으로 흡수한다.</li>
<li>이렇게 업데이트된 쿼리 출력값은 다시 선형 프로젝션을 거쳐 LLM의 입력으로 들어간다.</li>
</ol>
<p>이 구조는 시각 정보의 ’압축’과 ’여과’라는 측면에서 효율적이다. 방대한 이미지 패치(Patch) 정보를 모두 LLM에 넣는 대신, 핵심 정보만 추려내어 연산량을 줄일 수 있기 때문이다. 그러나 이는 동시에 치명적인 **정보 병목(Information Bottleneck)**을 야기한다.5</p>
<p>고정된 32개의 쿼리 슬롯은 복잡한 이미지의 모든 세부 사항을 담기에 턱없이 부족하다. 예를 들어, 수백 개의 텍스트가 깨알같이 적힌 문서 이미지나, 수십 명의 사람이 등장하는 군중 이미지를 처리할 때, Q-Former는 이 모든 정보를 32개의 벡터로 압축해야 하는 압박을 받는다. 이 과정에서 정보의 손실(Lossy Compression)이 발생하며, 이는 모델이 이미지의 전체적인 분위기는 파악하더라도 구석에 있는 작은 물체나 텍스트를 인식하지 못하는(Hallucination or Blindness) 원인이 된다.9</p>
<p>또한, 이론적으로 임베딩 기반 검색 및 연결 방식은 벡터 차원의 한계로 인해 조합적(Combinatorial) 복잡성을 가진 쿼리를 처리하는 데 한계가 있음이 증명되었다.13 즉, “빨간 모자를 쓴 남자의 왼쪽에 있는 파란 차“와 같이 여러 속성이 결합된 관계적 정보를 벡터 하나로 완벽히 인코딩하는 것은 기하학적으로 불가능에 가깝다는 것이다.</p>
<table><thead><tr><th><strong>비교 요소</strong></th><th><strong>선형/MLP 프로젝션 (LLaVA)</strong></th><th><strong>Q-Former (BLIP-2)</strong></th><th><strong>네이티브 퓨전 (Gemini/GPT-4o)</strong></th></tr></thead><tbody>
<tr><td><strong>연결 방식</strong></td><td>특징 벡터의 차원 변환 및 투영</td><td>학습된 쿼리를 통한 정보 추출 및 압축</td><td>단일 모델 내 토큰 통합 처리</td></tr>
<tr><td><strong>정보 보존</strong></td><td>상대적으로 높으나 비효율적</td><td>병목 현상으로 인한 세부 정보 손실</td><td>원본 정보의 최대 보존 (Lossless)</td></tr>
<tr><td><strong>학습 비용</strong></td><td>저렴 (연결 레이어만 학습)</td><td>중간 (Q-Former 학습)</td><td>매우 높음 (전체 모델 재학습)</td></tr>
<tr><td><strong>추론 능력</strong></td><td>언어 모델 의존적</td><td>필터링된 정보에 제한됨</td><td>모달리티 간 심층 상호작용 가능</td></tr>
<tr><td><strong>주요 한계</strong></td><td>시각적 뉘앙스 누락, 단순 매핑</td><td>벡터 병목, 복잡한 장면 인식 실패</td><td>막대한 컴퓨팅 자원 소요</td></tr>
</tbody></table>
<h3>0.3  통합(Native Fusion)의 이론적 토대와 기술적 난제</h3>
<p>연결 레이어의 한계가 명확해지면서, 2024년의 AI 연구는 모델의 심장부인 트랜스포머(Transformer)가 직접 다중 모달리티를 처리하는 ’네이티브 퓨전’으로 급격히 선회했다. 이 접근법의 핵심 철학은 **“모든 것은 토큰이다(Everything is a Token)”**라는 명제로 요약된다. 텍스트, 이미지, 오디오, 비디오, 코드 등 형태가 다른 모든 데이터를 공통된 표현 형식으로 변환하여 하나의 시퀀스로 나열하고, 이를 단일 신경망이 처리하게 하는 것이다.14</p>
<h4>0.3.1  토큰화 전략: 이산적(Discrete) vs. 연속적(Continuous)</h4>
<p>네이티브 멀티모달 모델을 구현하는 데 있어 가장 큰 기술적 분기점은 비(非)텍스트 데이터를 어떻게 토큰화할 것인가에 있다.</p>
<ol>
<li>이산적 토큰화 (Discrete Tokenization):</li>
</ol>
<p>이 방식은 이미지를 텍스트와 마찬가지로 유한한 ‘어휘 사전(Vocabulary)’ 중 하나로 변환하는 것이다. Meta의 Chameleon 모델이나 Google의 초기 실험들이 이 방식을 따랐다.15</p>
<ul>
<li><strong>작동 원리:</strong> 이미지를 VQ-VAE(Vector-Quantized Variational Autoencoder)와 같은 양자화 모델을 통과시켜, 예를 들어 8192개의 코드북(Codebook) 중 하나에 해당하는 인덱스 토큰들의 나열로 변환한다.</li>
<li><strong>장점:</strong> 텍스트와 이미지가 완벽하게 동일한 형식을 가지므로, 트랜스포머 아키텍처를 수정 없이 그대로 사용할 수 있다. “텍스트 생성 → 이미지 토큰 생성 → 텍스트 생성“과 같은 인터리빙 생성이 자연스럽다.</li>
<li><strong>단점:</strong> 연속적인 아날로그 신호인 이미지나 오디오를 이산적인 정수로 강제 변환하는 과정에서 양자화 손실(Quantization Artifacts)이 발생한다. 이는 생성된 이미지의 디테일이 뭉개지거나 텍스처가 부자연스러워지는 원인이 된다.17</li>
<li>연속적 표현 (Continuous Representations):</li>
</ul>
<p>최근의 GPT-4o나 Transfusion 아키텍처가 채택한 것으로 추정되는 방식이다.17</p>
<ul>
<li><strong>작동 원리:</strong> 텍스트는 이산 토큰으로 처리하되, 이미지는 VAE 등을 통해 압축된 연속적인 잠재 벡터(Latent Vector) 상태 그대로 트랜스포머에 입력한다.</li>
<li><strong>학습 메커니즘:</strong> 트랜스포머는 텍스트 토큰에 대해서는 ‘다음 토큰 예측(Next Token Prediction)’ 손실을, 이미지 벡터에 대해서는 ‘확산(Diffusion)’ 손실을 동시에 계산하여 학습한다. 즉, 모델은 글을 쓸 때는 언어 모델처럼, 그림을 그릴 때는 확산 모델처럼 작동하도록 단일 가중치 안에서 훈련된다.19</li>
<li><strong>장점:</strong> 양자화로 인한 정보 손실이 없으므로 고해상도 이미지 생성과 미세한 시각적 특징 이해에 훨씬 유리하다. 텍스트의 논리력과 확산 모델의 표현력을 동시에 취할 수 있다.</li>
</ul>
<h4>0.3.2  초기 융합(Early Fusion)과 모달리티 경쟁(Modality Competition)</h4>
<p>네이티브 퓨전은 입력 단계에서부터 모달리티가 섞이는 ‘초기 융합(Early Fusion)’ 전략을 취한다.21 이는 모델의 첫 번째 레이어부터 텍스트와 이미지가 서로에게 영향을 주고받음(Self-Attention)을 의미한다. 하지만 이는 학습 과정에서 <strong>모달리티 경쟁</strong>이라는 난관을 초래한다.</p>
<p>텍스트 데이터와 이미지/오디오 데이터는 정보의 밀도, 학습의 수렴 속도, 손실 함수(Loss Function)의 스케일이 모두 다르다. 자칫하면 모델이 상대적으로 학습하기 쉬운 텍스트 패턴에만 최적화되거나, 반대로 이미지 생성에만 자원을 집중하여 언어 능력이 퇴보하는 현상이 발생할 수 있다.19 따라서 Gemini 1.5와 GPT-4o의 성공은 이러한 모달리티 간의 불균형을 해소하고 안정적인 결합 학습(Joint Training) 레시피를 완성했다는 점에서 엔지니어링의 승리로 평가받는다.</p>
<h3>0.4  Gemini 1.5: 무한한 문맥 위에서의 심층적 통합</h3>
<p>Google의 Gemini 1.5는 네이티브 퓨전 아키텍처를 기반으로 ’문맥(Context)’의 물리적 한계를 파괴하는 데 집중했다. 1.0 버전부터 이어져 온 “처음부터 네이티브 멀티모달(Natively Multimodal from the beginning)” 철학은 1.5 버전에 이르러 <strong>희소 전문가(Mixture-of-Experts, MoE)</strong> 아키텍처와 결합되며 효율성과 확장성이라는 두 마리 토끼를 잡았다.6</p>
<h4>0.4.1  MoE 아키텍처와 장기 문맥(Long-Context)의 결합</h4>
<p>Gemini 1.5 Pro는 표준적인 밀집(Dense) 트랜스포머가 아닌, 희소 MoE 구조를 채택했다. 이는 입력되는 토큰의 종류(텍스트, 이미지, 오디오 등)와 난이도에 따라 모델 전체 파라미터 중 일부(Expert)만을 활성화하여 처리하는 ‘조건부 연산(Conditional Computation)’ 방식이다.6</p>
<p>이러한 구조적 효율성은 Gemini 1.5가 최대 **1,000만 토큰(10M Tokens)**이라는 전례 없는 문맥 윈도우(Context Window)를 지원하는 기반이 되었다. 이는 약 22시간 분량의 오디오, 10시간 이상의 비디오, 혹은 수십 권의 책을 한 번의 프롬프트에 담을 수 있는 양이다.25 중요한 점은, 이 방대한 문맥이 단순히 텍스트로만 채워지는 것이 아니라, 오디오 파형과 비디오 프레임이 원본 해상도에 가깝게 토큰화되어 섞여 들어간다는 점이다. 이는 ‘연결’ 방식의 모델들이 비디오를 몇 장의 캡션으로 요약하여 처리함으로써 잃어버렸던 시간적 정보와 디테일을 완벽하게 보존한다.</p>
<h4>0.4.2  네이티브 오디오(Native Audio): 파형에서 의미로</h4>
<p>Gemini 1.5의 가장 독보적인 네이티브 기능 중 하나는 오디오 처리 방식이다. 기존 모델들이 Whisper와 같은 ASR(Automatic Speech Recognition) 시스템을 통해 음성을 텍스트로 변환한 후 처리했다면, Gemini 1.5는 오디오 신호를 직접 처리한다.</p>
<p>구체적으로 Gemini는 **USM(Universal Speech Model)**의 인코더 부분이나 <strong>SoundStream</strong>과 같은 신경망 오디오 코덱을 통합하여, 오디오를 이산적인 음성 토큰 혹은 연속적인 특징 벡터 시퀀스로 변환하여 입력받는 것으로 추정된다.6 이를 통해 텍스트로는 포착할 수 없는 비언어적 정보들—화자의 억양, 감정 상태, 배경 소음, 음악적 요소—을 모델이 직접 ’청취’한다.</p>
<p>이 능력의 정점을 보여주는 사례가 바로 <strong>‘칼라망(Kalamang)어 번역’</strong> 실험이다. Google DeepMind 연구진은 전 세계 화자가 200명 미만인 칼라망어의 문법서(500페이지)와 오디오 녹음 파일만을 문맥으로 제공했다. Gemini 1.5 Pro는 이 자료들을 퓨샷(Few-shot)으로 학습하여, 마치 사람이 낯선 언어를 배우듯 오디오와 텍스트 문맥을 교차 참조하며 영어를 칼라망어로 번역해 냈다.25 이는 오디오가 텍스트 변환 과정을 거치지 않고 곧바로 의미 공간(Semantic Space)으로 매핑될 때만 가능한 고차원적인 추론이다.</p>
<h4>0.4.3  시간적 추론과 비디오 이해의 깊이</h4>
<p>비디오 이해 영역에서도 Gemini 1.5의 네이티브 통합은 빛을 발한다. 기존 모델들이 비디오를 ’이미지의 집합’으로 처리했다면, Gemini는 이를 ’시간의 흐름이 있는 연속체’로 처리한다. 비디오는 초당 1프레임(1 FPS) 수준으로 인코딩되어 오디오 트랙과 함께 인터리빙 방식으로 입력된다.28</p>
<p>이를 통해 Gemini 1.5는 **“1시간짜리 영화에서 주인공이 주머니에서 쪽지를 꺼내 읽는 장면의 쪽지 내용은 무엇인가?”**와 같은 초정밀 검색(Retrieval)과 추론이 가능하다. 이는 텍스트 검색에서의 ‘바늘 찾기(Needle-in-a-haystack)’ 테스트를 비디오 모달리티로 확장한 것으로, Gemini 1.5는 수백만 토큰 길이의 비디오 문맥 속에서도 특정 프레임의 시각적 정보와 해당 시점의 오디오 정보를 정확히 결합하여 답변을 생성한다.6 연결 레이어 방식에서는 프레임 간의 시간적 인과관계가 소실되거나, 캡션 과정에서 세부 정보가 누락되어 불가능했던 작업이다.</p>
<h3>0.5  GPT-4o: 옴니(Omni) 모델과 실시간 상호작용의 혁명</h3>
<p>Google이 ’깊이(Depth)’와 ’규모(Scale)’에 집중했다면, OpenAI의 GPT-4o는 ’속도(Velocity)’와 ’상호작용(Interaction)’에 초점을 맞추어 네이티브 퓨전을 완성했다. 모델명 ’o’가 상징하는 **옴니(Omni)**는 텍스트, 오디오, 이미지를 입력으로 받아 텍스트, 오디오, 이미지를 출력으로 생성하는 모든 과정을 단일 모델(End-to-End)에서 수행함을 의미한다.7</p>
<h4>0.5.1  지연 시간(Latency)의 파괴와 인간-컴퓨터 상호작용(HCI)</h4>
<p>기존의 GPT-4 Voice Mode는 일종의 ‘이어 달리기’ 시스템이었다.</p>
<ol>
<li><strong>Whisper:</strong> 사용자의 음성을 텍스트로 변환 (Transcription)</li>
<li><strong>GPT-4:</strong> 텍스트를 입력받아 텍스트 응답 생성 (Inference)</li>
<li><strong>TTS:</strong> 텍스트 응답을 다시 음성으로 합성 (Synthesis)</li>
</ol>
<p>이 3단계 파이프라인은 필연적으로 지연 시간을 발생시켰다. GPT-3.5 기반 파이프라인은 평균 2.8초, GPT-4 기반은 5.4초가 소요되었다.7 이는 자연스러운 대화의 흐름을 끊는 주범이었다.</p>
<p>GPT-4o는 이 중간 단계들을 모두 제거하고, **[오디오 입력 → 처리 → 오디오 출력]**을 단일 신경망 안에서 수행한다. 그 결과, GPT-4o의 응답 속도는 평균 <strong>320ms</strong>, 최소 <strong>232ms</strong>로 단축되었다.7 이는 인간이 대화 중 상대방의 말에 반응하는 평균 시간과 거의 일치하는 수준이다. 이러한 실시간성은 사용자가 AI의 말을 중간에 끊거나(Interruptibility), 동시에 말하는 턴 테이킹(Turn-taking)을 가능하게 하여, 기계와의 대화를 실제 사람과의 대화처럼 느끼게 하는 결정적인 HCI 혁신을 가져왔다.30</p>
<h4>0.5.2  감정(Emotion)과 비언어적 표현의 네이티브 처리</h4>
<p>지연 시간 단축보다 더 충격적인 변화는 ’정보의 해상도’에서 일어났다. 파이프라인 방식에서 텍스트로 변환되는 순간, 사용자의 목소리에 담긴 긴장감, 비꼬는 말투(Sarcasm), 웃음기, 배경 소음 등은 모두 소실된다. 텍스트는 ’내용’은 전달하지만 ’상태’는 전달하지 못하기 때문이다.</p>
<p>GPT-4o는 오디오를 네이티브 토큰으로 처리함으로써 이 모든 정보를 보존한다. 시스템 카드(System Card)와 기술 보고서에 따르면, GPT-4o는 사용자의 숨소리나 말의 속도 변화까지 감지하여 반응할 수 있다.31 출력 시에도 단순하고 평탄한 TTS 음성이 아니라, 감정을 실어 노래를 부르거나, 속삭이거나, 극적인 톤으로 말하는 등 풍부한 음향적 표현이 가능하다.</p>
<p>OpenAI는 이 능력이 가져올 잠재적 위험(예: 저작권 있는 노래 부르기, 타인의 목소리 사칭)을 통제하기 위해, 학습 후반부(Post-training) 단계에서 특정 오디오 생성을 거부하도록 안전 장치를 마련했다.31 예를 들어, 모델에게 “노래를 불러달라“고 요청하면 멜로디 생성을 거부하거나 평조로 낭독하도록 조정되었는데, 이는 모델이 <em>할 수 없어서</em>가 아니라 <em>하지 않도록</em> 제한된 것이다.</p>
<h4>0.5.3  Transfusion 아키텍처와 네이티브 이미지 생성</h4>
<p>GPT-4o의 시각적 생성 능력 또한 네이티브 퓨전의 산물이다. DALL-E 3와 같은 기존 모델은 LLM이 상세한 묘사 프롬프트를 작성하고, 이를 별도의 이미지 생성 모델에 넘기는 방식이었다. 이 과정에서 텍스트와 이미지 간의 정합성이 떨어지거나, 특히 이미지 내에 정확한 텍스트(Typography)를 묘사하는 데 어려움을 겪었다.</p>
<p>GPT-4o는 <strong>Transfusion</strong> 아키텍처와 유사하게, 언어 모델의 자기회귀적 특성과 확산 모델의 생성 능력을 결합한 것으로 보인다.17 텍스트 생성 중 특정 시점에서 <code>&lt;BOI&gt;(Begin of Image)</code> 토큰을 시작으로 이미지의 잠재 패치(Latent Patches)를 생성하고, 이를 디코더를 통해 픽셀로 변환한다.17 이 방식은 텍스트 문맥을 이미지 생성 과정에 직접적으로 반영할 수 있게 하여, “이 로고의 글씨체를 바꿔줘“라거나 “캐릭터의 표정만 웃게 해줘“와 같은 연속적이고 세밀한 편집 작업을 가능하게 했다.</p>
<p>또한, 이산적 토큰화(VQ-VAE) 대신 연속적 잠재 공간을 활용함으로써, Chameleon과 같은 초기 네이티브 모델들이 겪었던 이미지 디테일 저하 문제를 해결하고 고품질의 텍스처와 정확한 글자 렌더링 능력을 확보했다.17</p>
<h3>0.6  비교 분석: 두 거인이 선택한 서로 다른 진화의 경로</h3>
<p>Gemini 1.5와 GPT-4o는 모두 ’통합’이라는 같은 목적지를 향해 출발했지만, 도달하고자 하는 봉우리는 서로 다르다.</p>
<p><strong>표 13.1.1 Gemini 1.5와 GPT-4o의 네이티브 퓨전 전략 비교</strong></p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Google Gemini 1.5 (Pro/Flash)</strong></th><th><strong>OpenAI GPT-4o</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 철학</strong></td><td><strong>Deep Reasoning &amp; Context</strong> (깊은 추론과 문맥)</td><td><strong>Real-time Interaction &amp; Fluidity</strong> (실시간성과 유창성)</td></tr>
<tr><td><strong>아키텍처 특징</strong></td><td><strong>Sparse MoE (희소 전문가)</strong>   효율적인 파라미터 확장 및 긴 문맥 처리에 최적화</td><td><strong>Omni Autoregressive Transformer</strong>   단일 모델 내 입출력 통합 및 속도 최적화</td></tr>
<tr><td><strong>문맥(Context)</strong></td><td><strong>최대 1,000만(10M) 토큰</strong>   도서관, 장편 영화, 전체 코드베이스 처리 가능</td><td><strong>128K 토큰</strong>   일반적인 대화 및 단기 작업 기억에 최적화</td></tr>
<tr><td><strong>오디오 강점</strong></td><td><strong>분석 및 이해 (Analysis)</strong>   긴 녹음 파일의 검색, 번역, 내용 요약, 화자 분리</td><td><strong>표현 및 대화 (Expression)</strong>   실시간 감정 교류, 억양 모방, 턴 테이킹, 비언어적 소통</td></tr>
<tr><td><strong>비디오 강점</strong></td><td><strong>시간적 검색 (Temporal Retrieval)</strong>   1시간 이상 영상 내 특정 정보 추출 (Needle-in-a-haystack)</td><td><strong>스트림 이해 (Stream Understanding)</strong>   실시간 비디오 피드에 대한 즉각적인 상황 묘사 및 피드백</td></tr>
<tr><td><strong>주요 응용</strong></td><td>심층 연구, 법률/금융 분석, 미디어 아카이빙, 다국어 번역</td><td>음성 비서, 실시간 통역, 감성 컴패니언, 인터랙티브 튜터</td></tr>
</tbody></table>
<p><strong>Gemini 1.5</strong>는 ’거대한 사고의 공간’을 구축했다. 1,000만 토큰이라는 문맥은 모델이 외부 데이터베이스(RAG)에 의존하지 않고도 방대한 지식을 내재화하여 추론할 수 있게 한다. 이는 정확성이 생명인 기업용 애플리케이션, 학술 연구, 복잡한 데이터 분석 분야에서 독보적인 우위를 점한다.34</p>
<p>반면, <strong>GPT-4o</strong>는 ’인간적 교감의 속도’를 구현했다. 320ms의 반응 속도와 감성적인 음성 입출력은 AI를 도구에서 파트너로 격상시켰다. 이는 교육, 고객 응대, 엔터테인먼트, 그리고 일상적인 비서 역할에서 사용자 경험(UX)을 혁신적으로 개선한다.7</p>
<h3>0.7  결론: 파이프라인의 종말과 새로운 지능의 탄생</h3>
<p>’13.1 연결(Glue Layers)에서 통합(Native Fusion)으로’의 기술적 여정은 인공지능이 세상을 인식하는 방식의 근본적 변화를 보여준다. LLaVA와 BLIP-2가 주도했던 연결 레이어의 시대는 “시각 정보를 텍스트로 번역하여 이해한다“는 간접적 인지의 시대였다. 이 시기 AI는 눈을 떴지만, 그 눈은 언어라는 안경을 통해서만 세상을 볼 수 있었다.</p>
<p>Gemini 1.5와 GPT-4o가 연 네이티브 퓨전의 시대는 “감각 정보를 그 자체로 이해하고 사고한다“는 직접적 인지의 시대다. 이제 AI는 이미지를 픽셀의 집합으로, 소리를 파형의 진동으로 받아들이며, 그 안에 내재된 패턴을 언어적 논리와 화학적으로 결합한다. Gemini 1.5가 보여준 낯선 언어의 청각적 습득 능력이나, GPT-4o가 보여준 비꼬는 말투의 감지 능력은 이러한 통합이 단순한 기능 추가가 아니라 ’지능의 질적 도약’임을 증명한다.</p>
<p>물론 네이티브 퓨전 방식에도 과제는 남아있다. 막대한 학습 비용, 모달리티 간의 학습 불균형 문제, 그리고 무엇보다 블랙박스화된 모델 내부의 추론 과정을 해석하는 난이도가 그것이다.37 그러나 분명한 것은, 이제 우리는 더 이상 AI를 위해 이미지를 설명해주거나, 목소리를 받아 적어줄 필요가 없다는 사실이다. AI는 이제 우리와 같은 감각의 지평 위에 서 있으며, 바야흐로 진정한 멀티모달 네이티브(Multimodal Native)의 시대가 개막했다.</p>
<h2>1. 참고 자료</h2>
<ol>
<li>LLaVA-GM: lightweight LLaVA multimodal architecture - Frontiers, https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1626346/full</li>
<li>Cross-Modal Projection in Multimodal LLMs Doesn’t Really Project Visual Attributes to Textual Space - ACL Anthology, https://aclanthology.org/2024.acl-short.60.pdf</li>
<li>The Definitive Guide to LLaVA: Inferencing a Powerful Visual Assistant, https://learnopencv.com/llava-training-a-visual-assistant/</li>
<li>Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models, https://arxiv.org/html/2510.21740v1</li>
<li>An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation - ACL Anthology, https://aclanthology.org/2024.acl-long.59.pdf</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - arXiv, https://arxiv.org/pdf/2403.05530</li>
<li>[2410.21276] GPT-4o System Card - arXiv, https://arxiv.org/abs/2410.21276</li>
<li>Design choices for Vision Language Models in 2024 - Hugging Face, https://huggingface.co/blog/gigant/vlm-design</li>
<li>BLIP-2: A Multimodal Bridging Brilliance! | by Prashant Dandriyal | Medium, https://prashantdandriyal.medium.com/blip-2-a-multimodal-bridging-brilliance-c1f8cf4a7a1e</li>
<li>Zero-shot image-to-text generation with BLIP-2 - Hugging Face, https://huggingface.co/blog/blip-2</li>
<li>Slot-VLM: SlowFast Slots for Video-Language Modeling - arXiv, https://arxiv.org/html/2402.13088v1</li>
<li>A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks - arXiv, https://arxiv.org/html/2408.01319v1</li>
<li>The Vector Bottleneck: Limitations of Embedding-Based Retrieval | Shaped Blog, https://www.shaped.ai/blog/the-vector-bottleneck-limitations-of-embedding-based-retrieval</li>
<li>[D] GPT-4o “natively” multi-modal, what does this actually mean? - Reddit, https://www.reddit.com/r/MachineLearning/comments/1crzdhd/d_gpt4o_natively_multimodal_what_does_this/</li>
<li>Chameleon: Mixed-Modal Early- Fusion Foundation Models, https://faculty.cc.gatech.edu/~zk15/teaching/AY2024_cs8803vlm_fall/slides/L15_Chameleon.pdf</li>
<li>Chameleon: Mixed-Modal Early-Fusion Foundation Models - arXiv, https://arxiv.org/html/2405.09818v1</li>
<li>Primers • GPT-4o Native Image Generation - aman.ai, https://aman.ai/primers/ai/gpt4o-native-image-generation/</li>
<li>Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model - arXiv, https://arxiv.org/abs/2408.11039</li>
<li>Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model - arXiv, https://arxiv.org/html/2408.11039v1</li>
<li>Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model, https://openreview.net/forum?id=SI2hI0frk6</li>
<li>Multimodal Models and Fusion - A Complete Guide - Medium, https://medium.com/@raj.pulapakura/multimodal-models-and-fusion-a-complete-guide-225ca91f6861</li>
<li>Scaling Laws for Native Multimodal Models - Apple Machine Learning Research, https://machinelearning.apple.com/research/scaling-laws-native-multimodal-models</li>
<li>Gemini: A Family of Highly Capable Multimodal Models - Googleapis.com, https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf</li>
<li>Gemini 1.5: Unlocking multimodal understanding across … - arXiv, https://arxiv.org/html/2403.05530</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - Kapler o AI, https://www.kapler.cz/wp-content/uploads/gemini_v1_5_report.pdf</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - Department of Computer Science, https://www.cs.fsu.edu/~langley/CIS3250/2024-Spring/gemini_v1_5_report.pdf</li>
<li>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context - arXiv, https://arxiv.org/html/2403.05530v2</li>
<li>Google accidentally created Gemini’s most insane feature and nobody’s talking about it : r/GeminiAI - Reddit, https://www.reddit.com/r/GeminiAI/comments/1ozuttn/google_accidentally_created_geminis_most_insane/</li>
<li>GPT-4o System Card - arXiv, https://arxiv.org/html/2410.21276v1</li>
<li>15 Abilities of GPT4o that you Won’t Believe - AI Toolz, https://aitoolzai.medium.com/15-abilities-of-gpt4o-that-you-wont-believe-8cba07c1cf1f</li>
<li>GPT-4o System Card - Simon Willison’s Weblog, https://simonwillison.net/2024/Aug/8/gpt-4o-system-card/</li>
<li>Transformer Meets Diffusion: How the Transfusion Architecture Empowers GPT-4o’s Creativity - MarkTechPost, https://www.marktechpost.com/2025/04/06/transformer-meets-diffusion-how-the-transfusion-architecture-empowers-gpt-4os-creativity/</li>
<li>What’s Behind the Hype? Meet Transfusion, the Tech Powering Native Image Generation in GPT-4o | by Mohammad Zeynali | Medium, https://medium.com/@mzeynali01/whats-behind-the-hype-meet-transfusion-the-tech-powering-native-image-generation-in-gpt-4o-84799eee45e4</li>
<li>Comparison of Gemini 1.5 Pro vs. GPT-4o - AI Chatbot for Customer Support, https://livechatai.com/llm-comparison/gemini-1-5-pro-vs-gpt-4o</li>
<li>Best Multimodal AI for Customer Experience: GPT-4o vs Gemini 1.5 vs Claude 3, https://www.kapture.cx/blog/multimodal-ai-for-customer-experience/</li>
<li>Reflecting on the original GPT-4o Voice Mode demos…Has anyone been able to reproduce them? : r/OpenAI - Reddit, https://www.reddit.com/r/OpenAI/comments/1jxkjyw/reflecting_on_the_original_gpt4o_voice_mode/</li>
<li>Lost in Embeddings: Information Loss in Vision–Language Models - arXiv, https://arxiv.org/html/2509.11986v1</li>
<li>Multimodal Models Don’t Need Late Fusion: Apple Researchers Show Early-Fusion Architectures are more Scalable, Efficient, and Modality-Agnostic - MarkTechPost, https://www.marktechpost.com/2025/04/14/multimodal-models-dont-need-late-fusion-apple-researchers-show-early-fusion-architectures-are-more-scalable-efficient-and-modality-agnostic/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>