# 야코비안 완전 정복


고등학교나 대학 1학년 미적분학에서 처음 만나는 도함수 $f'(x)$를 기억할 것이다. 도함수는 두 가지 강력한 의미를 지닌다. 첫째, 특정 지점에서의 '순간 변화율'을 나타낸다. 둘째, 복잡한 곡선 함수 $f(x)$를 특정 지점 근처에서 단순한 직선, 즉 접선으로 근사하는 '최고의 선형 근사' 도구다.1 이 두 가지 개념은 과학과 공학의 수많은 문제를 해결하는 열쇠가 된다.

하지만 현실 세계의 문제들은 대부분 하나의 변수가 하나의 결과에만 영향을 미치는 단순한 구조가 아니다. 로봇 팔의 움직임은 여러 관절 각도의 조합으로 끝점의 3차원 위치와 방향을 결정한다. 인공지능 신경망은 수백만 개의 입력 신호가 복잡한 연산을 거쳐 다양한 출력 결과를 만들어낸다. 이처럼 우리가 마주하는 대부분의 시스템은 여러 개의 입력이 여러 개의 출력에 동시에 영향을 미치는 다변수 벡터 함수($\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$)로 모델링된다.3

그렇다면 이런 복잡한 다차원 함수는 어떻게 '미분'할 수 있을까? 입력 벡터가 미세하게 변할 때 출력 벡터는 어떻게 변할까? 복잡하게 휘어진 공간을 다른 휘어진 공간으로 보내는 이 비선형 변환을 어떻게 국소적으로 분석할 수 있을까? 이 질문에 대한 답이 바로 **야코비안(Jacobian)**이다.

야코비안 행렬은 일변수 도함수의 핵심 개념(순간 변화율, 선형 근사)을 다차원으로 확장한 '자연스러운 일반화'다.1 야코비안은 단순히 계산 공식의 집합이 아니다. 그것은 복잡하고 비선형적인 세상을 특정 지점 근처에서는 단순한 선형 세상으로 바라볼 수 있게 해주는 강력한 '수학적 렌즈'와 같다. 이 '비선형성의 선형화'라는 철학은 야코비안의 모든 응용을 관통하는 핵심 아이디어다.

이 학습 교재에서는 야코비안의 정의부터 시작해 그 기하학적 의미를 파헤치고, 다중 적분에서의 좌표 변환, 로봇 공학의 운동학 및 특이점 분석, 수치 해석에서의 비선형 방정식 풀이, 그리고 머신러닝의 핵심인 역전파 알고리즘에 이르기까지 야코비안이 어떻게 핵심적인 역할을 하는지 심도 있게 탐구할 것이다.3 이 여정을 통해 당신은 야코비안이라는 언어를 자유자재로 구사하며 다양한 공학 및 과학 문제를 해결할 수 있는 강력한 무기를 얻게 될 것이다.


야코비안을 제대로 이해하려면 먼저 그것이 어떤 대상을 다루는지, 어떻게 정의되는지 명확히 해야 한다.


야코비안은 '벡터 값 함수'를 미분하기 위한 도구다. 벡터 값 함수란 입력과 출력이 모두 벡터인 함수를 말한다. 수학적으로는 $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ 와 같이 표기하며, 이는 $n$차원 실수 벡터 공간($\mathbb{R}^n$)의 한 점(벡터) $\mathbf{x}$를 입력받아 $m$차원 실수 벡터 공간($\mathbb{R}^m$)의 한 점(벡터) $\mathbf{y}$로 대응시키는 함수라는 의미다.1

입력 벡터 $\mathbf{x}$와 출력 벡터 $\mathbf{y}$는 다음과 같이 각 성분으로 표현할 수 있다.
$$
\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \quad \mathbf{y} = \mathbf{f}(\mathbf{x}) = \begin{bmatrix} f_1(x_1, x_2, \dots, x_n) \\ f_2(x_1, x_2, \dots, x_n) \\ \vdots \\ f_m(x_1, x_2, \dots, x_n) \end{bmatrix}
$$
여기서 각 출력 성분 $f_1, f_2, \dots, f_m$은 여러 개의 변수($x_1, \dots, x_n$)를 입력으로 받는 스칼라 함수다.9 예를 들어, 로봇 팔의 경우 입력 벡터 $\mathbf{x}$는 각 관절의 각도들이고, 출력 벡터 $\mathbf{y}$는 로봇 끝점의 3차원 좌표($x, y, z$)가 될 수 있다.


함수 $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$에 대한 **야코비안 행렬(Jacobian Matrix)** $J$는 이 함수의 모든 1차 편도함수(first-order partial derivatives)를 모아놓은 $m \times n$ 크기의 행렬이다.1 구체적인 정의는 다음과 같다.
$$
J = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}
$$
14

이 행렬의 $(i, j)$번째 원소 $J_{ij} = \frac{\partial f_i}{\partial x_j}$는 $j$번째 입력 변수 $x_j$의 미세한 변화가 $i$번째 출력 함수 $f_i$에 얼마나 영향을 미치는지를 나타내는 '민감도(sensitivity)'라고 해석할 수 있다.4 따라서 야코비안 행렬 전체는 $n$개의 입력 채널에서 $m$개의 출력 채널로 변화의 정보가 어떻게 전달되는지를 보여주는 '국소적 연결망' 또는 '미분 정보의 청사진'으로 볼 수 있다. 이 구조는 신경망에서 한 층의 입력이 다음 층의 출력에 미치는 영향을 분석하는 개념과 직접적으로 연결된다.


스칼라 함수 $g: \mathbb{R}^n \rightarrow \mathbb{R}$의 **그래디언트(gradient)**, 즉 $\nabla g$는 각 변수에 대한 1차 편도함수들을 모아놓은 벡터다.15
$$
\nabla g = \begin{bmatrix} \frac{\partial g}{\partial x_1} \\ \frac{\partial g}{\partial x_2} \\ \vdots \\ \frac{\partial g}{\partial x_n} \end{bmatrix}
$$
야코비안 행렬의 정의를 다시 살펴보면, $i$번째 행(row)은 벡터 값 함수 $\mathbf{f}$의 $i$번째 성분 함수인 $f_i$의 그래디언트를 가로로 눕혀놓은 것(전치, transpose)과 정확히 일치한다.5
$$
J = \begin{bmatrix}
(\nabla f_1)^T \\
(\nabla f_2)^T \\
\vdots \\
(\nabla f_m)^T
\end{bmatrix}
$$
즉, 야코비안 행렬은 그래디언트의 개념을 스칼라 출력에서 벡터 출력으로 자연스럽게 일반화한 것이다. 여러 스칼라 함수의 그래디언트들을 차곡차곡 쌓아 만든 행렬이 바로 야코비안인 셈이다.1


야코비안 행렬이 정사각 행렬일 때, 즉 입력과 출력의 차원이 같을 때($m=n$), 우리는 그 행렬식(determinant)을 계산할 수 있다. 이를 **야코비안 행렬식(Jacobian determinant)** 또는 줄여서 **야코비안**이라고 부른다.1

야코비안 행렬식은 다음과 같이 표기한다.
$$
\det(J) \quad \text{또는} \quad |J| \quad \text{또는} \quad \frac{\partial(f_1, \dots, f_n)}{\partial(x_1, \dots, x_n)}
$$
2

이 값은 3부에서 자세히 다룰 기하학적 의미 때문에 매우 중요하다.


정의만으로는 와닿지 않을 수 있으니, 몇 가지 구체적인 예제를 통해 야코비안을 직접 계산해보자.

예제 1 (2x2 비선형 함수)

함수 $\mathbf{f}: \mathbb{R}^2 \rightarrow \mathbb{R}^2$가 다음과 같이 주어졌다고 하자.
$$
\mathbf{f}(x, y) = \begin{bmatrix} f_1(x, y) \\ f_2(x, y) \end{bmatrix} = \begin{bmatrix} x^2y \\ 5x + \sin y \end{bmatrix}
$$
이 함수의 야코비안 행렬은 $2 \times 2$ 행렬이다. 각 편도함수를 계산해보자.20

- $\frac{\partial f_1}{\partial x} = 2xy$
- $\frac{\partial f_1}{\partial y} = x^2$
- $\frac{\partial f_2}{\partial x} = 5$
- $\frac{\partial f_2}{\partial y} = \cos y$

이들을 행렬에 배치하면 야코비안 행렬을 얻는다.
$$
J = \begin{bmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \\
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y}
\end{bmatrix}
=
\begin{bmatrix}
2xy & x^2 \\
5 & \cos y
\end{bmatrix}
$$
예제 2 (3x2 비선형 함수)

함수 $\mathbf{f}: \mathbb{R}^2 \rightarrow \mathbb{R}^3$가 다음과 같이 주어졌다고 하자.
$$
\mathbf{f}(u, v) = \begin{bmatrix} f_1(u, v) \\ f_2(u, v) \\ f_3(u, v) \end{bmatrix} = \begin{bmatrix} u^2 - v^2 \\ 2uv \\ u+v \end{bmatrix}
$$
이 함수의 야코비안 행렬은 $3 \times 2$ 행렬이다. 입력 변수는 $u, v$이고 출력 함수는 $f_1, f_2, f_3$이다.5
$$
J = \begin{bmatrix}
\frac{\partial f_1}{\partial u} & \frac{\partial f_1}{\partial v} \\
\frac{\partial f_2}{\partial u} & \frac{\partial f_2}{\partial v} \\
\frac{\partial f_3}{\partial u} & \frac{\partial f_3}{\partial v}
\end{bmatrix}
=
\begin{bmatrix}
2u & -2v \\
2v & 2u \\
1 & 1
\end{bmatrix}
$$
예제 3 (극좌표 변환)

4부에서 다시 다룰 매우 중요한 예제다. 극좌표 ($(r, \theta)$)를 직교좌표 ($(x, y)$)로 변환하는 함수를 생각해보자.

$x = r\cos\theta$

$y = r\sin\theta$

이 변환은 함수 $\mathbf{f}: \mathbb{R}^2 \rightarrow \mathbb{R}^2$로 볼 수 있으며, 입력은 ($(r, \theta)$), 출력은 ($(x, y)$)이다. 야코비안 행렬과 행렬식을 계산해보자.20

- $\frac{\partial x}{\partial r} = \cos\theta$
- $\frac{\partial x}{\partial \theta} = -r\sin\theta$
- $\frac{\partial y}{\partial r} = \sin\theta$
- $\frac{\partial y}{\partial \theta} = r\cos\theta$

야코비안 행렬은 다음과 같다.
$$
J = \frac{\partial(x,y)}{\partial(r,\theta)} = 
\begin{bmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}
\end{bmatrix}
=
\begin{bmatrix}
\cos\theta & -r\sin\theta \\
\sin\theta & r\cos\theta
\end{bmatrix}
$$
야코비안 행렬식은 다음과 같이 계산된다.
$$
\det(J) = (\cos\theta)(r\cos\theta) - (-r\sin\theta)(\sin\theta) = r\cos^2\theta + r\sin^2\theta = r(\cos^2\theta + \sin^2\theta) = r
$$
이 결과, $\det(J) = r$은 다중 적분에서 좌표계를 변환할 때 핵심적인 역할을 하게 된다.


야코비안의 진정한 힘은 그 기하학적 의미를 이해할 때 드러난다. 야코비안은 단순히 편도함수들을 모아놓은 행렬이 아니라, 복잡한 변환의 국소적 특성을 명쾌하게 설명하는 도구다. 핵심적인 기하학적 의미는 '국소적 선형 근사'와 '부피/넓이 스케일링 팩터' 두 가지다.


아무리 복잡하게 휘어지고 비틀어지는 비선형 변환이라도, 어떤 한 점 주변의 아주 작은 영역으로 현미경을 들이대듯 확대해서 보면 거의 평평한 선형 변환처럼 보인다.1 이것이 미분 가능한 모든 함수가 갖는 '국소적 선형성(local linearity)'이다.

선형 변환은 기하학적으로 세 가지 특징을 가진다. (1) 원점은 변환 후에도 원점으로 유지된다. (2) 격자선들은 변환 후에도 평행한 직선 형태를 유지한다. (3) 격자선 사이의 간격은 균등하다.10 비선형 변환은 일반적으로 이러한 특징을 만족하지 않는다. 예를 들어, 공간을 휘게 만드는 변환은 직선을 곡선으로 바꾸고 간격도 불균등하게 만든다.

하지만 특정 점 $\mathbf{p}$와 그 주변의 아주 작은 영역에만 집중하면, 이 비선형 변환은 마치 $\mathbf{p}$를 원점으로 하는 선형 변환처럼 행동한다. 이 국소적인 선형 변환을 행렬로 표현한 것이 바로 그 점에서의 **야코비안 행렬**이다.

이 개념은 다변수 테일러 급수(Taylor series)를 통해 수학적으로 명확해진다. 함수 $\mathbf{f}(\mathbf{x})$를 점 $\mathbf{p}$ 근처에서 1차 근사하면 다음과 같은 식을 얻는다.
$$
\mathbf{f}(\mathbf{x}) \approx \mathbf{f}(\mathbf{p}) + J_{\mathbf{f}}(\mathbf{p})(\mathbf{x} - \mathbf{p})
$$
1

이 식을 자세히 뜯어보자. $\mathbf{f}(\mathbf{p})$는 변환의 시작점을 알려주는 상수 벡터(이동)이고, $(\mathbf{x} - \mathbf{p})$는 점 $\mathbf{p}$로부터의 작은 변위 벡터다. 그리고 $J_{\mathbf{f}}(\mathbf{p})$는 이 변위 벡터에 곱해지는 행렬, 즉 선형 변환이다. 이 식은 일변수 미분에서 접선 공식 $f(x) \approx f(p) + f'(p)(x-p)$의 완벽한 다차원 확장판이다.1 일변수 도함수 

$f'(p)$가 점 $p$에서 함수의 국소적 행동을 가장 잘 나타내는 '기울기'인 것처럼, 야코비안 행렬 $J_{\mathbf{f}}(\mathbf{p})$는 점 $\mathbf{p}$에서 벡터 함수 $\mathbf{f}$의 국소적 행동을 가장 잘 모사하는 '선형 변환 행렬'인 것이다.


선형대수학에서 행렬식(determinant)의 기하학적 의미는 해당 선형 변환이 공간을 얼마나 확대 또는 축소하는지를 나타내는 비율이다. 예를 들어, 2차원 행렬의 행렬식 절댓값은 단위 정사각형(넓이 1)이 변환 후 평행사변형으로 바뀔 때 그 평행사변형의 넓이를 의미한다.27

이 개념을 야코비안에 바로 적용할 수 있다. 3.1절에서 야코비안 행렬이 특정 지점에서의 국소적 선형 변환이라고 했다. 그렇다면, 그 지점에서의 **야코비안 행렬식의 절댓값**은 그 변환에 의해 미소(infinitesimal) 넓이나 부피가 얼마나 변하는지를 나타내는 **'국소적 스케일링 팩터(local scaling factor)'**가 된다.1

예를 들어, ($(u,v)$) 좌표계의 아주 작은 직사각형 영역(넓이 $dA_{uv} = dudv$)이 함수 $\mathbf{f}$에 의해 ($(x,y)$) 좌표계로 변환된다고 하자. 변환된 영역은 아주 작은 평행사변형이 되며, 그 넓이 $dA_{xy}$는 원래 넓이에 야코비안 행렬식의 절댓값을 곱한 것과 같다.
$$
dA_{xy} = |\det(J)| dA_{uv} = \left|\frac{\partial(x,y)}{\partial(u,v)}\right| du dv
$$
22

이 관계는 4부에서 다룰 다중적분의 변수 변환에서 왜 야코비안 행렬식이 반드시 필요한지를 설명하는 핵심적인 근거가 된다.

또한, 행렬식의 부호는 '방향성(orientation)'에 대한 정보를 담고 있다. 행렬식이 양수이면 변환 후에도 공간의 방향이 유지되고(예: 오른손 좌표계가 그대로 오른손 좌표계로), 음수이면 방향이 뒤집힌다(예: 오른손 좌표계가 왼손 좌표계로, 마치 거울에 비친 상처럼).1

이 모든 논의의 바탕에는 '미분 가능성'이라는 조건이 있다. 함수가 어떤 점에서 미분 가능하다는 것은 그 점에서 '매끄럽다'는 뜻이고, 이는 충분히 확대하면 '평평하게' 보인다는 의미다. 이 '평평함'이 바로 국소적 선형성이며, 이 선형성을 대표하는 것이 야코비안 행렬이다. 그리고 모든 선형 변환은 고유한 스케일링 팩터인 행렬식을 가진다. 따라서 미분 가능한 함수의 변환 과정에서 미소 영역의 크기 변화를 추적하려면, 그 지점의 국소적 선형 변환(야코비안)의 스케일링 팩터(행렬식)를 사용하는 것이 논리적으로 필연적이다.


야코비안의 가장 고전적이면서도 중요한 응용 분야는 바로 다중 적분에서의 변수 변환이다. 복잡한 모양의 영역에 대한 적분을 간단한 모양의 영역에 대한 적분으로 바꾸어 계산을 용이하게 할 때, 야코비안은 필수적인 역할을 한다.


먼저 일변수 치환적분을 떠올려보자. $\int f(x) dx$에서 변수 $x$를 $x=g(u)$로 치환하면, 미소 길이 요소 $dx$는 $dx = g'(u)du$로 변환된다.30 여기서 $g'(u)$는 $u$의 작은 변화 $du$가 $x$의 변화 $dx$로 얼마나 확대 또는 축소되는지를 나타내는 스케일링 팩터다. 이 $g'(u)$가 바로 1차원에서의 야코비안이다.32

이 아이디어는 다차원으로 그대로 확장된다. ($(x,y)$) 좌표계에서의 이중적분 $\iint_R f(x,y) \,dx\,dy$를 ($(u,v)$) 좌표계를 이용한 변수 변환 $x=x(u,v), y=y(u,v)$를 통해 계산한다고 생각해보자. 이때 미소 면적 요소 $dxdy$는 단순히 $dudv$로 바뀌지 않는다. ($(u,v)$) 공간의 작은 사각형이 ($(x,y)$) 공간으로 변환되면서 겪는 넓이의 왜곡을 보정해주어야 한다. 이 보정 인자가 바로 3부에서 살펴본 국소적 스케일링 팩터, 즉 야코비안 행렬식의 절댓값 $|\det(J)|$이다.1

이 개념을 '좌표계의 언어를 번역할 때 필요한 환율'에 비유할 수 있다. $dxdy$와 $dudv$는 서로 다른 '통화 단위'를 가진 면적이다. 두 좌표계 사이의 변환이 위치에 따라 다르게 왜곡되는 비선형 변환이라면, '환율'도 위치마다 달라져야 한다.32 야코비안 행렬식 $|J(u,v)|$가 바로 이 '위치에 따라 변하는 환율' 역할을 한다.30 어떤 곳에서는 면적이 크게 팽창하고($|J|>1$), 어떤 곳에서는 수축한다($|J|<1$). 따라서 전체 합산인 적분을 할 때 이 환율을 곱해주지 않으면, 마치 달러와 원을 1:1로 더하는 것처럼 완전히 틀린 결과를 얻게 된다.

결론적으로, 다중 적분의 변수 변환 공식은 다음과 같다.
$$
\iint_R f(x,y) \,dx\,dy = \iint_S f(x(u,v), y(u,v)) \left|\frac{\partial(x,y)}{\partial(u,v)}\right| \,du\,dv
$$
30

여기서 $R$은 ($(x,y)$) 평면의 적분 영역이고, $S$는 변환에 의해 대응되는 ($(u,v)$) 평면의 적분 영역이다.


변수 변환의 주된 목적은 원, 타원, 평행사변형처럼 적분하기 까다로운 영역을 ($(u,v)$) 좌표계에서 간단한 직사각형 영역으로 바꾸어 적분을 쉽게 만드는 것이다.32 물리학과 공학에서는 특히 극좌표, 원통좌표, 구면좌표 변환이 자주 사용되므로, 이들의 야코비안은 암기해두는 것이 효율적이다.

**Table 1: 주요 좌표계 변환과 야코비안**

| 좌표계 | 변환식 (x,y,z) | 야코비안 행렬식 ∣J∣ |

| :--- | :--- | :--- |

| 극좌표 (Polar) | $x = r \cos \theta$, $y = r \sin \theta$ | $r$ |

| 원통 좌표 (Cylindrical) | $x = r \cos \theta$, $y = r \sin \theta$, $z=z$ | $r$ |

| 구면 좌표 (Spherical) | $x = \rho \sin \phi \cos \theta$, $y = \rho \sin \phi \sin \theta$, $z = \rho \cos \phi$ | $\rho^2 \sin \phi$ |

20

계산 예제 1 (극좌표)

반지름이 $a$인 원 $x^2+y^2 \le a^2$ 위에서 함수 $f(x,y) = \sqrt{x^2+y^2}$를 적분해보자. 직교좌표로는 계산이 매우 복잡하지만, 극좌표로 변환하면 간단해진다.

변환은 $x=r\cos\theta, y=r\sin\theta$이고, 적분 영역은 $0 \le r \le a, 0 \le \theta \le 2\pi$인 직사각형이 된다. 피적분 함수는 $f(r\cos\theta, r\sin\theta) = \sqrt{r^2\cos^2\theta + r^2\sin^2\theta} = r$이 된다.

면적 요소는 $dxdy = |J|drd\theta = rdrd\theta$로 바뀐다. 따라서 적분식은 다음과 같이 변환된다.38
$$
\iint_{x^2+y^2 \le a^2} \sqrt{x^2+y^2} \,dx\,dy = \int_0^{2\pi} \int_0^a (r) \cdot (r \,dr\,d\theta) = \int_0^{2\pi} \int_0^a r^2 \,dr\,d\theta
\\
= \int_0^{2\pi} \left[ \frac{1}{3}r^3 \right]_0^a \,d\theta = \int_0^{2\pi} \frac{a^3}{3} \,d\theta = \frac{a^3}{3} [\theta]_0^{2\pi} = \frac{2\pi a^3}{3}
$$
계산 예제 2 (구면 좌표)

반지름이 $R$인 구의 부피를 3중 적분으로 구해보자. 부피는 $\iiint_V 1 \,dV$로 계산할 수 있다. 구면좌표 변환을 사용하면 적분 영역은 $0 \le \rho \le R, 0 \le \theta \le 2\pi, 0 \le \phi \le \pi$인 직육면체가 된다.

부피 요소는 $dxdydz = |J|d\rho d\phi d\theta = \rho^2 \sin\phi \,d\rho d\phi d\theta$로 변환된다. 따라서 적분식은 다음과 같다.35
$$
V = \int_0^\pi \int_0^{2\pi} \int_0^R (1) \cdot (\rho^2 \sin\phi \,d\rho\,d\theta\,d\phi) = \int_0^\pi \sin\phi \,d\phi \int_0^{2\pi} d\theta \int_0^R \rho^2 \,d\rho
\\
= [-\cos\phi]_0^\pi \cdot [\theta]_0^{2\pi} \cdot \left[\frac{1}{3}\rho^3\right]_0^R = (1 - (-1)) \cdot (2\pi) \cdot \left(\frac{R^3}{3}\right) = \frac{4}{3}\pi R^3
$$
계산 예제 3 (일반 변환)

네 직선 $x-y=0, x-y=2, x+y=0, x+y=2$로 둘러싸인 평행사변형 영역 $R$에서 함수 $f(x,y) = (x-y)^2$를 적분해보자.

$u = x-y$, $v = x+y$로 치환하면 적분 영역은 $0 \le u \le 2, 0 \le v \le 2$인 정사각형 $S$가 된다.34

이제 $x, y$를 $u, v$로 표현해야 야코비안을 계산할 수 있다. $x = \frac{1}{2}(u+v), y = \frac{1}{2}(v-u)$.

이 변환의 야코비안 행렬식은 다음과 같다.
$$
J = \frac{\partial(x,y)}{\partial(u,v)} = \det \begin{bmatrix} \partial x/\partial u & \partial x/\partial v \\ \partial y/\partial u & \partial y/\partial v \end{bmatrix} = \det \begin{bmatrix} 1/2 & 1/2 \\ -1/2 & 1/2 \end{bmatrix} = \frac{1}{4} - (-\frac{1}{4}) = \frac{1}{2}
$$
따라서 $dxdy = |J|dudv = \frac{1}{2}dudv$이고, 피적분 함수는 $u^2$이 된다. 적분식은 다음과 같이 변환된다.30
$$
\iint_R (x-y)^2 \,dx\,dy = \iint_S u^2 \left|\frac{1}{2}\right| \,du\,dv = \frac{1}{2} \int_0^2 \int_0^2 u^2 \,du\,dv
\\
= \frac{1}{2} \int_0^2 \left[ \frac{1}{3}u^3 \right]_0^2 \,dv = \frac{1}{2} \int_0^2 \frac{8}{3} \,dv = \frac{1}{2} \cdot \frac{8}{3} \cdot [v]_0^2 = \frac{1}{2} \cdot \frac{8}{3} \cdot 2 = \frac{8}{3}
$$


야코비안은 순수 수학을 넘어 실제 공학 및 과학 문제 해결에 강력한 도구로 사용된다. 여기서는 로봇 공학, 수치 해석, 머신러닝이라는 세 가지 주요 분야에서 야코비안이 어떻게 활용되는지 심층적으로 탐구한다.


로봇 공학에서 야코비안은 로봇의 움직임을 분석하고 제어하는 데 있어 심장과 같은 역할을 한다.


로봇의 순기구학(Forward Kinematics)은 관절 각도 벡터 $\mathbf{q}$가 주어졌을 때 로봇 끝점(end-effector)의 위치와 방향 벡터 $\mathbf{x}$를 계산하는 함수 $\mathbf{x} = \mathbf{f}(\mathbf{q})$를 다룬다. 그렇다면 관절의 속도와 끝점의 속도는 어떤 관계가 있을까? 이 질문에 답하는 것이 속도 기구학이다.

$\mathbf{x} = \mathbf{f}(\mathbf{q})$ 식의 양변을 시간에 대해 미분하면 연쇄 법칙에 의해 다음 관계식을 얻는다.
$$
\dot{\mathbf{x}} = \frac{d\mathbf{f}(\mathbf{q})}{dt} = \frac{\partial \mathbf{f}}{\partial \mathbf{q}} \frac{d\mathbf{q}}{dt}
$$
여기서 $\dot{\mathbf{x}}$는 끝점의 선속도 및 각속도 벡터, $\dot{\mathbf{q}}$는 관절의 각속도 벡터이며, $\frac{\partial \mathbf{f}}{\partial \mathbf{q}}$가 바로 관절 각도 $\mathbf{q}$에 대한 야코비안 행렬 $J(\mathbf{q})$이다. 따라서 로봇의 속도 기구학은 다음과 같은 간결한 선형 관계로 표현된다.
$$
\mathbf{v} = J(\mathbf{q})\dot{\mathbf{q}}
$$
6

이 식은 특정 관절 자세 $\mathbf{q}$에서, 각 관절의 속도 $\dot{\mathbf{q}}$가 끝점의 속도 $\mathbf{v}$로 어떻게 변환되는지를 보여준다. 야코비안은 이 둘을 연결하는 '순간 변환 행렬'인 셈이다.


로봇 공학에서 **특이점(Singularity)**은 로봇이 특정 자세(configuration)를 취했을 때, 야코비안 행렬의 행렬식이 0이 되는 현상을 말한다 ($\det(J)=0$).45 이는 단순한 수학적 문제를 넘어, 로봇의 제어 불능과 물리적 손상을 유발할 수 있는 심각한 엔지니어링 문제다.

로봇 제어는 보통 원하는 끝점 속도 $\mathbf{v}$를 달성하기 위해 필요한 관절 속도 $\dot{\mathbf{q}}$를 계산하는 역기구학(Inverse Kinematics) 문제, 즉 $\dot{\mathbf{q}} = J^{-1}\mathbf{v}$를 푸는 방식으로 이루어진다.47

1. **제어 불능**: 특이점에서는 $\det(J)=0$이므로 야코비안의 역행렬 $J^{-1}$가 존재하지 않는다. 이는 특정 방향으로의 끝점 속도 $\mathbf{v}$를 만들어낼 수 없음을 의미한다. 즉, 로봇이 움직일 수 있는 방향이 제한되어 '자유도를 잃게' 된다.45 예를 들어, 팔을 완전히 뻗은 상태에서는 팔꿈치를 아무리 움직여도 손을 더 멀리 뻗을 수 없는 것과 같다.
2. **과도한 관절 속도**: 더 심각한 문제는 특이점 '근처'에서 발생한다. $\det(J)$가 0에 가까워지면, 역행렬 $J^{-1}$의 원소 값들은 매우 커진다. 이는 아주 작은 끝점 속도를 내기 위해 엄청나게 빠른(이론적으로는 무한한) 관절 속도가 필요함을 의미한다.46 이는 로봇 모터에 심각한 과부하를 주고, 예측 불가능한 급격한 움직임을 유발하여 로봇 자체나 주변 환경에 위험을 초래할 수 있다.

따라서 로봇의 경로를 계획하고 제어할 때, 이러한 특이점을 사전에 분석하고 회피하는 것은 안전과 직결되는 필수적인 과정이다.45


역기구학은 원하는 끝점의 위치/자세 $\mathbf{x}_d$를 달성하기 위한 관절 각도 $\mathbf{q}$를 찾는 문제다. 이는 비선형 연립방정식을 푸는 것과 같아 해를 직접 구하기 어렵다. 야코비안을 이용한 반복적인 접근법이 널리 사용된다. 현재 끝점 위치와 목표 위치의 오차 $\Delta \mathbf{x} = \mathbf{x}_d - \mathbf{x}_{current}$를 계산하고, $\Delta \mathbf{x} \approx J \Delta \mathbf{q}$ 관계를 이용하여 필요한 관절 각도의 변화량 $\Delta \mathbf{q} = J^{-1} \Delta \mathbf{x}$를 구한다. 이 과정을 반복하여 오차를 줄여나간다. 야코비안이 정사각 행렬이 아니거나 특이점 근처에 있을 때는 역행렬 대신 **의사 역행렬(pseudo-inverse, $J^{+}$)**을 사용하여 안정적인 해를 구한다.50


야코비안은 여러 개의 변수를 가진 비선형 연립방정식의 해를 찾는 데에도 강력한 도구로 사용된다. 대표적인 방법이 **뉴턴-랩슨 방법(Newton-Raphson Method)**이다.55

$n$개의 변수와 $n$개의 방정식으로 이루어진 시스템 $\mathbf{f}(\mathbf{x}) = \mathbf{0}$을 생각해보자.

일변수 뉴턴 방법이 곡선을 접선으로 근사하듯, 다변수 뉴턴-랩슨 방법은 함수 $\mathbf{f}(\mathbf{x})$를 현재 추정치 $\mathbf{x}_k$에서 야코비안을 이용해 선형으로 근사한다.16
$$
\mathbf{f}(\mathbf{x}) \approx \mathbf{f}(\mathbf{x}_k) + J(\mathbf{x}_k)(\mathbf{x} - \mathbf{x}_k)
$$
우리는 이 선형 근사식의 값이 $\mathbf{0}$이 되는 점을 다음 추정치 $\mathbf{x}_{k+1}$로 삼는다.
$$
\mathbf{0} = \mathbf{f}(\mathbf{x}_k) + J(\mathbf{x}_k)(\mathbf{x}_{k+1} - \mathbf{x}_k)
$$
이 식을 $\mathbf{x}_{k+1}$에 대해 정리하면 다음과 같은 반복 공식을 얻는다.
$$
\mathbf{x}_{k+1} = \mathbf{x}_k - J(\mathbf{x}_k)^{-1}\mathbf{f}(\mathbf{x}_k)
$$
16

실제 계산에서는 역행렬을 직접 구하는 것이 비효율적이고 수치적으로 불안정할 수 있으므로, 대신 **뉴턴 스텝(Newton step)** $\mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k$를 구하기 위해 선형 연립방정식 $J(\mathbf{x}_k)\mathbf{s}_k = -\mathbf{f}(\mathbf{x}_k)$를 푼다.59 그 후 $\mathbf{x}_{k+1} = \mathbf{x}_k + \mathbf{s}_k$로 다음 추정치를 업데이트한다. 이 방법은 초기 추정치가 해에 충분히 가깝다면 매우 빠르게 (이차 수렴) 해에 수렴하는 강력한 알고리즘이다.56


현대 머신러닝, 특히 심층 신경망(Deep Neural Network)의 학습 과정은 본질적으로 거대한 최적화 문제다. 야코비안은 이 과정의 핵심인 **역전파(Backpropagation)** 알고리즘의 수학적 기반을 이룬다.


신경망 학습의 목표는 예측값과 실제값의 차이를 나타내는 손실 함수(Loss function) $L$을 최소화하는 가중치(weight) $W$를 찾는 것이다. 이를 위해 경사 하강법(Gradient Descent)을 사용하며, 손실 함수를 각 가중치로 미분한 그래디언트 $\frac{\partial L}{\partial W}$를 계산해야 한다.7

신경망은 입력층부터 출력층까지 여러 개의 층(layer)이 연결된 구조로, 각 층은 선형 변환과 활성화 함수의 합성을 나타낸다. 따라서 전체 신경망은 수많은 함수가 중첩된 거대한 합성 함수($L(a_L(z_L(\dots a_1(z_1(\mathbf{x})))))$)로 볼 수 있다. 이러한 합성 함수의 미분은 **연쇄 법칙(Chain Rule)**을 통해 계산된다. 다변수 함수의 연쇄 법칙은 본질적으로 야코비안 행렬들의 곱으로 표현된다.17

예를 들어, 출력층 바로 전의 가중치 $W_L$에 대한 그래디언트는 $\frac{\partial L}{\partial W_L} = \frac{\partial L}{\partial a_L} \frac{\partial a_L}{\partial z_L} \frac{\partial z_L}{\partial W_L}$와 같이 계산된다. 여기서 각 항은 스칼라, 벡터, 또는 야코비안 행렬이 될 수 있다.

만약 이 연쇄 법칙을 모든 가중치에 대해 처음부터 끝까지 적용하여 모든 야코비안 행렬을 명시적으로 계산하고 곱한다면 엄청난 계산 비용과 메모리가 필요할 것이다. **역전파 알고리즘**은 이 과정을 매우 영리하고 효율적으로 수행하는 방법이다.

역전파의 핵심은 모든 야코비안 행렬을 직접 계산하는 대신, '야코비안-벡터 곱(Jacobian-Vector Product)'을 뒤에서부터(출력층에서 입력층 방향으로) 계산하는 것이다.65 최종적으로 필요한 것은 스칼라 손실 함수 

$L$에 대한 그래디언트(벡터)다. 연쇄 법칙을 오른쪽에서 왼쪽으로 적용하면, 출력층에서 계산된 오차 그래디언트(벡터)가 이전 층의 야코비안과 곱해져 그 층의 오차 그래디언트(벡터)를 계산하는 과정이 연쇄적으로 일어난다. 즉, (벡터) $\times$ (야코비안) $\times$ (야코비안)... 형태가 되는데, 이는 (벡터-야코비안 곱)을 반복적으로 수행하는 것과 같다. 이 방식은 거대한 야코비안 행렬들을 메모리에 저장할 필요 없이, 오차 신호(벡터)를 뒤로 전파시키면서 각 층의 그래디언트를 효율적으로 계산할 수 있게 해준다. 이것이 바로 '오차 역전파'라는 이름의 유래이자 알고리즘의 본질이다.64


딥러닝과 최적화 이론에서는 그래디언트, 야코비안, 헤시안이라는 세 가지 미분 관련 개념이 자주 등장하여 혼동을 일으키기 쉽다. 이들의 역할을 명확히 구분하는 것은 필수적이다.

**Table 2: 그래디언트, 야코비안, 헤시안 비교**

| 개념           | 함수 유형                                                    | 미분 차수 | 출력 형태           | 핵심 의미/역할           | 대표 응용                                     |
| -------------- | ------------------------------------------------------------ | --------- | ------------------- | ------------------------ | --------------------------------------------- |
| **그래디언트** | 스칼라 값 함수 ($f: \mathbb{R}^n \rightarrow \mathbb{R}$)    | 1차       | 벡터 ($n \times 1$) | 가장 가파른 상승 방향    | 경사 하강법                                   |
| **야코비안**   | 벡터 값 함수 ($\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$) | 1차       | 행렬 ($m \times n$) | 국소 선형 근사, 스케일링 | 연쇄 법칙, 변수 변환                          |
| **헤시안**     | 스칼라 값 함수 ($f: \mathbb{R}^n \rightarrow \mathbb{R}$)    | 2차       | 행렬 ($n \times n$) | 함수의 곡률(curvature)   | 2차 최적화(뉴턴 방법), 볼록성(convexity) 판단 |

15

- **그래디언트**는 스칼라 함수를 입력받아 벡터를 출력한다. 이 벡터는 함수 값이 가장 빠르게 증가하는 방향과 그 정도를 나타내며, 경사 하강법의 핵심이다.
- **야코비안**은 벡터 함수를 입력받아 행렬을 출력한다. 그래디언트의 일반화된 형태로, 복잡한 벡터 변환의 국소적 선형 근사를 나타낸다.
- **헤시안**은 스칼라 함수를 입력받아 행렬을 출력하지만, 2차 미분을 다룬다. 함수의 '곡률' 정보를 담고 있어, 어떤 지점이 최소값인지, 최대값인지, 아니면 안장점(saddle point)인지를 판단하는 데 사용된다. 수학적으로 헤시안 행렬은 그래디언트 벡터의 야코비안 행렬과 같다 ($H_f = J(\nabla f)$).70


지금까지 우리는 야코비안의 정의부터 기하학적 의미, 그리고 다양한 응용 분야까지 긴 여정을 함께했다. 이 모든 내용을 관통하는 단 하나의 핵심 아이디어를 꼽으라면, 그것은 바로 **'국소적 선형 근사(local linear approximation)'**이다.1

야코비안은 이 강력한 아이디어를 바탕으로 각 응용 분야의 특성에 맞게 다양한 모습으로 발현된다.

- **다중 적분**에서는 비선형 좌표 변환으로 인해 왜곡되는 미소 면적/부피를 보정해주는 **'스케일링 팩터'**의 얼굴을 하고 나타났다.
- **로봇 공학**에서는 관절의 속도와 로봇 끝점의 속도를 연결하는 **'속도 변환 행렬'**로 기능하며, 제어 불능 상태인 특이점을 정의하는 기준이 되었다.
- **수치 해석**에서는 풀기 어려운 비선형 연립방정식을 반복적으로 풀 수 있는 **'선형 모델'**을 제공하는 역할을 했다.
- **머신러닝**에서는 복잡하게 얽힌 신경망의 손실 그래디언트를 효율적으로 계산하기 위한 **'그래디언트 전파의 매개체'**로 활약했다.

이 모든 역할의 근간에는 '복잡하고 다루기 힘든 비선형 시스템을 아주 작은 영역에서는 단순하고 잘 알려진 선형 시스템으로 바라보려는' 수학적, 공학적 통찰이 깊숙이 자리 잡고 있다.

따라서 야코비안을 마스터한다는 것은 단순히 공식을 암기하고 계산을 반복하는 것을 넘어선다. 그것은 복잡한 현상 속에서 핵심적인 선형 관계를 꿰뚫어 보고, 이를 바탕으로 문제를 분석하고 해결하는 능력을 기르는 것이다. 이 학습 교재가 당신이 야코비안이라는 강력한 렌즈를 통해 세상을 더 깊이 이해하는 데 든든한 발판이 되었기를 바란다.


1. Jacobian matrix and determinant - Wikipedia, accessed July 23, 2025, https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant
2. What is the Jacobian matrix? - Mathematics Stack Exchange, accessed July 23, 2025, https://math.stackexchange.com/questions/14952/what-is-the-jacobian-matrix
3. Jacobian Matrix - (Calculus IV) - Vocab, Definition, Explanations | Fiveable, accessed July 23, 2025, https://library.fiveable.me/key-terms/calculus-iv/jacobian-matrix
4. Understanding the Jacobian Matrix and Determinant in Machine Learning | by Divyesh Bhatt, accessed July 23, 2025, https://medium.com/@dbhatt245/understanding-the-jacobian-matrix-and-determinant-in-machine-learning-298355ab0cb8
5. Understanding the role of the Jacobian, and how to interpret wiki : r/learnmath - Reddit, accessed July 23, 2025, https://www.reddit.com/r/learnmath/comments/15jvyml/understanding_the_role_of_the_jacobian_and_how_to/
6. 로봇제어공학 – Introduction to Robotics 5장 자코비안: 속도와 static forces III - Lilys AI, accessed July 23, 2025, https://lilys.ai/notes/789984
7. 야코비안에 대한 짧은 소개 - 네피리티, accessed July 23, 2025, https://www.nepirity.com/blog/a-gentle-introduction-to-the-jacobian/
8. Jacobian Matrix - velog, accessed July 23, 2025, https://velog.io/@doohyun/Jacobian-Matrix
9. The Jacobian Matrix and the Chain Rule Let Rn = {(x 1,...,xn), accessed July 23, 2025, https://jan.ucc.nau.edu/~ns46/238/jacobi.pdf
10. Geometric Meaning of Jacobian Matrix - 공돌이의 수학정리노트 (Angelo's Math Notes), accessed July 23, 2025, https://angeloyeo.github.io/2020/07/24/Jacobian_en.html
11. GENERAL FUNCTIONS AND JACOBIANS Please review vector-valued functions, functions of several variables, partial derivatives and g - UCL, accessed July 23, 2025, https://www.ucl.ac.uk/~ucahyna/Rutgers/math251fa11/Jacobians.pdf
12. 야코비 행렬 - 위키백과, 우리 모두의 백과사전, accessed July 23, 2025, [https://ko.wikipedia.org/wiki/%EC%95%BC%EC%BD%94%EB%B9%84_%ED%96%89%EB%A0%AC](https://ko.wikipedia.org/wiki/야코비_행렬)
13. byjus.com, accessed July 23, 2025, [https://byjus.com/maths/jacobian/#:~:text=Jacobian%20matrix%20is%20a%20matrix,in%20the%20transformation%20of%20coordinates.](https://byjus.com/maths/jacobian/#:~:text=Jacobian matrix is a matrix,in the transformation of coordinates.)
14. [Calculus] Jacobian matrix - ok-lab - 티스토리, accessed July 23, 2025, https://ok-lab.tistory.com/147
15. Gradient (그래디언트), Jacobian (자코비안) 및 Hessian (헤시안 ..., accessed July 23, 2025, https://gaussian37.github.io/math-calculus-jacobian/
16. Newton-Raphson Method and Arithmetic Mean Newton's Method for Solving Systems of Nonlinear Equations - Lakehead University, accessed July 23, 2025, https://www.lakeheadu.ca/sites/default/files/uploads/77/Jia.pdf
17. Jacobian, Chain rule and backpropagation, accessed July 23, 2025, [https://suzyahyah.github.io/calculus/machine%20learning/2018/04/04/Jacobian-and-Backpropagation.html](https://suzyahyah.github.io/calculus/machine learning/2018/04/04/Jacobian-and-Backpropagation.html)
18. Gradient, Jacobian, Hessian, Laplacian and all that - Najeeb Khan, accessed July 23, 2025, https://najeebkhan.github.io/blog/VecCal.html
19. The Jacobian Matrix of Differentiable Functions from Rn to Rm - Mathonline, accessed July 23, 2025, http://mathonline.wikidot.com/the-jacobian-matrix-of-differentiable-functions-from-rn-to-r
20. What is the Jacobian matrix? - Educative.io, accessed July 23, 2025, https://www.educative.io/answers/what-is-the-jacobian-matrix
21. 야코비안 - 나무위키, accessed July 23, 2025, [https://namu.wiki/w/%EC%95%BC%EC%BD%94%EB%B9%84%EC%95%88](https://namu.wiki/w/야코비안)
22. 야코비안 (r52 판) - 나무위키, accessed July 23, 2025, [https://namu.wiki/w/%EC%95%BC%EC%BD%94%EB%B9%84%EC%95%88?uuid=4df8b6f8-2c66-404a-89dc-e69b91fd6511](https://namu.wiki/w/야코비안?uuid=4df8b6f8-2c66-404a-89dc-e69b91fd6511)
23. [머신러닝을 위한 기초수학] - 야코비안과 체인룰 - velog, accessed July 23, 2025, [https://velog.io/@zlddp723/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%9D%84-%EC%9C%84%ED%95%9C-%EA%B8%B0%EC%B4%88%EC%88%98%ED%95%99-%EC%95%BC%EC%BD%94%EB%B9%84%EC%95%88%EA%B3%BC-%EC%B2%B4%EC%9D%B8%EB%A3%B0](https://velog.io/@zlddp723/머신러닝을-위한-기초수학-야코비안과-체인룰)
24. Jacobian matrix - BYJU'S, accessed July 23, 2025, https://byjus.com/maths/jacobian/
25. The Jacobian for Polar and Spherical Coordinates, accessed July 23, 2025, https://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/jacpol/jacpol.html
26. web.ma.utexas.edu, accessed July 23, 2025, [https://web.ma.utexas.edu/users/m408m/Display15-10-4.shtml#:~:text=Example%201%3A%20Compute%20the%20Jacobian,rcos%CE%B8%7C%20%3D%20r.](https://web.ma.utexas.edu/users/m408m/Display15-10-4.shtml#:~:text=Example 1%3A Compute the Jacobian,rcosθ| %3D r.)
27. 자코비안(Jacobian) 행렬의 기하학적 의미 - 공돌이의 수학정리노트 ..., accessed July 23, 2025, https://angeloyeo.github.io/2020/07/24/Jacobian.html
28. angeloyeo.github.io, accessed July 23, 2025, [https://angeloyeo.github.io/2020/07/24/Jacobian.html#:~:text=%ED%96%89%EB%A0%AC%EC%8B%9D%EC%9D%98%20%EA%B8%B0%ED%95%98%ED%95%99%EC%A0%81%20%EC%9D%98%EB%AF%B8.,%EB%B3%80%ED%99%94%20%EB%B9%84%EC%9C%A8%EC%9D%84%20%EB%A7%90%ED%95%B4%EC%A4%80%EB%8B%A4.](https://angeloyeo.github.io/2020/07/24/Jacobian.html#:~:text=행렬식의 기하학적 의미.,변화 비율을 말해준다.)
29. Mastering the Jacobian Determinant - Number Analytics, accessed July 23, 2025, https://www.numberanalytics.com/blog/mastering-jacobian-determinant
30. 3.8: Jacobians - Mathematics LibreTexts, accessed July 23, 2025, https://math.libretexts.org/Bookshelves/Calculus/Supplemental_Modules_(Calculus)/Vector_Calculus/3%3A_Multiple_Integrals/3.8%3A_Jacobians
31. 16.7 Change of Variables in Multiple Integrals - CSUN, accessed July 23, 2025, http://www.csun.edu/~hcmth008/250/bccalclt03_1607.pdf
32. Change of Variables for Multiple Integrals - Oregon State University, accessed July 23, 2025, https://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/change/change.html
33. [미적분학]다중적분 : 변수변환 & 야코비안(자코비안)_Calculus: Multiple Integral (change of variables & Jacobian) - Advanced Mathematics Lab (AML) - 티스토리, accessed July 23, 2025, https://hub1.tistory.com/31
34. Change Of Variables (How-To w/ Step-by-Step Examples!) - Calcworkshop, accessed July 23, 2025, https://calcworkshop.com/multiple-integrals/change-variables/
35. 13.2Changing Coordinate Systems: The Jacobian, accessed July 23, 2025, https://faculty.valpo.edu/calculus3ibl/ch13_02_3djacobian.html
36. Cylindrical and spherical coordinates, accessed July 23, 2025, https://web.ma.utexas.edu/users/m408m/Display15-10-8.shtml
37. Cylindrical coordinate system and transformation | Calculus IV Class Notes - Fiveable, accessed July 23, 2025, https://library.fiveable.me/calculus-iv/unit-14/cylindrical-coordinate-system-transformation/study-guide/gEqCuE73swBlfM0G
38. 2.5.3 Change of variables and Jacobians In the previous example we saw that, once we have identified the type of coordinates whi, accessed July 23, 2025, https://www.staff.city.ac.uk/o.castro-alvaredo/teaching/jacobians
39. Jacobian of the polar transformation - YouTube, accessed July 23, 2025, https://www.youtube.com/watch?v=P281uZ17fqU
40. 03장 로봇과 자코비안 행렬, accessed July 23, 2025, https://t1.daumcdn.net/tistoryfile/fs9/14_18_2_30_blog91661_attach_0_32.pdf?original
41. [로봇공학기초 / 2DOF] #4. Jacobian Matrix (자코비안 행렬), accessed July 23, 2025, https://jhpark0518.tistory.com/4
42. The Ultimate Guide to Jacobian Matrices for Robotics - Automatic Addison, accessed July 23, 2025, https://automaticaddison.com/the-ultimate-guide-to-jacobian-matrices-for-robotics/
43. Robot Jacobian - Robotics Explained, accessed July 23, 2025, https://robotics-explained.com/jacobian/
44. Velocity Kinematics and Statics - Foundations of Robot Motion - Northwestern University, accessed July 23, 2025, https://modernrobotics.northwestern.edu/nu-gm-book-resource/velocity-kinematics-and-statics/
45. Singularity and how to avoid it. Singularity in robotics refers to a... | by Eve Pardi | Medium, accessed July 23, 2025, https://medium.com/@evepardi/singularity-and-how-to-avoid-it-79ba9fcf85f9
46. Robot Singularities: What Are They and How to Beat Them - RoboDK blog, accessed July 23, 2025, https://robodk.com/blog/robot-singularities/
47. Singularity - Robotics Explained, accessed July 23, 2025, https://robotics-explained.com/singularity/
48. CS 4733 Class Notes: Kinematic Singularities and Jacobians, accessed July 23, 2025, https://www.cs.columbia.edu/~allen/F15/NOTES/jacobians.pdf
49. Robot Singularity (Not the Type You're Thinking of) - The ANSI Blog, accessed July 23, 2025, https://blog.ansi.org/ansi/robot-singularity-protection-ria-kinematic/
50. 자코비안(Jacobian)이란 무엇인가 - T-Robotics, accessed July 23, 2025, http://t-robotics.blogspot.com/2013/12/jacobian.html
51. Chapter 6. Inverse kinematics, accessed July 23, 2025, https://motion.cs.illinois.edu/RoboticSystems/InverseKinematics.html
52. Introduction to inverse kinematics with Jacobian transpose, pseudoinverse and damped least squares methods | Request PDF - ResearchGate, accessed July 23, 2025, https://www.researchgate.net/publication/228383965_Introduction_to_inverse_kinematics_with_Jacobian_transpose_pseudoinverse_and_damped_least_squares_methods
53. Looking for someone to explain inverse kinematics using the Jacobian and variants. - Reddit, accessed July 23, 2025, https://www.reddit.com/r/robotics/comments/3ytbnn/looking_for_someone_to_explain_inverse_kinematics/
54. Introduction to Inverse Kinematics with Jacobian Transpose, Pseudoinverse and Damped Least Squares methods - CMU School of Computer Science, accessed July 23, 2025, https://www.cs.cmu.edu/~15464-s13/lectures/lecture6/iksurvey.pdf
55. 뉴턴 방법 - 위키백과, 우리 모두의 백과사전, accessed July 23, 2025, [https://ko.wikipedia.org/wiki/%EB%89%B4%ED%84%B4_%EB%B0%A9%EB%B2%95](https://ko.wikipedia.org/wiki/뉴턴_방법)
56. 4. Newton Method for Inverse Kinematics - 센로그 - 티스토리, accessed July 23, 2025, https://grace7040.tistory.com/95
57. Equations for the Newton-Raphson Method, accessed July 23, 2025, https://wwwbrr.cr.usgs.gov/projects/GWC_coupled/phreeqc/html/final-11.html
58. Lesson 13: Newton in several variables - UBC Math, accessed July 23, 2025, https://www.math.ubc.ca/~israel/m210/lesson13.pdf
59. Newton for nonlinear systems - Fundamentals of Numerical Computation - GitHub Pages, accessed July 23, 2025, https://fncbook.github.io/v1.0/nonlineqn/newtonsys.html
60. Mastering Newton's Method for Nonlinear Systems - Number Analytics, accessed July 23, 2025, https://www.numberanalytics.com/blog/newtons-method-nonlinear-systems-ultimate-guide
61. Jacobian to do Newton's method. - Math Stack Exchange, accessed July 23, 2025, https://math.stackexchange.com/questions/2052358/jacobian-to-do-newtons-method
62. 4.5. Newton for nonlinear systems - Fundamentals of Numerical Computation - Toby Driscoll, accessed July 23, 2025, https://tobydriscoll.net/fnc-julia/nonlineqn/newtonsys.html
63. Newton's method - Wikipedia, accessed July 23, 2025, [https://en.wikipedia.org/wiki/Newton%27s_method](https://en.wikipedia.org/wiki/Newton's_method)
64. [DL] 역전파 알고리즘(backpropagation algorithm) - velog, accessed July 23, 2025, [https://velog.io/@cha-suyeon/DL-%EC%97%AD%EC%A0%84%ED%8C%8C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98backpropagation-algorithm](https://velog.io/@cha-suyeon/DL-역전파-알고리즘backpropagation-algorithm)
65. Backpropagation - Wikipedia, accessed July 23, 2025, https://en.wikipedia.org/wiki/Backpropagation
66. Efficient Backpropagation: Exploring the Role of Jacobian Matrices and the Chain Rule, accessed July 23, 2025, https://www.shivamdshinde.com/post/efficient-backpropagation-exploring-the-role-of-jacobian-matrices-and-the-chain-rule
67. Computing Gradients with Backpropagation - cs.Princeton, accessed July 23, 2025, https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/backprop.pdf
68. 다크 프로그래머 :: Gradient, Jacobian 행렬, Hessian 행렬, Laplacian - 티스토리, accessed July 23, 2025, https://darkpgmr.tistory.com/132
69. [Optimization] 그래디언트(Gradient), 헤시안(Hessian), 자코비안(Jacobian) 개념 정리 - 이지 데이터사이언스 - 티스토리, accessed July 23, 2025, https://exvarx.tistory.com/16
70. The connection between the Jacobian, Hessian and the gradient? - Math Stack Exchange, accessed July 23, 2025, https://math.stackexchange.com/questions/2053229/the-connection-between-the-jacobian-hessian-and-the-gradient
71. A Journey through Gradient, Jacobian, and Hessian | by Rahul Das - Medium, accessed July 23, 2025, https://medium.com/@r.das699/vector-calculus-matrix-calculus-dfa4bf77cfdb
72. Jacobian and Hessian Matrices : r/math - Reddit, accessed July 23, 2025, https://www.reddit.com/r/math/comments/2xzxfm/jacobian_and_hessian_matrices/

