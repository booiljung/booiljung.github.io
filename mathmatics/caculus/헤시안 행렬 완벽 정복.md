# 헤시안 행렬 완벽 정복 가이드

## 1.  헤시안 행렬의 정의와 기본 성질

### 1.1  2차 도함수의 일반화: 왜 헤시안이 필요한가?

일변수 함수 $f(x)$를 다룰 때, 1차 도함수 $f'(x)$는 함수의 특정 지점에서의 '기울기' 또는 '변화율'을 나타낸다. 2차 도함수 $f''(x)$는 한 걸음 더 나아가 그 기울기가 어떻게 변하는지를 보여주며, 이는 함수의 '곡률(concavity)'을 의미한다.1

- $f''(x) > 0$ 이면 함수는 아래로 볼록(U자 모양)이고, $f''(x) < 0$ 이면 위로 볼록(∩자 모양)이다.

이제 이 개념을 여러 변수를 갖는 다변수 함수 $f(x_1, x_2,..., x_n)$로 확장해 보자. 1차 도함수는 더 이상 하나의 숫자가 아니라, 각 변수에 대한 편미분 값들을 모아놓은 벡터, 즉 **그래디언트(gradient)** $\nabla f$가 된다.3 그래디언트 벡터는 특정 지점에서 함수 값이 가장 가파르게 증가하는 방향과 그 변화율을 알려준다.

그렇다면 2차 도함수는 어떻게 될까? 다변수 함수에서는 고려할 것이 훨씬 많다. 각 변수 방향으로의 곡률($\frac{\partial^2 f}{\partial x_1^2}$, $\frac{\partial^2 f}{\partial x_2^2}$ 등)뿐만 아니라, 한 변수의 변화가 다른 변수의 변화율에 어떤 영향을 미치는지에 대한 정보, 즉 **혼합 편도함수(mixed partial derivatives)**($\frac{\partial^2 f}{\partial x_1 \partial x_2}$ 등)도 알아야 한다. 이 모든 2차 미분 정보를 체계적으로 정리하여 하나의 객체에 담아낸 것이 바로 **헤시안 행렬(Hessian Matrix)**이다.3

결론적으로 헤시안은 다변수 함수의 '국소적 곡률(local curvature)'을 완벽하게 설명하는 핵심 도구다.1 함수가 그리는 다차원 표면이 특정 지점에서 아래로 볼록한 그릇 모양인지, 위로 볼록한 돔 모양인지, 아니면 방향에 따라 오목함과 볼록함이 뒤섞인 말 안장 모양인지를 판별해 주는 강력한 수학적 장치인 것이다.8

### 1.2  공식적 정의와 계산

$n$개의 실수 변수를 입력으로 받아 하나의 스칼라 값을 출력하는 함수 $f: \mathbb{R}^n \to \mathbb{R}$가 있다고 하자. 이 함수의 모든 2계 편도함수가 존재하고 연속일 때, 헤시안 행렬 $H_f$ (또는 $\nabla^2 f$, $H(f)$)는 다음과 같이 정의되는 $n \times n$ 정사각 행렬이다.3
$$
H_f(\mathbf{x}) =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$
여기서 행렬의 $i$번째 행, $j$번째 열의 원소 $(H_f)_{ij}$는 함수 $f$를 먼저 $x_j$로 편미분한 뒤, 그 결과를 다시 $x_i$로 편미분한 값, 즉 $\frac{\partial^2 f}{\partial x_i \partial x_j}$이다.6 헤시안 행렬은 항상 변수의 개수와 같은 크기의 정사각 행렬($n \times n$)이 된다.1

**계산 예제: $f(x, y) = x^3 - 2xy - y^6$**

이 함수의 헤시안 행렬을 구하는 과정은 다음과 같다.3

1. **1차 편도함수 계산 (그래디언트 찾기):**

   - $f_x = \frac{\partial f}{\partial x} = 3x^2 - 2y$
   - $f_y = \frac{\partial f}{\partial y} = -2x - 6y^5$

2. **2차 편도함수 계산:**

   - $f_{xx} = \frac{\partial}{\partial x}(3x^2 - 2y) = 6x$
   - $f_{yy} = \frac{\partial}{\partial y}(-2x - 6y^5) = -30y^4$
   - $f_{xy} = \frac{\partial}{\partial y}(3x^2 - 2y) = -2$
   - $f_{yx} = \frac{\partial}{\partial x}(-2x - 6y^5) = -2$

3. **행렬 구성:** 계산된 2차 편도함수들을 행렬에 배치한다.
   $$
   H_f(x, y) = \begin{bmatrix} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{bmatrix} = \begin{bmatrix} 6x & -2 \\ -2 & -30y^4 \end{bmatrix}
   $$
   중요한 점은 헤시안 행렬 그 자체는 변수 $x, y$를 포함하는 **행렬 값 함수(matrix-valued function)**라는 것이다. 특정 지점에서의 곡률을 알고 싶다면, 그 지점의 좌표를 대입하여 숫자 행렬을 얻어야 한다.3 예를 들어, 위 예제에서 점 

$(1, 2)$에서의 헤시안은 다음과 같다.
$$
H_f(1, 2) = \begin{bmatrix} 6(1) & -2 \\ -2 & -30(2)^4 \end{bmatrix} = \begin{bmatrix} 6 & -2 \\ -2 & -480 \end{bmatrix}
$$
이 숫자 행렬이 바로 점 $(1, 2)$에서의 국소적 곡률에 대한 모든 정보를 담고 있다.

### 1.3  대칭성과 클레로의 정리

위의 계산 예제에서 $f_{xy}$와 $f_{yx}$의 값이 $-2$로 동일하게 나온 것은 우연이 아니다. 이는 **클레로의 정리(Clairaut's Theorem)** 또는 **슈바르츠의 정리(Schwarz's Theorem)**로 알려진 다변수 미적분학의 기본 정리 때문이다.1

**클레로의 정리:** 함수 $f$의 모든 2계 편도함수가 어떤 점 근방에서 존재하고 연속이라면, 그 점에서 혼합 편도함수의 미분 순서는 결과에 영향을 미치지 않는다.5 즉, 다음이 성립한다.
$$
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}
$$
이 정리 덕분에 헤시안 행렬의 $(i, j)$ 원소와 $(j, i)$ 원소는 항상 같아진다. 이는 헤시안 행렬이 주대각선을 기준으로 대칭인 **대칭 행렬(symmetric matrix)**임을 의미한다 ($H = H^T$).4

헤시안이 대칭이라는 사실은 매우 강력한 수학적 의미를 가진다. 선형대수학에서 대칭 행렬은 다음과 같은 중요한 특성을 갖기 때문이다.

1. **모든 고윳값(eigenvalue)이 실수(real number)이다**.4
2. 서로 다른 고윳값에 대응하는 고유벡터(eigenvector)들은 서로 직교(orthogonal)한다.

만약 고윳값이 복소수라면 '크다' 또는 '작다'를 비교하여 함수의 볼록성을 판단하기 어렵다. 헤시안의 대칭성 덕분에 우리는 함수의 곡률을 실수 고윳값이라는 명확한 척도로 분석할 수 있게 되며, 이는 3장에서 다룰 극값 판정의 이론적 기반이 된다.

물론, 2계 편도함수가 연속이 아닌 매우 특수한 함수에서는 이 대칭성이 깨질 수도 있다.11 하지만 공학, 과학, 머신러닝 등에서 다루는 대부분의 함수는 이 조건을 만족하므로, 헤시안은 대칭이라고 가정해도 무방하다.14

### 1.4  야코비 행렬과의 비교

다변수 미적분학을 공부할 때 헤시안 행렬과 자주 혼동되는 것이 **야코비 행렬(Jacobian Matrix)**이다. 둘은 밀접한 관련이 있지만, 근본적으로 다른 개념이므로 명확히 구분해야 한다.

가장 큰 차이는 다루는 대상과 미분의 차수다.

- **야코비 행렬**은 여러 변수를 입력받아 여러 값을 출력하는 **벡터 값 함수**($\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$)의 **1차** 편도함수들을 모아놓은 행렬이다.9
- **헤시안 행렬**은 여러 변수를 입력받아 하나의 값을 출력하는 **스칼라 값 함수**($f: \mathbb{R}^n \to \mathbb{R}$)의 **2차** 편도함수들을 모아놓은 행렬이다.6

이 관계를 더 깊이 파고들면 흥미로운 연결고리를 발견할 수 있다. 스칼라 함수 $f(\mathbf{x})$의 그래디언트 $\nabla f(\mathbf{x})$를 생각해보자. 그래디언트는 각 변수에 대한 1차 편도함수로 이루어진 벡터이므로, 그 자체로 하나의 벡터 값 함수($\mathbf{g}(\mathbf{x}) = \nabla f(\mathbf{x})$)로 볼 수 있다. 이 함수는 $\mathbb{R}^n$에서 $\mathbb{R}^n$으로 매핑된다.

이제 이 그래디언트 함수 $\mathbf{g}(\mathbf{x})$의 야코비 행렬 $J_{\mathbf{g}}$를 계산하면 어떻게 될까? 야코비 행렬의 정의에 따라 $(J_{\mathbf{g}})_{ij} = \frac{\partial g_i}{\partial x_j}$ 이다. 여기서 $g_i$는 그래디언트의 $i$번째 성분, 즉 $\frac{\partial f}{\partial x_i}$이므로, $(J_{\mathbf{g}})_{ij} = \frac{\partial}{\partial x_j} \left( \frac{\partial f}{\partial x_i} \right) = \frac{\partial^2 f}{\partial x_j \partial x_i}$가 된다.

이는 헤시안 행렬 $H_f$의 $(j, i)$ 원소와 같다. 즉, $J_{\nabla f} = (H_f)^T$ 이다. 그런데 클레로의 정리에 의해 헤시안은 대칭 행렬이므로 $(H_f)^T = H_f$이다. 따라서 우리는 다음과 같은 매우 중요하고 간결한 관계를 얻는다: **"헤시안 행렬은 그래디언트의 야코비 행렬이다"**.6 이 관계는 헤시안을 1차 미분 연산자인 야코비를 1차 도함수 벡터인 그래디언트에 적용한 결과로 이해할 수 있게 해준다.

두 행렬의 차이점은 다음 표와 같이 요약할 수 있다.

| 특성 (Attribute) | 야코비 행렬 (Jacobian Matrix)                                | 헤시안 행렬 (Hessian Matrix)                        |
| ---------------- | ------------------------------------------------------------ | --------------------------------------------------- |
| **미분 차수**    | 1차 (First-order) 9                                          | 2차 (Second-order) 9                                |
| **대상 함수**    | 벡터 값 함수 ($\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$) 15 | 스칼라 값 함수 ($f: \mathbb{R}^n \to \mathbb{R}$) 6 |
| **행렬 형태**    | 일반적인 직사각 행렬 ($m \times n$) 15                       | 항상 정사각 행렬 ($n \times n$) 9                   |
| **대칭성**       | 보장되지 않음                                                | 연속 함수에 대해 항상 대칭 9                        |
| **주요 용도**    | 선형 근사, 변수 변환, 임계점 탐색 9                          | 2차 근사, 곡률 분석, 극값 판정 4                    |

## 2.  헤시안과 함수 근사

### 2.1  다변수 테일러 급수와 2차 근사

헤시안 행렬의 가장 중요한 역할 중 하나는 다변수 함수를 국소적으로 근사하는 데 사용된다는 것이다. 일변수 함수 $f(x)$를 점 $a$ 근방에서 2차 다항식으로 근사하는 테일러 급수는 다음과 같다.
$$
f(x) \approx f(a) + f'(a)(x-a) + \frac{1}{2}f''(a)(x-a)^2
$$
이 아이디어를 다변수 함수 $f(\mathbf{x})$를 점 $\mathbf{x}_0$ 근방에서 근사하는 식으로 확장할 수 있다. 이때 그래디언트와 헤시안이 등장한다.3
$$
f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T H_f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)
$$
이 식의 각 항은 명확한 기하학적 의미를 가진다.

- $f(\mathbf{x}_0)$: 근사의 기준이 되는 높이 (상수항)
- $\nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0)$: 그래디언트를 이용한 1차 선형 근사. 이는 함수 표면에 접하는 **접평면(tangent plane)**의 방정식을 나타낸다.19
- $\frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T H_f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)$: 헤시안을 이용한 2차항. 이것이 바로 함수 표면의 휘어진 정도, 즉 **곡률**을 모델링하는 부분이다. 이 항은 **이차 형식(Quadratic Form)**이라고 불리며, 함수를 접평면보다 더 정교하게, 즉 포물면(paraboloid) 형태로 근사하게 해준다.20

물리학의 운동 개념에 비유하면 이해가 쉽다. 최적화 문제에서 함수 값 $f(\mathbf{x})$를 '위치'나 '에너지'라고 생각할 수 있다. 그렇다면 그래디언트 $\nabla f$는 위치의 1차 미분이므로 '속도'에 해당한다. 그래디언트가 크다는 것은 함수 값이 빠르게 변한다는 의미다. 같은 논리로, 헤시안 $H_f$는 그래디언트의 변화율, 즉 위치의 2차 미분이므로 '가속도'에 비유할 수 있다.1 헤시안은 우리가 그래디언트 방향으로 움직일 때, 그래디언트 벡터 자체가 어떻게 변하는지를 알려준다. 이 '가속도' 정보가 바로 함수 표면이 어떻게 휘어져 있는지를 나타내는 곡률이며, 이 정보를 활용하면 단순한 경사하강법보다 훨씬 더 지능적으로 최적점을 찾아갈 수 있다.

### 2.2  함수의 볼록성 판정

최적화 이론에서 **볼록 함수(convex function)**는 매우 특별하고 중요한 위치를 차지한다. 볼록 함수의 그래프는 아래로 오목한 그릇 모양과 같아서, 어떤 지점에서 찾은 **국소 최솟값(local minimum)**이 항상 **전역 최솟값(global minimum)**임을 보장하기 때문이다.23 이는 최적화 알고리즘이 엉뚱한 지점에 수렴할 걱정 없이 유일한 해를 향해 나아갈 수 있다는 의미다.

일변수 함수에서는 $f''(x) \ge 0$ 이라는 조건으로 볼록성을 판별할 수 있었다. 다변수 함수에서는 헤시안 행렬이 이 역할을 그대로 이어받는다.1

**판정 기준:** 함수 $f$의 정의역(domain) 내의 모든 점 $\mathbf{x}$에 대하여 다음이 성립한다.9

- 헤시안 $H_f(\mathbf{x})$가 **양의 준정부호(positive semidefinite, PSD)**이면, $f$는 **볼록 함수**이다.
- 헤시안 $H_f(\mathbf{x})$가 **양의 정부호(positive definite, PD)**이면, $f$는 **강한 볼록 함수(strictly convex)**이다.

여기서 '정부호성(definiteness)'이란 행렬이 이차 형식으로 사용될 때 그 값이 항상 특정 부호를 갖는 성질을 말한다. 예를 들어, 행렬 $H$가 양의 정부호(PD)라는 것은 0이 아닌 모든 벡터 $\mathbf{v}$에 대하여 이차 형식 $\mathbf{v}^T H \mathbf{v}$의 값이 항상 0보다 크다는 의미다. 기하학적으로 이는 모든 방향으로의 곡률이 양수, 즉 어느 방향에서 보아도 함수 표면이 아래로 볼록한 완벽한 그릇 모양임을 뜻한다.8

행렬의 정부호성은 헤시안의 모든 고윳값을 계산하여 간단하게 판별할 수 있다.1

- 모든 고윳값 $\lambda_i \ge 0$ ⟹ 양의 준정부호 (Positive Semidefinite, PSD)
- 모든 고윳값 $\lambda_i > 0$ ⟹ 양의 정부호 (Positive Definite, PD)
- 모든 고윳값 $\lambda_i \le 0$ ⟹ 음의 준정부호 (Negative Semidefinite, NSD)
- 모든 고윳값 $\lambda_i < 0$ ⟹ 음의 정부호 (Negative Definite, ND)
- 양수와 음수 고윳값이 섞여 있음 ⟹ 부정부호 (Indefinite)

따라서 함수의 볼록성을 확인하고 싶다면, 정의역 내 모든 점에서 헤시안 행렬을 계산하고 그 고윳값들이 모두 0 이상인지 확인하면 된다. 이는 많은 최적화 알고리즘의 수렴성을 증명하는 데 핵심적인 역할을 한다.

## 3.  응용 1: 극값 판정과 최적화

### 3.1  2계 편도함수 판정법의 원리

함수의 최솟값이나 최댓값을 찾는 최적화 문제의 첫 단계는 **임계점(critical point)**을 찾는 것이다. 임계점이란 그래디언트가 0 벡터가 되는 지점, 즉 $\nabla f(\mathbf{x}_0) = \mathbf{0}$인 점 $\mathbf{x}_0$를 말한다.2 기하학적으로 이 지점에서는 함수 표면에 그은 접평면이 수평이 되어, 더 이상 오르거나 내려갈 방향이 없는 상태다.

하지만 임계점이라고 해서 모두 극값(극소점 또는 극대점)은 아니다. 임계점은 **극소점(local minimum)**, **극대점(local maximum)**, 또는 **안장점(saddle point)**일 수 있다.7 안장점은 말 안장처럼 어떤 방향에서 보면 극소점 같지만, 다른 방향에서 보면 극대점처럼 보이는 지점이다.

이 세 가지 경우를 어떻게 구별할 수 있을까? 바로 헤시안 행렬이 답을 제공한다. 임계점 $\mathbf{x}_0$에서는 $\nabla f(\mathbf{x}_0) = \mathbf{0}$이므로, 2.1절에서 본 2차 테일러 근사식의 1차항이 사라지고 다음과 같이 매우 단순한 형태로 남는다.
$$
f(\mathbf{x}) - f(\mathbf{x}_0) \approx \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T H_f(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)
$$
이 식은 임계점 근방에서 함수 값의 변화($f(\mathbf{x}) - f(\mathbf{x}_0)$)가 전적으로 헤시안 행렬 $H_f(\mathbf{x}_0)$이 만드는 이차 형식에 의해 결정된다는 것을 보여준다. 따라서 극값을 판정하는 문제는 결국 임계점에서의 헤시안 행렬의 정부호성을 판정하는 문제와 같아진다.

- 만약 $H_f(\mathbf{x}_0)$가 **양의 정부호(PD)**라면, 이차 형식은 항상 양수다. 따라서 임계점 근방의 모든 점 $\mathbf{x}$에 대해 $f(\mathbf{x}) > f(\mathbf{x}_0)$가 성립하므로, $\mathbf{x}_0$는 **극소점**이다.1
- 만약 $H_f(\mathbf{x}_0)$가 **음의 정부호(ND)**라면, 이차 형식은 항상 음수다. 따라서 $f(\mathbf{x}) < f(\mathbf{x}_0)$가 성립하므로, $\mathbf{x}_0$는 **극대점**이다.1
- 만약 $H_f(\mathbf{x}_0)$가 **부정부호(indefinite)**라면, 방향에 따라 이차 형식의 부호가 달라진다. 즉, 어떤 방향으로는 함수 값이 증가하고 다른 방향으로는 감소하므로, 이는 **안장점**이다.9
- 만약 $H_f(\mathbf{x}_0)$가 양의 준정부호 또는 음의 준정부호라서 0인 고윳값을 갖는다면 (즉, 행렬식이 0이라면), 2차 근사만으로는 곡률을 판단하기에 정보가 부족하다. 이 경우 테스트는 **판정 불가(inconclusive)**이며, 더 높은 차수의 도함수를 분석해야 한다.2

### 3.2  판정 기준: 2변수 vs. n변수

위의 원리를 실제 문제에 적용하기 위한 구체적인 판정 기준은 다음과 같다.

#### 3.2.1 일반적인 n-변수 경우 (고윳값 기반)

가장 일반적이고 근본적인 방법은 임계점 $\mathbf{x}_0$에서 헤시안 행렬 $H_f(\mathbf{x}_0)$의 고윳값을 직접 계산하는 것이다.24

- **모든 고윳값이 양수 > 0:** 극소점 (Local Minimum)
- **모든 고윳값이 음수 < 0:** 극대점 (Local Maximum)
- **양수와 음수 고윳값이 공존:** 안장점 (Saddle Point)
- **0인 고윳값이 있고 나머지가 모두 같은 부호:** 판정 불가 (Inconclusive)

#### 3.2.2 특별한 2변수 경우 (행렬식 기반)

변수가 2개인 함수 $f(x, y)$의 경우, $2 \times 2$ 행렬의 고윳값을 직접 계산하는 대신, 헤시안 행렬식(Hessian determinant)과 2차 편도함수 $f_{xx}$의 부호를 이용하는 더 간편한 방법이 있다.3 헤시안 행렬식을 종종 **판별식(Discriminant)**이라고 부르며, $D = \det(H) = f_{xx}f_{yy} - f_{xy}^2$로 계산된다.1

이 판별법이 고윳값 기반 판정법과 어떻게 연결되는지 이해하는 것이 중요하다. $2 \times 2$ 행렬의 두 고윳값을 $\lambda_1, \lambda_2$라 하면, 행렬식 $D = \lambda_1 \lambda_2$ 이고, 행렬의 대각합(trace) $tr(H) = f_{xx} + f_{yy} = \lambda_1 + \lambda_2$ 이다.

- $D > 0$ 이면 $\lambda_1 \lambda_2 > 0$ 이므로, 두 고윳값의 부호가 같다.
  - 이때 $f_{xx} > 0$ 이면, $D>0$ 조건 때문에 $f_{yy}$도 양수여야 하고, 따라서 $tr(H) = f_{xx} + f_{yy} > 0$ 이다. 두 수의 합과 곱이 모두 양수이므로 $\lambda_1, \lambda_2$는 모두 양수다. ⟹ **극소점**.
  - 이때 $f_{xx} < 0$ 이면, $f_{yy}$도 음수여야 하고, 따라서 $tr(H) < 0$ 이다. 합은 음수, 곱은 양수이므로 $\lambda_1, \lambda_2$는 모두 음수다. ⟹ **극대점**.
- $D < 0$ 이면 $\lambda_1 \lambda_2 < 0$ 이므로, 두 고윳값의 부호가 다르다. ⟹ **안장점**.
- $D = 0$ 이면 $\lambda_1 \lambda_2 = 0$ 이므로, 고윳값 중 적어도 하나가 0이다. ⟹ **판정 불가**.

결국 2변수 판정법은 고윳값 계산을 피하기 위한 영리한 대수적 트릭일 뿐, 근본 원리는 n변수 경우와 완벽히 동일하다.

판정 기준을 요약하면 다음과 같다.

| 2계 도함수 판정법 요약                                       |
| ------------------------------------------------------------ |
| **2변수 함수 $f(x,y)$ (임계점 $(a,b)$에서, $D = f_{xx}f_{yy}-f_{xy}^2$)** |
| **조건:** $D > 0$ 이고 $f_{xx}(a,b) > 0$                     |
| **조건:** $D > 0$ 이고 $f_{xx}(a,b) < 0$                     |
| **조건:** $D < 0$                                            |
| **조건:** $D = 0$                                            |
| **n변수 함수 $f(\mathbf{x})$ (임계점 $\mathbf{x}_0$에서)**   |
| **조건:** $H_f(\mathbf{x}_0)$의 모든 고윳값이 양수 (Positive Definite) |
| **조건:** $H_f(\mathbf{x}_0)$의 모든 고윳값이 음수 (Negative Definite) |
| **조건:** 양수와 음수 고윳값이 공존 (Indefinite)             |
| **조건:** 0인 고윳값이 존재 (Semidefinite)                   |

### 3.3  종합 예제: 3변수 함수 극값 판정

함수 $f(x, y, z) = x^3 + xyz + y^2 - 3x$ 의 극값을 판정하는 전체 과정을 살펴보자.25

1. **임계점 찾기:** 그래디언트 $\nabla f = \mathbf{0}$이 되는 점을 찾기 위해 연립방정식을 푼다.

   - $f_x = 3x^2 + yz - 3 = 0$

   - $f_y = xz + 2y = 0$

   - $f_z = xy = 0$

     세 번째 식에서 $x=0$ 또는 $y=0$이다. 만약 $x=0$이면 첫 번째 식은 $-3=0$이 되어 모순이므로, $x \neq 0$이다. 따라서 반드시 $y=0$이어야 한다. $y=0$을 첫 번째와 두 번째 식에 대입하면 $3x^2 - 3 = 0$과 $xz = 0$을 얻는다. 첫 식에서 $x = \pm 1$이고, 두 번째 식에서 $z=0$이다. 따라서 임계점은 $(1, 0, 0)$과 $(-1, 0, 0)$ 두 개다.

2. **헤시안 행렬 계산:** 일반적인 헤시안 행렬을 구한다.
   $$
   H_f(x,y,z) = \begin{bmatrix}
   f_{xx} & f_{xy} & f_{xz} \\
   f_{yx} & f_{yy} & f_{yz} \\
   f_{zx} & f_{zy} & f_{zz}
   \end{bmatrix} = \begin{bmatrix}
   6x & z & y \\
   z & 2 & x \\
   y & x & 0
   \end{bmatrix}
   $$
   **각 임계점에서 판정:**

   - 점 $(1, 0, 0)$에서:

     헤시안 행렬은 $H_f(1,0,0) = \begin{bmatrix} 6 & 0 & 0 \\ 0 & 2 & 1 \\ 0 & 1 & 0 \end{bmatrix}$ 이다.

     이 행렬의 특성방정식 $\det(H - \lambda I) = 0$을 풀면,

     $(6-\lambda)((2-\lambda)(-\lambda) - 1) = (6-\lambda)(\lambda^2 - 2\lambda - 1) = 0$

     이므로, 고윳값은 $\lambda_1 = 6$, $\lambda_{2,3} = \frac{2 \pm \sqrt{4+4}}{2} = 1 \pm \sqrt{2}$ 이다.

     고윳값은 $6, 1+\sqrt{2}, 1-\sqrt{2}$ 이며, 이 중 $1-\sqrt{2}$는 음수다. 양수와 음수 고윳값이 공존하므로, 점 (1, 0, 0)은 안장점이다.

   - 점 $(-1, 0, 0)$에서:

     헤시안 행렬은 $H_f(-1,0,0) = \begin{bmatrix} -6 & 0 & 0 \\ 0 & 2 & -1 \\ 0 & -1 & 0 \end{bmatrix}$ 이다.

     이 행렬의 특성방정식은

     $(-6-\lambda)((2-\lambda)(-\lambda) - 1) = (-6-\lambda)(\lambda^2 - 2\lambda - 1) = 0$

     이므로, 고윳값은 $\lambda_1 = -6$, $\lambda_{2,3} = 1 \pm \sqrt{2}$ 이다.

     고윳값은 $-6, 1+\sqrt{2}, 1-\sqrt{2}$ 이며, 이번에도 양수와 음수 고윳값이 공존한다. 따라서 점 (-1, 0, 0)도 안장점이다.

결론적으로 이 함수는 두 개의 임계점을 가지며, 둘 다 안장점이다.

## 4.  응용 2: 뉴턴 최적화 방법

### 4.1  2차 최적화의 원리: 언덕을 더 똑똑하게 내려가는 법

최적화 알고리즘의 목표는 함수의 최저점을 찾는 것이다. 가장 기본적이고 널리 알려진 방법은 **경사하강법(Gradient Descent)**이다.

- **경사하강법:** 1차 정보(그래디언트)만 사용한다. 현재 위치에서 가장 가파른 내리막길 방향($-\nabla f$)으로 정해진 보폭(learning rate)만큼 이동하는 것을 반복한다. 이 방법은 구현이 간단하지만, 함수 표면이 좁고 긴 계곡 형태(ill-conditioned)일 경우, 계곡의 양쪽 벽을 오가며 지그재그로 비효율적인 경로를 따라 움직이는 단점이 있다.28

**뉴턴 방법(Newton's Method)**은 여기에 2차 정보(헤시안)를 추가하여 훨씬 더 효율적인 경로를 찾는다.

- **뉴턴 방법:** 현재 위치에서 함수를 가장 잘 근사하는 2차 함수, 즉 포물면(paraboloid)을 만든다. 그리고 그 포물면의 최저점으로 한 번에 점프하는 방식을 취한다.17 이는 단순히 언덕의 기울기만 보는 것이 아니라, 언덕이 얼마나 휘어져 있는지(곡률)까지 고려하여 최저점을 정확히 예측하는 것과 같다.

### 4.2  뉴턴 스텝과 업데이트 규칙

뉴턴 방법의 업데이트 규칙은 2.1절의 2차 테일러 근사식으로부터 유도된다. 현재 위치를 $\mathbf{x}_k$라 할 때, 함수를 근사하는 포물면은 다음과 같다.
$$
f(\mathbf{x}) \approx q(\mathbf{x}) = f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^T (\mathbf{x} - \mathbf{x}_k) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_k)^T H_k (\mathbf{x} - \mathbf{x}_k)
$$
(단, $H_k = H_f(\mathbf{x}_k)$이다)

우리의 목표는 이 근사 함수 $q(\mathbf{x})$를 최소화하는 다음 지점 $\mathbf{x}$를 찾는 것이다. $q(\mathbf{x})$를 $\mathbf{x}$에 대해 미분하여 0으로 놓으면 최저점을 찾을 수 있다.
$$
\nabla q(\mathbf{x}) = \nabla f(\mathbf{x}_k) + H_k (\mathbf{x} - \mathbf{x}_k) = \mathbf{0}
$$
이 식을 $\mathbf{x}$에 대해 정리하면 다음 스텝 $\mathbf{x}_{k+1} = \mathbf{x}$를 얻는다.
$$
H_k (\mathbf{x}_{k+1} - \mathbf{x}_k) = -\nabla f(\mathbf{x}_k)
$$

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - H_k^{-1} \nabla f(\mathbf{x}_k)
$$

이것이 바로 뉴턴 방법의 업데이트 규칙이다.28 여기서 $\Delta\mathbf{x}_k = -H_k^{-1} \nabla f(\mathbf{x}_k)$를 **뉴턴 스텝(Newton step)**이라고 부른다.30

### 4.3  수렴성과 한계: 빠르지만 비싸다

뉴턴 방법은 장점과 단점이 매우 명확하다.

장점: 2차 수렴 (Quadratic Convergence)

뉴턴 방법의 가장 큰 장점은 최적점에 충분히 가까워졌을 때 매우 빠르게 수렴한다는 것이다. 2차 수렴이란 각 반복 단계에서 오차의 제곱에 비례하여 오차가 줄어드는 것을 의미한다. 예를 들어 오차가 0.1이었다면 다음 스텝에서는 약 0.01, 그 다음에는 0.0001로 기하급수적으로 줄어든다. 이는 경사하강법의 선형 수렴(오차가 일정한 비율로 줄어드는 것)보다 훨씬 빠르다.17

단점: 막대한 계산 비용과 불안정성

이러한 빠른 속도에도 불구하고 뉴턴 방법은 실용적으로 사용하기 어려운 경우가 많다.

1. **헤시안 계산 비용:** 변수의 개수가 $n$일 때, $n \times n$ 헤시안 행렬을 계산하려면 약 $n^2/2$개의 서로 다른 2차 편도함수를 계산해야 한다. 변수가 수천, 수만 개에 달하는 딥러닝 모델 같은 문제에서는 이는 사실상 불가능한 계산량이다.28

2. **역행렬 계산 비용:** $n \times n$ 행렬의 역행렬을 계산하는 것은 일반적으로 $O(n^3)$의 계산 복잡도를 가진다. 이 또한 대규모 문제에서 뉴턴 방법을 비실용적으로 만드는 주된 원인이다.28 (실제 구현에서는 역행렬을 직접 구하기보다 

   $H_k \Delta\mathbf{x}_k = -\nabla f(\mathbf{x}_k)$ 형태의 선형 시스템을 푸는 것이 더 효율적이지만, 이 역시 $O(n^3)$에 가까운 비용이 든다 28).

3. **수치적 불안정성:**

   - 최적점에서 멀리 떨어져 있거나, 현재 지점이 극소점이 아닌 경우(예: 안장점 근처) 헤시안 행렬이 양의 정부호가 아닐 수 있다. 이 경우 뉴턴 스텝이 엉뚱한 방향을 가리켜 오히려 함수 값을 증가시키거나 알고리즘이 발산할 수 있다.31

   - 헤시안이 특이 행렬(singular, 역행렬이 존재하지 않음)이거나 거의 특이 행렬에 가까운 경우(ill-conditioned), 역행렬 계산이 수치적으로 매우 불안정해져서 정확한 스텝을 계산할 수 없다.23 이를 해결하기 위해 헤시안에 

     $H + \lambda I$ 와 같이 단위 행렬을 더해 강제로 양의 정부호 행렬로 만들어주는 **정규화(regularization)** 기법(예: Levenberg-Marquardt 알고리즘)을 사용하기도 한다.31

### 4.4  준-뉴턴 방법으로의 확장: 현실적인 대안

경사하강법은 싸지만 느리고, 뉴턴 방법은 빠르지만 비싸다. 이 둘 사이의 합리적인 절충안은 없을까? 이 질문에 대한 답이 바로 **준-뉴턴 방법(Quasi-Newton Method)**이다.23

준-뉴턴 방법의 핵심 아이디어는 다음과 같다: **"비싼 헤시안의 역행렬 $H_k^{-1}$을 직접 계산하지 말고, 더 저렴한 비용으로 근사(approximate)하자."**

준-뉴턴 방법은 매 스텝에서 그래디언트의 변화를 관찰하여 $H_k^{-1}$의 근사치인 행렬 $B_k$를 점진적으로 업데이트한다. 즉, 이전 스텝의 정보($\mathbf{x}_{k} - \mathbf{x}_{k-1}$와 $\nabla f(\mathbf{x}_k) - \nabla f(\mathbf{x}_{k-1})$)를 이용하여 현재의 곡률 정보를 추정하고 $B_k$를 갱신하는 것이다.23

가장 유명하고 널리 사용되는 준-뉴턴 알고리즘은 **BFGS(Broyden-Fletcher-Goldfarb-Shanno)**이다. BFGS는 근사 행렬 $B_k$가 항상 대칭이고 양의 정부호라는 성질을 유지하도록 업데이트하여 알고리즘의 안정성을 크게 높인다.33

결론적으로, 준-뉴턴 방법은 뉴턴 방법의 빠른 수렴 속도(초선형 수렴, superlinear convergence)를 상당 부분 유지하면서도, 헤시안을 직접 다루는 막대한 계산 부담($O(n^3)$)을 $O(n^2)$ 수준으로 크게 줄여준다. 이 덕분에 준-뉴턴 방법은 대규모 최적화 문제에서 매우 효과적이고 실용적인 표준 알고리즘 중 하나로 자리 잡았다.23

#### **참고 자료**

1. Hessian Matrix in mathematics | Medium, accessed July 23, 2025, https://sid-sharma1990.medium.com/hessian-matrix-f9863f934075
2. Second partial derivative test (article) - Khan Academy, accessed July 23, 2025, https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/second-partial-derivative-test
3. The Hessian matrix | Multivariable calculus (article) | Khan Academy, accessed July 23, 2025, https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/quadratic-approximations/a/the-hessian
4. [Optimization] 그래디언트(Gradient), 헤시안(Hessian), 자코비안(Jacobian) 개념 정리 - 이지 데이터사이언스 - 티스토리, accessed July 23, 2025, https://exvarx.tistory.com/16
5. The Hessian matrix (video) - Khan Academy, accessed July 23, 2025, https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/quadratic-approximations/v/the-hessian-matrix
6. Hessian matrix - Wikipedia, accessed July 23, 2025, https://en.wikipedia.org/wiki/Hessian_matrix
7. Hessian matrix (second derivative test) - MIT, accessed July 23, 2025, https://www.mit.edu/~ashrstnv/hessian-matrix.html
8. 헤시안 행렬 (Hessian matrix) / Frangi filter - 날아가는 개발자, accessed July 23, 2025, https://sjpyo.tistory.com/31
9. All About the Hessian Matrix, Convexity, and Optimization | System ..., accessed July 23, 2025, https://resources.system-analysis.cadence.com/blog/msa2022-all-about-the-hessian-matrix-convexity-and-optimization
10. resources.system-analysis.cadence.com, accessed July 23, 2025, [https://resources.system-analysis.cadence.com/blog/msa2022-all-about-the-hessian-matrix-convexity-and-optimization#:~:text=The%20Symmetry%20of%20a%20Hessian%20Matrix&text=According%20to%20Schwarz's%20theorem%20or,and%20y%20in%20different%20orders.](https://resources.system-analysis.cadence.com/blog/msa2022-all-about-the-hessian-matrix-convexity-and-optimization#:~:text=The Symmetry of a Hessian Matrix&text=According to Schwarz's theorem or,and y in different orders.)
11. Symmetry of second derivatives - Wikipedia, accessed July 23, 2025, https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives
12. Chapter 4. Numerical Computation - DP-Story - 티스토리, accessed July 23, 2025, https://dp-story.tistory.com/8
13. When is a Hessian matrix symmetric? - Math Stack Exchange, accessed July 23, 2025, https://math.stackexchange.com/questions/5041961/when-is-a-hessian-matrix-symmetric
14. A quick question about the Hessian matrix - Mathematics Stack Exchange, accessed July 23, 2025, https://math.stackexchange.com/questions/122169/a-quick-question-about-the-hessian-matrix
15. Understanding Jacobian and Hessian matrices with example, accessed July 23, 2025, http://www.sefidian.com/2022/03/02/understand-jacobian-and-hessian-matrices-with-example/
16. Gradient (그래디언트), Jacobian (자코비안) 및 Hessian (헤시안) - gaussian37 - JINSOL KIM, accessed July 23, 2025, https://gaussian37.github.io/math-calculus-jacobian/
17. Optimization and Convergence - Physics-based Deep Learning, accessed July 23, 2025, https://physicsbaseddeeplearning.org/overview-optconv.html
18. resources.system-analysis.cadence.com, accessed July 23, 2025, [https://resources.system-analysis.cadence.com/blog/msa2022-all-about-the-hessian-matrix-convexity-and-optimization#:~:text=A%20Hessian%20matrix%20consists%20of,but%20first%2Dorder%20partial%20derivatives.](https://resources.system-analysis.cadence.com/blog/msa2022-all-about-the-hessian-matrix-convexity-and-optimization#:~:text=A Hessian matrix consists of,but first-order partial derivatives.)
19. Hessian Matrix, Taylor Series, and the Newton-Raphson Method - PyImageSearch, accessed July 23, 2025, https://pyimagesearch.com/2025/03/10/hessian-matrix-taylor-series-and-the-newton-raphson-method/
20. The Hessian Matrix - Ximera - The Ohio State University, accessed July 23, 2025, https://ximera.osu.edu/mklynn2/multivariable/content/04_3_hessian/hessian
21. Local Extrema - Ximera - The Ohio State University, accessed July 23, 2025, https://ximera.osu.edu/mklynn2/multivariable/content/04_4_local_extrema/local_extrema
22. [벡터 미적분학] 해시안(Hessian), accessed July 23, 2025, https://deep-learning-study.tistory.com/m/740
23. 5. Quasi-Newton Method for Inverse Kinematics - 센로그 - 티스토리, accessed July 23, 2025, https://grace7040.tistory.com/96
24. Second partial derivative test - Wikipedia, accessed July 23, 2025, https://en.wikipedia.org/wiki/Second_partial_derivative_test
25. [선형대수학] 헤시안(Hessian) 행렬과 극소점, 극대점, 안장점 by ..., accessed July 23, 2025, [https://bskyvision.com/entry/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99-%ED%97%A4%EC%8B%9C%EC%95%88Hessian-%ED%96%89%EB%A0%AC%EA%B3%BC-%EA%B7%B9%EC%86%8C%EC%A0%90-%EA%B7%B9%EB%8C%80%EC%A0%90-%EC%95%88%EC%9E%A5%EC%A0%90](https://bskyvision.com/entry/선형대수학-헤시안Hessian-행렬과-극소점-극대점-안장점)
26. Hessian Matrix | Brilliant Math & Science Wiki, accessed July 23, 2025, https://brilliant.org/wiki/hessian-matrix/
27. The second derivative test | Calculus in a Nutshell | LetThereBeMath | - YouTube, accessed July 23, 2025, https://www.youtube.com/watch?v=MLuYzfFirAY
28. Newton's method for optimization | Computational Mathematics Class Notes - Fiveable, accessed July 23, 2025, https://library.fiveable.me/computational-mathematics/unit-5/newtons-method-optimization/study-guide/llakMCMdai6Sb26Q
29. 뉴턴 방법, 준 뉴턴 방법 - 추천시스템 맛집 - 티스토리, accessed July 23, 2025, [https://ekdud7667.tistory.com/entry/%EB%89%B4%ED%84%B4-%EB%B0%A9%EB%B2%95-%EC%A4%80-%EB%89%B4%ED%84%B4-%EB%B0%A9%EB%B2%95](https://ekdud7667.tistory.com/entry/뉴턴-방법-준-뉴턴-방법)
30. 뉴턴방법 (Newton's Method) - DeepCampus - 티스토리, accessed July 23, 2025, https://pasus.tistory.com/197
31. Newton's method in optimization - Wikipedia, accessed July 23, 2025, [https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization](https://en.wikipedia.org/wiki/Newton's_method_in_optimization)
32. 4. The Hessian Matrix, accessed July 23, 2025, http://norman3.github.io/prml/docs/chapter05/4.html
33. Mastering Newton's Method for Optimization - Number Analytics, accessed July 23, 2025, https://www.numberanalytics.com/blog/advanced-newtons-method-optimization

