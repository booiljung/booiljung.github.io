# 선형대수학 마스터를 위한 종합 로드맵
[선형대수](./index.md)

------


이 파트에서는 선형대수학의 근본적인 동기와 구성 요소를 확립합니다. 선형 연립방정식 풀이라는 구체적인 문제에서 시작하여, 이러한 문제를 효율적으로 표현하고 해결할 수 있게 해주는 객체인 벡터와 행렬을 소개합니다. 처음부터 대수적 유창함과 기하학적 직관을 함께 개발하는 데 중점을 둡니다.



선형대수학의 여정은 하나의 근본적인 문제, 즉 $m$개의 선형방정식과 $n$개의 미지수로 구성된 시스템을 푸는 것에서 시작합니다. 이 시스템은 대수적으로 다음과 같이 표현될 수 있습니다.
$$
\begin{align*}
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n &= b_1 \\
a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n &= b_2 \\
\vdots \qquad & \qquad \vdots \\
a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n &= b_m
\end{align*}
$$
이러한 시스템의 해집합(solution set)은 세 가지 가능성 중 하나를 가집니다: 유일한 해(unique solution)가 존재하거나, 무수히 많은 해(infinitely many solutions)가 존재하거나, 해가 존재하지 않는(no solution) 경우입니다. 기하학적으로 2차원 또는 3차원에서 각 방정식은 직선이나 평면을 나타내며, 해는 이들의 교점(또는 교선, 교면)에 해당합니다. 따라서 해가 하나인 경우는 한 점에서 만나는 것이고, 무수히 많은 해는 선이나 면에서 겹치는 것이며, 해가 없는 경우는 평행하여 만나지 않는 상황으로 시각화할 수 있습니다. 행렬식과 크래머 공식(Cramer's Rule)과 같은 후반부 개념들은 이러한 시스템을 체계적으로 분석하고 풀기 위한 필요성에서 자연스럽게 파생됩니다.1


복잡한 연립방정식을 풀기 위한 가장 기본적이고 강력한 알고리즘은 가우스 소거법(Gaussian Elimination)입니다. 이 방법의 목표는 주어진 시스템을 풀이가 더 쉬운 동등한 시스템으로 변환하는 것입니다. 구체적으로는 행렬을 **상삼각행렬(upper triangular form)** 형태로 만드는 것을 목표로 합니다.3 이 과정은 세 가지 **기본 행 연산(Elementary Row Operations, EROs)**을 통해 수행됩니다.4

1. **교환(Interchange/Permutation):** 두 행의 위치를 바꿉니다.
2. **스케일링(Scaling):** 한 행에 0이 아닌 상수를 곱합니다.
3. **대치(Replacement):** 한 행의 상수배를 다른 행에 더합니다.

이러한 연산들은 연립방정식의 해집합을 변경하지 않으면서 시스템을 단순화합니다.6 가우스 소거법을 통해 행렬을 **행 사다리꼴(row echelon form)**로 만들 수 있으며, 여기서 더 나아가 **가우스-조던 소거법(Gauss-Jordan Elimination)**을 적용하면 **기약 행 사다리꼴(reduced row echelon form)**을 얻게 됩니다. 기약 행 사다리꼴은 주어진 행렬에 대해 유일하게 결정되는 형태로, 해를 직접적으로 읽어낼 수 있게 해줍니다.7

이 과정에서 **피벗(pivot)**의 개념이 등장하는데, 이는 행 사다리꼴에서 각 행의 첫 번째 0이 아닌 원소를 의미합니다. 피벗이 있는 열에 해당하는 변수를 **피벗 변수(pivot variable)**, 그렇지 않은 변수를 **자유 변수(free variable)**라고 부릅니다.7 자유 변수의 존재는 시스템이 무수히 많은 해를 가짐을 의미하며, 해집합을 매개변수 방정식 형태로 표현하는 데 핵심적인 역할을 합니다.


연립방정식을 보다 효율적으로 다루기 위해 우리는 이를 **첨가 행렬(augmented matrix)**, 즉 $[A|b]$ 형태로 추상화합니다. 여기서 A는 계수 행렬(coefficient matrix)이고 b는 상수 벡터(constant vector)입니다.6 이 표현법은 방정식의 본질적인 정보만을 담고 있어 가우스 소거법을 적용하기 위한 완벽한 도구가 됩니다.

가우스 소거법은 단순히 계산 절차에 그치지 않고, 시스템의 근본적인 구조를 드러내는 강력한 분석 도구입니다. 소거 과정이 끝나고 남은 피벗의 개수는 행렬의 **계수(rank)**를 결정하며, 이는 시스템 내에 존재하는 "실질적으로 독립적인" 방정식의 수를 나타냅니다.10 계수는 시스템의 해가 존재하는지(일관성, consistency), 그리고 존재한다면 유일한지를 판별하는 기준이 됩니다.

더 깊이 들어가면, 각각의 기본 행 연산은 **기본 행렬(elementary matrix)** $E$를 왼쪽에 곱하는 것과 동일한 효과를 가집니다.11 예를 들어, 행렬 $A$에 기본 행 연산을 적용하여 행렬 $B$를 얻는 것은 $B=EA$라는 행렬 곱셈으로 표현할 수 있습니다. 따라서 가우스 소거법의 전체 과정, 즉 $A$를 상삼각행렬 $U$로 변환하는 과정은 일련의 기본 행렬들을 순차적으로 곱하는 것과 같습니다: $(E_k \cdots E_2 E_1)A = U$. 이 관계는 겉보기에는 평범한 방정식 풀이 과정이 실제로는 행렬의 선형 변환과 동일하다는 심오한 연결고리를 보여줍니다. 이는 나중에 배울 $A=LU$ 분해의 기초가 되며, 여기서 $L$은 소거 과정에 사용된 기본 행렬들의 곱의 역행렬입니다. 이처럼, 선형 연립방정식 풀이라는 구체적인 작업은 사실상 행렬 분해라는 선형대수학의 중심 주제와의 첫 만남인 셈입니다.



선형대수학의 가장 기본적인 구성 요소는 벡터입니다. 벡터는 두 가지 관점에서 이해할 수 있습니다.

- **대수적 정의:** 벡터는 숫자의 순서 있는 목록으로, 보통 세로로 나열된 **열벡터(column vector)** 또는 가로로 나열된 **행벡터(row vector)**로 표현됩니다.12 특별한 언급이 없는 한, 열벡터를 표준으로 간주합니다.13 예를 들어, $n$차원 벡터 $\vec{v}$는 다음과 같이 나타낼 수 있습니다.

$$
\vec{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}
$$

- **기하학적 정의:** $R^n$ 공간의 벡터는 $n$차원 공간의 한 점으로 해석되거나, 원점에서 그 점을 향하는 화살표로 시각화될 수 있습니다. 이 화살표는 크기(magnitude)와 방향(direction)을 모두 가집니다.12

벡터의 기본 연산은 덧셈과 스칼라 곱셈입니다. 벡터 덧셈은 같은 차원의 두 벡터에 대해 각 성분별로 더하는 것으로 정의되며, 기하학적으로는 한 벡터의 끝에 다른 벡터의 시작을 이어 붙이는 '머리-꼬리 연결(head-to-tail)'로 표현됩니다.12 스칼라 곱셈은 벡터의 각 성분에 특정 상수(스칼라)를 곱하는 것으로, 기하학적으로는 벡터의 길이를 스칼라배만큼 늘리거나 줄이고, 스칼라가 음수일 경우 방향을 반대로 바꾸는 것을 의미합니다.12


행렬은 숫자를 직사각형 배열로 나열한 것입니다. $m$개의 행과 $n$개의 열을 가진 행렬을 $m \times n$ 행렬이라고 부릅니다.13 행렬의 덧셈과 스칼라 곱셈은 벡터와 유사하게 성분별로 정의됩니다.14

행렬 연산 중 가장 중요하고 독특한 것은 **행렬 곱셈** $AB=C$입니다. 이 연산은 다음과 같은 규칙을 따릅니다.

- **조건:** 곱셈이 정의되려면 앞 행렬 $A$의 열의 개수와 뒤 행렬 $B$의 행의 개수가 일치해야 합니다. 즉, $A$가 $m \times n$ 행렬이라면 $B$는 $n \times k$ 형태여야 하며, 결과 행렬 $C$는 $m \times k$ 크기를 갖습니다.15
- **계산:** 결과 행렬 $C$의 $(i, j)$ 성분, 즉 $C_{ij}$는 $A$의 $i$번째 행벡터와 $B$의 $j$번째 열벡터의 **내적(inner product)**으로 계산됩니다.17

$$
C_{ij} = \sum_{k=1}^{n} A_{ik}B_{kj}
$$

- **성질:** 행렬 곱셈은 **결합법칙**($(AB)C = A(BC)$)과 **분배법칙**($A(B+C) = AB + AC$)을 만족합니다. 그러나 가장 중요한 특징은 실수의 곱셈과 달리 **교환법칙이 성립하지 않는다**는 것입니다. 즉, 일반적으로 $AB \neq BA$입니다.18

| 표 1: 행렬 연산의 주요 성질 |                                                |
| --------------------------- | ---------------------------------------------- |
| **성질**                    | **예시**                                       |
| 덧셈의 교환법칙             | $A + B = B + A$                              |
| 덧셈의 결합법칙             | $(A + B) + C = A + (B + C)$                  |
| 곱셈의 결합법칙             | $(AB)C = A(BC)$                              |
| 분배법칙                    | $A(B + C) = AB + AC$, $(B + C)A = BA + CA$ |
| **곱셈의 교환법칙**         | **성립하지 않음 ($AB \neq BA$)**             |
| 항등원의 존재               | $A + O = A$, $AI = IA = A$                 |
| 전치 행렬의 곱              | $(AB)^T = B^T A^T$                           |

이 표는 행렬 대수의 기본 규칙을 요약하며, 특히 스칼라 대수와 다른 점, 즉 곱셈의 비가환성을 강조하여 학습자가 흔히 저지르는 실수를 방지하고 핵심 개념을 강화하는 데 도움을 줍니다.


행렬과 벡터의 곱 $Ax=b$는 선형대수학의 핵심 개념으로, 두 가지 근본적인 방식으로 해석될 수 있습니다.

1. **행 관점 (Row Picture):** 결과 벡터 $b$의 각 성분은 행렬 $A$의 해당 행벡터와 벡터 $x$의 내적입니다. 이 관점은 $Ax=b$를 연립방정식의 집합으로 보는 것과 직접적으로 연결됩니다.
2. **열 관점 (Column Picture):** 결과 벡터 $b$는 행렬 $A$의 열벡터들의 **선형 결합(linear combination)**입니다. 이때 벡터 $x$의 성분들이 각 열벡터에 곱해지는 가중치(weights) 역할을 합니다. 즉, $b = x_1(\text{col}_1) + x_2(\text{col}_2) + \dots + x_n(\text{col}_n)$ 입니다. 이 해석은 벡터 공간을 이해하는 데 훨씬 더 강력하고 중요한 관점을 제공합니다.13

기하학적으로, 행렬 $A$는 입력 벡터 $x$를 출력 벡터 $b=Ax$로 **변환(transformation)**하는 연산자 또는 함수로 볼 수 있습니다.13 이는 행렬이 단순히 숫자의 배열이 아니라, 공간 자체를 움직이고 변형시키는 동적인 객체임을 시사하며, 

**선형 변환**이라는 개념의 첫걸음이 됩니다.21


몇 가지 중요한 특수 행렬들이 있습니다.

- **단위 행렬(Identity Matrix, $I$):** 대각선 성분이 모두 1이고 나머지 성분은 0인 정사각행렬입니다. 곱셈의 항등원 역할을 하여 $AI = IA = A$를 만족합니다.19
- **영 행렬(Zero Matrix, $O$):** 모든 성분이 0인 행렬로, 덧셈의 항등원입니다.19
- **역행렬(Inverse Matrix, $A^{-1}$):** $AA^{-1} = A^{-1}A = I$를 만족하는 행렬입니다. 모든 정사각행렬이 역행렬을 갖는 것은 아니며, 역행렬이 존재하는 행렬을 **가역(invertible)** 또는 **비특이(non-singular)** 행렬이라고 부릅니다.14
- **전치 행렬(Transpose, $A^T$):** 행과 열을 맞바꾼 행렬입니다. 전치 연산의 가장 중요한 성질은 곱셈에 대한 규칙으로, $(AB)^T = B^T A^T$와 같이 순서가 바뀐다는 점입니다.17

행렬 곱셈의 비가환성($AB \neq BA$)은 단순한 대수적 불편함이 아니라, 기하학적 변환의 순서가 중요하다는 근본적인 진리를 반영합니다. 행렬을 공간을 변형시키는 선형 변환으로 이해하면 18, 행렬 곱 

$AB$는 변환 $B$를 먼저 적용하고 그 결과에 변환 $A$를 적용하는 것(합성 변환)에 해당합니다. 반면 $BA$는 변환 $A$를 먼저 적용하고 변환 $B$를 적용하는 것입니다. 예를 들어, 90도 회전 변환과 x축 방향으로의 밀림 변환(shear)을 생각해보면, 어떤 순서로 적용하느냐에 따라 최종 결과 벡터의 위치가 완전히 달라집니다. 따라서 $AB \neq BA$라는 대수적 성질은 변환의 순서가 결과에 영향을 미친다는 기하학적 현실의 직접적인 표현입니다. 이 이해는 단순한 규칙 암기를 넘어, 대수와 기하학 사이의 깊은 연관성을 깨닫게 합니다.

------


이 파트에서는 $R^n$과 행렬이라는 구체적인 세계에서 벗어나 벡터 공간이라는 추상적인 틀로 나아갑니다. 이 추상화는 선형대수학의 심장부로서, 함수나 다항식과 같은 다양한 수학적 객체에 동일한 원리를 적용할 수 있게 해줍니다. 이후 행렬을 해부하여 네 개의 기본 부분 공간을 발견하고, 이를 통해 행렬이 변환으로서 어떻게 작동하는지에 대한 완전한 그림을 제공합니다.



**벡터 공간(vector space)**은 스칼라 체(field) $F$(일반적으로 실수 집합 $R$) 위의 객체(벡터라고 불림)들의 집합 $V$로, 이 집합에는 벡터 덧셈과 스칼라 곱셈이라는 두 가지 연산이 정의되어 있습니다.23 이 연산들은 다음의 

**8가지 공리(axioms)**(보통 10가지로 세분화됨)를 만족해야 합니다 23:

1. **덧셈에 대한 닫힘:** $u, v \in V$이면 $u+v \in V$이다.
2. **덧셈의 교환법칙:** $u+v = v+u$
3. **덧셈의 결합법칙:** $(u+v)+w = u+(v+w)$
4. **영벡터의 존재:** 모든 $u \in V$에 대해 $u+0 = u$를 만족하는 영벡터 $0$이 $V$에 존재한다.
5. **덧셈의 역원 존재:** 각 $u \in V$에 대해 $u+(-u) = 0$을 만족하는 역원 $-u$가 $V$에 존재한다.
6. **스칼라 곱셈에 대한 닫힘:** $c \in F$이고 $u \in V$이면 $cu \in V$이다.
7. **분배법칙 1:** $c(u+v) = cu + cv$
8. **분배법칙 2:** $(c+d)u = cu + du$
9. **결합법칙(스칼라 곱):** $c(du) = (cd)u$
10. **항등원(스칼라 곱):** $1u = u$

이러한 추상화의 힘은 다양한 수학적 대상을 하나의 틀 안에서 다룰 수 있다는 데 있습니다. 예를 들어, 우리가 익히 아는 유클리드 공간 $R^n$, $m \times n$ 행렬들의 집합 $M_{mn}$, 그리고 차수가 $n$ 이하인 모든 다항식의 집합 $P_n$은 모두 이 공리들을 만족하는 벡터 공간입니다.23 이는 행렬이나 다항식도 '벡터'로 간주될 수 있음을 의미하며, 벡터 공간의 이론을 이들에게도 적용할 수 있게 합니다.


벡터 공간 $V$의 **부분 공간(subspace)** $H$는 $V$의 부분집합이면서 그 자체로 벡터 공간을 이루는 집합입니다.28 어떤 집합 

$H$가 $V$의 부분 공간인지 확인하기 위해 8가지 공리를 모두 점검할 필요는 없습니다. 나머지 공리들은 상위 공간 $V$로부터 상속받기 때문에, 다음 세 가지 조건만 확인하면 충분합니다 (부분 공간 판정법) 28:

1. **영벡터의 존재:** $V$의 영벡터가 $H$에 포함되어야 한다.
2. **덧셈에 대한 닫힘:** $H$에 속한 임의의 두 벡터 $u, v$에 대해, 그 합 $u+v$도 $H$에 포함되어야 한다.
3. **스칼라 곱셈에 대한 닫힘:** $H$에 속한 임의의 벡터 $u$와 임의의 스칼라 $c$에 대해, 그 곱 $cu$도 $H$에 포함되어야 한다.

예를 들어, $R^3$에서 원점을 지나는 평면은 이 세 조건을 모두 만족하므로 $R^3$의 부분 공간입니다. 하지만 원점을 지나지 않는 평면은 영벡터를 포함하지 않으므로 부분 공간이 될 수 없습니다.29


모든 $m \times n$ 행렬 $A$에 대해, 우리는 네 개의 매우 중요한 부분 공간을 정의할 수 있습니다. 이들은 행렬 $A$가 나타내는 변환의 모든 측면을 설명합니다.32

1. **열 공간(Column Space), $C(A)$:** 행렬 $A$의 열벡터들의 모든 가능한 선형 결합으로 이루어진 집합입니다. 이는 변환의 '치역(range)'으로, $Ax=b$라는 방정식이 해를 갖기 위해 벡터 $b$가 존재해야 하는 공간입니다. $C(A)$는 출력 공간인 $R^m$의 부분 공간입니다.32
2. **영 공간(Null Space), $N(A)$:** $Ax=0$을 만족하는 모든 벡터 $x$들의 집합입니다. 이는 변환 $A$에 의해 원점으로 '붕괴되는' 입력 벡터들의 공간을 나타냅니다. $N(A)$는 입력 공간인 $R^n$의 부분 공간입니다.32
3. **행 공간(Row Space), $C(A^T)$:** 행렬 $A$의 행벡터들의 모든 선형 결합으로 이루어진 집합입니다. (이는 $A^T$의 열 공간과 같습니다.) $C(A^T)$는 입력 공간인 $R^n$의 부분 공간입니다.
4. **좌측 영 공간(Left Null Space), $N(A^T)$:** $A^Ty=0$ (또는 $y^TA=0$)을 만족하는 모든 벡터 $y$들의 집합입니다. $N(A^T)$는 출력 공간인 $R^m$의 부분 공간입니다.

이 네 부분 공간은 단순히 네 개의 개별적인 정의가 아닙니다. 이들은 입력 공간 $R^n$과 출력 공간 $R^m$을 완벽하게 분할하는 두 쌍의 직교 공간을 형성합니다. $Ax=0$이라는 영 공간의 정의를 살펴보면, 벡터 $x$가 $N(A)$에 속한다는 것은 $x$가 $A$의 *모든* 행벡터와 직교한다는 것을 의미합니다 (각 행과 $x$의 내적이 0이므로). 만약 $x$가 모든 행벡터와 직교한다면, 그들의 모든 선형 결합과도 직교해야 합니다. 행벡터들의 모든 선형 결합의 집합은 정의상 행 공간 $C(A^T)$입니다. 따라서, 영 공간 $N(A)$는 행 공간 $C(A^T)$ 전체에 직교하는 모든 벡터들로 구성됩니다. 이것이 바로 **직교 여공간(orthogonal complement)**의 정의입니다. 이 놀라운 관계, 즉 $C(A^T) \perp N(A)$는 **선형대수학의 기본 정리(Fundamental Theorem of Linear Algebra)**의 첫 번째 부분입니다. 이는 입력 공간 $R^n$의 두 기본 부분 공간이 기하학적으로 어떻게 완벽하게 분리되는지를 보여줍니다. 비슷한 논리로 출력 공간 $R^m$에서도 $C(A) \perp N(A^T)$가 성립합니다. 이로써 네 부분 공간은 단순한 목록이 아니라, 아름다운 대칭성을 가진 상호 연결된 구조임이 드러납니다.



벡터들의 집합 $\{v_1, \dots, v_p\}$가 **선형 독립(linearly independent)**이라는 것은, 벡터 방정식 $c_1v_1 + \dots + c_p v_p = 0$을 만족하는 유일한 해가 모든 계수가 0인 **자명한 해(trivial solution)**, 즉 $c_1 = \dots = c_p = 0$뿐이라는 의미입니다.34

만약 0이 아닌 계수들의 조합으로 영벡터를 만들 수 있다면, 그 벡터 집합은 **선형 종속(linearly dependent)**이라고 합니다. 이는 집합 내의 적어도 하나의 벡터가 다른 벡터들의 선형 결합으로 표현될 수 있음을 의미하며, 즉 그 벡터는 정보의 관점에서 '중복'됩니다.33

행렬의 열벡터들이 선형 독립일 필요충분조건은 그 행렬의 영 공간 $N(A)$가 오직 영벡터만을 포함하는 것입니다.33


벡터 집합 $\{v_1, \dots, v_p\}$의 **생성(span)**은 이 벡터들의 모든 가능한 선형 결합으로 만들어지는 벡터들의 집합입니다. 어떤 벡터들의 생성은 항상 부분 공간을 이룹니다.30 우리는 한 벡터 집합이 $R^3$과 같은 전체 공간을 생성하는지, 아니면 그 안의 평면이나 직선과 같은 부분 공간만을 생성하는지에 관심을 가집니다.


벡터 공간 $V$의 **기저(basis)**는 다음 두 조건을 모두 만족하는 벡터들의 집합입니다 30:

1. 선형 독립이다.
2. 벡터 공간 $V$ 전체를 생성한다.

기저는 "최소 생성 집합(minimal spanning set)" 또는 "최대 독립 집합(maximal independent set)"으로 생각할 수 있습니다. 즉, 기저는 중복 없이 전체 공간을 설명하는 데 필요한 정확한 양의 정보를 담고 있습니다.30 예를 들어, $R^n$ 공간의 **표준 기저(standard basis)**는 $n \times n$ 단위행렬의 열벡터들로 구성되며, 흔히 $e_1, e_2, \dots, e_n$으로 표기합니다.33


벡터 공간 $V$의 **차원(dimension)**, 즉 $\dim(V)$는 그 공간의 기저를 이루는 벡터의 개수입니다. 이 개수는 주어진 공간에 대해 항상 유일합니다.13

- 행렬 $A$의 열 공간의 차원 $\dim(C(A))$는 행렬의 **계수(rank)**와 같습니다. 이는 행 사다리꼴의 피벗 개수와 동일합니다.
- 행렬 $A$의 영 공간의 차원 $\dim(N(A))$는 **퇴화차수(nullity)**라고 합니다.

이 두 차원은 **계수-퇴화차수 정리(Rank-Nullity Theorem)**라는 중요한 관계로 연결됩니다. $m \times n$ 행렬 $A$에 대해 다음이 성립합니다:
$$
\dim(C(A)) + \dim(N(A)) = n
$$
즉, (계수) + (퇴화차수) = (열의 개수) 입니다. 이 정리는 선형대수학의 핵심적인 정리 중 하나입니다.

기저의 개념은 모든 추상적인 벡터 공간에 대한 좌표계를 제공한다는 점에서 매우 강력합니다. 일단 기저가 선택되면, 모든 추상적인 벡터는 $R^n$의 구체적인 좌표 벡터로 유일하게 표현될 수 있습니다. 예를 들어, 2차 다항식 공간 $P_2$를 생각해 봅시다.23 이 공간의 원소인 $p(t) = a + bt + ct^2$는 추상적인 객체입니다. 이 공간의 표준 기저로 $\mathcal{B} = \{1, t, t^2\}$를 선택하면, 모든 다항식은 이 기저 벡터들의 유일한 선형 결합으로 표현됩니다. 이 선형 결합의 계수 $(a, b, c)는 해당 다항식의 좌표 벡터가 됩니다. 이를 통해 우리는 추상적인 공간 P2와 구체적인 공간 R3사이에 일대일 대응(동형사상, isomorphism)을 만들 수 있습니다:a+bt+ct2↔[a,b,c]T. 이것은 우리가 Rn에 대해 개발한 모든 도구(행렬, 가우스 소거법 등)를 *모든* 유한 차원 벡터 공간의 문제를 푸는 데 사용할 수 있음을 의미합니다. 단지 문제를 먼저 좌표로 변환하기만 하면 됩니다. 이처럼 기저는 추상적인 세계와 구체적인 세계를 잇는 다리 역할을 합니다.39

------


이 파트에서는 내적(inner product)을 통해 벡터 공간에 길이, 거리, 각도와 같은 기하학적 구조를 도입합니다. 이를 통해 수직성(직교성, orthogonality)이라는 개념을 공식화할 수 있으며, 이는 해가 없는 시스템에 대한 '최상의' 근사해를 찾는 것부터 매우 효율적인 기저를 구성하는 것에 이르기까지 심오한 이론적, 실제적 결과를 낳습니다.



$R^n$ 공간의 두 벡터 $u$와 $v$에 대한 **내적(inner product)** 또는 **점곱(dot product)**은 $u \cdot v = u^T v = u_1v_1 + \dots + u_nv_n$으로 정의됩니다.15

이 개념은 추상적인 **내적 공간(inner product space)**으로 일반화될 수 있습니다. 내적 공간은 선형성, 대칭성, 양의 정부호성(positive-definiteness)이라는 공리를 만족하는 내적이 부여된 벡터 공간입니다.41 이를 통해 $R^n$이 아닌 함수 공간 등에서도 기하학적 개념을 다룰 수 있습니다.


내적을 사용하면 다음과 같은 기하학적 속성들을 정의할 수 있습니다.

- **길이(Length) 또는 노름(Norm):** 벡터 $v$의 길이는 $\|v\| = \sqrt{v \cdot v}$로 정의됩니다.15 가장 흔히 사용되는 L2 노름 외에도, 각 성분의 절댓값의 합인 L1 노름, 성분 중 최댓값인 L-무한대 노름 등이 있습니다.15
- **거리(Distance):** 두 벡터 $u$와 $v$ 사이의 거리는 $\|u - v\|로 계산됩니다.
- **직교성(Orthogonality):** 두 벡터 $u$와 $v$는 만약 그들의 내적이 0이면, 즉 $u \cdot v = 0$이면 **직교(orthogonal)**한다고 말합니다.45 이는 기하학적으로 두 벡터가 서로 수직임을 의미합니다.
- **각도(Angle):** 두 벡터 사이의 각도 $\theta$는 다음 공식을 통해 정의됩니다: $u \cdot v = \|u\| \|v\| \cos(\theta)$.15


한 벡터를 다른 벡터 위로 '그림자'를 내리는 것을 **직교 사영(orthogonal projection)**이라고 합니다. 벡터 $y$를 벡터 $u$ 위로 사영시킨 결과는 다음과 같이 계산됩니다:
$$
\text{proj}_u(y) = \frac{y \cdot u}{u \cdot u} u
$$
이 벡터는 $y$의 성분 중 $u$ 방향으로 놓인 부분입니다.15 그러면 $y$에서 이 사영 벡터를 뺀 $z = y - \text{proj}_u(y)$는 $u$에 직교하는 성분이 됩니다. 이를 통해 임의의 벡터 $y$를 $u$에 평행한 성분과 수직인 성분으로 유일하게 분해할 수 있습니다.


선형 연립방정식 $Ax=b$에 해가 없는 경우는 언제일까요? 이는 벡터 $b$가 $A$의 열 공간 $C(A)$에 포함되지 않을 때 발생합니다. 이런 경우, 우리는 정확한 해 대신 '가장 좋은' 근사해를 찾고자 합니다.

**최소제곱해(least-squares solution)** $\hat{x}$는 오차 벡터 $b - Ax$의 길이, 즉 $\|b - Ax\|를 최소로 만드는 벡터 $x$입니다.

기하학적으로 생각하면, 오차 $\|b - A\hat{x}\|가 최소가 되는 지점은 벡터 $b$에서 열 공간 $C(A)$ 위로 내린 직교 사영점 $p = A\hat{x}$일 때입니다. 이 경우, 오차 벡터 $b - A\hat{x}$는 열 공간 $C(A)$ 전체에 대해 직교해야 합니다.

이 기하학적 조건은 **정규 방정식(normal equations)**이라는 대수적 해법으로 이어집니다:
$$
A^T A \hat{x} = A^T b
$$
이 방정식을 풀면 우리가 원하는 최소제곱해 $\hat{x}$를 얻을 수 있습니다.48 이 방법은 통계학의 회귀 분석과 데이터 피팅의 이론적 기반이 되는 매우 중요한 응용입니다.

정규 방정식은 단순한 대수적 트릭이 아니라 직교 사영이라는 기하학적 원리를 그대로 수식으로 옮긴 것입니다. 가장 가까운 해를 찾는다는 최적화 문제는 $b$를 $C(A)$에 사영시키는 것과 같습니다. 이는 오차 벡터 $e = b - A\hat{x}$가 $C(A)$에 수직이어야 함을 의미합니다. 즉, $e$는 $C(A)$를 생성하는 모든 열벡터 $a_i$와 수직이어야 하므로, 모든 $i$에 대해 $a_i^T (b - A\hat{x}) = 0$이 성립해야 합니다. 이 개별적인 방정식들을 하나의 행렬 방정식으로 묶으면 $A^T (b - A\hat{x}) = 0$이 되고, 이를 정리하면 바로 정규 방정식 $A^T A \hat{x} = A^T b$가 유도됩니다. 이처럼, 최적의 근사해를 찾는 문제는 직교성이라는 순수한 기하학적 원리에 의해 해결됩니다.



- **직교 집합(Orthogonal Set):** 집합 내의 모든 벡터 쌍이 서로 직교하는 벡터들의 집합입니다.49
- **정규 직교 집합(Orthonormal Set):** 직교 집합이면서 각 벡터의 길이가 1인(즉, 단위 벡터인) 집합입니다.45

정규 직교 기저는 엄청난 장점을 가집니다. 만약 $\{u_1, \dots, u_p\}$가 어떤 공간의 정규 직교 기저라면, 그 공간에 속한 임의의 벡터 $y$의 좌표는 매우 쉽게 구할 수 있습니다. 가우스 소거법 같은 복잡한 계산 없이, 각 기저 벡터와의 내적만으로 좌표를 즉시 알 수 있습니다:
$$
y = (y \cdot u_1)u_1 + (y \cdot u_2)u_2 + \dots + (y \cdot u_p)u_p
$$


**그람-슈미트 과정(Gram-Schmidt Process)**은 임의의 기저 $\{x_1, \dots, x_p\}$를 입력받아, 동일한 공간을 생성하는 직교 기저 $\{v_1, \dots, v_p\}$를 만들어내는 알고리즘입니다.45

이 과정은 반복적으로 진행됩니다. 첫 번째 직교 기저 벡터 $v_1$은 원래의 $x_1$과 같게 둡니다. 그 다음 벡터 $v_2$는 $x_2$에서 $v_1$ 위로의 사영을 빼서 구합니다. $v_3$는 $x_3$에서 이미 구한 $v_1$과 $v_2$ 위로의 사영들을 모두 빼서 구합니다. 이 과정을 계속 반복하여 직교 기저를 완성합니다.49

- $v_1 = x_1$
- $v_2 = x_2 - \text{proj}_{v_1}(x_2)$
- $v_3 = x_3 - \text{proj}_{v_1}(x_3) - \text{proj}_{v_2}(x_3)$
- ...

이렇게 얻은 직교 기저의 각 벡터 $v_i$를 자신의 길이 $\|v_i\|$로 나누어 정규화하면 정규 직교 기저를 얻을 수 있습니다.49


행렬 $A$의 열들이 선형 독립일 때, 그람-슈미트 과정은 $A = QR$라는 행렬 분해로 표현될 수 있습니다.

- $Q$: $m \times n$ 크기의 행렬로, 그 열들은 $A$의 열 공간 $C(A)$에 대한 정규 직교 기저를 이룹니다. 이 때문에 $Q$는 $Q^T Q = I$라는 중요한 성질을 만족하는 **직교 행렬(orthogonal matrix)**입니다.
- $R$: $n \times n$ 크기의 상삼각행렬이며, 가역(invertible)입니다.

QR 분해는 최소제곱 문제를 매우 효율적으로 만듭니다.

정규 방정식 $A^T A x = A^T b$에 $A=QR$을 대입하면 $(QR)^T QR x = (QR)^T b$가 되고, 이를 정리하면 $R^T Q^T Q R x = R^T Q^T b$가 됩니다. $Q^T Q = I$이므로, $R^T R x = R^T Q^T b$가 되고, $R^T$가 가역이므로 최종적으로 $Rx = Q^T b$라는 매우 간단한 식으로 축약됩니다. $R$이 상삼각행렬이므로, 이 시스템은 **후방 대입법(back substitution)**을 통해 쉽게 풀 수 있습니다.53

그람-슈미트 과정과 QR 분해는 어떤 공간이든 항상 직교적인 관점에서 설명될 수 있음을 보여줍니다. 이는 마치 우리가 관점을 회전시켜 어떤 공간의 기본 축들이 서로 수직이 되도록 만들 수 있다는 것과 같습니다. 그람-슈미트 과정 50은 임의의 '삐뚤어진' 기저를 '바로 펴서' 직교 기저로 만드는 건설적인 절차입니다. QR 분해는 이 과정을 행렬로 공식화한 것입니다. 행렬 $A$는 원래의 삐뚤어진 기저 벡터들을 열로 갖고, 행렬 $Q$는 새롭게 바로 편 정규 직교 기저 벡터들을 열로 갖습니다. 행렬 $R$은 원래의 기저($A$)가 새로운 정규 직교 기저($Q$)로부터 어떻게 구성되는지에 대한 '변환 정보'를 담고 있는 '기저 변환 행렬' 역할을 합니다. 이 직교적 관점은 계산적으로 안정적이고 개념적으로 단순하기 때문에 이론과 응용 모두에서 매우 중요합니다.

------


이 파트에서는 정사각행렬이 나타내는 선형 변환의 핵심적인 속성을 깊이 파고듭니다. 변환의 '영혼'이라 할 수 있는 불변 방향과 스케일링 인자, 즉 고유값과 고유벡터를 발견하게 됩니다. 이 부분은 개념적으로 가장 풍부하며, 입문 선형대수학에서 가장 널리 응용되는 내용입니다.



**행렬식(determinant)**은 정사각행렬에 부여되는 고유한 스칼라 값으로, $\det(A)$ 또는 $|A|$로 표기합니다.2 행렬식은 행렬이 가진 여러 중요한 성질을 요약해주는 값입니다.

행렬식의 계산 방법은 행렬의 크기에 따라 다릅니다.

- **2x2 행렬:** 행렬 $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$의 행렬식은 $\det(A) = ad - bc$ 입니다.54
- **3x3 행렬:** **사뤼스 법칙(Sarrus's rule)**을 사용하여 간단히 계산할 수 있습니다.2
- **일반적인 n x n 행렬:** **여인수 전개(cofactor expansion)** 방법을 사용합니다. 임의의 한 행 또는 한 열을 선택하여 그 행(또는 열)의 각 성분과 그에 해당하는 여인수(cofactor)의 곱을 모두 더하여 계산합니다.2 여인수는 부호를 가진 소행렬식(minor)으로, 특정 원소가 속한 행과 열을 제외한 나머지 부분 행렬의 행렬식입니다.

삼각행렬(triangular matrix)의 행렬식은 주대각선 성분들의 곱과 같다는 유용한 성질이 있습니다.4


행렬식의 가장 중요한 직관적 의미는 기하학적 변환에서의 부피 변화율입니다. 행렬 $A$에 의한 선형 변환이 주어졌을 때, $\det(A)$의 절댓값은 해당 변환이 2차원에서는 넓이, 3차원에서는 부피, 그리고 $n$차원에서는 초부피(hypervolume)를 몇 배로 확대 또는 축소하는지를 나타냅니다.1

또한, 행렬식의 부호는 변환이 공간의 **방향성(orientation)**을 유지하는지 아니면 뒤집는지를 알려줍니다. $\det(A) > 0$이면 방향성이 보존되고, $\det(A) < 0$이면 거울상 반전(reflection)처럼 방향성이 뒤집힙니다.55


행렬식은 여러 중요한 대수적 성질을 가집니다.

- **가역성 판별:** $\det(A) = 0$일 필요충분조건은 행렬 $A$가 **특이(singular)** 행렬, 즉 역행렬이 존재하지 않는다는 것입니다. 이는 행렬식의 가장 핵심적인 성질로, 변환 $A$가 공간을 더 낮은 차원으로 '납작하게' 만든다는 것을 의미합니다.54
- **곱셈 법칙:** $\det(AB) = \det(A)\det(B)$
- **전치 불변성:** $\det(A^T) = \det(A)$
- **역행렬:** $\det(A^{-1}) = 1/\det(A)$
- **기본 행 연산의 영향:** 행 교환은 행렬식의 부호를 바꾸고, 한 행의 상수배는 행렬식도 그 상수배만큼 바꾸며, 한 행의 상수배를 다른 행에 더하는 대치 연산은 행렬식의 값을 바꾸지 않습니다.4

$\det(AB) = \det(A)\det(B)$라는 곱셈 법칙은 단순한 대수적 공식이 아니라, 행렬식의 기하학적 의미에서 비롯된 필연적인 결과입니다. 변환 $A$와 $B$의 합성 변환은 행렬 곱 $AB$로 표현됩니다. 변환 $B$가 부피를 $\det(B)$배만큼 변화시키고, 그 결과에 변환 $A$가 다시 부피를 $\det(A)$배만큼 변화시킨다면, 전체 합성 변환에 의한 총 부피 변화율은 각 변화율의 곱인 $\det(A)\det(B)$가 될 것입니다. 이 총 변화율은 합성 변환 행렬 $AB$의 행렬식과 같아야 하므로, $\det(AB) = \det(A)\det(B)$라는 관계가 성립합니다. 이는 대수와 기하가 어떻게 깊이 연결되어 있는지를 보여주는 아름다운 예시입니다.



$n \times n$ 정사각행렬 $A$에 대해, **고유벡터(eigenvector)** $x$는 0이 아닌 벡터로서, 행렬 $A$에 의해 변환되었을 때 자기 자신의 스칼라배가 되는 벡터입니다.57


$$Ax = \lambda x$$


이때 스칼라 $\lambda$는 해당 고유벡터 $x$에 대응하는 **고유값(eigenvalue)**입니다. $\lambda$는 고유벡터가 변환에 의해 얼마나 늘어나거나 줄어드는지를 나타내는 스케일링 인자입니다.60

기하학적으로 고유벡터는 선형 변환 $A$에 의해 그 **방향이 변하지 않는** 벡터들을 의미합니다. 이 벡터들은 변환의 '축(axes)' 역할을 하며, 변환의 가장 본질적인 특성을 드러냅니다.58 예를 들어, 3차원 공간에서의 회전 변환이 있다면, 그 회전축 상에 있는 벡터들은 방향이 변하지 않으므로 고유벡터가 되고, 크기도 변하지 않으므로 고유값은 1이 됩니다.57

##### **8.2. 고유값과 고유벡터 찾기**

고유값-고유벡터 방정식 $Ax = \lambda x$는 대수적으로 조작하여 해를 찾을 수 있습니다.

$$Ax - \lambda x = 0 \implies Ax - \lambda I x = 0 \implies (A - \lambda I)x = 0$$

여기서 $I$는 단위행렬입니다.63 이 방정식이 0이 아닌 해 $x$를 가지려면, 행렬 $(A - \lambda I)$가 반드시 특이(singular) 행렬이어야 합니다. 즉, 역행렬이 존재해서는 안 됩니다.

따라서, 행렬 $(A - \lambda I)$의 행렬식은 0이 되어야 합니다.


$$\det(A - \lambda I) = 0$$


이 방정식을 행렬 $A$의 **특성 방정식(characteristic equation)**이라고 부릅니다.57 이 방정식은 

$\lambda$에 대한 다항식이므로, 이 방정식을 풀면 행렬 $A$의 고유값 $\lambda$들을 모두 구할 수 있습니다.

각각의 고유값 $\lambda$에 대해, 해당하는 고유벡터들은 동차 연립방정식 $(A - \lambda I)x = 0$을 풀어 찾습니다. 이 방정식의 해집합, 즉 행렬 $(A - \lambda I)$의 **영 공간(null space)**이 바로 고유값 $\lambda$에 대응하는 **고유 공간(eigenspace)**입니다.66 고유 공간은 해당 고유값에 대응하는 모든 고유벡터들과 영벡터로 이루어진 부분 공간입니다.

##### **8.3. 고유값의 성질**

고유값들은 행렬의 다른 속성들과 밀접하게 연관되어 있습니다.

- 행렬의 모든 고유값들의 합은 행렬의 **대각합(trace)**, 즉 주대각선 성분들의 합과 같습니다.
- 행렬의 모든 고유값들의 곱은 행렬의 **행렬식(determinant)**과 같습니다.
- 삼각행렬의 고유값들은 그 행렬의 주대각선 성분들 자체입니다.67

고유값, 영 공간, 가역성, 행렬식이라는 네 가지 핵심 개념은 $\lambda=0$이라는 특별한 경우를 통해 우아하게 연결됩니다. $\lambda=0$이 고유값이라는 것은, 정의에 따라 $Ax = 0x = 0$을 만족하는 0이 아닌 벡터 $x$가 존재한다는 의미입니다. 이는 곧 행렬 $A$의 영 공간 $N(A)$가 자명하지 않음(영벡터 외의 원소를 가짐)을 뜻합니다. 행렬의 영 공간이 자명하지 않을 필요충분조건은 그 행렬이 특이(singular) 행렬, 즉 비가역적이라는 것입니다. 그리고 행렬이 특이 행렬일 필요충분조건은 그 행렬식이 0이라는 것입니다. 따라서, '$\lambda=0$이 고유값이다'라는 말은 'A가 비가역적이다', '$\det(A)=0$이다', '$N(A)$가 자명하지 않다'는 말들과 모두 동치입니다. 이 연결고리는 선형대수학의 여러 개념들이 어떻게 하나의 통합된 구조를 이루는지를 명확히 보여줍니다.

#### **Section 9: 대각화와 그 힘**

##### **9.1. 대각화 정리**

$n \times n$ 행렬 $A$가 **대각화 가능(diagonalizable)**하다는 것은, 이 행렬이 $A = PDP^{-1}$ 형태로 인수분해될 수 있음을 의미합니다.58 여기서 각 행렬은 다음과 같습니다.

- $D$: $A$의 고유값들을 주대각선 성분으로 갖는 **대각행렬(diagonal matrix)**.
- $P$: $A$의 고유벡터들을 열벡터로 갖는 **가역 행렬(invertible matrix)**.

한 행렬이 대각화 가능할 필요충분조건은 그 행렬이 $n$개의 선형 독립인 고유벡터를 갖는 것입니다. 만약 $n \times n$ 행렬이 $n$개의 서로 다른 고유값을 갖는다면, 이는 항상 보장됩니다.

##### **9.2. 대각화의 힘: $A^k$ 계산**

행렬의 거듭제곱 $A^k$를 직접 계산하는 것은 $k$가 클 경우 매우 많은 연산을 필요로 합니다. 하지만 $A$가 대각화 가능하다면 이 계산은 극적으로 단순해집니다.
$$
A^k = (PDP^{-1})^k = (PDP^{-1})(PDP^{-1})\cdots(PDP^{-1}) = PD^kP^{-1}
$$
중간의 $P^{-1}P$ 항들이 모두 단위행렬 $I$로 상쇄되기 때문입니다. 대각행렬의 거듭제곱 $D^k$는 각 대각 성분(고유값)을 $k$ 제곱하는 것만으로 간단히 계산할 수 있습니다.59 이 방법은 마르코프 연쇄(Markov chains)와 같이 시간이 지남에 따라 변화하는 시스템을 분석하는 데 매우 강력한 도구를 제공합니다.

##### **9.3. 대칭 행렬의 대각화**

**대칭 행렬(symmetric matrix)**, 즉 $A = A^T$를 만족하는 행렬은 대각화와 관련하여 특별하고 강력한 성질을 가집니다.

1. 모든 고유값이 실수입니다.
2. 서로 다른 고유값에 대응하는 고유벡터들은 서로 **직교(orthogonal)**합니다.

**스펙트럼 정리(Spectral Theorem)**에 따르면, 모든 대칭 행렬은 $A = PDP^T$ 형태로 대각화될 수 있습니다. 여기서 $P$는 **직교 행렬(orthogonal matrix)**, 즉 $P^{-1} = P^T$를 만족하며, 그 열들은 정규 직교하는 고유벡터들로 이루어져 있습니다.68 이는 일반적인 대각화보다 훨씬 더 안정적이고 기하학적으로 의미 있는 형태입니다.

대각화는 본질적으로 '기저 변환(change of basis)' 연산입니다. $A = PDP^{-1}$라는 식은 $AP = PD$로 다시 쓸 수 있는데, 이는 $A$로 고유벡터(P의 열들)를 변환한 결과가 단순히 고유값(D의 대각 성분)만큼 스케일링한 것과 같다는 의미입니다. 표준 기저에서는 $A$의 변환 작용이 회전, 밀림, 스케일링이 복잡하게 얽혀 나타날 수 있지만, 고유벡터로 이루어진 '고유 기저'에서는 변환의 작용이 각 기저 방향으로의 단순한 스케일링으로 나타납니다. 즉, 고유 기저에서의 변환 행렬이 바로 대각행렬 $D$입니다. 행렬 $P$는 고유 기저에서 표준 기저로 변환하는 역할을, $P^{-1}$는 표준 기저에서 고유 기저로 변환하는 역할을 합니다. 따라서 $A = PDP^{-1}$는 어떤 벡터 $x$에 변환 $A$를 적용하는 과정을 (1) $x$를 고유 기저의 좌표로 변환하고 ($P^{-1}x$), (2) 그 간단한 기저에서 스케일링 변환을 적용한 뒤 ($D(P^{-1}x)$), (3) 다시 표준 기저로 되돌리는 ($P(D(P^{-1}x))$) 세 단계로 해석할 수 있습니다. 이는 대각화가 단순한 계산 트릭이 아니라, 문제를 가장 단순하게 볼 수 있는 '더 나은' 좌표계로의 개념적 전환임을 보여줍니다.

------

### **Part V: 고급 주제와 현대적 응용**

이 마지막 파트에서는 행렬 분해의 정점이라 할 수 있는 특이값 분해(SVD)를 소개하고, 선형대수학의 전체 이론적 프레임워크가 어떻게 데이터 과학, 컴퓨터 그래픽스, 양자 컴퓨팅과 같은 강력한 응용 분야의 엔진 역할을 하는지 보여줍니다.

#### **Section 10: 특이값 분해 (SVD)**

##### **10.1. 대각화를 넘어서: \*모든\* 행렬을 위한 SVD**

고유값 분해는 정사각행렬에만 적용 가능하며, 대각화를 위해서는 충분한 수의 선형 독립인 고유벡터가 필요하다는 한계가 있습니다. **특이값 분해(Singular Value Decomposition, SVD)**는 이러한 한계를 극복하여 **모든** $m \times n$ 행렬 $A$에 적용할 수 있는 가장 일반적이고 강력한 행렬 분해 방법입니다.69

SVD는 행렬 $A$를 세 개의 행렬의 곱으로 분해합니다: $A = U\Sigma V^T$.

- $U$: $m \times m$ 크기의 **직교 행렬(orthogonal matrix)**입니다. $U$의 열들(좌측 특이벡터)은 $AA^T$의 정규 직교 고유벡터들로 구성됩니다.
- $\Sigma$: $m \times n$ 크기의 **대각 행렬**입니다. 주대각선 성분 $\sigma_i$들은 $A$의 **특이값(singular values)**이라고 불립니다. 이 값들은 $A^T A$ (또는 $AA^T$)의 고유값들의 양의 제곱근이며, 크기 순서대로 내림차순으로 정렬됩니다.
- $V$: $n \times n$ 크기의 직교 행렬입니다. $V$의 열들(우측 특이벡터)은 $A^T A$의 정규 직교 고유벡터들로 구성됩니다. 70

| 표 2: 주요 행렬 분해 기법 비교 |                     |                              |                                              |                                            |
| ------------------------------ | ------------------- | ---------------------------- | -------------------------------------------- | ------------------------------------------ |
| **분해**                       | **형태**            | **적용 대상 행렬 A**       | **구성 요소의 성질**                         | **주요 용도**                              |
| **LU 분해**                    | $A = LU$          | 정사각행렬 (가역)            | $L$: 하삼각, $U$: 상삼각                 | 선형 연립방정식 풀이                       |
| **QR 분해**                    | $A = QR$          | 열이 선형 독립인 행렬        | $Q$: 직교, $R$: 상삼각                   | 최소제곱 문제, 고유값 계산                 |
| **고유값 분해 (EVD)**          | $A = PDP^{-1}$    | 정사각행렬 (대각화 가능)     | $P$: 가역 (고유벡터), $D$: 대각 (고유값) | 동적 시스템 분석, 마르코프 연쇄            |
| **특이값 분해 (SVD)**          | $A = U\Sigma V^T$ | **모든** $m \times n$ 행렬 | $U, V$: 직교, $\Sigma$: 대각 (특이값)    | 데이터 압축, 노이즈 제거, PCA, 추천 시스템 |

이 표는 각 분해법의 적용 범위와 목적을 명확히 하여, 학습자가 문제 상황에 맞는 적절한 도구를 선택하는 데 도움을 줍니다. LU는 정방형 시스템, QR은 최소제곱, EVD는 정방행렬의 동적 특성 분석, 그리고 SVD는 모든 행렬에 대한 데이터 분석의 만능 도구임을 알 수 있습니다.

##### **10.2. SVD의 기하학적 해석**

SVD는 모든 선형 변환 $A$가 세 가지 기본적인 기하학적 연산의 조합으로 이루어짐을 보여줍니다.

1. 입력 공간에서의 **회전** (또는 반사) ($V^T$)
2. 새로운 축을 따른 **스케일링** ($\Sigma$)
3. 출력 공간에서의 **회전** (또는 반사) ($U$)

즉, 아무리 복잡해 보이는 변환이라도 결국 공간을 적절히 회전시키고, 각 축 방향으로 늘리거나 줄인 다음, 다시 회전시키는 과정으로 분해할 수 있다는 것입니다.

##### **10.3. 저계수 근사**

대각행렬 $\Sigma$에 있는 특이값 $\sigma_i$들은 각 차원의 '중요도'를 측정하는 척도와 같습니다. 큰 특이값에 대응하는 차원일수록 데이터의 분산이나 정보량이 많습니다. SVD의 가장 강력한 응용 중 하나는 가장 큰 $k$개의 특이값만 남기고 나머지를 0으로 만들어 행렬 $A$를 근사하는 **저계수 근사(low-rank approximation)**입니다. 이렇게 만들어진 $A_k = U_k \Sigma_k V_k^T$는 원래 행렬 $A$에 가장 가까운 계수 $k$의 행렬입니다. 이 원리는 이미지 압축, 데이터 노이즈 제거 등 다양한 분야의 핵심 기술입니다.74

SVD는 단순히 행렬을 분해하는 것을 넘어, 행렬의 네 가지 기본 부분 공간에 대한 '가장 좋은' 기저를 제공합니다. $A = U\Sigma V^T$를 $AV = U\Sigma$로 다시 쓰면, $Av_i = \sigma_i u_i$라는 관계를 얻습니다. 여기서 $r$을 $A$의 계수(0이 아닌 특이값의 개수)라고 할 때, $i \le r$에 대해 $u_1, \dots, u_r$은 열 공간 $C(A)$의 정규 직교 기저가 되고, $v_1, \dots, v_r$은 행 공간 $C(A^T)$의 정규 직교 기저가 됩니다. 반면 $i > r$에 대해 $\sigma_i = 0$이므로 $Av_i = 0$이 되어, $v_{r+1}, \dots, v_n$은 영 공간 $N(A)$의 정규 직교 기저가 됩니다. 마찬가지로 $u_{r+1}, \dots, u_m$은 좌측 영 공간 $N(A^T)$의 정규 직교 기저가 됩니다. 이처럼 SVD는 하나의 분해를 통해 네 개의 기본 부분 공간 모두에 대한 완벽한 정규 직교 기저를 동시에 구축해주는, 가장 심오한 구조적 통찰을 제공하는 도구입니다.

#### **Section 11: 데이터 과학 및 머신러닝 응용**

##### **11.1. 주성분 분석 (PCA)**

**주성분 분석(Principal Component Analysis, PCA)**은 고차원 데이터를 정보 손실을 최소화하면서 저차원 공간에 표현하기 위한 **차원 축소(dimensionality reduction)** 기법입니다.75

PCA의 과정은 선형대수학의 개념들로 완벽하게 설명됩니다.

1. 데이터의 각 변수(특성)에서 평균을 빼서 데이터를 **중심화(center)**합니다.
2. 중심화된 데이터의 **공분산 행렬(covariance matrix)**을 계산합니다. 이 행렬은 항상 대칭 행렬입니다.68
3. 공분산 행렬의 고유값과 고유벡터를 계산합니다 (즉, 고유값 분해를 수행합니다).72
4. 계산된 고유벡터들은 데이터가 가장 큰 분산(variance)을 보이는 방향을 나타내며, 이들을 **주성분(principal components)**이라고 부릅니다. 각 고유벡터에 대응하는 고유값은 해당 주성분 방향으로의 분산의 크기를 나타냅니다.
5. 차원을 축소하기 위해, 가장 큰 고유값들을 갖는 몇 개의 고유벡터(주성분)들로 생성되는 부분 공간에 원본 데이터를 정사영시킵니다.75

결론적으로, PCA는 데이터의 공분산 행렬에 스펙트럼 정리를 적용하는 것과 같습니다. 데이터의 '본질적인 축'을 찾는 과정이 바로 대칭 행렬의 직교 고유벡터를 찾는 과정인 것입니다.

##### **11.2. 추천 시스템**

추천 시스템의 목표 중 하나는 사용자가 아직 평가하지 않은 아이템에 대한 평점을 예측하는 것입니다. 이는 보통 사용자와 아이템 간의 평점 정보를 담은 거대한 **희소 행렬(sparse matrix)** $R$로부터 시작됩니다.

**행렬 분해(Matrix Factorization)** 기반의 추천 시스템은 사용자와 아이템 모두 소수의 **잠재 요인(latent factors)**(예: 영화의 경우 장르, 배우, 감독 등)으로 설명될 수 있다고 가정합니다. 이 아이디어는 거대하고 희소한 평점 행렬 $R$을 두 개의 작고 조밀한(dense) 행렬 $P$와 $Q$의 곱, 즉 $R \approx PQ^T$로 근사하는 것입니다. 여기서 $P$는 사용자들이 각 잠재 요인을 얼마나 선호하는지를, $Q$는 아이템들이 각 잠재 요소를 얼마나 포함하는지를 나타냅니다.

SVD는 이러한 잠재 요인 행렬을 찾는 데 직접적으로 사용될 수 있습니다. SVD를 이용한 저계수 근사는 최소제곱법 관점에서 최적의 근사 행렬을 제공하기 때문입니다.70 하지만 실제 추천 시스템에서는 평점 행렬의 대부분이 비어있기 때문에(희소성), 표준 SVD를 직접 적용하기 어렵습니다. 따라서 실제로는 경사 하강법(Gradient Descent) 등을 사용하여 결측치를 무시하고 잠재 요인 행렬을 학습하는 FunkSVD나 교대 최소제곱법(Alternating Least Squares)과 같은 변형된 기법들이 사용됩니다.78

#### **Section 12: 특수 분야 응용**

##### **12.1. 컴퓨터 그래픽스**

컴퓨터 그래픽스의 핵심 과제는 3차원 객체를 2차원 화면에 표현하고 조작하는 것입니다. 이를 위해 선형대수학, 특히 행렬 변환이 필수적으로 사용됩니다.

이동(translation), 회전(rotation), 크기 조절(scaling)은 3D 그래픽스의 기본 변환입니다. 회전과 크기 조절은 행렬 곱셈으로 표현되는 선형 변환이지만, 이동은 벡터 덧셈으로 표현되어 일관성을 해칩니다. 이 문제를 해결하기 위해 **동차 좌표계(homogeneous coordinates)**라는 기법이 사용됩니다.79

동차 좌표계는 3차원 점 $(x, y, z)$를 4차원 벡터 $(x, y, z, 1)$로 표현합니다. 이렇게 차원을 하나 확장함으로써, 3차원에서의 이동 변환을 4차원 공간에서의 밀림 변환(shear) 또는 회전 변환과 같은 행렬 곱셈으로 통합할 수 있습니다.81 결과적으로 모든 기하 변환(이동, 회전, 크기 조절)을 단일 $4 \times 4$ 행렬의 곱셈으로 표현할 수 있게 되어, 여러 변환을 합성하는 과정을 매우 효율적이고 우아하게 만듭니다.

##### **12.2. 양자 컴퓨팅**

선형대수학은 양자역학의 자연스러운 언어입니다.83 양자 컴퓨팅의 기본 원리는 선형대수학의 개념들로 완벽하게 기술됩니다.

- **양자 상태와 벡터:** 양자 시스템의 상태, 예를 들어 **큐비트(qubit)**는 복소수 벡터 공간(보통 **힐베르트 공간(Hilbert space)**이라 불림)의 벡터로 표현됩니다. 단일 큐비트의 기본 상태인 $\|0\rangle$과 $\|1\rangle$은 각각 $^T$와 $^T$라는 정규 직교 기저 벡터에 해당합니다.83 일반적인 큐비트 상태는 이 두 기저 상태의 선형 결합(중첩, superposition)으로 표현됩니다: 
  $$
  \alpha|0\rangle + \beta|1\rangle
  $$
  **양자 연산과 행렬:** 큐비트를 조작하는 연산인 **양자 게이트(quantum gate)**는 **유니터리 행렬(unitary matrix)**로 표현됩니다. 유니터리 행렬 $U$는 자신의 켤레 전치(conjugate transpose)가 역행렬이 되는 행렬, 즉 $U^{-1} = U^\dagger$를 만족하는 복소 행렬입니다. 유니터리 변환은 벡터의 길이를 보존하는 성질이 있는데, 이는 양자역학에서 총 확률이 항상 1로 보존되어야 한다는 물리적 요구사항과 정확히 일치합니다.84

이처럼 양자역학의 상태는 벡터로, 연산은 행렬로 기술되며, 시스템의 측정은 내적과 관련이 있습니다. 따라서 선형대수학을 깊이 이해하는 것은 양자 컴퓨팅의 원리를 파악하는 데 필수적입니다.

### **결론**

선형대수학은 연립방정식 풀이라는 구체적인 문제에서 출발하여 벡터와 행렬이라는 강력한 도구를 도입하고, 이를 벡터 공간이라는 추상적인 프레임워크로 일반화하는 학문이다. 이 여정을 통해 학습자는 대수적 조작 능력과 기하학적 직관을 동시에 함양하게 된다.

기저와 차원의 개념은 모든 유한 차원 공간의 구조를 밝히고, 직교성은 기하학적 의미를 부여하며 최소제곱법과 같은 최적화 문제의 해법을 제시한다. 나아가 고유값과 고유벡터는 선형 변환의 본질적인 특성을 드러내며, 시스템의 동적 거동을 이해하는 열쇠가 된다. 이 모든 이론은 특이값 분해(SVD)에서 정점을 이루며, 이는 모든 행렬을 기하학적으로 의미 있는 구성 요소로 분해하는 가장 일반적인 방법이다.

결론적으로, 선형대수학은 단순히 계산 기술의 집합이 아니다. 이는 공간, 변환, 데이터의 구조를 이해하는 근본적인 사유의 틀이며, 데이터 과학, 컴퓨터 그래픽스, 양자 컴퓨팅 등 현대 과학 기술의 거의 모든 분야를 뒷받침하는 핵심적인 언어이자 엔진이다. 이 로드맵을 따라 기초부터 응용까지 체계적으로 학습함으로써, 학습자는 복잡한 문제를 모델링하고 해결하는 데 필요한 강력하고 보편적인 지적 도구를 갖추게 될 것이다.

#### **참고 자료**

1. 행렬식(determinant)의 기하학적 의미 - 오다기리 박의 알고리즘 노트, accessed July 20, 2025, [https://wjdgh283.tistory.com/entry/%ED%96%89%EB%A0%AC%EC%8B%9Ddeterminant%EC%9D%98-%EA%B8%B0%ED%95%98%ED%95%99%EC%A0%81-%EC%9D%98%EB%AF%B8](https://wjdgh283.tistory.com/entry/행렬식determinant의-기하학적-의미)
2. [행렬대수학] 행렬식(Determinant) 1 - 행렬식의 개념 - 간토끼 DataMining Lab - 티스토리, accessed July 20, 2025, https://datalabbit.tistory.com/39
3. 2. 가우스 소거법을 이용한 연립방정식의 풀이, accessed July 20, 2025, https://blueberrypie.tistory.com/5
4. 가우스 소거법 - 위키백과, 우리 모두의 백과사전, accessed July 20, 2025, [https://ko.wikipedia.org/wiki/%EA%B0%80%EC%9A%B0%EC%8A%A4_%EC%86%8C%EA%B1%B0%EB%B2%95](https://ko.wikipedia.org/wiki/가우스_소거법)
5. [선형대수학] 가우스-요르단 소거법으로 n차 정방행렬의 역행렬 구하기 - bskyvision.com, accessed July 20, 2025, [https://bskyvision.com/entry/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99-%EA%B0%80%EC%9A%B0%EC%8A%A4-%EC%9A%94%EB%A5%B4%EB%8B%A8-%EC%86%8C%EA%B1%B0%EB%B2%95%EC%9C%BC%EB%A1%9C-3%EC%B0%A8-%EC%A0%95%EB%B0%A9%ED%96%89%EB%A0%AC%EC%9D%98-%EC%97%AD%ED%96%89%EB%A0%AC-%EA%B5%AC%ED%95%98%EA%B8%B0](https://bskyvision.com/entry/선형대수학-가우스-요르단-소거법으로-3차-정방행렬의-역행렬-구하기)
6. 가우스-조던 행렬 소거법의 기하학적 의미 - 공돌이의 수학정리노트 (Angelo's Math Notes), accessed July 20, 2025, https://angeloyeo.github.io/2019/09/09/Gauss_Jordan.html
7. 1.2 가우스 소거법 - 개발자할래요 - 티스토리, accessed July 20, 2025, https://iskull-dev.tistory.com/96
8. 연립 일차 방정식, 행렬로 풀기/ 가우스조던 소거법, 역행렬 - 결국 빛나리라 - 티스토리, accessed July 20, 2025, https://westshine-data-analysis.tistory.com/48
9. 가우스 소거법(Gaussian elimination)의 이해 - 선형대수 2-2강 :: Data 쿡북, accessed July 20, 2025, https://datacookbook.kr/68
10. 가우스 소거법 - velog, accessed July 20, 2025, [https://velog.io/@wbsl0427/%EA%B0%80%EC%9A%B0%EC%8A%A4-%EC%86%8C%EA%B1%B0%EB%B2%95](https://velog.io/@wbsl0427/가우스-소거법)
11. 기본 행 연산과 기본행렬 (Elementary row operation and Elementary matrix) - 단아한섭동, accessed July 20, 2025, https://gosamy.tistory.com/21
12. [선형대수학] 벡터와 행렬 기초 - 기본 개념과 연산 방법 1 - Savvy, accessed July 20, 2025, https://savvy0402.tistory.com/11
13. 1 선형 대수의 기초( 벡터, 행렬, 기저, 행렬 연산) - 인공지능 학습하는 오복이, accessed July 20, 2025, https://happy-obok.tistory.com/25
14. 행렬 VS 벡터: 선형 대수학에서 어느 것이 더 기본적인 개념일까? 🧮🔢 - 재능넷, accessed July 20, 2025, https://www.jaenung.net/tree/2694
15. 행렬과 벡터의 기초 연산 - 스파이 펭귄의 이글루, accessed July 20, 2025, https://bestech49.tistory.com/25
16. 행렬 곱 의미 생각하기, accessed July 20, 2025, https://people-analysis.tistory.com/23
17. 10.행렬의 전치 - 민주의 하루, accessed July 20, 2025, https://mindole94.tistory.com/291
18. 행렬곱 - 나무위키, accessed July 20, 2025, [https://namu.wiki/w/%ED%96%89%EB%A0%AC%EA%B3%B1](https://namu.wiki/w/행렬곱)
19. 행렬 곱셈의 성질 (개념 이해하기) | 행렬 | Khan Academy - 칸아카데미, accessed July 20, 2025, https://ko.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:properties-of-matrix-multiplication/a/properties-of-matrix-multiplication
20. 앤드류 응의 머신러닝 (3-5) : 행렬 곱셈의 속성 - 브런치, accessed July 20, 2025, https://brunch.co.kr/@linecard/454
21. 14주차-2. 선형변환, accessed July 20, 2025, https://erase-jeong.tistory.com/83
22. Gauss Jordan으로 역행렬(Inverse Matrix) 구하기 - 파고파고 - 티스토리, accessed July 20, 2025, https://dippingtodeepening.tistory.com/16?category=969044
23. 15. 벡터 공간 (Vector Space) - 베이지안의 고양이 !, accessed July 20, 2025, https://portrait-of-youngblood.tistory.com/27
24. 선형대수학 시리즈 0편(벡터공간이란 무엇인가?) - 물리 필기구, accessed July 20, 2025, [https://pilgigo.tistory.com/entry/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99-%EC%8B%9C%EB%A6%AC%EC%A6%88-0%ED%8E%B8%EB%B2%A1%ED%84%B0%EA%B3%B5%EA%B0%84%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80](https://pilgigo.tistory.com/entry/선형대수학-시리즈-0편벡터공간이란-무엇인가)
25. 벡터공간(vector space)의 또 다른 예, accessed July 20, 2025, https://jjycjnmath.tistory.com/148
26. 3.1a 벡터 공간 (Vector space) - 알기 쉬운 경제 과학 기술 상식, accessed July 20, 2025, [https://er5030000.tistory.com/entry/%EB%B2%A1%ED%84%B0-%EA%B3%B5%EA%B0%84-Vector-space](https://er5030000.tistory.com/entry/벡터-공간-Vector-space)
27. Vector Space (벡터 공간), Vector (벡터) - 딥러닝 지망생 - 티스토리, accessed July 20, 2025, https://tinyarchive.tistory.com/6
28. 3.1c 부분 공간(subspace) - 알기 쉬운 경제 과학 기술 상식, accessed July 20, 2025, [https://er5030000.tistory.com/entry/%EB%B6%80%EB%B6%84-%EA%B3%B5%EA%B0%84-subspace](https://er5030000.tistory.com/entry/부분-공간-subspace)
29. 벡터공간의 부분공간 (Subspace) - 단아한섭동 - 티스토리, accessed July 20, 2025, https://gosamy.tistory.com/44
30. [선형대수학] - 부분공간 (Subspace) 과 부분공간의 기저 (Baisis of Subspace), accessed July 20, 2025, [https://velog.io/@jailies/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99-%EB%B6%80%EB%B6%84%EA%B3%B5%EA%B0%84%EA%B3%BC-%EB%B6%80%EB%B6%84%EA%B3%B5%EA%B0%84%EC%9D%98-%EA%B8%B0%EC%A0%80](https://velog.io/@jailies/선형대수학-부분공간과-부분공간의-기저)
31. [선형대수학] 부분공간, 기저 (Subspace, Basis) - SUBORATORY - 티스토리, accessed July 20, 2025, https://subprofessor.tistory.com/51
32. 2장. 네 가지 기본적인 부분 공간, accessed July 20, 2025, [https://psh7286.tistory.com/entry/2%EC%9E%A5-%EB%84%A4-%EA%B0%80%EC%A7%80-%EA%B8%B0%EB%B3%B8%EC%A0%81%EC%9D%B8-%EB%B6%80%EB%B6%84-%EA%B3%B5%EA%B0%84](https://psh7286.tistory.com/entry/2장-네-가지-기본적인-부분-공간)
33. 09 선형 독립(Linear independence), Span, 기저(Basis) 그리고 차원(Dimension) - adioshun, accessed July 20, 2025, https://adioshun.gitbooks.io/linear-algebra/content/09c120-d615-b3c5-b9bd-span-ae30-c800-cc28-c6d0.html
34. [선형대수]선형 독립(linearly independent)과 기저(Basis) 개념정리 - 가동한의 공학공부방, accessed July 20, 2025, [https://gadonghan.tistory.com/entry/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%EC%84%A0%ED%98%95-%EB%8F%85%EB%A6%BDlinearly-independent%EA%B3%BC-%EA%B8%B0%EC%A0%80Basis-%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC](https://gadonghan.tistory.com/entry/선형대수선형-독립linearly-independent과-기저Basis-개념정리)
35. 벡터공간의 기저와 차원 - 미래로, accessed July 20, 2025, https://diffrentedcon.tistory.com/26
36. [선대] 2-7강. 선형 독립과 기저 (linearly independent & basis) 직관적 설명 - YouTube, accessed July 20, 2025, https://www.youtube.com/watch?v=mOOI4-BfjGQ
37. [선형대수] 벡터공간(vector space), 벡터 부분공간(vector subspace), 생성공간(span), 차원(dimension), accessed July 20, 2025, https://rfriend.tistory.com/173
38. [선형대수학] 기저와 차원, accessed July 20, 2025, [https://velog.io/@k_bobin/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99-%EA%B8%B0%EC%A0%80%EC%99%80-%EC%B0%A8%EC%9B%90](https://velog.io/@k_bobin/선형대수학-기저와-차원)
39. 선형대수학의 기본정리 - 나무위키, accessed July 20, 2025, [https://namu.wiki/w/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99%EC%9D%98%20%EA%B8%B0%EB%B3%B8%EC%A0%95%EB%A6%AC](https://namu.wiki/w/선형대수학의 기본정리)
40. 14. 벡터의 내적 (Inner Product of Vectors) - 베이지안의 고양이 ! - 티스토리, accessed July 20, 2025, https://portrait-of-youngblood.tistory.com/26
41. 내적의 개념 - CS STUDY - 티스토리, accessed July 20, 2025, https://mldlcvmjw.tistory.com/410
42. 내적 공간 - 위키백과, 우리 모두의 백과사전, accessed July 20, 2025, [https://ko.wikipedia.org/wiki/%EB%82%B4%EC%A0%81_%EA%B3%B5%EA%B0%84](https://ko.wikipedia.org/wiki/내적_공간)
43. 내적 - 나무위키, accessed July 20, 2025, [https://namu.wiki/w/%EB%82%B4%EC%A0%81](https://namu.wiki/w/내적)
44. 벡터 - 정의, 벡터의 연산, 노름(norm), 벡터 사이의 거리와각도 - velog, accessed July 20, 2025, [https://velog.io/@guts4/%EB%B2%A1%ED%84%B0-%EC%A0%95-%EB%B2%A1%ED%84%B0%EC%9D%98-%EC%97%B0%EC%82%B0-%EB%85%B8%EB%A6%84norm-%EB%B2%A1%ED%84%B0-%EC%82%AC%EC%9D%B4%EC%9D%98-%EA%B0%81%EB%8F%84-%EB%82%B4%EC%A0%81](https://velog.io/@guts4/벡터-정-벡터의-연산-노름norm-벡터-사이의-각도-내적)
45. 그램 슈미트 과정 - CS STUDY - 티스토리, accessed July 20, 2025, https://mldlcvmjw.tistory.com/412
46. 벡터와 행렬 - velog, accessed July 20, 2025, [https://velog.io/@dldydldy75/%EB%B2%A1%ED%84%B0%EC%99%80-%ED%96%89%EB%A0%AC](https://velog.io/@dldydldy75/벡터와-행렬)
47. [미적분학2] 10.3절 (2/3) - 두 벡터 사이의 각이란? - YouTube, accessed July 20, 2025, https://www.youtube.com/watch?v=4DSUFQRcHrc
48. [혁펜하임: 보이는 선대] 3강: 가우스-조던, 행렬식, trace, 최소자승법 - 데이터 분석 일지, accessed July 20, 2025, https://younhaxyz.tistory.com/39
49. [선형대수학] 그람-슈미트 과정 (Gram-Schmidt Process) 예제 - SUBORATORY - 티스토리, accessed July 20, 2025, https://subprofessor.tistory.com/70
50. [Linear Algebra] Lecture 17-(2) 직교행렬(Orthogonal Matrices)과 그람 슈미트 과정(Gram-Schmidt Process) - Learn Again! 러너게인 - 티스토리, accessed July 20, 2025, https://twlab.tistory.com/38
51. 그람-슈미트 과정 - 위키백과, 우리 모두의 백과사전, accessed July 20, 2025, [https://ko.wikipedia.org/wiki/%EA%B7%B8%EB%9E%8C-%EC%8A%88%EB%AF%B8%ED%8A%B8_%EA%B3%BC%EC%A0%95](https://ko.wikipedia.org/wiki/그람-슈미트_과정)
52. Gram–Schmidt(그람-슈미트) 직교화 - 다크 프로그래머 - 티스토리, accessed July 20, 2025, https://darkpgmr.tistory.com/165
53. [AI class day 8] 인공지능 수학- 자료의 정리 TIL - Rolling Snowball - 티스토리, accessed July 20, 2025, [https://rollingsnowball.tistory.com/entry/AI-class-day-8-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EC%88%98%ED%95%99-%EC%9E%90%EB%A3%8C%EC%9D%98-%EC%A0%95%EB%A6%AC-TIL](https://rollingsnowball.tistory.com/entry/AI-class-day-8-인공지능-수학-자료의-정리-TIL)
54. 행렬식과 역행렬 - 행렬식의 계산 방법, 성질, 기하학적 의미와 역행렬의 계산 및 응용을 포함합니다. | Flashcards World, accessed July 20, 2025, https://flashcards.world/flashcards/sets/9909aec3-b7d1-427b-8ee8-31397392ee35/
55. [Linear Algebra] Lecture 20-(2) 행렬식(Determinant)의 기하학적 해석(Geometrical Analysis), accessed July 20, 2025, https://twlab.tistory.com/44
56. 행렬식의 기하학적 의미 - 공돌이의 수학정리노트 (Angelo's Math Notes), accessed July 20, 2025, https://angeloyeo.github.io/2019/08/06/determinant.html
57. 고윳값과 고유 벡터 - 위키백과, 우리 모두의 백과사전, accessed July 20, 2025, [https://ko.wikipedia.org/wiki/%EA%B3%A0%EC%9C%B3%EA%B0%92%EA%B3%BC_%EA%B3%A0%EC%9C%A0_%EB%B2%A1%ED%84%B0](https://ko.wikipedia.org/wiki/고윳값과_고유_벡터)
58. 머신러닝 - 19. 고유값(eigenvalue), 고유벡터(eigenvector), 고유값 분해(eigen decomposition) - 귀퉁이 서재 - 티스토리, accessed July 20, 2025, [https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-19-%ED%96%89%EB%A0%AC](https://bkshin.tistory.com/entry/머신러닝-19-행렬)
59. [선형대수학 #3] 고유값과 고유벡터 (eigenvalue & eigenvector) - 다크 프로그래머 - 티스토리, accessed July 20, 2025, https://darkpgmr.tistory.com/105
60. 고윳값과 고유 벡터의 개념 - CS STUDY, accessed July 20, 2025, https://mldlcvmjw.tistory.com/416
61. [선형대수-1] 행렬대각화 : 행렬의 고유값과 고유벡터의 기하학적 의미 - 공부하는박사곰, accessed July 20, 2025, https://studyingrabbit.tistory.com/5
62. 선형대수 - Eigen vector(고유 벡터), Eigen value(고유 값)의 기하학적 의미 - 외쳐갓우찬, accessed July 20, 2025, https://woochan-autobiography.tistory.com/716
63. 고윳값 고유벡터의 기하학적 의미 - YouTube, accessed July 20, 2025, https://www.youtube.com/watch?v=7dmV3p3Iy90
64. 고윳값과 고유벡터 - 공돌이의 수학정리노트 (Angelo's Math Notes), accessed July 20, 2025, https://angeloyeo.github.io/2019/07/17/eigen_vector.html
65. 고유값(eigen value)과 고유벡터(eigen vector) - 생각과 고민., accessed July 20, 2025, https://gguguk.github.io/posts/eigenvalue_eigenvector/
66. 고유값과 고유벡터의 의미 - velog, accessed July 20, 2025, [https://velog.io/@guide333/%EA%B3%A0%EC%9C%A0%EA%B0%92%EA%B3%BC-%EA%B3%A0%EC%9C%A0%EB%B2%A1%ED%84%B0%EC%9D%98-%EC%9D%98%EB%AF%B8](https://velog.io/@guide333/고유값과-고유벡터의-의미)
67. [Linear Algebra] Lecture 21-(2) 고유값(eigenvalues)과 고유 벡터(eigenvectors) - Learn Again! 러너게인 - 티스토리, accessed July 20, 2025, https://twlab.tistory.com/47
68. [선형대수학] PCA(주성분 분석)란? - Logical Scribbles - 티스토리, accessed July 20, 2025, https://stydy-sturdy.tistory.com/36
69. 행렬분해, accessed July 20, 2025, http://matrix.skku.ac.kr/math4ai-intro/W6/
70. 특이값 분해(Singular Value Decompostion) - 생각과 고민., accessed July 20, 2025, https://gguguk.github.io/posts/SVD/
71. 데이터 분석(추천시스템): SVD (SVD와 Latent Factor 모형) - 잔재미코딩, accessed July 20, 2025, https://www.fun-coding.org/post/recommend_basic6.html
72. [재업]차원 축소와 주성분분석(Principal Component Analysis, PCA) :: 시드일지, accessed July 20, 2025, https://sidreco.tistory.com/16
73. [차원 축소] 주성분 분석 (PCA, Principal Component Analysis) - 이거슨무슨블로그 - 티스토리, accessed July 20, 2025, https://seongyun-dev.tistory.com/4
74. 추천 시스템 2 - Matrix Factorization(MF)를 이용한 협력 필터링 | by 김희규, accessed July 20, 2025, [https://heegyukim.medium.com/%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-2-matrix-factorization-mf-%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%ED%98%91%EB%A0%A5-%ED%95%84%ED%84%B0%EB%A7%81-11c10199a593](https://heegyukim.medium.com/추천-시스템-2-matrix-factorization-mf-를-이용한-협력-필터링-11c10199a593)
75. 주성분 분석(PCA) - 공돌이의 수학정리노트 (Angelo's Math Notes), accessed July 20, 2025, https://angeloyeo.github.io/2019/07/27/PCA.html
76. 고유값 분해와 뗄래야 뗄 수 없는 주성분분석(PCA) - bskyvision.com, accessed July 20, 2025, [https://bskyvision.com/entry/%EA%B3%A0%EC%9C%A0%EA%B0%92-%EB%B6%84%ED%95%B4%EC%99%80-%EB%97%84%EB%9E%98%EC%95%BC-%EB%97%84-%EC%88%98-%EC%97%86%EB%8A%94-%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9DPCA](https://bskyvision.com/entry/고유값-분해와-뗄래야-뗄-수-없는-주성분분석PCA)
77. [Linear Algebra] SVD(Singular Value Decomposition) - ok-lab - 티스토리, accessed July 20, 2025, https://ok-lab.tistory.com/105
78. [추천 시스템] Matrix Factorization (SGD) - 책 읽는 성키 - 티스토리, accessed July 20, 2025, https://sungkee-book.tistory.com/12
79. 2차원 그래픽스의 동차좌표계 변환 - Kim's Programming - 티스토리, accessed July 20, 2025, https://robodream.tistory.com/213
80. [CG]컴퓨터 그래픽스 03. 기하 변환 Geometric Transformation, accessed July 20, 2025, https://justdoitproject.tistory.com/39
81. 7. 동차 좌표계(Homogeneous Coordinates)와 아핀 변환 행렬(Affine Transformation Matrix) - 프로그래머 - 티스토리, accessed July 20, 2025, https://eunho5751.tistory.com/38
82. 컴퓨터그래픽스 필기노트 4강-좌표계와 변환 - MoonBug - 티스토리, accessed July 20, 2025, https://moonbug4.tistory.com/30
83. 양자 계산을 위한 선형대수학 -1 : 벡터 - 모카롤이 먹고 싶다, accessed July 20, 2025, https://mocharoll.tistory.com/4
84. 양자 계산을 위한 선형대수학 -2 : 행렬 - 모카롤이 먹고 싶다, accessed July 20, 2025, https://mocharoll.tistory.com/5
85. 2. 양자역학 입문, accessed July 20, 2025, https://teach-meaning.tistory.com/811

