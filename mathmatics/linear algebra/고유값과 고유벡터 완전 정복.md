# 고유값과 고유벡터 완전 정복
[선형대수](./index.md)


행렬은 단순히 숫자를 사각형 격자에 배열한 것이 아니다. 행렬은 벡터 공간을 변환하는 동적인 연산자이며, 모든 선형 변환의 본질을 담고 있다. 그렇다면 이 변환의 '영혼' 또는 'DNA'는 무엇일까? 어떤 행렬이 공간을 어떻게 늘리고, 회전시키고, 뒤트는지를 가장 근본적으로 설명하는 특성은 무엇일까? 그 해답은 바로 **고유값(eigenvalue)**과 **고유벡터(eigenvector)**에 있다.1

독일어 'eigen'은 '고유한', '특유의'라는 의미를 가진다.2 이름에서 알 수 있듯이, 고유값과 고유벡터는 행렬, 즉 선형 변환 그 자체가 가진 고유한 특성을 나타낸다. 어떤 복잡한 변환이 일어나더라도, 그 속에는 방향이 변하지 않는 특별한 벡터들(고유벡터)이 존재하며, 이 벡터들은 단지 특정 비율(고유값)만큼 크기만 변할 뿐이다.

이 개념은 순수 수학의 영역을 넘어 우리 세계를 설명하는 강력한 도구로 자리 잡았다. 구글의 페이지랭크 알고리즘은 웹페이지의 중요도를 결정하기 위해 거대한 링크 행렬의 고유벡터를 계산하고 6, 양자역학에서는 슈뢰딩거 방정식을 고유값 문제로 풀어 원자의 안정적인 에너지 상태를 찾아낸다.8 또한, 공학자들은 다리나 건물의 진동을 분석하여 공명으로 인한 붕괴를 막기 위해 고유값을 사용한다.10

이 자습서는 고유값과 고유벡터의 세계를 탐험하는 완전한 안내서가 될 것이다. 우리는 가장 단순한 기하학적 직관에서 출발하여 엄밀한 수학적 정의와 계산법을 익히고, 대각화와 같은 핵심 이론을 심도 있게 파고들 것이다. 나아가, 이 개념들이 현대 과학과 기술의 다양한 분야에서 어떻게 세상을 움직이는지 구체적인 응용 사례를 통해 살펴볼 것이다. 이 여정을 통해 독자들은 고유값과 고유벡터가 단순한 계산 절차를 넘어, 복잡한 시스템의 근본적인 구조를 드러내는 강력한 렌즈임을 이해하게 될 것이다.

------


고유값과 고유벡터의 엄밀한 수학적 정의에 앞서, 이들이 기하학적으로 무엇을 의미하는지 직관적으로 이해하는 것이 중요하다. 행렬을 숫자의 집합이 아닌, 공간을 움직이고 변형시키는 동적인 힘으로 상상해 보자.


선형대수학의 중심에는 **선형 변환(linear transformation)**이라는 개념이 있다. 이는 한 벡터를 다른 벡터로 옮기는 규칙으로, 행렬 곱셈을 통해 구현된다. $n \times n$ 행렬 A와 벡터 v가 주어졌을 때, Av는 행렬 A가 벡터 v에 가하는 변환의 결과를 나타내는 새로운 벡터이다.11

이 변환은 여러 가지 형태로 나타날 수 있다 6:

- **크기 조절 (Scaling):** 모든 벡터를 원점으로부터 일정 비율로 늘리거나 줄인다.
- **회전 (Rotation):** 모든 벡터를 원점을 중심으로 특정 각도만큼 회전시킨다.
- **전단 (Shear):** 공간을 한쪽으로 밀어 기울인다. 마치 카드 덱의 윗부분을 옆으로 미는 것과 같다.
- **사영 (Projection):** 벡터를 특정 선이나 평면 위로 "그림자"처럼 투영한다.

2차원 평면 위의 모든 벡터들이 행렬 변환에 의해 새로운 위치로 이동하는 모습을 상상해 보라. 대부분의 벡터는 원래의 방향과 크기가 모두 변하게 된다. 이처럼 공간 전체가 역동적으로 움직이는 와중에, 우리는 특별한 벡터에 주목하게 된다.


복잡한 변환 속에서도, 어떤 벡터들은 변환 후에도 자신의 원래 방향을 그대로 유지한다. 방향이 바뀌지 않거나, 정확히 180도 반대 방향으로 뒤집힐 뿐, 여전히 원래 벡터가 놓여 있던 직선 위에 존재한다.2 이 특별한 벡터들이 바로 해당 변환의 **고유벡터(eigenvector)**이다.

고유벡터는 변환의 '축' 또는 '골격'과 같다.11 공간의 모든 벡터들이 이 축을 중심으로 움직이고 변형된다. 고유벡터는 변환의 영향을 받지 않는다는 의미가 아니라, 변환의 효과가 가장 단순하게 나타나는 방향이라는 의미이다.

예를 들어, 수평 방향으로만 2배 늘리는 변환을 생각해 보자.

- 수평 벡터 (1, 0)은 변환 후 (2, 0)이 된다. 방향은 그대로이고 크기만 2배가 되었다. 따라서 (1, 0)은 이 변환의 고유벡터이다.
- 수직 벡터 (0, 1)은 변환 후에도 (0, 1)로 그대로이다. 방향과 크기가 모두 변하지 않았다. 이 또한 고유벡터이다.
- 하지만 대각선 벡터 (1, 1)은 변환 후 (2, 1)이 된다. 이 벡터는 방향이 변했으므로 고유벡터가 아니다.

고유벡터는 하나로 유일하게 정해지지 않는다. 어떤 고유벡터와 같은 방향(또는 반대 방향)에 있는 모든 0이 아닌 벡터는 같은 고유벡터의 성질을 가진다. 이 고유벡터들이 모여 이루는 직선이나 평면, 즉 부분 공간을 **고유공간(eigenspace)**이라고 부른다.6 고유공간은 변환에 의해 자기 자신으로 사상되는 안정적인 공간이다.


고유벡터가 변환의 '방향'을 알려준다면, **고유값(eigenvalue)** $\lambda$는 그 방향으로 얼마나 변했는지, 즉 '크기'의 변화를 알려주는 스칼라 값이다.4 고유값은 고유벡터가 변환에 의해 얼마나 늘어나거나 줄어드는지를 나타내는 배율이다.

고유값 $\lambda$의 값에 따라 변환의 성격이 다음과 같이 결정된다:

- $\lambda > 1$: 고유벡터는 원래보다 길어진다 (팽창). 시스템이 이 방향으로 불안정해지거나 성장함을 의미할 수 있다.1
- $0 < \lambda < 1$: 고유벡터는 원래보다 짧아진다 (수축). 시스템이 이 방향으로 안정화되거나 소멸함을 의미할 수 있다.6
- $\lambda = 1$: 고유벡터는 크기와 방향이 전혀 변하지 않는다. 이 상태를 **정상 상태(steady state)**라고 하며, 시간이 지나도 변하지 않는 안정적인 상태를 모델링하는 데 매우 중요하다 (예: 마르코프 연쇄).6
- $\lambda < 0$: 고유벡터의 방향이 정확히 180도 반대로 뒤집힌다. 크기는 $|\lambda|$배만큼 변한다.4
- $\lambda = 0$: 고유벡터는 변환 후 영벡터(원점)로 붕괴된다. 이는 해당 방향의 정보가 완전히 사라짐을 의미하며, 행렬이 비가역적(singular)이라는 중요한 단서가 된다.4


몇 가지 기본적인 변환의 고유값과 고유벡터를 살펴보면 직관을 더욱 확고히 할 수 있다.

- **사영 행렬 (Projection Matrix):** 2차원 벡터를 x축에 사영하는 변환을 생각해 보자.

  - x축 위의 벡터들(예: (1, 0))은 변환 후에도 그대로이다. 따라서 이들은 고유값 $\lambda=1$을 갖는 고유벡터이다.1
  - y축 위의 벡터들(예: (0, 1))은 변환 후 원점 (0, 0)으로 붕괴된다. 따라서 이들은 고유값 $\lambda=0$을 갖는 고유벡터이다.1

- **반사 행렬 (Reflection Matrix):** y축을 기준으로 벡터를 반사하는 변환을 생각해 보자.

  - y축 위의 벡터들(예: (0, 1))은 반사 후에도 변하지 않는다. 이들은 고유값 $\lambda=1$을 갖는 고유벡터이다.1
  - x축 위의 벡터들(예: (1, 0))은 반사 후 방향이 정반대인 (-1, 0)이 된다. 이들은 고유값 $\lambda=-1$을 갖는 고유벡터이다.1

- **회전 행렬 (Rotation Matrix):** 2차원 평면을 90도 회전시키는 변환을 생각해 보자.

  - 영벡터를 제외한 모든 벡터는 방향이 바뀐다. 따라서 이 변환은 실수 범위 내에서 고유벡터를 갖지 않는다.12 이는 회전과 같은 변환을 완전히 이해하기 위해서는 실수를 넘어 

    **복소수** 고유값과 고유벡터의 개념이 필요함을 시사한다.

이처럼 고유값과 고유벡터를 찾는 것은 단순히 방정식을 푸는 행위가 아니라, 주어진 선형 변환의 가장 본질적인 기하학적 특성, 즉 변하지 않는 축과 그 축에서의 스케일링 효과를 밝혀내는 과정이다. 이 기하학적 통찰은 행렬의 대수적 속성들이 왜 중요한지, 그리고 왜 이 개념들이 수많은 물리적, 데이터 기반 문제에 적용될 수 있는지를 이해하는 열쇠가 된다.

------


기하학적 직관을 바탕으로 이제 고유값과 고유벡터를 수학적으로 엄밀하게 정의하고 계산하는 방법을 살펴보자. 이 과정은 직관을 구체적인 수식으로 변환하고, 어떤 행렬이 주어지더라도 그 고유한 특성을 찾아낼 수 있는 체계적인 도구를 제공한다.


1장에서 살펴본 기하학적 관계, 즉 "행렬 A에 의한 변환 결과 Av가 원래 벡터 v와 방향이 같고 크기만 $\lambda$배 변한다"는 것을 하나의 방정식으로 표현할 수 있다.

$n \times n$ 정방행렬 A에 대하여, 0이 아닌 벡터 v와 스칼라 $\lambda$가 다음 방정식을 만족할 때,
$$
Av = \lambda v
$$
$\lambda$를 행렬 A의 **고유값(eigenvalue)**이라 하고, v를 고유값 $\lambda$에 대응하는 **고유벡터(eigenvector)**라고 한다.4

여기서 v는 영벡터가 아니어야 한다는 조건이 매우 중요하다. 만약 v=0이라면 `A0 = \lambda 0`은 모든 $\lambda$에 대해 항상 성립하므로 의미 있는 정보를 주지 못하기 때문이다.19


고유값 $\lambda$를 어떻게 찾을 수 있을까? 해답은 고유값 방정식 자체에 숨어 있다. 우리는 이 방정식을 논리적으로 변형하여 $\lambda$를 구하기 위한 새로운 방정식을 유도할 것이다.

1. **방정식 재정렬:** 고유값 방정식 Av = $\lambda$v에서 우변을 좌변으로 이항한다.
   $$
   Av - \lambda v = 0
   $$
   **항등행렬 도입:** 좌변의 두 항을 행렬 연산으로 묶기 위해, 스칼라 $\lambda$를 행렬 형태로 바꿔준다. 벡터 v에 항등행렬 I를 곱해도 v는 변하지 않으므로($Iv = v$), $\lambda v$를 $\lambda I v$로 쓸 수 있다.12
   $$
   Av - \lambda I v = 0
   $$
   **벡터 인수분해:** 이제 공통 인수인 벡터 v를 묶어낸다.
   $$
   (A - \lambda I)v = 0
   $$
   **비자명 해의 조건:** 이 방정식은 v에 대한 동차 선형 연립방정식이다. 우리는 정의에 따라 v $\neq$ 0인 **비자명 해(non-trivial solution)**를 찾고 있다. 동차 선형 연립방정식이 비자명 해를 가질 필요충분조건은 계수 행렬, 즉 (A - $\lambda$I)가 **비가역(singular, non-invertible)** 행렬이어야 한다는 것이다.24 만약 (A - 

   $\lambda$I)가 가역(invertible)이라면, 유일한 해는 `v = (A - \lambda I)^{-1}0 = 0`인 자명한 해(trivial solution)뿐이기 때문이다.

2. **행렬식 조건:** 정방행렬이 비가역일 필요충분조건은 그 행렬식이 0이라는 것이다. 따라서 비자명 해 v가 존재하려면 다음 조건이 반드시 만족되어야 한다.
   $$
   \det(A - \lambda I) = 0
   $$
   이 방정식을 행렬 A의 **특성방정식(characteristic equation)**이라고 한다.24


특성방정식 $\det(A - \lambda I) = 0$의 좌변은 $\lambda$에 대한 다항식이며, 이를 **특성다항식(characteristic polynomial)**이라고 한다.28 행렬 A의 고유값들은 바로 이 특성다항식의 근(root)이다.29

따라서 고유값을 찾는 과정은 특성방정식을 세우고, 이 다항방정식을 풀어 모든 근 $\lambda$를 찾는 것으로 귀결된다. $n \times n$ 행렬의 경우, 특성다항식은 n차 다항식이므로, 중근을 포함하여 n개의 고유값을 가진다 (복소수 범위까지 포함).


고유값 $\lambda$를 구했다면, 이제 각 고유값에 대응하는 고유벡터를 찾을 차례이다. 이는 앞서 유도한 동차 연립방정식 `(A - \lambda I)v = 0`으로 돌아가서 해결한다.32

구체적인 고유값(예: $\lambda_1$)을 이 식에 대입하면, (A - $\lambda_1$I)는 모든 원소가 숫자로 이루어진 행렬이 된다. 이제 우리는 이 행렬의 **영공간(null space)**, 즉 `(A - \lambda_1 I)v = 0`을 만족하는 모든 벡터 v의 집합을 찾으면 된다.31 이 영공간에 속하는 모든 0이 아닌 벡터가 바로 고유값 $\lambda_1$에 대응하는 고유벡터이다.

고유벡터는 유일하지 않다. 어떤 고유벡터 v에 0이 아닌 임의의 상수 c를 곱한 cv 역시 같은 고유값에 대한 고유벡터가 된다.3 이는 고유벡터가 '방향'을 나타내기 때문이다. 따라서 고유벡터를 구할 때는 보통 계산이 간편한 정수 벡터를 선택하거나, 크기를 1로 만드는 **정규화(normalization)** 과정을 거친다.15


아래 표는 2x2와 3x3 행렬에 대한 고유값과 고유벡터를 구하는 전체 과정을 단계별로 보여준다.

**표 2.1: 2x2 행렬의 고유값 및 고유벡터 계산**

| 단계  | 설명                                     | 예제: $A = \begin{pmatrix} 2 & 3 \\ 2 & 1 \end{pmatrix}$     |
| ----- | ---------------------------------------- | ------------------------------------------------------------ |
| **1** | **특성방정식 설정**                      | 행렬 `(A - \lambda I)`를 구성한다.  $A - \lambda I = \begin{pmatrix} 2 & 3 \\ 2 & 1 \end{pmatrix} - \lambda \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} 2-\lambda & 3 \\ 2 & 1-\lambda \end{pmatrix}$ |
| **2** | **특성다항식 계산**                      | $\det(A - \lambda I) = 0$을 계산한다.  $(2-\lambda)(1-\lambda) - (3)(2) = 0$  $\lambda^2 - 3\lambda + 2 - 6 = 0$  $\lambda^2 - 3\lambda - 4 = 0$ |
| **3** | **고유값 계산**                          | 특성방정식을 푼다.  $(\lambda-4)(\lambda+1) = 0$  따라서 고유값은 $\lambda_1 = 4$, $\lambda_2 = -1$ 이다. |
| **4** | **고유벡터 계산 (for $\lambda_1 = 4$)**  | $(A - 4I)v = 0$을 푼다.  $\begin{pmatrix} 2-4 & 3 \\ 2 & 1-4 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} -2 & 3 \\ 2 & -3 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$  $-2v_1 + 3v_2 = 0 \implies 2v_1 = 3v_2$.  $v_1=3, v_2=2$로 두면, 고유벡터는 $v_1 = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$ 이다. |
| **5** | **고유벡터 계산 (for $\lambda_2 = -1$)** | $(A - (-1)I)v = 0$ 즉, $(A + I)v = 0$을 푼다.  $\begin{pmatrix} 2+1 & 3 \\ 2 & 1+1 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 3 & 3 \\ 2 & 2 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$  $3v_1 + 3v_2 = 0 \implies v_1 = -v_2$.  $v_1=1, v_2=-1$로 두면, 고유벡터는 $v_2 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$ 이다. |

계산 과정 참조: 16

**표 2.2: 3x3 행렬의 고유값 및 고유벡터 계산**

| 단계  | 설명                                    | 예제: $A = \begin{pmatrix} 1 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1 \end{pmatrix}$ |
| ----- | --------------------------------------- | ------------------------------------------------------------ |
| **1** | **특성방정식 설정**                     | 행렬 `(A - \lambda I)`를 구성한다.  $A - \lambda I = \begin{pmatrix} 1-\lambda & -1 & 0 \\ -1 & 2-\lambda & -1 \\ 0 & -1 & 1-\lambda \end{pmatrix}$ |
| **2** | **특성다항식 계산**                     | $\det(A - \lambda I) = 0$을 계산한다. (예: 1행에 대한 여인수 전개)  $(1-\lambda) \begin{vmatrix} 2-\lambda & -1 \\ -1 & 1-\lambda \end{vmatrix} - (-1) \begin{vmatrix} -1 & -1 \\ 0 & 1-\lambda \end{vmatrix} = 0$  $(1-\lambda)[(2-\lambda)(1-\lambda)-1] + [-(1-\lambda)] = 0$  $(1-\lambda)[\lambda^2-3\lambda+2-1] - (1-\lambda) = 0$  $(1-\lambda)(\lambda^2-3\lambda+1) - (1-\lambda) = 0$  $(1-\lambda)(\lambda^2-3\lambda) = 0 \implies -\lambda(\lambda-1)(\lambda-3) = 0$ |
| **3** | **고유값 계산**                         | 특성방정식을 푼다.  따라서 고유값은 $\lambda_1 = 3$, $\lambda_2 = 1$, $\lambda_3 = 0$ 이다. |
| **4** | **고유벡터 계산 (for $\lambda_1 = 3$)** | $(A - 3I)v = 0$을 푼다.  $\begin{pmatrix} -2 & -1 & 0 \\ -1 & -1 & -1 \\ 0 & -1 & -2 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$  행렬을 행 간소 사다리꼴로 변환하면 $v_1=v_3, v_2=-2v_3$을 얻는다.  $v_3=1$로 두면, 고유벡터는 $v_1 = \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}$ 이다. |
| **5** | **고유벡터 계산 (for $\lambda_2 = 1$)** | $(A - I)v = 0$을 푼다.  $\begin{pmatrix} 0 & -1 & 0 \\ -1 & 1 & -1 \\ 0 & -1 & 0 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$  행렬을 변환하면 $v_2=0, v_1=-v_3$을 얻는다.  $v_3=1$로 두면, 고유벡터는 $v_2 = \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}$ 이다. |
| **6** | **고유벡터 계산 (for $\lambda_3 = 0$)** | $(A - 0I)v = Av = 0$을 푼다.  $\begin{pmatrix} 1 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \\ v_3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$  행렬을 변환하면 $v_1=v_3, v_2=v_3$을 얻는다.  $v_3=1$로 두면, 고유벡터는 $v_3 = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$ 이다. |

계산 과정 참조: 31

이처럼 고유값과 고유벡터를 계산하는 과정은 추상적인 정의를 구체적인 계산 절차로 연결하는 다리 역할을 한다. 이 절차를 이해하고 나면, 어떤 행렬이 주어져도 그 행렬의 핵심적인 특성을 분석할 준비가 된 것이다.

------


고유값과 고유벡터를 계산하는 방법을 익혔으니, 이제 이와 관련된 더 깊은 개념들을 탐구할 시간이다. 이 개념들은 행렬의 구조를 더욱 근본적으로 이해하고, 행렬을 더 간단한 형태로 분해하는 강력한 도구를 제공한다.


하나의 고유값 $\lambda$에 대응하는 고유벡터는 유일하지 않다. 사실, 그 고유값에 대응하는 모든 고유벡터들의 집합에 영벡터를 추가하면, 이는 벡터공간의 성질을 만족하는 **부분공간(subspace)**을 이룬다. 이 부분공간을 고유값 $\lambda$에 대한 **고유공간(eigenspace)**이라고 하며, $E_{\lambda}$로 표기한다.44

수학적으로, $E_{\lambda}$는 동차 연립방정식 $(A - \lambda I)v = 0$의 해공간, 즉 행렬 (A - $\lambda$I)의 **영공간(null space)**과 같다.36 영공간은 항상 부분공간이므로, 고유공간 역시 부분공간의 모든 성질(덧셈에 대해 닫혀 있고, 스칼라 곱에 대해 닫혀 있음)을 만족한다. 기하학적으로 고유공간은 원점을 지나는 직선, 평면, 또는 더 높은 차원의 공간으로 나타난다.


하나의 고유값이 여러 번 나타날 수 있다. 이를 설명하기 위해 두 가지 '중복도' 개념이 사용된다.

- **대수적 중복도 (Algebraic Multiplicity, AM):** 어떤 고유값 $\lambda$가 특성다항식의 근으로 몇 번 나타나는지를 의미한다. 즉, 특성다항식을 인수분해했을 때 `(x-\lambda)` 인수의 지수이다.47 예를 들어, 특성다항식이 

  $(\lambda-2)^3(\lambda-5)=0$이라면, 고유값 2의 대수적 중복도는 3이고, 고유값 5의 대수적 중복도는 1이다.

- **기하적 중복도 (Geometric Multiplicity, GM):** 해당 고유값 $\lambda$에 대한 고유공간 $E_{\lambda}$의 차원이다. 이는 그 고유값에 대응하는 선형 독립인 고유벡터의 최대 개수를 의미한다.47

이 두 중복도 사이에는 매우 중요한 관계가 있다. 어떤 고유값 $\lambda$에 대해서든, 그 기하적 중복도는 대수적 중복도보다 크거나 같을 수 없다.
$$
1 \le \text{GM}(\lambda) \le \text{AM}(\lambda)
$$
이 부등식은 행렬의 가장 중요한 성질 중 하나인 대각화 가능성을 결정하는 핵심 열쇠가 된다.47


많은 경우, 행렬 A가 나타내는 복잡한 선형 변환을 더 단순한 형태로 분해할 수 있다. **대각화(diagonalization)**는 행렬 A를 세 행렬의 곱, $A = PDP^{-1}$로 표현하는 과정이다.14

- D는 대각행렬(diagonal matrix)로, 대각선 원소는 A의 고유값들이고 나머지 원소는 모두 0이다.
- P는 가역행렬(invertible matrix)로, 그 열(column)들은 D의 고유값에 각각 대응하는 A의 고유벡터들이다.
- $P^{-1}$는 P의 역행렬이다.

대각화의 의미와 힘:

대각화는 좌표계의 변환으로 이해할 수 있다. 행렬 P는 표준 좌표계에서 고유벡터로 이루어진 **고유기저(eigenbasis)**로의 '변환'을 수행한다. 이 고유기저에서는 원래의 복잡한 변환 A가 단순히 각 축을 고유값만큼 스케일링하는 대각행렬 D로 표현된다. $P^{-1}$는 다시 원래의 표준 좌표계로 돌아오는 역할을 한다.51 즉, 복잡한 변환을 (1) 고유기저로 이동, (2) 단순한 스케일링, (3) 원래 기저로 복귀라는 세 단계로 분해하는 것이다.

대각화의 가장 큰 장점 중 하나는 행렬의 거듭제곱을 매우 쉽게 계산할 수 있다는 점이다.
$$
A^k = (PDP^{-1})^k = (PDP^{-1})(PDP^{-1})\cdots(PDP^{-1}) = PD(P^{-1}P)D(P^{-1}P)\cdots DP^{-1} = PD^kP^{-1}
$$
대각행렬 D의 거듭제곱 $D^k$는 단순히 각 대각 원소를 k제곱하는 것이므로 계산이 매우 간단하다.49 이 속성은 마르코프 연쇄나 미분방정식 시스템처럼 시간이 지남에 따라 변화하는 시스템을 분석하는 데 결정적인 역할을 한다.1

대각화 가능성 정리 (The Diagonalization Theorem):

$n \times n$ 행렬 A가 대각화 가능하다는 것은 다음 조건들과 동치이다 49:

1. A가 n개의 선형 독립인 고유벡터를 갖는다.
2. 모든 고유값에 대해, 기하적 중복도와 대수적 중복도가 같다 ($\text{GM}(\lambda) = \text{AM}(\lambda)$).54


고유값과 고유벡터의 세계는 다양한 현상을 포함한다. 이를 몇 가지 중요한 범주로 나누어 볼 수 있다.

- **결함 행렬 (Defective Matrices):** 대각화가 불가능한 행렬을 '결함이 있다(defective)'고 말한다.55 이는 적어도 하나의 고유값에 대해 기하적 중복도가 대수적 중복도보다 작은 경우(

  $\text{GM}(\lambda) < \text{AM}(\lambda)$)에 발생한다.57 이는 해당 고유공간에 충분한 수의 선형 독립인 고유벡터가 존재하지 않음을 의미하며, 이 변환이 단순한 스케일링만으로는 설명될 수 없는 '전단(shear)' 효과를 포함하고 있음을 시사한다.12

- **대칭 행렬 (Symmetric Matrices):** $A^T = A$를 만족하는 대칭 행렬은 매우 특별하고 좋은 성질을 가진다.

  - 모든 고유값은 항상 실수이다.11
  - 서로 다른 고유값에 대응하는 고유벡터들은 항상 서로 직교(orthogonal)한다.
  - 항상 **직교 대각화(orthogonally diagonalizable)**가 가능하다. 즉, $A = PDP^T$로 분해할 수 있으며, 여기서 P는 열벡터들이 정규직교(orthonormal)인 직교행렬($P^{-1} = P^T$)이다.59 이 속성은 주성분 분석(PCA)과 같은 분야에서 핵심적인 역할을 한다.

- **복소 고유값 (Complex Eigenvalues):** 실수 성분만 갖는 행렬이라도 복소수인 고유값을 가질 수 있다. 이는 보통 변환이 회전 요소를 포함할 때 나타난다.6 실수 행렬의 복소 고유값은 항상 켤레 복소수 쌍으로 존재한다.

- **0인 고유값 (Zero Eigenvalue):** 고유값 $\lambda=0$은 방정식 $Av = 0v = 0$을 의미한다. 이는 고유벡터 v가 행렬 A의 영공간(null space)에 속하는 0이 아닌 벡터임을 뜻한다.19

  **행렬이 0을 고유값으로 가질 필요충분조건은 그 행렬이 비가역(singular)이라는 것이다**.4 이는 행렬식의 값이 0이라는 것과도 동치이며, 가역 행렬 정리(Invertible Matrix Theorem)의 중요한 한 부분이다.

결론적으로, 대수적 중복도와 기하적 중복도 사이의 관계는 행렬의 '성격'을 결정하는 최종 심판관과 같다. 이 관계는 변환이 순수한 '늘림/줄임'으로 분해될 수 있는지(대각화 가능), 아니면 분해할 수 없는 '비틀림' 요소를 포함하는지(결함 있음)를 판별한다. 이 구분은 단순한 수학적 호기심을 넘어, 시스템의 동적 거동을 이해하는 데 깊은 물리적 의미를 가진다.

------


지금까지 우리는 특성방정식을 직접 풀어 고유값을 찾는 대수적인 방법을 배웠다. 하지만 이 방법은 작은 크기의 행렬에나 적용 가능하며, 현실의 대규모 문제에서는 거의 사용되지 않는다. 이 장에서는 컴퓨터가 실제로 거대 행렬의 고유값을 계산하는 데 사용하는 강력한 수치적 알고리즘들을 소개한다.


왜 우리는 대수적 방법만으로는 부족할까? 그 이유는 아벨-루피니 정리(Abel-Ruffini theorem)에 있다. 이 정리에 따르면, 일반적인 5차 이상의 다항방정식은 사칙연산과 거듭제곱근만으로는 해를 구할 수 있는 일반 공식이 존재하지 않는다. $n \times n$ 행렬의 특성방정식은 n차 다항식이므로, n $\ge$ 5인 행렬에 대해서는 특성방정식의 근, 즉 고유값을 대수적으로 정확히 구하는 것이 불가능하다.61

따라서 실제 공학, 과학, 데이터 분석 문제에서 마주치는 거대한 행렬(수천, 수만 차원)의 고유값을 찾기 위해서는 반드시 **반복적인(iterative)** 수치해석적 방법이 필요하다.


거듭제곱법은 가장 개념적으로 간단하고 직관적인 고유값 알고리즘이다. 이 방법은 행렬의 모든 고유값 중 절댓값이 가장 큰 **주 고유값(dominant eigenvalue)**과 그에 대응하는 고유벡터를 찾아낸다.62

- **알고리즘:**

  1. 임의의 초기 벡터 $b_0$를 선택한다. (보통 정규화된 랜덤 벡터를 사용한다.)
  2. 다음의 반복 계산을 수행한다: $b_{k+1} = A b_k$.
  3. 각 단계마다 벡터가 너무 커지거나 작아지는 것을 막기 위해 $b_{k+1}$을 정규화한다 (예: $b_{k+1} = \frac{A b_k}{\|A b_k\|}$).
  4. k가 충분히 커지면, 벡터 $b_k$는 주 고유값에 대응하는 고유벡터로 수렴한다.
  5. 이때 주 고유값 $\lambda_1$은 레일리 몫(Rayleigh quotient) $\lambda_1 \approx \frac{b_k^T A b_k}{b_k^T b_k}$를 통해 근사할 수 있다.

- 직관적 이해:

  초기 벡터 $b_0$는 모든 고유벡터들의 선형 결합으로 표현될 수 있다. 행렬 A를 반복적으로 곱할 때마다, 각 고유벡터 성분은 해당 고유값만큼 스케일링된다. 주 고유값은 절댓값이 가장 크므로, 이 고유값에 해당하는 고유벡터 성분이 다른 성분들보다 훨씬 빠르게 증폭된다. 수많은 반복을 거치면 다른 성분들의 영향력은 미미해지고, 최종 벡터는 거의 순수한 주 고유벡터 방향을 가리키게 된다.64

- 주요 응용:

  이 단순하지만 강력한 아이디어는 구글의 페이지랭크 알고리즘의 핵심 원리이다. 웹 링크 구조를 나타내는 거대한 행렬에 거듭제곱법을 적용하여 가장 중요한 웹페이지(주 고유벡터의 가장 큰 성분에 해당하는 페이지)를 찾아낸다.63


거듭제곱법이 단 하나의 (가장 큰) 고유값만 찾는 반면, **QR 알고리즘**은 현대 수치 선형대수 라이브러리에서 행렬의 **모든** 고유값을 찾는 표준적인 방법으로 사용되는 매우 강력하고 안정적인 알고리즘이다.61

- **기본 아이디어:**

  1. 초기 행렬을 $A_0 = A$로 둔다.
  2. 반복적으로 k번째 행렬 $A_k$에 대해 **QR 분해**를 수행한다: $A_k = Q_k R_k$.
     - $Q_k$는 직교행렬(orthogonal matrix, $Q_k^T Q_k = I$)이다.
     - $R_k$는 상삼각행렬(upper triangular matrix)이다.
  3. 분해된 두 행렬의 순서를 바꾸어 곱하여 다음 단계의 행렬을 만든다: $A_{k+1} = R_k Q_k$.

- 수렴:

  이 과정을 반복하면, 놀랍게도 행렬 $A_k$는 점차 상삼각행렬(또는 블록 상삼각행렬) 형태로 수렴한다. 수렴이 완료되면, 대각선상의 원소들이 바로 원래 행렬 A의 고유값들이 된다.61 왜냐하면 

  $A_{k+1} = R_k Q_k = (Q_k^T A_k) Q_k = Q_k^T A_k Q_k$ 이므로, 각 단계의 행렬은 서로 닮음(similar) 관계에 있어 항상 같은 고유값을 공유하기 때문이다.

- 알고리즘 가속화:

  실제 구현에서는 수렴 속도를 극적으로 높이기 위해 몇 가지 중요한 기법이 추가된다.

  - **헤센베르크(Hessenberg) 형태로 변환:** QR 분해를 직접 적용하기 전에, 유한한 단계의 연산을 통해 행렬을 헤센베르크 행렬(대각선 바로 아래쪽을 제외한 나머지 하부 삼각형 원소가 모두 0인 행렬)로 변환한다. 이는 각 QR 분해 단계의 계산량을 크게 줄여준다.61
  - **이동(Shifts) 기법:** 매 단계에서 $A_k$ 대신 $A_k - s_k I$와 같은 '이동된' 행렬을 분해한다. 적절한 이동값 $s_k$를 선택하면(보통 현재 행렬의 오른쪽 아래 원소를 사용), 특정 고유값으로의 수렴이 매우 빨라진다.66

QR 알고리즘은 대수적 해법에서 수치적 수렴으로의 패러다임 전환을 상징한다. 이는 현대 응용 수학의 핵심 주제로, 이론적으로 완벽한 해를 찾는 대신, 대규모 문제에 대해 계산적으로 실현 가능한 근사 해를 찾는 것의 중요성을 보여준다.

------


고유값과 고유벡터는 이론적인 개념을 넘어, 다양한 과학 및 공학 분야에서 복잡한 문제를 해결하는 데 사용되는 실용적인 도구이다. 이 장에서는 고유값과 고유벡터가 어떻게 현실 세계의 문제들을 해결하는지 구체적인 사례를 통해 살펴본다. 각 사례에서 우리는 (1) 해결하고자 하는 문제, (2) 문제를 모델링하는 행렬, (3) 고유값과 고유벡터의 해석이라는 일관된 틀로 접근할 것이다.

**표 5.1: 고유값 및 고유벡터 응용 요약**

| 응용 분야                 | 핵심 문제                                      | 관련 행렬                                       | 고유값의 의미                              | 고유벡터의 의미                                          |
| ------------------------- | ---------------------------------------------- | ----------------------------------------------- | ------------------------------------------ | -------------------------------------------------------- |
| **주성분 분석 (PCA)**     | 데이터의 차원 축소 및 핵심 구조 파악           | 공분산 행렬 (Covariance Matrix)                 | 각 주성분이 설명하는 데이터의 분산(정보량) | 주성분(Principal Components), 데이터 분산이 가장 큰 방향 |
| **페이지랭크 (PageRank)** | 웹페이지의 상대적 중요도 순위 결정             | 구글 행렬 (Google Matrix)                       | 시스템의 안정성 (주 고유값 = 1)            | 페이지랭크 벡터, 웹페이지의 중요도 점수                  |
| **양자역학**              | 양자 시스템의 허용된 에너지 준위 결정          | 해밀토니안 연산자 (Hamiltonian Operator)        | 양자화된 에너지 준위                       | 정상 상태(Stationary States)의 파동함수                  |
| **진동 분석**             | 구조물의 공명 방지를 위한 고유 진동수 파악     | 강성/질량 행렬 (Stiffness/Mass Matrix)          | 고유 진동수의 제곱 (진동의 빠르기)         | 모드 형상 (Mode Shapes), 진동 시의 변형 형태             |
| **경제/금융**             | 동적 시스템의 안정성 분석 및 포트폴리오 최적화 | 전이/공분산 행렬 (Transition/Covariance Matrix) | 안정성(크기<1), 리스크의 크기              | 안정 상태, 독립적인 리스크 포트폴리오                    |
| **스펙트럼 군집화**       | 비선형적 데이터의 군집 발견                    | 그래프 라플라시안 (Graph Laplacian)             | 군집의 개수 (0의 개수)                     | 데이터를 선형 분리 가능한 저차원 공간으로 사영           |


- **문제:** 수백, 수천 개의 변수(차원)를 가진 고차원 데이터가 있을 때, 정보 손실을 최소화하면서 데이터를 더 낮은 차원으로 축소하는 방법은 무엇일까? 70
- **행렬:** 데이터의 **공분산 행렬(Covariance Matrix)**. 이 행렬의 각 원소는 두 변수가 함께 어떻게 변하는지를 나타낸다. 대각 원소는 각 변수의 분산을 의미한다.71
- **고유-해석:**
  - **고유벡터:** 공분산 행렬의 고유벡터는 데이터의 **주성분(Principal Components)**이라고 불린다. 이들은 데이터의 분산이 가장 큰 방향을 나타내는 새로운 축들이다. 첫 번째 주성분은 데이터가 가장 넓게 퍼져 있는 방향을 가리킨다.70
  - **고유값:** 각 고유벡터(주성분)에 대응하는 고유값은 그 방향으로 데이터가 얼마나 많이 퍼져있는지, 즉 해당 주성분이 데이터의 전체 분산 중 얼마나 많은 부분을 설명하는지를 나타낸다. 고유값이 클수록 더 중요한 주성분이다.71 PCA는 고유값이 큰 순서대로 몇 개의 주성분만 선택하여 데이터를 저차원 공간에 사영함으로써 차원을 축소한다.


- **문제:** 수십억 개의 웹페이지가 서로 연결된 거대한 네트워크에서 어떤 페이지가 더 중요한지 어떻게 순위를 매길 수 있을까? 7
- **행렬:** 웹의 링크 구조를 기반으로 한 **수정된 인접 행렬(modified adjacency matrix)**, 또는 **구글 행렬(Google Matrix)**. 이 행렬의 각 원소 Aij는 '임의의 웹 서퍼'가 j 페이지에서 i 페이지로 링크를 따라 이동할 확률을 나타낸다. 이 행렬은 마르코프 행렬의 일종이다.7
- **고유-해석:**
  - **고유벡터:** 이 시스템의 **정상 상태 분포(stationary distribution)**를 나타내는 고유벡터가 바로 **페이지랭크 벡터**이다. 이 벡터는 주 고유값 $\lambda=1$에 대응하는 고유벡터이며, 각 원소는 해당 웹페이지의 최종적인 중요도 점수(페이지랭크)를 나타낸다. 즉, 수많은 클릭 끝에 서퍼가 특정 페이지에 도달할 장기적인 확률이다.65
  - **고유값:** 마르코프 행렬의 주 고유값은 항상 **1**이다.6 이는 시스템이 시간이 지남에 따라 유일하고 안정적인 평형 상태(페이지랭크 분포)로 수렴한다는 것을 보장한다.


- **문제:** 원자나 분자와 같은 양자 시스템은 왜 불연속적인, 특정 에너지 값만 가질 수 있을까? 9
- **방정식:** 시간에 무관한 슈뢰딩거 방정식($\hat{H}\psi = E\psi$)은 그 자체가 고유값 방정식이다.8
- **고유-해석:**
  - **연산자(행렬):** **해밀토니안 연산자($\hat{H}$)**는 시스템의 총 에너지(운동 에너지 + 위치 에너지)를 나타내는 선형 연산자이다.
  - **고유벡터:** **파동함수($\psi$)** 또는 **고유상태(eigenstate)**는 시스템이 가질 수 있는 안정적인 상태를 기술한다. 예를 들어, 원자 내 전자의 오비탈이 이에 해당한다.8
  - **고유값:** **에너지(E)**는 해당 고유상태에서 시스템이 가질 수 있는 양자화된(quantized) 에너지 준위를 나타낸다. 시스템은 오직 이러한 고유값에 해당하는 에너지만을 가질 수 있다.8


- **문제:** 다리, 건물, 비행기 날개와 같은 구조물은 외부 힘에 의해 특정 주파수에서 격렬하게 진동하여 파괴될 수 있다(공명 현상). 이 위험한 '고유 진동수'를 어떻게 예측하고 피할 수 있을까? 81
- **방정식:** 다자유도 시스템의 운동 방정식은 $(K - \omega^2 M)x = 0$ 형태의 일반화된 고유값 문제로 표현될 수 있다. 여기서 K는 강성 행렬, M은 질량 행렬이다.81
- **고유-해석:**
  - **고유값:** 고유값은 구조물의 **고유 각진동수의 제곱($\omega^2$)**과 관련이 있다. 이 값들로부터 구조물이 외부 가진력 없이 자연적으로 진동하려는 경향이 있는 고유 진동수를 계산할 수 있다.81


1. Machine Learning & Linear Algebra - Eigenvalue and eigenvector | by Jonathan Hui, accessed July 20, 2025, https://jonathan-hui.medium.com/machine-learning-linear-algebra-eigenvalue-and-eigenvector-f8d0493564c9
2. Eigen Intuitions: Understanding Eigenvectors and Eigenvalues - Towards Data Science, accessed July 20, 2025, https://towardsdatascience.com/eigen-intuitions-understanding-eigenvectors-and-eigenvalues-630e9ef1f719/
3. Chapter 12 Eigenvalues and Eigenvectors | Linear Algebra for Data Science, accessed July 20, 2025, https://shainarace.github.io/LinearAlgebra/eigen.html
4. Eigenvalues and eigenvectors - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors
5. Math Origins: Eigenvectors and Eigenvalues | Mathematical Association of America, accessed July 20, 2025, https://old.maa.org/press/periodicals/convergence/math-origins-eigenvectors-and-eigenvalues
6. Eigenvectors and Eigenvalues explained visually - Setosa.IO, accessed July 20, 2025, https://setosa.io/ev/eigenvectors-and-eigenvalues/
7. PageRank - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/PageRank
8. chem.libretexts.org, accessed July 20, 2025, [https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Physical_Chemistry_(LibreTexts)/03%3A_The_Schrodinger_Equation_and_a_Particle_in_a_Box/3.03%3A_The_Schrodinger_Equation_is_an_Eigenvalue_Problem#:~:text=Both%20time%2Ddependent%20and%20time,levels%20of%20the%20quantum%20system.&text=The%20object%20on%20the%20left,an%20example%20of%20an%20operator.](https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Physical_Chemistry_(LibreTexts)/03%3A_The_Schrodinger_Equation_and_a_Particle_in_a_Box/3.03%3A_The_Schrodinger_Equation_is_an_Eigenvalue_Problem#:~:text=Both time-dependent and time,levels of the quantum system.&text=The object on the left,an example of an operator.)
9. Unlocking Eigenvalues in Quantum Mechanics - Number Analytics, accessed July 20, 2025, https://www.numberanalytics.com/blog/eigenvalues-in-quantum-mechanics-for-quantum-computing
10. Applications of Eigenvalues and Eigenvectors - GeeksforGeeks, accessed July 20, 2025, https://www.geeksforgeeks.org/engineering-mathematics/applications-of-eigenvalues-and-eigenvectors/
11. How to intuitively understand eigenvalue and eigenvector? - Mathematics Stack Exchange, accessed July 20, 2025, https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector
12. I need an intuitive explanation of eigenvalues and eigenvectors - Math Stack Exchange, accessed July 20, 2025, https://math.stackexchange.com/questions/1814197/i-need-an-intuitive-explanation-of-eigenvalues-and-eigenvectors
13. 고유벡터/고유값을 어떻게 직관적으로 이해할 수 있을까? : r/learnmath - Reddit, accessed July 20, 2025, https://www.reddit.com/r/learnmath/comments/10zznq9/how_can_i_intuitively_understand/?tl=ko
14. 고유값, 고유벡터, 고유값 분해 - Ruby, Data - 티스토리, accessed July 20, 2025, https://jaaamj.tistory.com/66
15. 머신러닝 - 19. 고유값(eigenvalue), 고유벡터(eigenvector), 고유값 분해(eigen decomposition) - 귀퉁이 서재 - 티스토리, accessed July 20, 2025, [https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-19-%ED%96%89%EB%A0%AC](https://bkshin.tistory.com/entry/머신러닝-19-행렬)
16. Eigenspace - LearnDataSci, accessed July 20, 2025, https://www.learndatasci.com/glossary/eigenspace/
17. [선형대수] 고유값(eigenvalue), 고유벡터(eigenvector) 의 정의, accessed July 20, 2025, https://rfriend.tistory.com/181
18. Day 1 of Learning Quantum Computing: Linear Algebra & Eigenvectors | by Arpita Mallik, accessed July 20, 2025, https://medium.com/@arpitamallik13/day-1-of-learning-quantum-computing-linear-algebra-eigenvectors-165b3f79c9ac
19. textbooks.math.gatech.edu, accessed July 20, 2025, https://textbooks.math.gatech.edu/ila/eigenvectors.html
20. Visualizing Eigenvalues and Eigenvectors: Geometry Unveiled - YouTube, accessed July 20, 2025, https://www.youtube.com/watch?v=pthF2D6BUyA
21. 7.1: Eigenvalues and Eigenvectors of a Matrix - Mathematics LibreTexts, accessed July 20, 2025, https://math.libretexts.org/Bookshelves/Linear_Algebra/A_First_Course_in_Linear_Algebra_(Kuttler)/07%3A_Spectral_Theory/7.01%3A_Eigenvalues_and_Eigenvectors_of_a_Matrix
22. 고유값과 고유벡터(Eigenvalues and Eigenvectors) - codingfarm - 티스토리, accessed July 20, 2025, https://codingfarm.tistory.com/m/195
23. Linear algebra (eigenvalues and eigenvectors) - Project Rhea, accessed July 20, 2025, https://www.projectrhea.org/rhea/index.php/Linear_algebra_(eigenvalues_and_eigenvectors)
24. 4.6 Eigenvalues and the Characteristic Equation of a Matrix, accessed July 20, 2025, https://ocw.mit.edu/ans7870/18/18.013a/textbook/HTML/chapter04/section06.html
25. How to Find? | Eigenvalues and Eigenvectors - Cuemath, accessed July 20, 2025, https://www.cuemath.com/algebra/eigenvectors/
26. 고유값(eigen value)과 고유벡터(eigen vector) - 생각과 고민., accessed July 20, 2025, https://gguguk.github.io/posts/eigenvalue_eigenvector/
27. Eigenvalues and the Characteristic Equation - Math Stack Exchange, accessed July 20, 2025, https://math.stackexchange.com/questions/491633/eigenvalues-and-the-characteristic-equation
28. Facts About Eigenvalues, accessed July 20, 2025, https://www.adelaide.edu.au/mathslearning/ua/media/120/evalue-magic-tricks-handout.pdf
29. math.libretexts.org, accessed July 20, 2025, [https://math.libretexts.org/Bookshelves/Linear_Algebra/Interactive_Linear_Algebra_(Margalit_and_Rabinoff)/05%3A_Eigenvalues_and_Eigenvectors/5.02%3A_The_Characteristic_Polynomial#:~:text=Theorem%205.2.-,1%3A%20Eigenvalues%20are%20Roots%20of%20the%20Characteristic%20Polynomial,(%CE%BB0)%3D0.](https://math.libretexts.org/Bookshelves/Linear_Algebra/Interactive_Linear_Algebra_(Margalit_and_Rabinoff)/05%3A_Eigenvalues_and_Eigenvectors/5.02%3A_The_Characteristic_Polynomial#:~:text=Theorem 5.2.-,1%3A Eigenvalues are Roots of the Characteristic Polynomial,(λ0)%3D0.)
30. Characteristic polynomial - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/Characteristic_polynomial
31. 19. The characteristic equation If v is an eigenvector with eigenvalue λ then Av = λv. As observed above if we rewrite the RHS - UCSD Math Department, accessed July 20, 2025, https://www.math.ucsd.edu/~jmckerna/Teaching/14-15/Autumn/20F/l_19.pdf
32. [선형대수학] 특성방정식, 고윳값과 고유벡터 구하기 - SUBORATORY, accessed July 20, 2025, https://subprofessor.tistory.com/57
33. Eigenvector - BYJU'S, accessed July 20, 2025, https://byjus.com/maths/eigenvector/
34. How do I find Eigenvectors, explained like I'm 5? : r/learnmath - Reddit, accessed July 20, 2025, https://www.reddit.com/r/learnmath/comments/a0oq8u/how_do_i_find_eigenvectors_explained_like_im_5/
35. A guide to solving linear systems - UNL Math, accessed July 20, 2025, https://www.math.unl.edu/~mbrittenham2/classwk/221s03/inclass/linear_systems.pdf
36. Eigenspace Definition | DeepAI, accessed July 20, 2025, https://deepai.org/machine-learning-glossary-and-terms/eigenspace
37. 고유공간(Eigenspace) - 단아한섭동 - 티스토리, accessed July 20, 2025, https://gosamy.tistory.com/264
38. [선형대수학 #3] 고유값과 고유벡터 (eigenvalue & eigenvector) - 다크 프로그래머 - 티스토리, accessed July 20, 2025, https://darkpgmr.tistory.com/105
39. Why eigenvectors seem incorrect in python? - ResearchGate, accessed July 20, 2025, https://www.researchgate.net/post/Why-eigenvectors-seem-incorrect-in-python
40. Understanding the process of finding eigenvectors - Math Stack Exchange, accessed July 20, 2025, https://math.stackexchange.com/questions/192801/understanding-the-process-of-finding-eigenvectors
41. 7. Eigenvalues and Eigenvectors - Interactive Mathematics, accessed July 20, 2025, https://www.intmath.com/matrices-determinants/7-eigenvalues-eigenvectors.php
42. Eigenvalues and Eigenvectors - matrix - Numeracy, Maths and Statistics - Academic Skills Kit, accessed July 20, 2025, https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/core-mathematics/pure-maths/matrices/eigenvalues-and-eigenvectors.html
43. In this lecture we will find the eigenvalues and eigenvectors of - UCL, accessed July 20, 2025, https://www.ucl.ac.uk/~ucahmdl/LessonPlans/Lesson12.pdf
44. Eigenspace - (Linear Algebra and Differential Equations) - Vocab, Definition, Explanations | Fiveable, accessed July 20, 2025, https://library.fiveable.me/key-terms/linear-algebra-and-differential-equations/eigenspace
45. velog.io, accessed July 20, 2025, [https://velog.io/@guide333/%EA%B3%A0%EC%9C%A0%EA%B0%92%EA%B3%BC-%EA%B3%A0%EC%9C%A0%EB%B2%A1%ED%84%B0%EC%9D%98-%EC%9D%98%EB%AF%B8#:~:text=%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC)-,%EA%B3%A0%EC%9C%A0%EA%B3%B5%EA%B0%84,%EA%B3%B5%EA%B0%84(eigenspace)%EB%9D%BC%EA%B3%A0%20%ED%95%9C%EB%8B%A4.](https://velog.io/@guide333/고유값과-고유벡터의-의미#:~:text=위키백과)-,고유공간,공간(eigenspace)라고 한다.)
46. 선형변환의 고유공간과 기하적 중복도 - 생새우초밥집, accessed July 20, 2025, https://freshrimpsushi.github.io/ko/posts/3349/
47. Algebraic and Geometric Multiplicity - GeeksforGeeks, accessed July 20, 2025, https://www.geeksforgeeks.org/engineering-mathematics/algebraic-and-geometric-multiplicity/
48. Algebraic and geometric multiplicity of eigenvalues - StatLect, accessed July 20, 2025, https://www.statlect.com/matrix-algebra/algebraic-and-geometric-multiplicity-of-eigenvalues
49. 20. Diagonalisation Definition 20.1. Let A and B be two square n × n matrices. We say that A and B are similar if there is an - UCSD Math Department, accessed July 20, 2025, https://www.math.ucsd.edu/~jmckerna/Teaching/14-15/Autumn/20F/l_20.pdf
50. [선형대수학] 4.3 대각화(Diagonalization), 행렬 대각화하기 - 딥러닝 공부방, accessed July 20, 2025, https://deep-learning-study.tistory.com/341
51. How can I intuitively understand eigenvectors/eigenvalues? : r/learnmath - Reddit, accessed July 20, 2025, https://www.reddit.com/r/learnmath/comments/10zznq9/how_can_i_intuitively_understand/
52. vector spaces - Matrix diagonalization. Is $A = PDP^{-1} = P^{-1}DP - Math Stack Exchange, accessed July 20, 2025, https://math.stackexchange.com/questions/3346006/matrix-diagonalization-is-a-pdp-1-p-1dp
53. Section 5.3 Diagonalization - Purdue Math, accessed July 20, 2025, https://www.math.purdue.edu/~zhan4306/spring_2022_la/sec_5_3.pdf
54. Diagonalization, accessed July 20, 2025, https://textbooks.math.gatech.edu/ila/diagonalization.html
55. web.uvic.ca, accessed July 20, 2025, [https://web.uvic.ca/~tbazett/diffyqs/sec_multeigen.html#:~:text=If%20an%20n%20%C3%97%20n,multiplicities%20we%20call%20the%20defect.](https://web.uvic.ca/~tbazett/diffyqs/sec_multeigen.html#:~:text=If an n × n,multiplicities we call the defect.)
56. Defective matrix - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/Defective_matrix
57. Defective eigenvalue - (Linear Algebra and Differential Equations) - Vocab, Definition, Explanations | Fiveable, accessed July 20, 2025, https://library.fiveable.me/key-terms/linear-algebra-and-differential-equations/defective-eigenvalue
58. Defective Matrices: A Deep Dive into Linear Algebra - Number Analytics, accessed July 20, 2025, https://www.numberanalytics.com/blog/defective-matrices-deep-dive-linear-algebra
59. 대각화와 고유값 분해, accessed July 20, 2025, http://contents.kocw.or.kr/contents4/document/lec/2013/Chungbuk/LeeGeonmyeong1/12.pdf
60. Repeated Eigenvalues and Symmetric Matrices, accessed July 20, 2025, https://sheffield.ac.uk/media/32040/download?attachment
61. The QR Algorithm - Ethz, accessed July 20, 2025, https://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter4.pdf
62. Power iteration - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/Power_iteration
63. Power method | Numerical Analysis II Class Notes - Fiveable, accessed July 20, 2025, https://library.fiveable.me/numerical-analysis-ii/unit-4/power-method/study-guide/pKbG5q5tM74b93PN
64. Mastering Power Iteration for Eigenvalues - Number Analytics, accessed July 20, 2025, https://www.numberanalytics.com/blog/ultimate-guide-power-iteration-eigenvalues
65. PageRank Algorithm & Linear Algebra | by Ashutosh Kumar - Medium, accessed July 20, 2025, https://medium.com/@ashu1069/pagerank-algorithm-linear-algebra-cea39a887bd7
66. QR algorithm - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/QR_algorithm
67. Computing Eigenvalues and Eigenvectors using QR Decomposition - andreinc, accessed July 20, 2025, https://www.andreinc.net/2021/01/25/computing-eigenvalues-and-eigenvectors-using-qr-decomposition
68. The QR Method - Python Numerical Methods, accessed July 20, 2025, https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter15.03-The-QR-Method.html
69. Understanding the QR eigenvalue finding algorithm - Mathematics Stack Exchange, accessed July 20, 2025, https://math.stackexchange.com/questions/1196559/understanding-the-qr-eigenvalue-finding-algorithm
70. Principal component analysis - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/Principal_component_analysis
71. The Mathematics Behind Principal Component Analysis (PCA) | by Rishabh Singh | Medium, accessed July 20, 2025, https://medium.com/@RobuRishabh/the-mathematics-behind-principal-component-analysis-pca-1321f6aeb2f7
72. 주성분 분석(PCA) - 공돌이의 수학정리노트 (Angelo's Math Notes), accessed July 20, 2025, https://angeloyeo.github.io/2019/07/27/PCA.html
73. PCA(Principal Component Analysis) - velog, accessed July 20, 2025, https://velog.io/@lighthouse97/PCAPrincipal-Component-Analysis
74. PCA 4: principal components = eigenvectors - YouTube, accessed July 20, 2025, https://www.youtube.com/watch?v=fKivxsVlycs
75. 주성분 분석(PCA)이란 무엇인가요? - IBM, accessed July 20, 2025, https://www.ibm.com/kr-ko/think/topics/principal-component-analysis
76. Lecture #3: PageRank Algorithm - The Mathematics of Google Search, accessed July 20, 2025, https://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html
77. [그래프] The PageRank Citation Ranking: Bringing Order to the Web - WonderLand, accessed July 20, 2025, https://99moments.tistory.com/168
78. "선형대수학과 구글(Google) 검색엔진" - 성균관대학교, accessed July 20, 2025, http://matrix.skku.ac.kr/2012-e-Books/KMS-News-LA-Google-SGLee.pdf
79. Schrödinger equation - Wikipedia, accessed July 20, 2025, [https://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation](https://en.wikipedia.org/wiki/Schrödinger_equation)
80. 5. The Schrödinger Equations - Physics, accessed July 20, 2025, https://physics.weber.edu/schroeder/quantum/Schrodinger.pdf
81. Eigenfrequency Analysis - COMSOL, accessed July 20, 2025, https://www.comsol.com/multiphysics/eigenfrequency-analysis
82. Boundary Conditions of Eigenvalue Analysis [IAD 4] - Midas Civil, accessed July 20, 2025, https://www.midasoft.com/mechanical/blog/intro-to-analysis-for-designers-4
83. 모드해석 - modal analysis - 반디통, accessed July 20, 2025, https://www.banditong.com/cae-dict/modal_analysis
84. 기계진동학 4.2 고유값과 고유진동수 - YouTube, accessed July 20, 2025, https://www.youtube.com/watch?v=lWhsoQZIocw
85. www.numberanalytics.com, accessed July 20, 2025, [https://www.numberanalytics.com/blog/eigenvalues-finance-deep-dive#:~:text=used%20in%20finance%3F-,Eigenvalues%20are%20scalar%20values%20that%20represent%20how%20much%20a%20linear,maximize%20returns%20while%20minimizing%20risk.](https://www.numberanalytics.com/blog/eigenvalues-finance-deep-dive#:~:text=used in finance%3F-,Eigenvalues are scalar values that represent how much a linear,maximize returns while minimizing risk.)
86. Eigenvalues and eigenvectors | Intro to Mathematical Economics Class Notes - Fiveable, accessed July 20, 2025, https://library.fiveable.me/introduction-to-mathematical-economics/unit-2/eigenvalues-eigenvectors/study-guide/R5WPY6HtysDsJZl6
87. Eigenvalues in Economics: A Comprehensive Guide - Number Analytics, accessed July 20, 2025, https://www.numberanalytics.com/blog/ultimate-guide-eigenvalues-linear-algebra-economics
88. Eigenvalues in Finance: A Deep Dive - Number Analytics, accessed July 20, 2025, https://www.numberanalytics.com/blog/eigenvalues-finance-deep-dive
89. Spectral Clustering: The Basics - Medium, accessed July 20, 2025, https://medium.com/@cjunwon.tech/spectral-clustering-the-basics-608720f8124c
90. Spectral Clustering - Machine Learning for Engineers - APMonitor, accessed July 20, 2025, https://apmonitor.com/pds/index.php/Main/SpectralClustering
91. Spectral clustering - Wikipedia, accessed July 20, 2025, https://en.wikipedia.org/wiki/Spectral_clustering
92. Was Euler's theorem in differential geometry motivated by matrices and eigenvalues?, accessed July 20, 2025, https://hsm.stackexchange.com/questions/1799/was-eulers-theorem-in-differential-geometry-motivated-by-matrices-and-eigenvalu
93. www.reddit.com, accessed July 20, 2025, [https://www.reddit.com/r/learnmath/comments/nrz2f1/linear_algebra_confused_about_what_multiplicity/#:~:text=There%20are%20two%20multiplicities%20of,root%20of%20the%20characteristic%20equation.](https://www.reddit.com/r/learnmath/comments/nrz2f1/linear_algebra_confused_about_what_multiplicity/#:~:text=There are two multiplicities of,root of the characteristic equation.)
94. Course Information - University of Pittsburgh - Acalog ACMS™, accessed July 20, 2025, [https://catalog.upp.pitt.edu/content.php?catoid=5&navoid=77&filter%5Bitem_type%5D=3&filter%5Bonly_active%5D=1&filter%5B3%5D=1&filter%5Bcpage%5D=47&expand=1&print](https://catalog.upp.pitt.edu/content.php?catoid=5&navoid=77&filter[item_type]=3&filter[only_active]=1&filter[3]=1&filter[cpage]=47&expand=1&print)
95. 선형대수학 - 나무위키, accessed July 20, 2025, [https://namu.wiki/w/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99](https://namu.wiki/w/선형대수학)

