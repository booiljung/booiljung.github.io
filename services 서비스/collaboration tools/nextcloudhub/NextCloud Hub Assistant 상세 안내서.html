<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:NextCloud Hub Assistant 상세 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>NextCloud Hub Assistant 상세 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">서비스 (Services)</a> / <a href="../index.html">협업 도구</a> / <a href="index.html">NextCloud Hub</a> / <span>NextCloud Hub Assistant 상세 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>NextCloud Hub Assistant 상세 안내서</h1>
<h2>1.  Nextcloud Assistant 서론</h2>
<h3>1.1  개발 철학: 디지털 주권과 온프레미스(On-Premise) AI</h3>
<p>Nextcloud Assistant의 개발 철학은 ’디지털 주권(Digital Sovereignty)’이라는 핵심 가치에 깊이 뿌리내리고 있다. 이는 사용자와 조직이 자신의 데이터를 완벽하게 통제하고, 제3자 기술 제공업체의 종속에서 벗어나 독립적인 디지털 인프라를 구축할 권리를 의미한다.1 현대의 AI 기술, 특히 대규모 언어 모델(LLM)은 Microsoft 365 Copilot이나 Google Workspace의 Gemini와 같이 소수의 빅테크 기업이 주도하는 클라우드 기반 서비스에 집중되어 있다.3 이러한 중앙 집중식 모델은 편리함을 제공하지만, 사용자의 민감한 데이터가 외부 서버로 전송되어 AI 모델 훈련에 사용될 수 있다는 심각한 개인정보 보호 문제를 야기한다.4</p>
<p>Nextcloud는 이러한 시장 동향에 대한 근본적인 대안을 제시하고자 Assistant를 설계했다. Assistant의 핵심 목표는 사용자가 자신의 서버, 즉 온프레미스 환경에서 AI의 모든 기능을 활용할 수 있도록 하는 것이다.4 이를 통해 문서, 이메일, 채팅 메시지 등 조직의 가장 중요한 데이터가 외부로 유출될 위험 없이(no data leaks) 안전하게 보호된다.4 이 접근 방식은 단순한 기술적 선택을 넘어, 데이터 통제권을 사용자에게 돌려주려는 이념적 선언과 같다.</p>
<p>이러한 철학을 구체화하고 사용자에게 실질적인 선택 기준을 제공하기 위해 Nextcloud는 ’Ethical AI’라는 전략적 프레임워크를 도입했다. 시장의 지배적인 AI 솔루션들이 데이터 프라이버시에 대한 우려를 낳는 상황에서, Nextcloud는 ’디지털 주권’과 ’개인정보 보호’를 핵심 가치로 내세워 차별화를 시도한다.1 그러나 이러한 추상적인 가치는 사용자가 기술을 선택하는 데 직접적인 도움이 되기 어렵다. 따라서 Nextcloud는 ’Ethical AI Rating’이라는 독자적인 평가 시스템을 개발하여 이 문제를 해결했다.8 이 평가 시스템은 AI 모델의 투명성, 개방성, 데이터 처리 방식 등을 기준으로 등급을 부여함으로써, 관리자가 조직의 보안 정책과 규정 준수 요구사항에 부합하는 AI 기술을 정보에 입각하여 선택할 수 있도록 지원한다. 결과적으로 Ethical AI Rating은 단순한 기능 설명이 아니라, 규제가 엄격한 공공 부문이나 유럽 연합(EU) 내 기업과 같이 데이터 주권을 최우선으로 여기는 특정 고객층을 겨냥한 핵심적인 전략적 자산이자 신뢰 구축 메커니즘으로 작용한다.</p>
<h3>1.2  아키텍처 개요: 모듈성과 유연성</h3>
<p>Nextcloud Assistant의 아키텍처는 개발 철학인 유연성과 데이터 통제권을 기술적으로 구현하기 위해 고도로 모듈화된 구조를 채택하고 있다. Assistant는 단일 기능이 아니라, 여러 앱이 유기적으로 결합하여 작동하는 하나의 생태계이다.10</p>
<p>이 아키텍처의 중심에는 <code>assistant</code> 앱이 있다. 이 앱은 사용자가 AI 기능과 상호작용하는 그래픽 사용자 인터페이스(GUI)와 핵심 프레임워크를 제공한다.10 그러나 <code>assistant</code> 앱 자체는 텍스트 요약이나 이미지 생성과 같은 실제 AI 연산을 수행하지 않는다. 이러한 기능들은 별도의 백엔드 앱을 통해 구현된다. 예를 들어, 로컬 대규모 언어 모델을 사용한 텍스트 처리는 <code>llm2</code> 앱이 담당하고, 음성-텍스트 변환은 <code>stt_whisper2</code> 앱이, 사용자의 데이터에 대한 문맥 기반 채팅은 <code>context_chat</code> 앱이 처리하는 방식이다.10</p>
<p>모든 AI 관련 요청은 Nextcloud 코어에 통합된 ’AI Task Processing API’를 통해 표준화된 방식으로 처리된다.10 Mail, Text, Talk와 같은 다양한 애플리케이션은 이 통합 API를 호출하여 일관된 방식으로 AI 기능을 요청할 수 있다. 작업이 요청되면, API는 관리자가 설정한 백엔드 앱으로 해당 작업을 전달하여 처리하고 결과를 반환한다.</p>
<p>이러한 모듈식 구조는 관리자에게 전례 없는 유연성을 제공한다. 조직의 보안 정책과 예산에 따라, 모든 데이터를 내부 서버에서 처리하는 완전한 온프레미스 솔루션(예: <code>llm2</code> 앱을 통한 자체 호스팅 모델)을 구축할 수도 있고, 특정 작업을 외부 클라우드 기반 서비스(AI-as-a-Service, AIaaS)에 위임할 수도 있다.3 이처럼 사용자가 자신의 필요에 맞게 AI 백엔드를 자유롭게 선택하고 조합할 수 있다는 점이 Nextcloud Assistant 아키텍처의 가장 큰 특징이다.</p>
<h2>2.  핵심 기능 및 역량</h2>
<h3>2.1  AI 에이전시(AI Agency): 능동적인 비서</h3>
<p>Nextcloud Assistant 3.0에서 도입된 AI 에이전시 기능은 Assistant를 단순한 정보 제공 도구에서 벗어나 사용자를 대신하여 실제 작업을 수행하는 능동적인 비서로 격상시켰다.13 이는 단일 명령에 대한 응답을 넘어, 여러 Nextcloud 애플리케이션에 걸쳐 있는 다단계 워크플로우를 자율적으로 계획하고 실행하는 능력을 의미한다.7</p>
<p>AI 에이전시는 사용자의 자연어 요청을 해석하여 복잡한 작업 순서로 분해하고, 각 단계를 수행하는 데 필요한 앱과 데이터에 접근한다. 예를 들어, 사용자가 “이 PDF 문서를 요약하고, 그 결과를 ’마케팅팀’에게 이메일로 공유한 뒤, 내일 오후 2시에 관련 논의를 위한 회의를 잡아줘“라고 요청할 수 있다. 이 경우 Assistant는 다음과 같은 일련의 작업을 순차적으로 수행한다 13:</p>
<ol>
<li>
<p><strong>정보 수집 (Files)</strong>: 지정된 PDF 파일에 접근하여 내용을 읽는다.</p>
</li>
<li>
<p><strong>작업 수행 (Text Processing)</strong>: LLM 백엔드를 사용하여 텍스트를 요약한다.</p>
</li>
<li>
<p><strong>정보 공유 (Mail)</strong>: 요약된 내용을 본문으로 하는 이메일을 작성하고, 주소록(Contacts)에서 ’마케팅팀’의 이메일 주소를 찾아 수신자로 지정한 후 발송한다.</p>
</li>
<li>
<p><strong>일정 생성 (Calendar)</strong>: 지정된 시간에 회의를 생성하고, 관련 팀원들을 초대한다.</p>
</li>
</ol>
<p>이 기능은 Nextcloud 내부 앱에만 국한되지 않는다. 외부 서비스와의 연동도 가능하여, “내 동료의 위치를 기반으로 현재 날씨를 알려줘“와 같은 요청을 처리할 수 있다.13 또한, 외부 프로젝트 관리 도구와의 통합도 가능하다. 예를 들어, “지난주에 수신된 모든 이메일에서 ’기능 요청’이라는 키워드가 포함된 내용을 찾아, 각각을 OpenProject에 새로운 태스크로 생성하고 팀원들에게 견적을 요청해줘“와 같은 복잡한 비즈니스 프로세스 자동화를 구현할 수 있다.13 이처럼 AI 에이전시는 반복적인 관리 업무를 자동화하여 사용자의 생산성을 극대화하는 강력한 도구이다.</p>
<h3>2.2  문맥 인식 인텔리전스: Context Chat과 Context Write</h3>
<p>Nextcloud Assistant의 핵심적인 차별점은 사용자의 데이터라는 ’문맥(context)’을 깊이 이해하고 활용하는 능력에 있다. 이는 Context Chat과 Context Write라는 두 가지 주요 기능을 통해 구현된다.</p>
<p><strong>Context Chat</strong>은 사용자가 자신의 Nextcloud 인스턴스 내에 저장된 파일, 문서, 보고서, 노트 등 방대한 데이터에 대해 자연어로 질문하고 지능적인 답변을 얻을 수 있는 기능이다.7 이는 일반적인 챗봇처럼 외부의 일반 지식을 제공하는 것이 아니라, 조직 내부의 특정 데이터에 기반한 맞춤형 정보를 제공한다. 예를 들어, “지난 4분기 마케팅 보고서의 주요 성과 지표를 요약해줘” 또는 “A 프로젝트 계약서에서 지불 조건과 관련된 조항을 찾아줘“와 같은 질문에 답할 수 있다. 사용자는 수동으로 검색 범위를 특정 폴더나 파일로 제한하여 답변의 정확도와 관련성을 높일 수 있다.7 기술적으로 Context Chat은 AppAPI를 통해 배포되는 외부 앱(ExApps)에 의존하므로, 이 기능을 사용하기 위해서는 관리자의 사전 구성이 필수적이다.3</p>
<p><strong>Context Write</strong>는 한 걸음 더 나아가, 기존 문서의 문맥을 활용하여 새로운 콘텐츠를 생성하는 기능이다.6 이 기능은 단순히 정보를 복사하는 것이 아니라, 참조된 문서의 고유한 톤, 문체, 단어 선택, 심지어 구조까지 학습하여 새로운 텍스트에 일관성 있게 적용한다.4 예를 들어, 사용자가 자신의 이력서 파일과 최근 프로젝트 경험에 대한 몇 가지 핵심 사항을 제공하면, Assistant는 기존 이력서의 형식을 유지하면서 새로운 경력을 자연스럽게 통합한 업데이트된 이력서를 생성해준다. 또 다른 예로, 회의록 녹취 파일과 과거에 작성했던 보고서 샘플을 입력으로 제공하면, Assistant는 회의 내용을 바탕으로 기존 보고서와 유사한 형식과 스타일을 갖춘 새로운 보고서 초안을 자동으로 작성할 수 있다.4</p>
<h3>2.3  콘텐츠 생성 및 처리</h3>
<p>Assistant는 다양한 형태의 디지털 콘텐츠를 생성하고 처리하는 강력한 기능을 제공하여 사용자의 창의성과 생산성을 지원한다.</p>
<p><strong>텍스트 처리</strong>는 Assistant의 가장 기본적인 동시에 광범위하게 활용되는 기능이다. 사용자는 간단한 명령을 통해 다양한 텍스트 조작 작업을 수행할 수 있다. 여기에는 긴 문서를 몇 개의 문장으로 압축하는 ‘요약’, 글의 핵심을 담은 ‘헤드라인 생성’, 문장의 의미는 유지하되 표현을 바꾸는 ‘텍스트 재구성(reword)’, 글의 분위기를 공식적이거나 비공식적으로 바꾸는 ‘어조 변경’, 그리고 문법 및 철자 오류를 수정하는 ‘교정’ 등이 포함된다.7 이러한 기능들은 문서 작성, 이메일 회신, 보고서 준비 등 일상적인 업무의 효율을 크게 향상시킨다.</p>
<p><strong>이미지 생성</strong> 기능은 사용자가 텍스트 프롬프트(prompt)를 입력하여 고유한 시각적 콘텐츠를 만들어내는 기능이다.7 “미래 도시의 모습을 사이버펑크 스타일로 그려줘“와 같은 구체적인 요구사항을 입력하면, Assistant는 연결된 이미지 생성 모델을 통해 해당 설명에 부합하는 이미지를 생성한다. 이 기능은 프레젠테이션 자료, 소셜 미디어 게시물, 문서 삽화 등을 제작할 때 외부 스톡 이미지 사이트를 검색하는 시간을 절약해주고, 저작권 문제 없이 맞춤형 이미지를 확보할 수 있게 해준다. Nextcloud는 Stable Diffusion, DALL-E 등 다양한 온프레미스 및 클라우드 기반 이미지 생성 모델과의 연동을 지원한다.11</p>
<p><strong>오디오 전사(Transcription)</strong> 기능은 음성을 텍스트로 변환하는 역할을 한다.7 사용자는 회의, 강의, 인터뷰 등을 녹음한 오디오 파일을 업로드하거나, 실시간으로 마이크에 대고 말하여 그 내용을 텍스트로 변환할 수 있다. 이 기능은 주로 OpenAI가 개발한 강력한 음성 인식 모델인 Whisper를 기반으로 하며, 높은 정확도로 음성 데이터를 검색 가능하고 편집 가능한 텍스트로 전환해준다.11 이는 회의록 작성, 인터뷰 내용 정리, 영상 자막 제작 등의 작업을 자동화하는 데 매우 유용하다.</p>
<h3>2.4  앱 통합: 워크플로우의 중심</h3>
<p>Nextcloud Assistant의 진정한 강점은 독립적인 기능이 아니라, Nextcloud Hub를 구성하는 여러 핵심 애플리케이션에 깊숙이 통합되어 사용자의 워크플로우를 끊김 없이 지원한다는 점에 있다.</p>
<ul>
<li>
<p><strong>Text</strong>: 문서 편집기인 Text 앱에서는 모든 문단 옆에 작은 ✨ 아이콘이 표시된다.4 사용자가 텍스트를 선택하고 이 아이콘을 클릭하면, 현재 작업 중인 문서의 맥락을 벗어나지 않고도 즉시 해당 텍스트를 요약, 번역, 또는 재구성할 수 있다. 결과물은 클릭 한 번으로 문서에 바로 삽입할 수 있어, 편집 과정의 흐름을 방해하지 않는다.7</p>
</li>
<li>
<p><strong>Mail</strong>: Mail 앱에서 Assistant는 지능적인 이메일 관리 비서 역할을 한다. 길고 복잡한 이메일 스레드를 간결하게 요약하여 사용자가 빠르게 핵심 내용을 파악하도록 돕는다.4 또한, 수신된 이메일의 내용을 분석하여 상황에 맞는 답장 초안을 여러 개 제안해주며, 사용자는 이 중 하나를 선택하여 신속하게 회신할 수 있다.4 더 나아가, 발신자가 답장을 기대하는지 여부를 판단하여 후속 조치가 필요한 메일을 별도의 ‘Follow up’ 섹션으로 자동 분류해준다.4</p>
</li>
<li>
<p><strong>Talk</strong>: 실시간 협업 도구인 Talk에서 Assistant는 커뮤니케이션의 장벽을 허문다. 다국적 팀원 간의 대화에서 채팅 메시지를 실시간으로 번역하여 원활한 소통을 지원한다.4 또한, 화상 통화 내용을 자동으로 녹취하고 요약하여 회의에 참석하지 못한 팀원도 주요 논의 사항을 쉽게 따라잡을 수 있게 한다.16 사용자는 스마트 피커(<code>/ai</code>) 명령어를 통해 채팅창에서 직접 텍스트나 이미지를 생성하여 대화에 활용할 수도 있다.4</p>
</li>
<li>
<p><strong>Calendar</strong>: Calendar 앱과의 통합은 일정 관리를 자동화한다. 예를 들어, Mail 앱에서 회의 일정을 조율하는 이메일을 받으면, Assistant가 해당 내용을 기반으로 자동으로 Calendar에 이벤트를 생성한다.4 이때 이벤트 제목, 설명, 참석자 목록뿐만 아니라 회의 안건 초안까지 제안하여 회의 준비 시간을 대폭 단축시킨다. 또한, 참석자들의 기존 일정을 분석하여 모두가 참여 가능한 최적의 시간을 지능적으로 제안하는 기능도 제공한다.7</p>
</li>
<li>
<p><strong>Office</strong>: Nextcloud Office의 문서, 스프레드시트, 프레젠테이션 편집기 내에서도 Assistant의 기능을 직접 사용할 수 있다. 편집기 상단 툴바나 ‘삽입’ 탭을 통해 텍스트 생성, 번역, 교정 기능을 호출하여, 별도의 창을 열지 않고도 문서 작업의 효율성을 높일 수 있다.18</p>
</li>
</ul>
<h2>3.  Ethical AI 프레임워크</h2>
<h3>3.1  Ethical AI Rating 이해하기</h3>
<p>Nextcloud는 AI 기술 도입에 따른 윤리적 문제와 데이터 프라이버시 위험을 사용자가 명확히 인지하고 통제할 수 있도록 ’Ethical AI Rating’이라는 독자적인 평가 시스템을 개발했다.8 이는 단순히 AI 기능을 제공하는 것을 넘어, 각 기능이 사용자의 데이터 주권을 얼마나 존중하는지에 대한 투명한 정보를 제공하려는 노력의 일환이다. 이 평가 시스템은 관리자와 사용자가 자신의 조직이 추구하는 가치와 보안 정책에 부합하는 AI 도구를 선택할 수 있도록 돕는 중요한 의사결정 프레임워크 역할을 한다.</p>
<p>Ethical AI Rating은 다음 세 가지 핵심 기준을 바탕으로 AI 솔루션을 평가한다 5:</p>
<ol>
<li>
<p><strong>소프트웨어의 오픈 소스 여부</strong>: AI 모델을 실행하는 추론(inferencing) 소프트웨어와 모델을 훈련하는(training) 소프트웨어가 모두 오픈 소스인지 평가한다. 이는 코드의 투명성과 사용자의 수정 및 통제 가능성을 보장하는 기본 전제이다.</p>
</li>
<li>
<p><strong>훈련된 모델의 자유로운 셀프 호스팅 가능 여부</strong>: 사전에 훈련된 AI 모델을 사용자가 자신의 서버에 자유롭게 다운로드하여 설치하고 운영(셀프 호스팅)할 수 있는지 평가한다. 이는 데이터가 외부 서버로 전송되는 것을 원천적으로 차단하고 완전한 데이터 통제를 가능하게 하는 핵심 요소이다.</p>
</li>
<li>
<p><strong>훈련 데이터의 공개 및 자유로운 사용 가능 여부</strong>: AI 모델을 훈련시키는 데 사용된 데이터셋이 공개되어 있고, 사용자가 이를 자유롭게 접근하고 사용할 수 있는지 평가한다. 이는 모델에 내재된 편향(bias)을 분석하고, 필요 시 특정 목적에 맞게 모델을 재훈련하거나 미세 조정(fine-tuning)할 수 있는 가능성을 열어준다.</p>
</li>
</ol>
<p>이 세 가지 기준의 충족 여부에 따라 AI 기능은 네 가지 등급 중 하나를 부여받는다 8:</p>
<ul>
<li>
<p><code>Green 🟢</code>: 세 가지 조건을 모두 충족하는 경우. 최고의 투명성과 데이터 통제권을 제공하며, 사용자가 AI 시스템의 모든 측면을 직접 검토하고 운영할 수 있다.</p>
</li>
<li>
<p><code>Yellow 🟡</code>: 세 가지 조건 중 두 가지를 충족하는 경우. 일반적으로 소프트웨어와 모델은 개방되어 있으나, 훈련 데이터가 비공개인 경우가 많다. 데이터는 온프레미스에서 처리되지만, 모델의 편향성이나 내부 작동 원리를 완전히 파악하기는 어렵다.</p>
</li>
<li>
<p><code>Orange 🟠</code>: 세 가지 조건 중 한 가지만 충족하는 경우. 보통 외부 상용 서비스를 연동하는 경우로, 모델은 자유롭게 사용할 수 있지만 소프트웨어나 훈련 데이터는 비공개이며, 데이터가 외부로 전송될 수 있다.</p>
</li>
<li>
<p><code>Red 🔴</code>: 세 가지 조건 중 어느 것도 충족하지 못하는 경우. 완전한 독점(proprietary) 솔루션으로, 소프트웨어, 모델, 훈련 데이터가 모두 비공개이며 데이터는 외부 서비스 제공업체에 의해 처리된다. 사용자의 통제권이 가장 제한된다.</p>
</li>
</ul>
<h3>3.2  AI 기능 및 제공자별 등급 분석</h3>
<p>Nextcloud에서 사용 가능한 주요 AI 기능과 이를 제공하는 백엔드 솔루션들은 Ethical AI Rating에 따라 명확히 구분된다. 이 등급 시스템은 관리자가 기술을 선택할 때 성능이나 기능뿐만 아니라, 데이터 처리 방식과 그에 따른 위험 수준을 종합적으로 고려할 수 있도록 돕는다. 중요한 점은 이 등급이 AI의 성능이나 결과물의 품질을 의미하는 것이 아니라, ’데이터 통제 수준’과 ’투명성’을 나타내는 지표라는 것이다. 예를 들어, ‘Red’ 등급의 외부 서비스가 ‘Yellow’ 등급의 로컬 모델보다 특정 작업에서 더 나은 성능을 보일 수 있다. 따라서 관리자는 이 등급을 ’성능’이 아닌 ’위험 관리’의 척도로 이해하고, 조직의 보안 정책과 규정 준수 요구사항(예: GDPR)에 따라 적절한 트레이드오프를 결정해야 한다.</p>
<p>예를 들어, 한 관리자가 조직에 기계 번역 기능을 도입하려 한다고 가정해보자. 만약 조직의 규정상 민감한 데이터가 EU 외부로 전송되어서는 안 된다면, 아래 표를 통해 ’Local Machine Translation 2 (ExApp)’가 Green 등급으로 데이터를 온프레미스에 유지하는 반면, ’DeepL integration’은 Red 등급으로 데이터가 외부로 전송됨을 즉시 확인할 수 있다. 이를 통해 관리자는 기술적 선택을 조직의 정책과 손쉽게 일치시킬 수 있다.</p>
<p>다음 표는 주요 AI 기능과 제공자별 Ethical AI Rating을 요약한 것이다.8</p>
<table><thead><tr><th>기능 (Feature)</th><th>제공 앱/서비스 (App / Provider)</th><th>등급 (Rating)</th><th>핵심 모델 (Key Model)</th><th>데이터 위치 (Data Location)</th><th>비고 (Notes)</th></tr></thead><tbody>
<tr><td>텍스트 처리</td><td><code>Local large language model 2 (llm2)</code></td><td>Green</td><td>Llama 3.1</td><td>On-Premise</td><td>완전한 데이터 주권 확보</td></tr>
<tr><td>텍스트 처리</td><td><code>OpenAI and LocalAI integration</code> (via OpenAI API)</td><td>Red</td><td>GPT-4 등</td><td>OpenAI Cloud</td><td>데이터가 외부로 전송됨</td></tr>
<tr><td>텍스트 처리</td><td><code>OpenAI and LocalAI integration</code> (via LocalAI)</td><td>Yellow</td><td>Llama 등</td><td>On-Premise</td><td>훈련 데이터 비공개</td></tr>
<tr><td>기계 번역</td><td><code>Local Machine Translation 2 (translate2)</code></td><td>Green</td><td>MADLAD</td><td>On-Premise</td><td>완전한 데이터 주권 확보</td></tr>
<tr><td>기계 번역</td><td><code>DeepL integration</code></td><td>Red</td><td>DeepL</td><td>DeepL Cloud</td><td>데이터가 외부로 전송됨</td></tr>
<tr><td>음성-텍스트</td><td><code>Local Whisper Speech-To-Text 2 (stt_whisper2)</code></td><td>Yellow</td><td>Whisper</td><td>On-Premise</td><td>모델은 오픈 소스이나 훈련 데이터는 비공개</td></tr>
<tr><td>이미지 생성</td><td><code>Local Stable Diffusion</code></td><td>Yellow</td><td>StableDiffusion XL</td><td>On-Premise</td><td>모델은 오픈 소스이나 훈련 데이터는 비공개</td></tr>
<tr><td>이미지 생성</td><td><code>OpenAI and LocalAI integration</code> (via OpenAI API)</td><td>Red</td><td>DALL·E</td><td>OpenAI Cloud</td><td>데이터가 외부로 전송됨</td></tr>
<tr><td>Context Chat</td><td><code>Nextcloud Assistant Context Chat</code></td><td>Yellow</td><td>-</td><td>On-Premise</td><td>훈련 데이터 비공개</td></tr>
<tr><td>Context Agent</td><td><code>Nextcloud Context Agent</code></td><td>Green</td><td>-</td><td>On-Premise</td><td>완전한 데이터 주권 확보</td></tr>
</tbody></table>
<h2>4.  설치 및 관리</h2>
<h3>4.1  시스템 요구 사항</h3>
<p>Nextcloud Assistant를 성공적으로 배포하고 운영하기 위해서는 서버의 시스템 요구 사항을 정확히 이해하고 준비하는 것이 중요하다. 요구 사항은 일반적인 Nextcloud 서버 운영과 로컬 AI 모델 호스팅의 두 가지 경우로 나누어 고려해야 한다.</p>
<p><strong>일반 Nextcloud 서버 요구 사항</strong>은 Assistant를 외부 AI 서비스(AIaaS)와 연동하여 사용할 때 적용된다. 권장 사양은 다음과 같다 22:</p>
<ul>
<li>
<p><strong>CPU</strong>: 64비트 아키텍처</p>
</li>
<li>
<p><strong>메모리</strong>: 프로세스당 최소 512MB RAM 권장. 사용자 수, 활성화된 앱, 서버 활동량에 따라 증가해야 한다.</p>
</li>
<li>
<p><strong>운영 체제</strong>: Ubuntu 24.04 LTS 또는 Red Hat Enterprise Linux 9 권장.</p>
</li>
<li>
<p><strong>데이터베이스</strong>: MariaDB 10.11+ 또는 MySQL 8.4+ 권장. InnoDB 스토리지 엔진 사용은 필수이다.</p>
</li>
<li>
<p><strong>웹 서버</strong>: Apache 2.4 (mod_php 또는 php-fpm) 또는 Nginx (php-fpm).</p>
</li>
<li>
<p><strong>PHP</strong>: 8.3 버전 권장. 필요한 PHP 모듈이 모두 설치되어 있어야 한다.</p>
</li>
</ul>
<p><strong>로컬 AI 모델 호스팅 요구 사항</strong>은 <code>llm2</code>와 같은 앱을 사용하여 온프레미스 환경에서 직접 대규모 언어 모델을 실행할 때 필요한 추가 사양이다. 이는 상당한 시스템 자원을 요구하므로 신중한 계획이 필요하다 23:</p>
<ul>
<li>
<p><strong>시스템 RAM</strong>: 최소 12GB 이상. 모델의 크기에 따라 더 많은 메모리가 필요할 수 있다.</p>
</li>
<li>
<p><strong>CPU</strong>: AVX 및 AVX2 명령어 세트를 지원하는 x86_64 CPU. 코어 수가 많고 성능이 높을수록 추론 속도가 향상된다.</p>
</li>
<li>
<p><strong>GPU</strong>: NVIDIA GPU 사용이 강력히 권장된다. 최소 8GB 이상의 VRAM을 가진 GPU가 필요하며, 호스트 시스템에는 CUDA v12.4 이상이 설치되어 있어야 한다.</p>
</li>
<li>
<p><strong>분리된 환경</strong>: 로컬 AI 모델은 기본적으로 사용 가능한 모든 CPU 코어를 점유하려는 경향이 있으므로, Nextcloud 애플리케이션 서버와 물리적으로 분리된 별도의 전용 서버에서 AI 작업을 실행하는 것이 성능 안정성을 위해 권장된다.5</p>
</li>
</ul>
<h3>4.2  설치 및 초기 설정</h3>
<p>Nextcloud Assistant와 관련 AI 기능의 설치는 Nextcloud의 앱 관리 시스템을 통해 이루어진다. 기본적인 설치 절차는 다음과 같다.</p>
<ol>
<li>
<p><strong>관리자 계정으로 로그인</strong>: Nextcloud 웹 인터페이스에 관리자 권한을 가진 계정으로 로그인한다.</p>
</li>
<li>
<p><strong>앱 스토어 이동</strong>: 화면 우측 상단의 프로필 아이콘을 클릭한 후, 드롭다운 메뉴에서 ’앱’을 선택하여 앱 관리 페이지로 이동한다.</p>
</li>
<li>
<p><strong>Assistant 앱 설치</strong>: 검색창에 ’assistant’를 입력하여 <code>Nextcloud Assistant</code> 앱을 찾는다. ‘다운로드 및 활성화’ 버튼을 클릭하여 설치를 완료한다.10</p>
</li>
<li>
<p><strong>백엔드 앱 설치</strong>: 사용하려는 AI 기능에 따라 필요한 백엔드 앱을 추가로 설치한다. 예를 들어, 로컬 텍스트 처리를 원한다면 <code>Local large language model (llm2)</code>를, 문맥 기반 채팅을 원한다면 <code>Context Chat</code>과 <code>Context Chat Backend</code>를 설치해야 한다.10 각 앱은</p>
</li>
</ol>
<p><code>assistant</code> 앱과 동일한 방식으로 검색하여 설치할 수 있다.</p>
<p>서버에 직접 접근할 수 있는 경우, <code>occ</code> (ownCloud Console) 명령줄 도구를 사용하여 앱을 활성화할 수도 있다. 이는 서버 자동화 스크립트에 유용하다. <code>assistant</code> 앱을 활성화하는 명령어는 다음과 같다 10:</p>
<pre><code class="language-sh">sudo -E -u www-data php occ app:enable assistant
</code></pre>
<p>여기서 <code>www-data</code>는 웹 서버가 실행되는 사용자 계정이며, 시스템 환경에 따라 다를 수 있다. 다른 백엔드 앱들도 동일한 <code>app:enable</code> 명령어를 사용하여 활성화할 수 있다.</p>
<h3>4.3  관리자 구성</h3>
<p>Assistant 및 관련 AI 기능의 설치가 완료되면, 관리자는 조직의 요구사항에 맞게 세부적인 구성을 진행해야 한다. 모든 관련 설정은 관리자 설정 메뉴의 ‘인공지능(Artificial intelligence)’ 섹션에서 찾을 수 있다.10</p>
<p>주요 구성 작업은 다음과 같다:</p>
<ul>
<li><strong>백엔드 선택</strong>: 이 섹션에서는 설치된 모든 AI 백엔드 앱과 이들이 제공하는 작업 유형(Task Type) 목록을 확인할 수 있다. 관리자는 ‘기계 번역’, ‘텍스트 처리’, ‘이미지 생성’ 등 각 작업 유형에 대해 어떤 백엔드 앱을 사용할지 지정해야 한다.10 예를 들어, 텍스트 처리 작업에는</li>
</ul>
<p><code>llm2</code>를, 이미지 생성에는 <code>integration_openai</code>를 할당할 수 있다.</p>
<ul>
<li>
<p><strong>기능 활성화/비활성화</strong>: 특정 AI 기능을 조직 전체에서 사용하지 못하도록 전역적으로 비활성화할 수 있다. 예를 들어, 이미지 생성 기능이 조직의 정책에 부합하지 않는 경우 해당 작업 유형을 비활성화하면, 사용자 인터페이스에서 관련 메뉴가 사라진다. <code>occ</code> 명령어를 사용하면 더 세밀한 제어가 가능하다 10:</p>
<pre><code class="language-sh"># 특정 작업 유형(ID로 식별)을 비활성화(0) 또는 활성화(1)
sudo -E -u www-data php occ taskprocessing:task-type:set-enabled $TASK_TYPE_ID 0
</code></pre>
</li>
</ul>
<pre><code>
- **스마트 피커 관리**: 스마트 피커는 Text나 Talk와 같은 앱에서 `/` 문자를 입력했을 때 나타나는 빠른 AI 명령어 실행 도구이다. 관리자는 '인공지능' 설정 페이지 하단에서 또는 `occ` 명령어를 사용하여 특정 스마트 피커(예: 텍스트 생성, 이미지 생성)의 표시 여부를 제어할 수 있다.10

  ```sh
  # 텍스트-이미지 스마트 피커 비활성화
  sudo -E -u www-data php occ config:app:set assistant text_to_image_picker_enabled --value=0 --type=string
</code></pre>
<ul>
<li><strong>UI 요소 제어</strong>: 웹 인터페이스 우측 상단에 표시되는 Assistant 바로가기 버튼의 표시 여부도 <code>occ</code> 명령어를 통해 제어할 수 있다.10</li>
</ul>
<h3>4.4  성능 최적화: AI 작업자(Worker) 구성</h3>
<p>Nextcloud Assistant, 특히 온프레미스 AI 모델을 사용하는 환경에서 최상의 성능과 사용자 경험을 제공하기 위해서는 AI 작업 처리 방식을 최적화하는 것이 매우 중요하다.</p>
<p>Nextcloud의 기본 백그라운드 작업 처리 방식은 시스템 cron을 통해 <code>cron.php</code> 스크립트를 주기적으로 (보통 5분마다) 실행하는 것이다. AI 작업 역시 이 시스템을 통해 처리되도록 기본 설정되어 있다. 하지만 이 방식은 실시간 상호작용이 중요한 AI 챗봇 기능에는 치명적인 약점을 가진다. 사용자가 질문을 입력한 후, 해당 작업이 처리되기까지 최대 5분의 지연이 발생할 수 있다.3 이로 인해 사용자는 기능이 멈추거나 오류가 발생했다고 오인하게 되며, 이는 ’417 Expectation Failed’와 같은 HTTP 오류로 나타나기도 한다.25 사용자가 반복적으로 요청을 재시도하면 시스템 부하는 가중되고, 결국 기능 자체에 대한 신뢰도가 하락하게 된다.</p>
<p>이러한 문제를 해결하기 위해, Nextcloud는 전용 AI 작업자(worker)를 구성하는 방법을 제공한다. 이는 선택 사항이 아니라, 원활한 Assistant 운영을 위한 필수 구성으로 간주되어야 한다. 전용 작업자는 <code>systemd</code>와 같은 서비스 관리자를 통해 상시 실행되는 프로세스로, AI 작업 큐에 새로운 작업이 등록되는 즉시 이를 감지하고 처리한다. 이를 통해 cron 주기에 관계없이 거의 실시간에 가까운 응답성을 확보할 수 있다.3</p>
<p>관리자는 사용자에게 Assistant 기능을 공개하기 전에 반드시 전용 AI 작업자를 구성해야 한다. TrueNAS SCALE 환경에서 Docker 컨테이너로 실행되는 Nextcloud를 위한 <code>systemd</code> 서비스 구성 예시는 다음과 같다. 먼저, 지속적으로 <code>background-job:worker</code> 명령을 실행하는 셸 스크립트(<code>nextcloud-ai-worker.sh</code>)를 작성한다 25:</p>
<pre><code class="language-sh">#!/bin/bash
CONTAINER_NAME="your-nextcloud-container-name"
while true; do
  docker exec -u www-data ${CONTAINER_NAME} php occ background-job:worker -t 60 'OC\TaskProcessing\SynchronousBackgroundJob'
  sleep 1
done
</code></pre>
<p>그 다음, 이 스크립트를 백그라운드에서 실행하고 시스템 부팅 시 자동으로 시작되도록 <code>systemd</code> 서비스 유닛 파일(<code>/etc/systemd/system/nextcloud-ai-worker.service</code>)을 생성한다. 이 구성을 통해 관리자는 AI 작업 처리의 지연 문제를 근본적으로 해결하고, 사용자에게 빠르고 안정적인 Assistant 경험을 제공할 수 있다.</p>
<h2>5.  AI 모델 연동 및 사용자 정의</h2>
<h3>5.1  온프레미스 vs. AI-as-a-Service (AIaaS)</h3>
<p>Nextcloud Assistant의 AI 기능을 구동하기 위한 백엔드 솔루션은 크게 두 가지 접근 방식으로 나뉜다: 모든 것을 자체 서버에서 운영하는 ‘온프레미스’ 방식과 외부 전문 업체의 서비스를 구독하여 사용하는 ‘AI-as-a-Service(AIaaS)’ 방식이다. 각 방식은 명확한 장단점을 가지고 있으므로, 관리자는 조직의 예산, 기술 역량, 그리고 무엇보다 데이터 보안 정책에 따라 전략적인 선택을 해야 한다.</p>
<p><strong>온프레미스 방식</strong>(<code>llm2</code>, <code>LocalAI</code> 등)은 데이터 주권과 개인정보 보호를 최우선으로 하는 조직에 가장 이상적인 선택이다. 모든 데이터 처리 과정이 내부 네트워크 안에서 이루어지므로, 민감한 정보가 외부로 유출될 위험이 원천적으로 차단된다.26 또한, 초기 하드웨어 투자 이후에는 별도의 구독 비용이 발생하지 않아 장기적인 비용 통제에 유리하다. 하지만 이는 높은 수준의 기술적 전문성을 요구한다. 대규모 언어 모델을 운영하기 위한 고사양의 서버(특히 GPU)를 구축하고 유지보수해야 하며, 모델 설정, 최적화, 문제 해결 등 복잡한 관리 업무를 직접 수행해야 한다.23</p>
<p><strong>AIaaS 방식</strong>(<code>integration_openai</code>, OVHcloud, IONOS 등)은 신속한 도입과 편의성에 중점을 둔다. 복잡한 하드웨어 구성이나 모델 관리 없이, API 키를 발급받아 간단한 설정만으로 강력한 AI 기능을 즉시 사용할 수 있다.5 항상 최신 고성능 모델을 사용할 수 있다는 장점도 있다. 그러나 이는 데이터가 외부 서비스 제공업체의 서버로 전송됨을 의미하며, 이는 엄격한 데이터 보안 규정을 가진 조직에게는 수용하기 어려운 위험일 수 있다.3 또한, 사용량에 따라 지속적으로 구독 비용이 발생하므로 총소유비용(TCO)이 증가할 수 있다.</p>
<p>다음 표는 각 AI 백엔드 제공자 유형의 특징을 비교하여 관리자의 의사결정을 돕는다.</p>
<table><thead><tr><th>백엔드 (Backend)</th><th>주요 사용 사례</th><th>개인정보보호 (최고/중간/최저)</th><th>성능 (높음/가변적)</th><th>비용 (초기투자/구독)</th><th>설정 복잡도 (높음/중간/낮음)</th></tr></thead><tbody>
<tr><td><code>llm2</code> (On-Premise)</td><td>텍스트 처리</td><td>최고</td><td>하드웨어 의존적</td><td>높은 초기 투자</td><td>높음</td></tr>
<tr><td><code>LocalAI</code> (On-Premise)</td><td>다목적 (텍스트, 이미지 등)</td><td>최고</td><td>하드웨어 의존적</td><td>높은 초기 투자</td><td>매우 높음</td></tr>
<tr><td><code>integration_openai</code></td><td>고성능 텍스트/이미지</td><td>최저</td><td>높음</td><td>구독 (사용량 기반)</td><td>낮음</td></tr>
<tr><td><code>AIaaS</code> (OVH, IONOS)</td><td>주권 클라우드 AI</td><td>중간</td><td>높음</td><td>구독</td><td>낮음</td></tr>
</tbody></table>
<h3>5.2  로컬 LLM 구성 (llm2 및 LocalAI)</h3>
<p>온프레미스 AI를 구축하기로 결정했다면, LocalAI와 같은 로컬 추론 엔진을 설정하고 Nextcloud와 연동하는 과정이 필요하다. LocalAI는 OpenAI API와 호환되는 인터페이스를 제공하여 다양한 오픈 소스 모델을 로컬에서 실행할 수 있게 해주는 강력한 도구이다.24</p>
<p>설정 과정은 다음과 같은 단계로 진행된다 24:</p>
<ol>
<li>
<p><strong>LocalAI 서비스 설치 및 실행</strong>: 별도의 서버에 LocalAI를 설치하고, 사용할 LLM 모델(예: Llama, Vicuna)을 다운로드하여 로드한다. 보안을 강화하기 위해 환경 변수를 통해 API 키(<code>LOCALAI_API_KEY</code>)를 설정하는 것이 권장된다.</p>
</li>
<li>
<p><strong>Nextcloud에 연동 앱 설치</strong>: Nextcloud 앱 스토어에서 <code>OpenAI and LocalAI integration</code> 앱을 설치하고 활성화한다.</p>
</li>
<li>
<p><strong>연결된 계정(Connected Accounts) 구성</strong>: Nextcloud 관리자 설정의 ‘연결된 계정’ 메뉴로 이동하여 ‘OpenAI and LocalAI integration’ 항목을 찾는다.</p>
</li>
<li>
<p><strong>서비스 정보 입력</strong>: ‘서비스 URL’ 필드에 실행 중인 LocalAI 서버의 주소(예: <code>http://&lt;localai-server-ip&gt;:8080</code>)를 입력한다. ’서비스 이름’은 구분을 위한 별칭이므로 자유롭게 지정한다.</p>
</li>
<li>
<p><strong>인증 정보 입력</strong>: ‘인증’ 섹션의 ‘API 키’ 필드에 LocalAI 서비스에서 설정한 <code>LOCALAI_API_KEY</code>와 동일한 값을 입력한다. API 키를 설정하지 않았다면 비워둘 수 있다.</p>
</li>
<li>
<p><strong>AI 설정에서 제공자 지정</strong>: 마지막으로, 관리자 설정의 ‘인공지능’ 섹션으로 돌아가 텍스트 처리, 이미지 생성 등 각 기능에 대해 방금 설정한 LocalAI 서비스를 제공자로 선택한다.</p>
</li>
</ol>
<p>이 과정을 통해 Nextcloud Assistant는 모든 AI 요청을 외부 클라우드가 아닌 내부 LocalAI 서버로 보내게 되며, 완전한 온프레미스 AI 환경이 구축된다.</p>
<h3>5.3  외부 API 연동</h3>
<p>외부 AI 서비스의 강력한 성능과 편의성을 활용하고자 할 경우, Nextcloud는 다양한 외부 API와의 연동을 지원한다.</p>
<p>Nextcloud의 <code>integration_openai</code> 앱은 기본적으로 OpenAI API 사양과 호환되는 엔드포인트를 사용하도록 설계되었다.3 이는</p>
<p><code>/v1/chat/completions</code>와 같은 특정 API 경로를 통해 요청을 보낸다는 의미이다. 따라서 OpenAI, Microsoft Azure OpenAI, 그리고 Ollama와 같이 OpenAI 호환 API를 제공하는 서비스들은 별도의 수정 없이 직접 연동이 가능하다.3</p>
<p>하지만 Amazon Bedrock, Google Vertex AI, Anthropic의 Claude와 같이 자체적인 고유 API 구조를 사용하는 서비스들은 직접 연동할 수 없다. 이러한 비호환성 문제를 해결하기 위해 <code>LiteLLM Proxy</code>와 같은 미들웨어 프록시를 도입해야 한다.3 LiteLLM Proxy는 ‘번역 계층(translation layer)’ 역할을 수행한다. 즉, Nextcloud Assistant가 보낸 OpenAI 형식의 API 요청을 가로채서, 목표 서비스(예: Amazon Bedrock)가 이해할 수 있는 형식으로 변환하여 전달하고, 그 응답을 다시 OpenAI 형식으로 변환하여 Nextcloud에 반환한다. 이 미들웨어를 사용하면 사실상 100개 이상의 다양한 LLM 및 추론 제공업체와 Nextcloud Assistant를 유연하게 통합할 수 있다.</p>
<h3>5.4 주요 테이블 3: Assistant 관련 주요 <code>occ</code> 명령어 참조</h3>
<p>Nextcloud 서버 관리자는 웹 UI 외에도 <code>occ</code> 명령줄 인터페이스를 통해 Assistant 관련 설정을 효율적으로 관리하고 자동화할 수 있다. 다음 표는 관리자가 반드시 알아야 할 주요 <code>occ</code> 명령어들을 정리한 것이다. 이 명령어들을 셸 스크립트나 Ansible과 같은 자동화 도구와 함께 사용하면, 서버 구성 작업을 일관되고 반복 가능하게 만들어 운영 효율성을 크게 향상시킬 수 있다.</p>
<table><thead><tr><th>명령어 (Command)</th><th>설명 (Description)</th><th>예시 (Example)</th></tr></thead><tbody>
<tr><td><code>app:enable assistant</code></td><td>Assistant 앱을 활성화한다.</td><td><code>... occ app:enable assistant</code></td></tr>
<tr><td><code>app:disable assistant</code></td><td>Assistant 앱을 비활성화한다.</td><td><code>... occ app:disable assistant</code></td></tr>
<tr><td><code>config:app:set assistant assistant_enabled --value=0</code></td><td>상단 바의 Assistant 버튼을 모든 사용자에게서 비활성화한다.</td><td><code>... occ... --value=0 --type=string</code></td></tr>
<tr><td><code>config:app:set assistant free_prompt_picker_enabled --value=1</code></td><td>텍스트 생성 스마트 피커(<code>/ai generate text</code>)를 활성화한다.</td><td><code>... occ... --value=1 --type=string</code></td></tr>
<tr><td><code>taskprocessing:task:list</code></td><td>현재 처리 중이거나 대기 중인 모든 AI 작업을 나열한다.</td><td><code>... occ taskprocessing:task:list</code></td></tr>
<tr><td><code>taskprocessing:task-type:set-enabled $ID 0</code></td><td>ID로 식별된 특정 AI 작업 유형(예: 요약)을 비활성화한다.</td><td><code>... occ...:set-enabled 5 0</code></td></tr>
<tr><td><code>config:app:set assistant chat_last_n_messages --value=10</code></td><td>‘AI와 채팅’ 시 컨텍스트로 고려할 최근 메시지 수를 설정한다.</td><td><code>... occ... --value=10</code></td></tr>
<tr><td><code>config:app:set assistant max_image_generation_idle_time --value=90</code></td><td>생성된 이미지가 조회되지 않을 경우 보관할 기간(일)을 설정한다.</td><td><code>... occ... --value=90 --type=integer</code></td></tr>
</tbody></table>
<h2>6.  실제 사용 사례 및 워크플로우</h2>
<p>Nextcloud Assistant는 다양한 앱과의 유기적인 통합을 통해 이론적인 기능을 넘어 실제 업무 환경에서 강력한 생산성 향상 도구로 활용될 수 있다.</p>
<h3>6.1  커뮤니케이션 향상 (Talk &amp; Mail)</h3>
<p>팀 커뮤니케이션은 협업의 핵심이며, Assistant는 이 과정에서 발생하는 비효율을 크게 줄여준다. 예를 들어, 휴가에서 복귀한 직원은 수백 개의 읽지 않은 Talk 메시지와 긴 이메일 스레드에 직면할 수 있다. 이때 Assistant를 사용하여 각 대화방의 주요 논의 내용과 이메일 스레드의 핵심 결정을 요약본으로 받아보면, 몇 시간 걸릴 작업을 단 몇 분 만에 완료하고 빠르게 업무에 복귀할 수 있다.4</p>
<p>또한, 다국적 팀과의 협업 시 언어 장벽은 큰 걸림돌이 될 수 있다. Talk 채팅 중 Assistant의 실시간 번역 기능을 사용하면, 각자 자신의 모국어로 메시지를 입력하더라도 상대방에게는 번역된 내용이 표시되어 원활한 의사소통이 가능하다.4 외부 파트너로부터 받은 중요한 이메일에 신속하게 회신해야 할 경우, Assistant가 제안하는 여러 답장 초안 중 가장 적절한 것을 선택하고 약간만 수정하여 응답 시간을 획기적으로 단축할 수 있다.4</p>
<h3>6.2  정리 및 계획 간소화 (Calendar &amp; Deck)</h3>
<p>프로젝트 관리와 일정 조율은 종종 많은 수작업을 동반한다. Assistant는 이러한 과정을 자동화하여 사용자가 더 중요한 본질적인 업무에 집중할 수 있도록 돕는다. 예를 들어, 클라이언트가 Mail을 통해 새로운 프로젝트 요구사항과 함께 회의를 요청하는 이메일을 보냈다고 가정하자. 사용자는 이 이메일을 기반으로 Assistant에게 “이 내용으로 Calendar에 회의를 잡고, 관련 내용을 Deck 보드의 ‘신규 프로젝트’ 목록에 카드로 추가해줘“라고 지시할 수 있다. Assistant는 이메일 내용을 분석하여 회의 안건 초안을 작성하고 Calendar 이벤트를 생성하며, 동시에 Deck에 상세 내용이 포함된 새로운 태스크 카드를 만들어준다.4</p>
<p>반복적인 프로젝트를 진행할 때도 Assistant는 유용하다. 과거에 성공적으로 완료한 프로젝트의 Deck 보드를 Assistant에게 복제하도록 지시하면, 기존의 태스크 목록, 레이블, 기한 설정 등을 그대로 가져와 새로운 프로젝트를 신속하게 시작할 수 있다.16 이는 프로젝트 셋업에 드는 시간을 절약하고, 표준화된 프로세스를 유지하는 데 도움을 준다.</p>
<h3>6.3  콘텐츠 제작 가속화 (Files &amp; Office)</h3>
<p>보고서, 제안서, 프레젠테이션 등 다양한 콘텐츠를 제작하는 과정은 많은 시간과 노력을 필요로 한다. Assistant는 이 과정을 가속화하는 창의적인 파트너 역할을 한다. 예를 들어, 분기별 실적 보고서를 작성해야 할 때, 사용자는 Files에 저장된 이전 분기 보고서와 최근 실적 데이터가 담긴 스프레드시트를 Assistant에게 제공하며 “이 자료들을 바탕으로 Context Write 기능을 사용해 새로운 분기 보고서 초안을 작성해줘“라고 요청할 수 있다.4 Assistant는 기존 보고서의 형식과 스타일을 유지하면서 새로운 데이터를 반영한 초안을 생성하여, 사용자가 처음부터 글을 쓰는 부담을 덜어준다.</p>
<p>Nextcloud Office에서 프레젠테이션을 제작하는 도중, 특정 슬라이드에 어울리는 이미지가 필요하다면, “데이터 분석 그래프를 시각화하는 추상적인 이미지“와 같이 프롬프트를 입력하여 즉시 이미지를 생성하고 슬라이드에 삽입할 수 있다.7 또한, 조직의 지식 베이스를 관리하는 Collectives 페이지에 관련 프로젝트의 Deck 보드나 팀의 일정이 담긴 Calendar를 대화형 위젯으로 삽입하여, 모든 관련 정보를 한 곳에서 통합적으로 확인하고 관리할 수 있다.29</p>
<h2>7.  문제 해결 및 알려진 한계</h2>
<h3>7.1  일반적인 문제 및 해결 방안</h3>
<p>Nextcloud Assistant는 강력한 기능을 제공하지만, 복잡한 아키텍처로 인해 다양한 문제가 발생할 수 있다. 관리자는 일반적인 문제 유형과 해결 방안을 숙지하고 있어야 한다.</p>
<ul>
<li>
<p><strong>‘417 Expectation Failed’ 오류</strong>: 이 오류는 LocalAI와 같은 온프레미스 백엔드와 연동할 때 가장 빈번하게 발생하는 문제이다. 주된 원인은 Nextcloud의 기본 백그라운드 작업 처리 방식(cron)의 지연 때문이다. AI 작업이 즉시 처리되지 않고 큐에 대기하면서 타임아웃이 발생하는 것이다. 이 문제의 근본적인 해결책은 제4.4장에서 상세히 설명한 바와 같이, <code>systemd</code> 등을 사용하여 전용 AI 작업자(worker)를 구성하여 AI 작업을 실시간으로 처리하도록 하는 것이다.25</p>
</li>
<li>
<p><strong>UI 버그 및 이슈</strong>: GitHub의 <code>nextcloud/assistant</code> 저장소 이슈 트래커에는 사용자들이 보고한 다양한 버그와 기능 개선 요청이 등록되어 있다. 여기에는 오디오 채팅 기능의 불안정한 작동, UI 요소가 서버 테마 색상을 따르지 않는 문제, <code>context_agent</code> 활성화 시 LLM으로 사용자 지침이 제대로 전송되지 않는 오류 등이 포함된다.30 특정 문제가 발생했을 때, 가장 먼저 공식 이슈 트래커를 검색하여 유사한 사례나 해결책이 있는지 확인하는 것이 좋다.</p>
</li>
<li>
<p><strong>구성 오류</strong>: 로컬 추론 엔진(LocalAI, Ollama 등)에 연결할 때 접속 오류가 발생한다면, Nextcloud의 <code>config/config.php</code> 파일에 다음 설정이 누락되었을 수 있다. 이 설정은 Nextcloud 서버가 로컬 네트워크 내의 다른 서버로 요청을 보낼 수 있도록 허용한다.31</p>
<pre><code class="language-PHP">'allow_local_remote_servers' =&gt; true,
</code></pre>
</li>
</ul>
<pre><code>
- **로그 확인**: 문제의 원인을 파악하기 위한 가장 기본적인 단계는 로그 파일을 분석하는 것이다. 관리자는 다음 로그 파일들을 우선적으로 확인해야 한다 32:

- **Nextcloud 로그**: Nextcloud 데이터 디렉토리 내 `data/nextcloud.log`

- **웹 서버 오류 로그**: (예: Apache) `/var/log/apache2/error.log`

- **PHP-FPM 로그**: 시스템 구성에 따라 위치가 다름

### 7.2  로컬 모델의 성능 및 한계


온프레미스 AI 모델을 운영하는 것은 데이터 주권을 확보하는 가장 확실한 방법이지만, 그에 따르는 기술적 한계와 제약을 명확히 인지해야 한다.

- **하드웨어 제약**: 대규모 언어 모델은 막대한 계산 자원을 필요로 한다. 특히 고성능 GPU 없이는 모델의 추론 속도가 현저히 느려져, 사용자가 실시간으로 응답을 받기 어려울 수 있다.23 CPU만으로 운영할 경우, 간단한 요청에도 수십 초 이상이 소요될 수 있어 사용자 경험이 크게 저하될 수 있다.

- **모델 자체의 한계**: LLM은 방대한 텍스트 데이터를 기반으로 확률적으로 가장 그럴듯한 다음 단어를 예측하는 방식으로 작동한다. 이로 인해 몇 가지 본질적인 한계를 가진다.

- **환각(Hallucination)**: 모델이 사실이 아닌 정보를 마치 사실인 것처럼 자신감 있게 생성하는 현상이 발생할 수 있다.

- **추론 및 수학 능력 부족**: 복잡한 논리적 추론이나 정확한 수학적 계산에 취약한 경향이 있다.

- 이러한 이유로, AI가 생성한 결과물은 중요한 의사결정에 직접 사용되어서는 안 되며, 항상 인간이 검토하고 수정하는 과정을 거쳐야 한다. AI의 결과물은 최종 결과물이 아닌, 업무를 시작하기 위한 '초안'으로 활용하는 것이 가장 바람직하다.23

- **에너지 소비**: LLM을 구동하는 데는 상당한 양의 전력이 소모된다. 이는 서버 운영 비용 증가로 이어진다. 서버 부하와 에너지 소비를 줄이고자 한다면, 성능은 다소 저하되더라도 파라미터 수가 더 적은 소형 모델이나, 정밀도를 낮춰 모델 크기를 줄인 양자화(quantized) 모델을 사용하는 것을 고려할 수 있다.23

### 7.3  로그 분석 및 디버깅


Assistant 관련 문제 해결의 핵심은 정확한 로그 분석에 있다. 문제 발생 시, 관리자는 `config/config.php` 파일에서 디버그 모드를 활성화하여 더 상세한 로그를 수집해야 한다.32

```PHP
'debug' =&gt; true,
'loglevel' =&gt; 0, // 0은 DEBUG 레벨
</code></pre>
<p>디버그 모드를 활성화하면 <code>data/nextcloud.log</code> 파일에 애플리케이션의 내부 동작에 대한 훨씬 더 자세한 정보가 기록된다. 로그를 분석할 때는 오류 메시지와 함께 기록되는 ’Request ID’를 주목해야 한다. 이 ID를 통해 특정 사용자 요청과 관련된 모든 로그 항목을 필터링하여 문제의 흐름을 추적할 수 있다.32</p>
<p>웹 인터페이스에서 발생하는 문제의 경우, 브라우저의 개발자 도구(F12)를 열어 ‘콘솔(Console)’ 탭에서 자바스크립트 오류가 있는지 확인하고, ‘네트워크(Network)’ 탭에서 특정 API 요청이 실패(예: 4xx, 5xx 상태 코드)하는지 확인하는 것도 중요하다.</p>
<p>데스크톱 클라이언트와의 동기화 문제가 의심될 경우, 클라이언트 설정에서 로그 파일을 활성화하고 해당 로그(<code>nextcloud.cfg</code> 파일 위치는 OS마다 다름)를 분석하여 서버와의 통신 문제를 진단할 수 있다.33 문제 해결이 완료되면, 보안 및 성능상의 이유로 반드시 디버그 모드를 다시 비활성화(<code>'debug' =&gt; false,</code>)해야 한다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Integrating AI into your business without selling your data to Big Tech - Nextcloud, https://nextcloud.com/blog/integrating-ai-into-your-business-without-selling-your-data-to-big-tech/</li>
<li>Ethical use of AI: five major challenges Nextcloud, https://nextcloud.com/blog/ethical-use-of-ai-5-major-challenges/</li>
<li>Integrating Nextcloud AI Assistant with Inference Engines &amp; APIs - Autoize, https://autoize.com/integrating-nextcloud-ai-assistant-with-inference-engines-apis/</li>
<li>Meet the first open-source AI assistant that doesn’t prey on your data - Nextcloud, https://nextcloud.com/blog/first-open-source-ai-assistant/</li>
<li>Nextcloud releases Assistant 2.0 and pushes AI-as-a-Service, https://nextcloud.com/blog/nextcloud-releases-assistant-2-0-and-pushes-ai-as-a-service/</li>
<li>Nextcloud - Open source content collaboration platform, https://nextcloud.com/</li>
<li>The first local AI assistant built into a collaboration platform - Nextcloud, https://nextcloud.com/assistant/</li>
<li>AI in Nextcloud: what, why and how - Nextcloud, https://nextcloud.com/blog/ai-in-nextcloud-what-why-and-how/</li>
<li>Nextcloud Ethical AI Rating, https://nextcloud.com/blog/nextcloud-ethical-ai-rating/</li>
<li>Nextcloud Assistant — Nextcloud latest Administration Manual latest documentation, https://docs.nextcloud.com/server/latest/admin_manual/ai/app_assistant.html</li>
<li>Nextcloud Assistant - GitHub, https://github.com/nextcloud/assistant</li>
<li>Nextcloud features that put you in control, https://nextcloud.com/features/</li>
<li>Nextcloud Assistant 3.0: AI Agency for efficient collaboration - YouTube, https://www.youtube.com/watch?v=NCIIwiYnp84&amp;pp=0gcJCf8Ao7VqN5tD</li>
<li>Nextcloud AI Assistant 2.0 - sovereign, AI-powered workspace with …, https://www.youtube.com/watch?v=kMl9OdP10EY</li>
<li>Nextcloud Assistant - Apps - App Store, https://apps.nextcloud.com/apps/assistant</li>
<li>Nextcloud Hub 10 – your unified, modular digital workspace …, https://nextcloud.com/blog/nextcloud-hub10/</li>
<li>Regain control over your time with Nextcloud Hub 8, https://nextcloud.com/blog/nextcloud-hub8/</li>
<li>Hub 10 Nextcloud Assistant 3.0: Transcripts and rewriting of documents with fully transparent AI - YouTube, https://www.youtube.com/watch?v=BO7N_TvaNGA</li>
<li>Nextcloud introduces Ethical AI Rating for user transparency and security - Sendent, https://sendent.com/nextcloud-introduces-ethical-ai-rating-for-user-transparency-and-security/</li>
<li>Open-source AI models that give you privacy back - Nextcloud, https://nextcloud.com/blog/how-open-source-ai-models-can-help-you-take-control-of-your-privacy/</li>
<li>Overview of AI features - Nextcloud Documentation, https://docs.nextcloud.com/server/latest/admin_manual/ai/overview.html</li>
<li>System requirements — Nextcloud latest Administration Manual …, https://docs.nextcloud.com/server/latest/admin_manual/installation/system_requirements.html</li>
<li>App: Local large language model (llm2) - Nextcloud Documentation, https://docs.nextcloud.com/server/latest/admin_manual/ai/app_llm2.html</li>
<li>How to Setup the Nextcloud Assistant with a LocalAI Service - RCA Systems, https://rcasys.com/en/blog/how-to-setup-the-nextcloud-assistant-with-a-localai-service</li>
<li>Persistent 417 Nextcloud Assistant and LocalAI on NC 30.0.5 …, https://help.nextcloud.com/t/persistent-417-nextcloud-assistant-and-localai-on-nc-30-0-5-despite-common-solutions/218954</li>
<li>Create Your Own Local AI Assistant for Enhanced Privacy, https://www.cognativ.com/blogs/post/create-your-own-local-ai-assistant-for-enhanced-privacy/271</li>
<li>Hub 10 Nextcloud Assistant 3.0: Run your own ethical AI services - YouTube, https://www.youtube.com/watch?v=TUsrqBzPhSc</li>
<li>Nextcloud Flow makes it easy to automate workflows and actions, https://nextcloud.com/blog/nextcloud-flow-makes-it-easy-to-automate-actions-and-workflows/</li>
<li>Guide to interactive widgets in Nextcloud Hub, https://nextcloud.com/blog/guide-to-interactive-widgets-in-nextcloud-hub/</li>
<li>Issues · nextcloud/assistant - GitHub, https://github.com/nextcloud/assistant/issues</li>
<li>Nextcloud Assistant Not Working Properly After Latest Nextcloud Update - Cloudron Forum, https://forum.cloudron.io/topic/13888/nextcloud-assistant-not-working-properly-after-latest-nextcloud-update</li>
<li>General troubleshooting — Nextcloud latest Administration Manual latest documentation, https://docs.nextcloud.com/server/latest/admin_manual/issues/general_troubleshooting.html</li>
<li>Troubleshooting — Nextcloud latest Administration Manual latest documentation, https://docs.nextcloud.com/server/latest/admin_manual/desktop/troubleshooting.html</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>