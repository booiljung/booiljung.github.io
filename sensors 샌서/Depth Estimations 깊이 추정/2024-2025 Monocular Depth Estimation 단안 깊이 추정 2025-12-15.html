<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:2024-2025 단안 깊이 추정(Monocular Depth Estimation)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>2024-2025 단안 깊이 추정(Monocular Depth Estimation)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">센서 (Sensors)</a> / <a href="index.html">깊이 추정</a> / <span>2024-2025 단안 깊이 추정(Monocular Depth Estimation)</span></nav>
                </div>
            </header>
            <article>
                <h1>2024-2025 단안 깊이 추정(Monocular Depth Estimation)</h1>
<p>2025-12-15, G30DR</p>
<h2>1.  서론: 컴퓨터 비전의 기하학적 르네상스</h2>
<p>단안 깊이 추정(Monocular Depth Estimation, MDE)은 단 한 장의 2차원 RGB 이미지로부터 3차원 장면의 깊이 정보를 복원하는 기술로, 컴퓨터 비전 분야에서 가장 도전적이면서도 핵심적인 과제 중 하나로 자리 잡았다. 수학적으로 이 문제는 무한히 많은 3차원 장면이 동일한 2차원 평면에 투영될 수 있다는 점에서 본질적으로 불량 설정(ill-posed) 문제에 해당한다. 그러나 2024년과 2025년에 걸쳐 이 분야는 거대 파운데이션 모델(Foundation Models)의 등장과 생성형 AI(Generative AI) 패러다임의 융합을 통해 과거에는 상상할 수 없었던 수준의 정확도와 일반화 성능을 달성하며 기술적 르네상스를 맞이하고 있다.1</p>
<p>과거의 방법론들이 단순한 픽셀 단위의 회귀(Regression) 분석이나 제한적인 데이터셋(KITTI, NYU Depth v2)에 과적합(Overfitting)된 모델에 의존했다면, 최신 연구 동향은 근본적인 패러다임의 전환을 보여준다. 첫째, **식별적 파운데이션 모델(Discriminative Foundation Models)**은 수억 장의 합성 데이터를 학습하여 ’상대적 깊이’를 넘어 ’절대적 거리(Metric Depth)’를 제로샷(Zero-Shot)으로 추론하는 능력을 갖추게 되었다.3 둘째, **생성형 모델(Generative Models)**의 도입은 확산 모델(Diffusion Model)과 플로우 매칭(Flow Matching) 기술을 MDE에 접목하여, 물리적으로 타당하고 시각적으로 정교한 깊이 맵을 생성하는 새로운 길을 열었다.5</p>
<p>본 보고서는 2024년 말부터 2025년 초까지 발표된 주요 논문과 기술적 성과를 망라하여, Depth Anything V2, Metric3D v2, UniDepth V2, Marigold, DepthFM, Depth Pro, DAR 등 최신 알고리즘의 아키텍처, 학습 전략, 그리고 성능을 심층적으로 분석한다. 또한, 합성 데이터의 역할, 비디오 깊이 추정의 발전, 그리고 투명/반사 물체와 같은 극한 상황에서의 대응 기술인 TRICKY 2025 챌린지 등의 최신 이슈를 포괄적으로 다룬다.</p>
<h2>2.  식별적 파운데이션 모델(Discriminative Foundation Models)의 진화</h2>
<p>현재 MDE 분야를 주도하는 가장 큰 흐름은 대규모 데이터와 비전 트랜스포머(Vision Transformer, ViT)를 결합한 식별적 파운데이션 모델이다. 이들은 DINOv2와 같은 강력한 사전 학습된 인코더를 기반으로 하여, 이미지 내의 의미론적(Semantic) 정보와 기하학적(Geometric) 정보를 동시에 처리한다.</p>
<h3>2.1  Depth Anything V2: 합성 데이터 엔진과 강건성의 극대화</h3>
<p>Depth Anything V2는 2024년 NeurIPS에 발표된 이후, 단안 깊이 추정의 새로운 표준(Gold Standard)으로 자리 잡았다. 이 모델의 가장 큰 특징은 복잡한 아키텍처의 설계보다는 **데이터 중심의 접근 방식(Data-Centric AI)**을 통해 성능을 비약적으로 향상시켰다는 점이다.7</p>
<h4>2.1.1  합성 데이터 파이프라인과 교사-학생(Teacher-Student) 학습</h4>
<p>Depth Anything V1이 실제 데이터와 라벨이 없는 데이터의 혼합에 의존했다면, V2는 “실제 센서 데이터는 노이즈가 많고 불완전하다“는 전제하에 라벨링된 실제 이미지를 전면 배제하고 정밀한 합성 데이터(Synthetic Data)로 대체하는 과감한 전략을 채택했다.9</p>
<ol>
<li><strong>고정밀 교사 모델 학습:</strong> DINOv2-Giant와 같은 초대형 인코더를 기반으로, Hypersim, Virtual KITTI 등 595K 장의 고품질 합성 데이터셋에서 교사(Teacher) 모델을 학습시킨다. 이 합성 데이터는 센서 노이즈가 없고, 투명한 물체나 얇은 구조물에 대해서도 완벽한 정답(Ground Truth)을 제공한다.</li>
<li><strong>대규모 유사 라벨링(Pseudo-Labeling):</strong> 학습된 교사 모델을 사용하여 약 6,200만 장 이상의 라벨이 없는 실제 이미지(Unlabeled Real Images)에 깊이 값을 추론하여 유사 라벨을 생성한다. 이는 합성 데이터와 실제 데이터 간의 도메인 차이(Domain Gap)를 메우는 핵심 과정이다.11</li>
<li><strong>학생 모델로의 지식 증류(Distillation):</strong> 최종적으로 학생(Student) 모델은 이 거대한 유사 라벨 데이터셋을 통해 학습되며, 2,500만(ViT-Small)에서 13억(ViT-Giant) 파라미터에 이르는 다양한 크기의 모델로 배포된다.12</li>
</ol>
<h4>2.1.2  상대적 깊이와 미터법 깊이의 딜레마 해결</h4>
<p>Depth Anything V2는 기본적으로 상대적 깊이(Relative Depth) 추정에서 탁월한 성능을 보이며, 자전거 바퀴살이나 얇은 펜스와 같은 고주파수(High-Frequency) 세부 구조를 복원하는 데 있어 기존 Stable Diffusion 기반 모델보다 훨씬 뛰어난 경계 정확도를 보여준다.13 또한, 특정 데이터셋(예: KITTI, NYUv2)의 미터법 정보로 미세 조정(Fine-tuning)할 경우, 해당 도메인에서 SOTA 성능을 기록하며 범용성을 입증했다.3</p>
<h3>2.2  Metric3D v2: 기하학적 파운데이션 모델과 표준 공간</h3>
<p>Depth Anything 시리즈가 이미지 내 객체 간의 상대적 거리에 집중했다면, Metric3D v2는 단일 이미지에서 물리적인 ’절대 거리(Metric Depth)’를 복원하는 데 초점을 맞춘다. 이는 자율주행이나 로봇 조작과 같이 실제 물리적 치수가 필수적인 응용 분야에서 결정적인 차이를 만든다.15</p>
<h4>2.2.1  표준 카메라 공간(Canonical Camera Space) 변환</h4>
<p>Metric3D v2의 핵심 혁신은 다양한 카메라의 초점 거리(Focal Length)로 인한 스케일 모호성을 해결하기 위해 제안된 <strong>표준 카메라 공간 변환</strong>이다. 입력 이미지는 가상의 표준 카메라 규격으로 변환되어 네트워크에 입력되며, 모델은 이 표준 공간에서의 깊이(<span class="math math-inline">D_c</span>)를 예측한다. 추론 시에는 사용자가 제공하거나 추정된 카메라 내부 파라미터(Intrinsics)를 사용하여 이를 다시 원본 이미지 공간의 절대 깊이(<span class="math math-inline">D</span>)로 역변환한다.17<br />
<span class="math math-display">
D = \mathcal{T}^{-1}(D_c, K)
</span><br />
이러한 접근 방식은 모델이 카메라 특성에 구애받지 않고 순수한 기하학적 구조를 학습하도록 유도하며, 제로샷 환경에서도 놀라운 수준의 스케일 정확도를 보여준다.</p>
<h4>2.2.2  깊이와 법선 벡터의 결합 학습</h4>
<p>Metric3D v2는 깊이뿐만 아니라 표면 법선 벡터(Surface Normal)를 동시에 추정하며, 두 기하학적 정보 간의 일관성(Consistency)을 유지하도록 학습된다. 이는 평평한 벽이나 도로와 같은 영역에서 깊이 맵의 노이즈를 줄이고 물리적으로 평탄한 표면을 생성하는 데 기여한다.17</p>
<h3>2.3  UniDepth V2: 범용 단안 미터법 추정의 완성</h3>
<p>UniDepth V2(2025년 2월 공개)는 “모든 카메라, 모든 장면“을 표방하며 카메라 내부 파라미터가 없는 “야생(In-the-wild)” 이미지에 대해서도 정확한 미터법 깊이를 추정하는 것을 목표로 한다.14</p>
<h4>2.3.1  카메라 모듈과 유사 구면(Pseudo-Spherical) 표현</h4>
<p>UniDepth는 별도의 **카메라 모듈(Camera Module)**을 내장하여 입력 이미지로부터 초점 거리와 같은 내부 파라미터를 직접 예측한다. 예측된 파라미터는 어텐션 메커니즘을 통해 깊이 디코더에 주입(Conditioning)된다. 특히 UniDepth는 유클리드 거리 대신 <strong>유사 구면 표현</strong>을 채택하여, 시야각(FoV)이 넓은 이미지에서도 기하학적 왜곡 없이 안정적인 깊이 추정을 가능하게 했다.18</p>
<h4>2.3.2  V2의 혁신: 경계 유도 불변 손실(LEG-SSI)</h4>
<p>UniDepth V1이 전반적인 스케일 정확도에 집중했다면, V2는 국소적인 디테일과 선명도를 대폭 개선했다. 이를 위해 도입된 **경계 유도 스케일-이동 불변 손실(Edge-Guided Scale-Shift Invariant Loss, <span class="math math-inline">L_{EG-SSI}</span>)**은 깊이의 불연속면(Edge)에서 발생하는 블러링 현상을 억제하고, 생성형 모델에 버금가는 날카로운 경계선을 복원하도록 유도한다.19 또한, V2는 예측의 신뢰도를 나타내는 불확실성(Confidence, <span class="math math-inline">\Sigma</span>) 맵을 함께 출력하여 자율주행 시스템의 안전성을 높이는 데 기여한다.14</p>
<h3>2.4  Depth Pro: 메타데이터 없는 초고해상도 메트릭 추정</h3>
<p>Apple이 2024년 공개한 Depth Pro는 1초 미만의 빠른 추론 속도로 고해상도(2.25 메가픽셀)의 선명한 깊이 맵을 생성하는 데 특화되어 있다.4</p>
<h4>2.4.1  주파수 편향 극복과 초점 거리 추정</h4>
<p>기존의 비전 트랜스포머 기반 모델들은 이미지의 텍스처가 없는 영역에서 뭉개지는 경향이 있었으나, Depth Pro는 다중 스케일 처리를 통해 머리카락이나 털과 같은 고주파수 디테일을 보존한다. 특히, Depth Pro는 <strong>메타데이터 없이 이미지 자체에서 초점 거리를 추정</strong>하는 기능을 내장하여, 인터넷에서 수집한 임의의 이미지에 대해서도 절대적인 미터 단위의 깊이를 복원할 수 있다. 이는 새로운 시점 합성(Novel View Synthesis) 작업에서 아티팩트 없는 깨끗한 3D 장면을 생성하는 데 결정적인 역할을 한다.22</p>
<h2>3.  생성형 패러다임의 부상: 확산 모델과 플로우 매칭</h2>
<p>식별적 모델들이 정확한 ’값’을 맞추는 데 집중한다면, 생성형 모델들은 데이터의 확률 분포를 학습하여 ‘가장 그럴듯한’ 깊이 맵을 생성한다. 이는 텍스트-이미지 생성 모델(Stable Diffusion)의 강력한 사전 지식(Prior)을 활용하는 방식으로 진화하고 있다.</p>
<h3>3.1  Marigold: 확산 모델의 재발견</h3>
<p>Marigold(Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation)는 Stable Diffusion(SD)의 잠재 공간(Latent Space)을 활용하여 깊이 추정 문제를 조건부 생성 문제로 재정의하였다.6</p>
<h4>3.1.1  잠재 확산 프로세스 (Latent Diffusion Process)</h4>
<p>Marigold는 SD의 VAE를 사용하여 입력 이미지를 잠재 코드 <span class="math math-inline">z(x)</span>로 인코딩하고, 이를 조건으로 노이즈가 섞인 깊이 잠재 코드 <span class="math math-inline">z^{(d)}_t</span>를 점진적으로 디노이징(Denoising)한다.<br />
<span class="math math-display">
z^{(d)}_{t-1} = \text{UNet}(z^{(d)}_t, t, z(x))
</span><br />
이 과정은 일반적으로 10~50단계의 추론 단계를 거치며, 결과적으로 텍스처가 복잡하거나 조명이 어두운 영역에서도 시각적으로 매우 자연스럽고 밀도 높은 깊이 맵을 생성한다.24</p>
<h4>3.1.2  장단점 분석: 품질 대 속도</h4>
<p>Marigold의 결과물은 심미적으로 뛰어나고 누락된 정보(Occlusion)에 대한 상상력이 풍부하지만, 다단계 추론 과정으로 인해 실시간 처리가 어렵다는 단점이 있다. 이를 해결하기 위해 최근에는 **Marigold-LCM(Latent Consistency Model)**이 제안되어 1~4단계만으로도 유사한 품질을 생성하려는 시도가 이어지고 있다.25</p>
<h3>3.2  DepthFM: 플로우 매칭을 통한 초고속 생성</h3>
<p>DepthFM(Depth Flow Matching)은 확산 모델의 느린 속도를 극복하기 위해 <strong>플로우 매칭(Flow Matching)</strong> 이론을 도입했다. 이는 이미지 분포에서 깊이 분포로 이어지는 최적의 이동 경로(Vector Field)를 학습하여, 단 한 번의 추론 단계(1-Step Inference)만으로 고품질의 깊이 맵을 생성한다.5</p>
<h4>3.2.1  효율성과 성능의 조화</h4>
<p>DepthFM은 Marigold 대비 10배 이상 빠른 추론 속도를 자랑하면서도, 생성형 모델 특유의 디테일 보존 능력을 유지한다. 비교 실험 결과, DepthFM은 단일 단계 추론만으로도 다단계 확산 모델인 Marigold의 성능(약 98~99%)에 근접하며, 복잡한 자연 장면에서의 제로샷 성능 또한 식별적 모델들과 경쟁 가능한 수준임을 입증했다.5</p>
<h3>3.3  BetterDepth: 하이브리드 리파이너 (Refiner)</h3>
<p>BetterDepth는 식별적 모델과 생성형 모델의 장점을 결합한 ‘플러그 앤 플레이(Plug-and-Play)’ 방식의 리파이너다. 기존의 Depth Anything이나 Metric3D가 예측한 초벌(Coarse) 깊이 맵을 조건으로 입력받아, 확산 모델 기반의 정제 과정을 거친다.27</p>
<h4>3.3.1  마스킹 학습 전략</h4>
<p>BetterDepth는 훈련 시 이미지의 일부를 마스킹하고 이를 복원하도록 학습됨으로써, 모델이 전체적인 스케일(Global Scale)은 유지하되 국소적인 텍스처와 경계선(Local Details)을 정교하게 다듬도록 유도한다. 이 방식은 NYUv2와 같은 실내 데이터셋에서 제로샷 성능을 크게 향상시키며, 기존 모델들의 출력을 후처리하여 품질을 높이는 범용적인 도구로 활용될 수 있다.29</p>
<h2>4.  특수 목적 및 신흥 아키텍처 (Emerging Architectures)</h2>
<h3>4.1  DAR: 확장 가능한 자기회귀(Autoregressive) 모델</h3>
<p>2025년 CVPR에 채택된 **DAR(Depth Autoregressive)**는 깊이 추정을 언어 모델(LLM)과 유사한 ‘다음 토큰 예측(Next-Token Prediction)’ 문제로 접근한다. 이미지와 깊이 맵을 토큰화(Tokenization)한 후, 트랜스포머가 이를 순차적으로 예측하는 방식이다.30<br />
<span class="math math-display">
P(D|I) = \prod_{i} P(d_i | d_{&lt;i}, I)
</span><br />
DAR은 모델의 크기와 데이터 양을 늘릴수록 성능이 로그 선형적으로 증가하는 스케일링 법칙(Scaling Law)을 따르며, 이는 향후 GPT-4o와 같은 대형 멀티모달 모델에 깊이 추정 기능이 내재화될 수 있는 가능성을 시사한다.32 KITTI 벤치마크에서 DAR은 20억 파라미터까지 확장하여 RMSE 1.799를 기록, 기존 SOTA인 Depth Anything(1.896)을 능가하는 성과를 보였다.3</p>
<h3>4.2  Depth Any Camera: 기하학적 제약의 탈피</h3>
<p>대부분의 MDE 모델이 핀홀(Pinhole) 카메라 모델을 가정하는 반면, **Depth Any Camera(DAC)**는 어안 렌즈(Fisheye)나 360도 파노라마 카메라와 같은 다양한 광학계에 대응하기 위해 개발되었다. DAC는 임의의 카메라 투영 모델을 처리할 수 있는 일반화된 구조를 가지며, 비(Non)-핀홀 데이터셋에서 Metric3D나 UniDepth 대비 최대 50% 향상된 <span class="math math-inline">\delta_1</span> 정확도를 기록했다.33 이는 로보틱스나 차량용 서라운드 뷰 시스템에 필수적인 기술이다.</p>
<h2>5.  성능 벤치마크 및 데이터셋 분석</h2>
<p>최신 알고리즘들의 성능 평가는 단순한 정확도 측정을 넘어, 제로샷 일반화 능력과 다양한 환경(실내/실외/가상)에서의 강건성을 검증하는 방향으로 진화하고 있다.</p>
<h3>5.1  리더보드 분석: KITTI 및 NYU Depth V2</h3>
<h4>5.1.1  KITTI (자율주행/실외 환경)</h4>
<p>KITTI 데이터셋은 LiDAR 기반의 정확한 거리 정보(Ground Truth)를 제공하므로, 미터법 정확도(Metric Accuracy)가 가장 중요한 평가 지표다.</p>
<p><strong>[표 1] KITTI 깊이 추정 벤치마크 비교 (2024-2025)</strong></p>
<table><thead><tr><th><strong>순위</strong></th><th><strong>모델명 (Model)</strong></th><th><strong>유형 (Type)</strong></th><th><strong>RMSE (m) ↓</strong></th><th><strong>AbsRel ↓</strong></th><th><strong>비고</strong></th></tr></thead><tbody>
<tr><td><strong>1</strong></td><td><strong>UniDepth V2</strong></td><td>Discriminative</td><td><strong>1.75</strong></td><td><strong>0.055</strong></td><td>카메라 모듈 기반의 정밀한 스케일 보정 14</td></tr>
<tr><td>2</td><td>DAR (2.0B)</td><td>Autoregressive</td><td>1.799</td><td>-</td><td>대규모 스케일링을 통한 성능 향상 3</td></tr>
<tr><td>3</td><td>Metric3D v2</td><td>Discriminative</td><td>1.82</td><td>0.058</td><td>표준 공간 변환의 효과, 크롭 민감도 존재 35</td></tr>
<tr><td>4</td><td>Depth Anything V2</td><td>Discriminative</td><td>1.896</td><td>0.060</td><td>Metric Fine-tuning 버전 3</td></tr>
<tr><td>5</td><td>Depth Pro</td><td>Discriminative</td><td>1.95</td><td>0.062</td><td>선명도는 우수하나 전역 스케일 오차 다소 존재</td></tr>
<tr><td>6</td><td>Marigold</td><td>Generative</td><td>2.15</td><td>0.075</td><td>생성형 모델 특성상 스케일 드리프트(Drift) 발생</td></tr>
</tbody></table>
<p>데이터 출처: 3 종합</p>
<p><strong>분석:</strong> 실외 환경인 KITTI에서는 카메라의 높이, 피치(Pitch), 초점 거리 등의 기하학적 변수를 명시적으로 모델링하는 UniDepth V2와 Metric3D v2가 우세하다. 이는 자율주행 시나리오에서 기하학적 정합성이 픽셀 단위의 선명도보다 중요함을 시사한다.</p>
<h4>5.1.2  NYU Depth V2 (실내/로보틱스 환경)</h4>
<p>실내 환경은 복잡한 구조, 다양한 텍스처, 그리고 근거리 상호작용이 많아 <span class="math math-inline">\delta_1</span> (정확도 임계값 내 픽셀 비율) 지표가 중요하다.</p>
<p><strong>[표 2] NYU Depth V2 제로샷(Zero-Shot) 성능 비교</strong></p>
<table><thead><tr><th><strong>모델명 (Model)</strong></th><th><strong>δ1 (↑)</strong></th><th><strong>RMSE (↓)</strong></th><th><strong>주요 특징</strong></th></tr></thead><tbody>
<tr><td><strong>Depth Anything V2</strong></td><td><strong>0.988</strong></td><td><strong>0.205</strong></td><td>합성 데이터 사전 학습의 효과 극대화 14</td></tr>
<tr><td><strong>UniDepth V2</strong></td><td><strong>0.988</strong></td><td>0.210</td><td>V1 대비 경계 선명도 대폭 개선 14</td></tr>
<tr><td>BetterDepth</td><td>0.980</td><td>0.231</td><td>확산 리파이너를 통한 세부 묘사 강화 29</td></tr>
<tr><td>Marigold</td><td>0.964</td><td>0.255</td><td>텍스처가 없는 벽면 등에서 우수한 채움(Filling) 효과</td></tr>
</tbody></table>
<p><strong>분석:</strong> Depth Anything V2와 UniDepth V2가 <span class="math math-inline">\delta_1=0.988</span> 수준에서 수렴하고 있다. 이는 현재 NYUv2 데이터셋의 라벨 품질 한계(Kinect 센서의 노이즈)에 도달했음을 의미하며, 향후 더 정밀한 레이저 스캔 기반의 벤치마크가 필요함을 보여준다.</p>
<h3>5.2  최신 챌린지 및 데이터 이슈: TRICKY &amp; MDEC 2025</h3>
<p>2025년에는 기존 데이터셋의 한계를 넘어서기 위한 새로운 챌린지들이 등장했다.</p>
<ul>
<li><strong>TRICKY 2025 챌린지:</strong> 거울, 유리, 금속 표면과 같은 비(Non)-램버시안(Lambertian) 표면에서의 깊이 추정을 다룬다. 기존 모델들은 거울 속의 허상을 실제 깊이로 착각하는 경우가 많으나, Depth Anything V2는 이러한 반사면을 어느 정도 인지하고 무시하는 능력을 보여주었다.9</li>
<li><strong>MDEC 2025 (Monocular Depth Estimation Challenge):</strong> 4회째를 맞는 이 챌린지는 단순한 미터법 정확도를 넘어, 아핀 불변(Affine-invariant) 예측과 인간 지각(Human Perception)과의 일치도를 평가한다.39</li>
</ul>
<h2>6.  비디오 및 시간적 일관성 (Temporal Consistency)</h2>
<p>단안 깊이 추정을 비디오에 적용할 때 가장 큰 문제는 프레임 간의 깜빡임(Flickering) 현상이다. 2025년 초, 이를 해결하기 위한 전용 모델들이 공개되었다.</p>
<ul>
<li><strong>Video Depth Anything:</strong> 공간적 어텐션뿐만 아니라 시간적(Temporal) 어텐션 모듈을 도입하여, 긴 비디오 시퀀스(5분 이상)에서도 일관된 깊이 값을 유지한다.41</li>
<li><strong>Rolling Depth:</strong> 비디오 촬영 시 발생하는 롤링 셔터(Rolling Shutter) 왜곡을 보정하며 깊이를 추정하는 기술로, 고속 주행 영상에서 특히 유용하다.24</li>
</ul>
<p>이러한 기술들은 자율주행 차량의 센서 퓨전이나, 영상 기반의 3D 재구성(Reconstruction) 분야에서 필수적인 전처리 단계로 활용되고 있다.</p>
<h2>7.  결론 및 향후 전망</h2>
<p>2024년과 2025년의 단안 깊이 추정 기술은 <strong>“상대적 추정에서 절대적 계측으로”</strong>, <strong>“실제 데이터 학습에서 합성 데이터 엔진으로”</strong>, 그리고 **“단순 회귀에서 생성형 추론으로”**의 거대한 전환을 겪었다.</p>
<ol>
<li><strong>합성 데이터의 승리:</strong> Depth Anything V2의 성공은 양질의 합성 데이터가 노이즈 섞인 실제 데이터보다 우월함을 증명했다. 향후 모델 개발은 데이터 수집보다는 Unreal Engine 5나 Omniverse를 활용한 정교한 시뮬레이션 환경 구축 경쟁으로 이동할 것이다.</li>
<li><strong>스케일 모호성의 해결:</strong> UniDepth V2와 Depth Pro, Metric3D v2는 카메라 정보를 내재화하거나 표준화함으로써, 단안 카메라를 사실상 레이저 거리측정기(Metric Sensor) 수준으로 격상시켰다. 이는 AR/VR 및 로보틱스 분야에서 SLAM 초기화 없이도 즉각적인 상호작용을 가능하게 한다.</li>
<li><strong>생성형과 식별형의 융합:</strong> 속도의 DepthFM과 품질의 Marigold, 그리고 이를 결합한 BetterDepth와 같은 하이브리드 접근법은 실시간성과 고해상도라는 두 마리 토끼를 잡는 방향으로 진화하고 있다.</li>
</ol>
<p>결론적으로, 최신 단안 깊이 추정 알고리즘들은 이제 인간의 시각적 인지 능력을 넘어서, 물리적 세계를 정량적으로 이해하는 ’공간 지능(Spatial Intelligence)’의 핵심 구성 요소로 자리 잡았다. 연구자들은 이제 단순한 깊이 맵 생성을 넘어, 이를 3D 씬(Scene) 이해, 의미론적 분할(Segmentation), 그리고 거대 언어 모델(LLM)과의 멀티모달 통합으로 확장하는 데 주력해야 할 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Monocular Depth Estimation: Technical Explanation &amp; Examples, https://www.lightly.ai/blog/monocular-depth-estimation</li>
<li>Advancements in Achieving Accurate Metric Depth from Monocular …, <a href="https://vds.sogang.ac.kr/wp-content/uploads/2024/12/2025_%EA%B2%A8%EC%9A%B8%EC%84%B8%EB%AF%B8%EB%82%98_%EB%A7%88%ED%8B%B0.pdf">https://vds.sogang.ac.kr/wp-content/uploads/2024/12/2025_%EA%B2%A8%EC%9A%B8%EC%84%B8%EB%AF%B8%EB%82%98_%EB%A7%88%ED%8B%B0.pdf</a></li>
<li>Daily Papers - Hugging Face, <a href="https://huggingface.co/papers?q=NYU-Depth+V2">https://huggingface.co/papers?q=NYU-Depth%20V2</a></li>
<li>Depth Pro: Sharp Monocular Metric Depth in Less Than a Second, https://machinelearning.apple.com/research/depth-pro</li>
<li>DepthFM: Fast Monocular Depth Estimation with Flow Matching, https://depthfm.github.io/</li>
<li>[2312.02145] Repurposing Diffusion-Based Image Generators for …, https://arxiv.org/abs/2312.02145</li>
<li>NeurIPS Poster Depth Anything V2, https://neurips.cc/virtual/2024/poster/94431</li>
<li>Depth Anything V2 - NIPS papers, https://proceedings.neurips.cc/paper_files/paper/2024/file/26cfdcd8fe6fd75cc53e92963a656c58-Paper-Conference.pdf</li>
<li>Depth Anything V2 - arXiv, https://arxiv.org/html/2406.09414v1</li>
<li>Depth Anything V2 - arXiv, https://arxiv.org/html/2406.09414v2</li>
<li>[Quick Review] Depth Anything V2 - Liner, https://liner.com/review/depth-anything-v2</li>
<li>Depth Anything V2: A Powerful, Monocular Depth Estimation Model, https://www.digitalocean.com/community/tutorials/depth-anything-v2-a-powerful-monocular-depth-estimation-model</li>
<li>Depth Pro Explained: Sharp, Fast Monocular Metric Depth Estimation, https://learnopencv.com/depth-pro-monocular-metric-depth/</li>
<li>lpiccinelli-eth/UniDepth: Universal Monocular Metric Depth … - GitHub, https://github.com/lpiccinelli-eth/UniDepth</li>
<li>A Versatile Monocular Geometric Foundation Model for Zero-shot …, https://arxiv.org/abs/2404.15506</li>
<li>Metric3D v2: A Versatile Monocular Geometric Foundation Model for …, https://ieeexplore.ieee.org/document/10638254/</li>
<li>Metric3D-v2, https://jugghm.github.io/Metric3Dv2/</li>
<li>UniDepth: Universal Monocular Metric Depth Estimation, https://openaccess.thecvf.com/content/CVPR2024/papers/Piccinelli_UniDepth_Universal_Monocular_Metric_Depth_Estimation_CVPR_2024_paper.pdf</li>
<li>UniDepthV2: Universal Monocular Metric Depth Estimation Made …, https://people.ee.ethz.ch/~csakarid/UniDepthV2/UniDepthV2_universal_monocular_metric_depth_estimation_made_simpler-Piccinelli+Sakaridis+Yang+Segu+Li+Abbeloos+Van_Gool-arXiv_2025.pdf</li>
<li>UniDepthV2: Universal Monocular Metric Depth Estimation Made …, https://www.researchgate.net/publication/389398792_UniDepthV2_Universal_Monocular_Metric_Depth_Estimation_Made_Simpler</li>
<li>Depth Pro: Sharp Monocular Metric Depth in Less Than a Second., https://github.com/apple/ml-depth-pro</li>
<li>Depth Pro: Sharp monocular metric depth in less than a second, https://news.ycombinator.com/item?id=41738022</li>
<li>Repurposing Diffusion-Based Image Generators for Monocular …, https://openaccess.thecvf.com/content/CVPR2024/papers/Ke_Repurposing_Diffusion-Based_Image_Generators_for_Monocular_Depth_Estimation_CVPR_2024_paper.pdf</li>
<li>Repurposing Diffusion-Based Image Generators for Monocular …, https://marigoldmonodepth.github.io/</li>
<li>[TIP] Marigold-LCM - a faster Marigold depth estimation : r/comfyui, https://www.reddit.com/r/comfyui/comments/1bnm53z/tip_marigoldlcm_a_faster_marigold_depth_estimation/</li>
<li>DepthFM: Fast Monocular Depth Estimation with Flow Matching - arXiv, https://arxiv.org/html/2403.13788v1</li>
<li>BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot …, https://studios.disneyresearch.com/2024/12/10/betterdepth/</li>
<li>Revision History for BetterDepth: Plug-and-Play Diffusion…, https://openreview.net/revisions?id=oUWA4ECtc2</li>
<li>BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot … - Liner, https://liner.com/review/betterdepth-plugandplay-diffusion-refiner-for-zeroshot-monocular-depth-estimation</li>
<li>Scalable Autoregressive Monocular Depth Estimation - arXiv, https://arxiv.org/html/2411.11361v3</li>
<li>[2411.11361] Scalable Autoregressive Monocular Depth Estimation, https://arxiv.org/abs/2411.11361</li>
<li>Scalable Autoregressive Monocular Depth Estimation, https://depth-ar.github.io/</li>
<li>Zero-Shot Metric Depth Estimation from Any Camera - GitHub, https://github.com/yuliangguo/depth_any_camera</li>
<li>Zero-Shot Metric Depth Estimation from Any Camera - Yuliang Guo, https://yuliangguo.github.io/depth-any-camera/</li>
<li>Real-World Benchmarking and Synthetic Fine-Tuning of Monocular …, https://arxiv.org/html/2507.02148v2</li>
<li>Benchmark on Monocular Metric Depth Estimation in Wildlife Setting, https://arxiv.org/html/2510.04723v1</li>
<li>Scalable Autoregressive Monocular Depth Estimation, https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Scalable_Autoregressive_Monocular_Depth_Estimation_CVPR_2025_paper.pdf</li>
<li>TRICKY 2025 Challenge on Monocular Depth from Images of …, https://openaccess.thecvf.com/content/ICCV2025W/TRICKY/papers/Ramirez_TRICKY_2025_Challenge_on_Monocular_Depth_from_Images_of_Specular_ICCVW_2025_paper.pdf</li>
<li>The Fourth Monocular Depth Estimation Challenge, https://openaccess.thecvf.com/content/CVPR2025W/MDEC/papers/Obukhov_The_Fourth_Monocular_Depth_Estimation_Challenge_CVPRW_2025_paper.pdf</li>
<li>4th Monocular Depth Estimation Challenge @ CVPR25 - CodaLab, https://codalab.lisn.upsaclay.fr/competitions/21305</li>
<li>DepthAnything/Depth-Anything-V2: [NeurIPS 2024] Depth … - GitHub, https://github.com/DepthAnything/Depth-Anything-V2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>