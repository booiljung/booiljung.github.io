<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:SuperPoint 자기 지도 학습 기반 특징점 추출</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>SuperPoint 자기 지도 학습 기반 특징점 추출</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">센서 (Sensors)</a> / <a href="index.html">특징점 추출</a> / <span>SuperPoint 자기 지도 학습 기반 특징점 추출</span></nav>
                </div>
            </header>
            <article>
                <h1>SuperPoint 자기 지도 학습 기반 특징점 추출</h1>
<h2>1.  특징점 기반 컴퓨터 비전의 패러다임 전환</h2>
<h3>1.1  관심 지점(Interest Point)과 기술자(Descriptor)의 본질</h3>
<p>컴퓨터 비전의 수많은 기하학적 문제 해결의 첫걸음은 이미지로부터 안정적이고 반복 가능한 특징점을 추출하는 것이다. 관심 지점(Interest Point)은 조명, 시점, 스케일 변화와 같은 변환에도 불구하고 이미지 내에서 일관되게 검출될 수 있는 2D 위치로 정의된다.1 이러한 점들은 동시적 위치추정 및 지도작성(SLAM), 동작 기반 구조 복원(Structure-from-Motion, SfM), 카메라 캘리브레이션 등 다중 시점 기하학(Multiple View Geometry) 분야의 근간을 형성하는 핵심 요소다.1</p>
<p>관심 지점이 ‘어디에’ 있는지를 나타낸다면, 기술자(Descriptor)는 그 지점이 ’무엇인지’를 설명한다. 기술자는 각 관심 지점 주변의 지역적 이미지 패턴을 고유하게 표현하는 고차원 벡터로, 서로 다른 이미지에서 동일한 3D 점에 해당하는 관심 지점들을 정합(matching)하는 데 결정적인 역할을 한다.4 따라서, 검출된 관심 지점의 반복성(repeatability)과 기술자의 변별력(discriminative power)은 후속 작업의 전반적인 정확성과 강건성을 좌우하는 가장 중요한 요소라 할 수 있다.4</p>
<h3>1.2  전통적 접근법의 공헌과 명백한 한계: SIFT와 ORB를 중심으로</h3>
<p>딥러닝 시대 이전, 수작업으로 설계된(hand-crafted) 특징점 알고리즘들이 이 분야를 지배했다. 그중 가장 대표적인 것은 SIFT(Scale-Invariant Feature Transform)와 ORB(Oriented FAST and Rotated BRIEF)이다.</p>
<p>SIFT는 가우시안 차분(Difference of Gaussians, DoG) 피라미드를 이용하여 스케일-공간에서 극점을 검출함으로써, 스케일과 회전 변화에 불변하는 강력한 특징점을 제공했다.6 그러나 그래디언트 방향 히스토그램에 기반한 128차원의 고차원 기술자를 계산하는 과정은 상당한 계산 비용을 수반하여, 실시간 성능이 요구되는 응용 분야에서는 사용이 제한되었다.8 또한, 강한 노이즈나 복잡한 배경을 가진 이미지에서는 성능이 저하되는 경향을 보였다.8</p>
<p>이러한 SIFT의 속도 문제를 해결하기 위해 등장한 것이 ORB이다. ORB는 FAST 코너 검출기와 BRIEF 이진 기술자를 결합하여 SIFT 대비 약 100배 빠른 속도를 달성했으며, 이는 Visual SLAM과 같이 실시간 성능이 중요한 분야에서 널리 채택되는 계기가 되었다.8 하지만 ORB는 속도를 얻는 대가로 정확성과 강건성을 일부 희생했다. SIFT에 비해 스케일, 조명, 회전 변화에 대한 불변성이 약하며, 특히 텍스처가 부족한(low-texture) 환경에서는 충분한 수의 특징점을 추출하지 못하는 문제를 드러냈다.8 또한, 검출된 특징점이 특정 영역에 밀집되는 경향이 있어 후속 기하학적 추정 작업의 정확도를 저해하기도 했다.2</p>
<h3>1.3  딥러닝과 자기 지도 학습의 부상: 패러다임의 전환</h3>
<p>전통적 방법론의 명백한 한계는 새로운 패러다임의 등장을 촉발했다. 특히 SLAM이나 증강 현실(AR)과 같은 응용 분야는 실시간 처리 능력과 높은 정확성을 동시에 요구하는데, SIFT는 너무 느렸고 ORB는 충분히 강건하지 못했다.11 이러한 ’속도-정확도 트레이드오프’는 수작업 설계 방식의 근본적인 한계였으며, 이 간극을 메우기 위해 학습 기반 접근법이 대두되었다.</p>
<p>그러나 특징점 검출 문제에 전통적인 지도 학습(supervised learning)을 적용하는 데에는 근본적인 난제가 존재했다. 인간의 얼굴 특징점과 같은 의미론적 키포인트(semantic keypoints)와 달리, 일반적인 이미지의 관심 지점은 “의미론적으로 정의하기 어렵다(semantically ill-defined)”.2 즉, 사람이 일관되고 반복적으로 동일한 지점을 정답으로 레이블링하는 것이 거의 불가능하다.2</p>
<p>이러한 레이블링의 부재 문제를 해결하기 위한 대안으로 자기 지도 학습(Self-Supervised Learning)이 부상했다. 자기 지도 학습은 데이터 자체로부터 감독 신호(supervisory signal)를 생성하여 모델을 학습시키는 방식이다. SuperPoint는 인간의 주석에 의존하는 대신, 모델 스스로가 생성한 의사-정답(pseudo-ground truth) 레이블을 통해 학습하는 혁신적인 프레임워크를 제안했다.1 SuperPoint의 핵심 기여는 완전 컨볼루션 신경망(FCN)을 통해 이미지 전체에서 픽셀 수준의 관심 지점 위치와 해당 기술자를 단일 순방향 패스(single forward pass)로 동시에 계산하는 통합 아키텍처를 제시한 것이다.1 이는 검출 후 기술자를 별도로 계산하던 전통적인 파이프라인을 개선하여 효율성과 표현력 공유를 극대화한, 진정한 패러다임의 전환이었다.</p>
<h2>2.  SuperPoint 네트워크 아키텍처 해부</h2>
<h3>2.1  설계 원칙: 공유와 효율성</h3>
<p>SuperPoint 아키텍처의 핵심 철학은 ’계산의 재사용’에 있다. 관심 지점 검출과 기술자 계산은 본질적으로 모서리, 질감 등 동일한 저수준 시각적 특징에 의존한다. 전통적인 방법들이 이 두 과정을 완전히 별개의 알고리즘으로 처리했던 것과 달리, SuperPoint는 이를 하나의 통합된 네트워크에서 효율적으로 처리하도록 설계되었다. 이를 위해 패치가 아닌 전체 크기 이미지(full-sized image)에 직접 작동하는 완전 컨볼루션 네트워크(FCN) 구조를 채택했다.1</p>
<p>가장 중요한 설계 특징은 단일 공유 인코더(shared encoder)와 두 개의 분리된 디코더 헤드(관심 지점용, 기술자용)로 구성된 구조다.1 이 구조는 이미지에 대한 풍부한 특징 표현을 한 번만 계산한 뒤, 이 공유된 정보를 바탕으로 두 가지 하위 작업을 동시에 수행한다. 이는 중복 계산을 제거하여 효율성을 극대화할 뿐만 아니라, 두 작업이 서로에게 유익한 정보를 제공하며 함께 학습될 수 있는 기반을 마련한다. 예를 들어, 좋은 기술자를 생성할 수 있는 위치가 좋은 관심 지점일 가능성이 높다는 암묵적인 제약이 학습 과정에 자연스럽게 반영될 수 있다.</p>
<h3>2.2  공유 인코더: VGG 스타일의 특징 추출기</h3>
<p>SuperPoint는 VGG 네트워크와 유사한 스타일의 인코더를 사용하여 입력 이미지로부터 계층적인 특징을 추출하고 차원을 점진적으로 줄여나간다.1 이 인코더는 총 8개의 3x3 컨볼루션 레이어로 구성되며, 채널 수는 64-64-64-64-128-128-128-128 순으로 깊어지는 구조를 가진다.2</p>
<p>두 개의 컨볼루션 레이어마다 2x2 최대 풀링(max-pooling) 연산이 적용되어 특징 맵의 공간적 해상도를 절반으로 줄인다. 총 3번의 풀링을 거치면서, 원본 이미지의 해상도 <code>H \times W</code>는 최종적으로 <span class="math math-inline">H_c \times W_c</span> (여기서 <span class="math math-inline">H_c = H/8</span>, <span class="math math-inline">W_c = W/8</span>)로 다운샘플링된다.14 모든 컨볼루션 레이어 뒤에는 비선형성을 부여하기 위한 ReLU 활성화 함수와 학습을 안정화시키기 위한 배치 정규화(BatchNorm)가 적용된다.2 이 과정을 통해 인코더는 이미지의 저수준 텍스처부터 고수준의 구조적 정보까지 다양한 스케일의 특징을 포함하는 밀집된 특징 맵을 생성한다.</p>
<h3>2.3  관심 지점 디코더 헤드(Interest Point Decoder Head): ‘Point-ness’ 확률 맵 생성</h3>
<p>관심 지점 디코더는 공유 인코더로부터 <span class="math math-inline">H_c \times W_c \times 128</span> 크기의 특징 맵을 입력받아, 각 픽셀이 관심 지점일 확률을 나타내는 히트맵을 생성한다. 이 디코더는 먼저 입력 특징 맵을 <span class="math math-inline">H_c \times W_c \times 65</span> 크기의 텐서로 변환한다.</p>
<p>여기서 65개의 채널은 <span class="math math-inline">8 \times 8</span> 크기의 지역 블록 내 각 픽셀 위치에 대한 ‘점으로서의 특성(point-ness)’ 확률을 나타내는 64개 채널과, 나머지 하나의 특별한 ‘관심 지점 없음(no-interest point)’ 채널로 구성된다.14 이 65번째 ‘dustbin’ 채널은 SuperPoint의 매우 영리한 공학적 설계 중 하나다. 이미지의 대부분은 특징이 없는 배경 영역이며, 이 배경 픽셀들의 활성화 값을 모두 이 dustbin 채널로 모으는 ‘쓰레기통’ 역할을 한다. 만약 이 채널이 없다면, 배경 픽셀들은 64개의 모든 채널에 낮은 확률 값을 골고루 분산시켜 히트맵 전체의 신호 대 잡음비를 떨어뜨릴 것이다. dustbin 채널은 네트워크가 “여기는 아무것도 아니다“라고 명시적으로 예측할 수 있는 선택지를 제공함으로써, 나머지 64개 채널이 실제 관심 지점 신호에만 집중하도록 유도하여 히트맵을 재보정(recalibrate)하고 훨씬 깨끗한 결과를 생성하게 한다.14</p>
<p>이 <span class="math math-inline">H_c \times W_c \times 65</span> 텐서는 채널 차원에 대해 소프트맥스(softmax) 연산을 거친 후, dustbin 채널이 제거된다. 최종적으로 남은 <span class="math math-inline">H_c \times W_c \times 64</span> 텐서는 <span class="math math-inline">H \times W \times 1</span> 형태의 최종 히트맵으로 재구성(reshape)된다. 이 과정은 파라미터가 없는 ‘depth to space’ 또는 ‘sub-pixel convolution’ 연산과 동일하며, 출력 히트맵의 각 픽셀 값은 원본 이미지의 해당 위치가 관심 지점일 확률을 나타낸다.1</p>
<h3>2.4  기술자 디코더 헤드(Descriptor Decoder Head): 고차원 불변 표현 생성</h3>
<p>기술자 디코더 헤드 역시 공유 인코더의 <span class="math math-inline">H_c \times W_c \times 128</span> 특징 맵을 입력으로 받는다. 이 헤드의 목표는 각 픽셀 위치에 대해 조명과 시점 변화에 강건한 고유한 기술자 벡터를 생성하는 것이다.</p>
<p>디코더는 입력 특징 맵을 처리하여 <span class="math math-inline">H_c \times W_c \times D</span> 크기의 준-밀집(semi-dense) 기술자 맵을 출력한다. 여기서 <span class="math math-inline">D</span>는 기술자의 차원으로, 일반적으로 256이 사용된다.14 이 준-밀집 기술자 맵은 원본 이미지 해상도와 맞추기 위해 쌍선형 보간(bilinear interpolation)을 통해 <span class="math math-inline">H \times W \times D</span> 크기로 업샘플링된다.2 이 보간 과정은 검출된 관심 지점의 위치가 정수 픽셀 좌표가 아닌 하위 픽셀(sub-pixel) 단위의 정밀도를 가질 때, 해당 위치에 대한 정확한 기술자를 계산하는 데 필수적이다.</p>
<p>마지막으로, 모든 픽셀 위치의 <span class="math math-inline">D</span>차원 기술자 벡터는 각각 L2-정규화(L2-normalization)를 거쳐 단위 길이(unit length)를 갖도록 만들어진다.2 이 과정은 기술자가 전체적인 밝기 변화에 둔감해지도록(조명 불변성) 만들고, 유클리드 거리나 코사인 유사도 기반의 매칭 과정에서 안정적인 성능을 보장하는 역할을 한다.</p>
<h2>3.  자기 지도 학습: 데이터로부터 스스로 학습하는 방법론</h2>
<p>SuperPoint의 훈련 과정은 ’부트스트래핑(bootstrapping)’의 전형적인 예시를 보여준다. 즉, 간단하고 제한된 지식(합성 데이터에서의 코너)에서 시작하여, 점진적으로 더 복잡하고 일반적인 지식(실제 이미지에서의 반복 가능한 특징점)을 스스로 구축해나가는 과정을 따른다.</p>
<h3>3.1  1단계 - MagicPoint: 합성 데이터를 통한 기초 검출기 사전 훈련</h3>
<p>SuperPoint 훈련의 첫 단계는 <code>MagicPoint</code>라는 기초 검출기를 만드는 것이다.1 이는 완벽하지만 비현실적인 세계에서 ’코너’라는 개념의 초기 버전을 학습하는 과정에 해당한다. 이를 위해, 코너 위치에 대한 기하학적 모호함이 전혀 없는 간단한 도형들(삼각형, 사각형, 별, 체커보드 등)로 구성된 합성 데이터셋</p>
<p><code>Synthetic Shapes</code>를 절차적으로 생성한다.1 이 데이터셋은 각 도형의 꼭짓점 좌표를 정답 레이블로 가지고 있기 때문에, 지도 학습 방식으로 네트워크를 훈련시킬 수 있다.5</p>
<p><code>MagicPoint</code>는 이 합성 데이터에 대해 훈련되며, 합성 이미지 내에서는 노이즈가 추가되더라도 기존의 코너 검출기보다 훨씬 강건한 성능을 보인다.14 하지만 이 모델은 실제 이미지의 복잡하고 다양한 텍스처와 패턴에 대한 일반화 성능은 아직 부족한 상태다.2</p>
<p><code>MagicPoint</code>는 최종 모델인 <code>SuperPoint</code>를 만들기 위한 ‘씨앗(seed)’ 역할을 한다.</p>
<h3>3.2  2단계 - 호모그래피 적응(Homographic Adaptation): 자기 지도 학습의 핵심</h3>
<p>호모그래피 적응은 합성 세계에서 얻은 초기 지식을 현실 세계로 확장하고 증류하는 ‘경작’ 과정이다. 이 단계의 목표는 합성 데이터로 훈련된 <code>MagicPoint</code>를 ’교사’로 활용하여, 레이블이 없는 대규모 실제 이미지 데이터셋(예: MS-COCO)에 대한 풍부한 의사-정답(pseudo-ground truth) 레이블을 생성하는 것이다.1</p>
<p>이 과정의 원리는 다음과 같다:</p>
<ol>
<li>
<p>임의의 실제 이미지 <span class="math math-inline">I</span>를 선택한다.</p>
</li>
<li>
<p>이 이미지에 다수의 랜덤 호모그래피(Homography) <span class="math math-inline">H</span>를 적용하여, 마치 카메라를 회전시키거나 다른 각도에서 본 것과 같은 여러 개의 왜곡된 이미지 <span class="math math-inline">I&#39;</span>를 생성한다.2</p>
</li>
<li>
<p>생성된 모든 왜곡 이미지 <span class="math math-inline">I&#39;</span>에 <code>MagicPoint</code> 검출기를 적용하여 관심 지점을 찾는다.</p>
</li>
<li>
<p>각 <span class="math math-inline">I&#39;</span>에서 검출된 관심 지점들을 다시 역 호모그래피 <span class="math math-inline">H^{-1}</span>를 적용하여 원본 이미지 <span class="math math-inline">I</span>의 좌표계로 투영한다.2</p>
</li>
<li>
<p>이 과정을 수백 개의 다른 호모그래피에 대해 반복하고, 원본 이미지 좌표계로 모인 모든 점들을 취합(aggregate)한다.</p>
</li>
</ol>
<p>이 과정을 통해 집계된 점들의 집합은 원본 이미지 <span class="math math-inline">I</span>에 대한 새로운 의사-정답 레이블이 된다. 이 레이블은 단일 시점에서 <code>MagicPoint</code>가 찾은 점들보다 훨씬 더 풍부하고, 여러 시점에서도 일관되게 검출되는 반복성 높은 점들로 구성된다.1 이처럼 강화된 검출 능력을 갖게 된 모델이 바로 <code>SuperPoint</code>다.1 여기서 제공되는 감독 신호는 특정 위치가 ’코너’라는 정적인 레이블이 아니라, “어떤 픽셀이 좋은 특징점이라면, 기하학적 변환 후에도 그에 해당하는 위치에서 일관되게 검출되어야 한다“는 동적인 ‘일관성(consistency)’ 원칙이다. 이것이 바로 수동 레이블링의 한계를 우회하는 자기 지도 학습의 정수다.</p>
<h3>3.3  3단계 - 통합 훈련(Joint Training): 검출과 기술을 동시에 최적화</h3>
<p>호모그래피 적응을 통해 실제 이미지에 대한 대규모 의사-정답 레이블이 확보되면, SuperPoint 네트워크 전체(인코더, 검출기 헤드, 기술자 헤드)를 end-to-end 방식으로 훈련시킨다.2 이 단계에서는 검출기와 기술자가 함께 최적화된다.</p>
<p>훈련은 이미지 쌍 <span class="math math-inline">(I, I&#39;)</span>을 사용하여 진행되며, <span class="math math-inline">I&#39;</span>는 <span class="math math-inline">I</span>에 랜덤 호모그래피 <span class="math math-inline">H</span>를 적용하여 생성된다. 총 손실 함수 <span class="math math-inline">L</span>은 관심 지점 손실 <span class="math math-inline">L_p</span>와 기술자 손실 <span class="math math-inline">L_d</span>의 가중 합으로 구성된다:</p>
<p><span class="math math-display">
L(X, D, X&#39;, D&#39;; H) = L_p(X, X&#39;) + λ * L_d(D, D&#39;; H)
</span><br />
<strong>관심 지점 손실 (<span class="math math-inline">L_p</span>):</strong> 이미지 <span class="math math-inline">I</span>와 <span class="math math-inline">I&#39;</span> 각각에 대해, 네트워크가 예측한 관심 지점 히트맵과 2단계에서 생성한 의사-정답 레이블 간의 픽셀 단위 교차 엔트로피 손실(pixel-wise cross-entropy loss)을 계산한다.5</p>
<p><strong>기술자 손실 (<span class="math math-inline">L_d</span>):</strong> 기술자 손실은 호모그래피 <span class="math math-inline">H</span>에 의해 대응 관계가 알려진 관심 지점 쌍에 대해 계산된다. <span class="math math-inline">d_i</span>를 이미지 <span class="math math-inline">I</span>의 관심 지점 <span class="math math-inline">p_i</span>에 대한 기술자라 하고, <span class="math math-inline">d&#39;_i</span>를 <span class="math math-inline">I&#39;</span>의 대응점 <span class="math math-inline">p&#39;_i = H(p_i)</span>에 대한 기술자라고 하자. 이 손실은 대응되는 기술자 쌍(긍정 쌍)은 서로 유사해지도록, 대응되지 않는 쌍(부정 쌍)은 서로 달라지도록 유도하는 쌍별 힌지 손실(pairwise hinge loss)을 사용한다.14 손실 함수의 형태는 다음과 같다:<br />
<span class="math math-display">
L_d(D, D&#39;; H) = \frac{1}{N_t} \sum_{(i,j) \in \mathcal{P}} \left( \max(0, m_p - \mathbf{d}_i^T \mathbf{d}&#39;_j) + \max(0, \mathbf{d}_i^T \mathbf{d}&#39;_k - m_n) \right)
</span><br />
여기서 <span class="math math-inline">(i, j)</span>는 긍정 쌍, <span class="math math-inline">(i, k)</span>는 부정 쌍을 의미하며, <span class="math math-inline">m_p</span>와 <span class="math math-inline">m_n</span>은 각각 긍정 및 부정 마진이다 (예: <span class="math math-inline">m_p = 1</span>, <span class="math math-inline">m_n = 0.2</span>). 이 손실 함수는 대응되는 기술자 쌍의 코사인 유사도는 <span class="math math-inline">m_p</span> 이상이 되도록, 대응되지 않는 쌍의 유사도는 <span class="math math-inline">m_n</span> 이하가 되도록 네트워크를 훈련시킨다.14 이 통합 훈련 과정을 통해 SuperPoint는 시점 변화에 불변하는 특징점을 정확하게 검출하고, 동시에 변별력 높은 기술자를 생성하는 능력을 학습하게 된다.</p>
<h2>4.  성능 벤치마크 및 심층 비교 분석</h2>
<h3>4.1  HPatches 데이터셋: 표준 평가의 장</h3>
<p>알고리즘의 성능을 객관적으로 평가하기 위해서는 표준화된 벤치마크가 필수적이다. 지역 특징점(local feature) 분야에서 HPatches 데이터셋은 사실상의 표준으로 통용된다.17 이 데이터셋은 116개의 다른 장면에 대해, 실제 시점(viewpoint) 변화와 합성 조명(illumination) 변화를 포함하는 이미지 시퀀스로 구성되어 있어, 검출기와 기술자의 강건성을 종합적으로 평가하는 데 매우 적합하다.15</p>
<h3>4.2  핵심 성능 지표 분석</h3>
<p>HPatches 벤치마크에서는 다음과 같은 여러 핵심 지표를 통해 알고리즘의 성능을 다각도로 측정한다:</p>
<ul>
<li>
<p><strong>반복성 (Repeatability):</strong> 두 이미지 간의 알려진 기하학적 변환(호모그래피) 하에서, 첫 번째 이미지에서 검출된 특징점이 두 번째 이미지의 대응 영역 내에서 얼마나 일관되게 다시 검출되는지를 백분율로 나타낸다.14 이는 안정적인 매칭의 가장 기본적인 전제 조건이다.</p>
</li>
<li>
<p><strong>평균 위치 결정 오차 (Mean Localization Error, MLE):</strong> 올바르게 대응된 특징점 쌍 간의 평균 픽셀 거리 오차를 측정한다. 이 값이 낮을수록 검출된 특징점의 위치 정확도가 높음을 의미한다.14</p>
</li>
<li>
<p><strong>최근접 이웃 평균 정밀도 (Nearest Neighbor mAP):</strong> 기술자의 변별력(discriminative power)을 평가하는 핵심 지표다. 한 이미지의 기술자가 다른 이미지에서 수많은 후보 기술자들 중 자신의 올바른 짝을 얼마나 정확하게 최근접 이웃으로 찾아내는지를 측정한다.14</p>
</li>
<li>
<p><strong>정합 점수 (Matching Score):</strong> 검출기와 기술자의 성능을 종합하여, 두 이미지 간에 올바른 대응점(correspondence)을 최종적으로 얼마나 많이 찾아내는지를 평가하는 실용적인 지표다.</p>
</li>
<li>
<p><strong>호모그래피 추정 (Homography Estimation):</strong> 검출된 대응점들을 사용하여 두 이미지 간의 호모그래피 행렬을 추정하고, 이를 실제 정답(ground-truth)과 비교하여 정확도를 측정한다. 이는 특징점의 품질이 실제 기하학적 문제 해결에 얼마나 효과적으로 기여하는지를 보여주는 최종적인 응용 수준의 평가 지표다.15</p>
</li>
</ul>
<h3>4.3  정량적 성능 비교표</h3>
<p>SuperPoint는 HPatches 벤치마크에서 SIFT, ORB와 같은 전통적인 방법들을 여러 핵심 지표에서 압도하며 최첨단(state-of-the-art) 성능을 입증했다.15 아래 표는 SuperPoint와 SIFT의 성능을 정량적으로 비교한 결과다.</p>
<table><thead><tr><th>지표</th><th>SuperPoint (사전 훈련)</th><th>SIFT (하위 픽셀 정확도)</th><th>설명</th></tr></thead><tbody>
<tr><td><strong>호모그래피 추정</strong></td><td></td><td></td><td>추정된 호모그래피의 정확도 (코너 오차 임계값 <span class="math math-inline">\epsilon=1, 3, 5</span> 픽셀)</td></tr>
<tr><td><span class="math math-inline">\epsilon = 1</span></td><td>0.44</td><td><strong>0.63</strong></td><td><span class="math math-inline">\epsilon=1</span>에서 SIFT가 더 높은데, 이는 SIFT가 더 적지만 매우 신뢰도 높은 점을 생성하는 경향과 관련될 수 있다.</td></tr>
<tr><td><span class="math math-inline">\epsilon = 3</span></td><td><strong>0.77</strong></td><td>0.76</td><td>더 관대한 임계값에서는 SuperPoint가 SIFT와 대등하거나 우세한 성능을 보인다.</td></tr>
<tr><td><span class="math math-inline">\epsilon = 5</span></td><td><strong>0.83</strong></td><td>0.79</td><td>SuperPoint가 더 많은 대응점을 제공하여 전반적인 강건성에서 우위를 점함을 시사한다.</td></tr>
<tr><td><strong>검출기 지표</strong></td><td></td><td></td><td>검출기 자체의 성능 지표.</td></tr>
<tr><td>반복성 (Repeatability)</td><td><strong>0.606</strong></td><td>0.51</td><td>SuperPoint가 시점/조명 변화에 대해 더 일관된 특징점을 검출한다.</td></tr>
<tr><td>MLE (px)</td><td><strong>1.14</strong></td><td>1.16</td><td>두 검출기의 위치 정확도는 유사하다 (값이 낮을수록 좋음).</td></tr>
<tr><td><strong>기술자 지표</strong></td><td></td><td></td><td>기술자 자체의 성능 지표.</td></tr>
<tr><td>NN mAP</td><td><strong>0.81</strong></td><td>0.70</td><td>SuperPoint 기술자가 SIFT보다 훨씬 더 변별력이 높아 올바른 대응점을 찾을 확률이 높다.</td></tr>
<tr><td>정합 점수 (Matching Score)</td><td><strong>0.55</strong></td><td>0.27</td><td>검출기와 기술자를 종합했을 때, SuperPoint가 올바른 매칭을 생성하는 능력이 SIFT의 두 배에 가깝다.</td></tr>
</tbody></table>
<p>데이터 출처:.17</p>
<p>이 표는 여러 중요한 사실을 보여준다. 첫째, SuperPoint는 반복성(0.606 vs 0.51)과 기술자 변별력(NN mAP 0.81 vs 0.70) 모두에서 SIFT를 능가한다. 이는 더 안정적인 지점을 찾고, 그 지점을 더 잘 구별해낸다는 의미다. 둘째, 이 두 장점이 결합되어 최종 정합 점수(0.55 vs 0.27)에서 두 배 이상의 압도적인 성능 차이를 만들어낸다.</p>
<p>흥미로운 점은 가장 엄격한 호모그래피 추정 조건(<span class="math math-inline">\epsilon=1</span>)에서 SIFT가 더 나은 성능을 보인다는 것이다. 이는 SIFT가 더 적은 수의, 하지만 매우 높은 정밀도를 가진 특징점을 추출하는 경향이 있기 때문일 수 있다. 반면 SuperPoint는 더 많은 특징점을 추출하여 장면 전체를 더 빽빽하게 표현한다. 따라서 매우 정밀한 추정이 필요할 때는 SIFT의 고신뢰도 소수점이 유리할 수 있지만, 약간의 오차를 허용하는 더 강건한 추정이 필요할 때는 SuperPoint의 풍부한 특징점 집합이 더 나은 결과를 가져온다 (<span class="math math-inline">\epsilon=3, 5</span>에서 성능 역전). 이는 응용 분야의 요구사항에 따라 최적의 특징점 추출기가 다를 수 있음을 시사한다.</p>
<h3>4.4  도전적인 시나리오에서의 강건성</h3>
<p>벤치마크 수치를 넘어, SuperPoint의 진정한 강점은 도전적인 실제 환경에서 발휘된다. 큰 시점 변화나 극심한 조명 변화가 있는 시나리오에서 SIFT, SURF, ORB와 같은 전통적인 방법들의 성능은 급격히 저하되는 반면, SuperPoint는 훨씬 더 강건한 성능을 유지한다.4</p>
<p>특히, 반복적인 패턴이나 텍스처가 거의 없는(low-texture) 환경은 전통적인 그래디언트 기반 방법들에게 매우 어려운 문제다.10 이러한 상황에서 기존 방법들은 특징점을 거의 찾지 못하거나 신뢰할 수 없는 점들을 추출하는 반면, SuperPoint는 데이터로부터 학습된 더 고차원적인 표현을 바탕으로 안정적으로 유의미한 특징점을 추출할 수 있다. 이는 학습 기반 접근법이 다양한 변환과 환경에 대한 불변성을 데이터로부터 내재적으로 학습하기 때문에 가능한 것이다.4</p>
<h2>5.  응용 분야: 이론에서 실제 시스템으로</h2>
<p>SuperPoint는 이론적 우수성을 넘어 다양한 실제 컴퓨터 비전 시스템의 성능을 한 단계 끌어올리는 핵심 구성 요소로 자리 잡았다.</p>
<h3>5.1  Visual SLAM: 로봇과 자율주행의 눈</h3>
<p>Visual SLAM 시스템의 정확성과 강건성은 전적으로 특징점 검출 및 정합의 품질에 달려있다.12 기존의 ORB-SLAM과 같은 시스템들은 ORB 특징점의 한계로 인해 조명 변화가 심하거나 텍스처가 부족한 환경에서 추적에 실패하는 경우가 많았다. SuperPoint는 이러한 한계를 극복하기 위한 강력한 대안으로 부상했다.11</p>
<p>SuperPoint의 가장 큰 실용적 영향은 ORB-SLAM과 같은 기존의 복잡한 시스템에서 가장 취약한 부분인 ‘프론트엔드’(특징 추출 및 매칭)를 교체할 수 있는 모듈화된 대안을 제공했다는 점이다. 연구자들은 ORB-SLAM의 검증된 백엔드(최적화, 맵 관리)는 그대로 유지하면서, 프론트엔드의 ORB 특징 추출 부분만 SuperPoint로 교체하는 ’SuperPoint-SLAM’을 구현했다.12 그 결과, 시스템 전체를 재설계할 필요 없이 위치 추정 정확도와 강건성을 크게 향상시킬 수 있었다.12 다만, 표준 SuperPoint는 GPU 연산을 요구하므로, 임베디드 시스템 적용을 위해 Depthwise Separable Convolution 등을 사용한 경량화 모델이 제안되었으며, 이를 통해 CPU만으로도 25Hz 이상의 실시간 구동이 가능해졌다.11</p>
<h3>5.2  3D 재구성 및 동작 기반 구조 복원(SfM)</h3>
<p>여러 장의 2D 이미지로부터 3D 장면 구조와 카메라의 움직임을 복원하는 SfM 파이프라인의 핵심 역시 다중 시점 간의 정확한 특징점 정합이다.1 SuperPoint는 더 많고 정확한 대응점을 제공함으로써 SfM의 성능을 크게 향상시킨다.</p>
<p>특히 건설 현장 매핑과 같이 복잡하고 규모가 큰 시나리오에서 SuperPoint 기반 SfM은 SIFT와 같은 전통적인 방법보다 더 완전하고 정밀한 3D 재구성 결과를 생성한다.24 또한, 촬영 정보가 부족하고 이미지 품질이 낮은 역사적 항공 사진과 같은 매우 어려운 데이터셋에서도 SuperPoint와 후술할 SuperGlue를 결합한 파이프라인은 기존 방법이 실패하는 경우에도 성공적으로 3D 복원을 수행하여, 과거 데이터의 디지털 아카이빙 및 분석에 새로운 가능성을 열었다.25</p>
<h3>5.3  이미지 스티칭 및 증강 현실(AR)</h3>
<p><strong>이미지 스티칭(Image Stitching):</strong> 여러 장의 겹치는 이미지를 하나의 거대한 파노라마 이미지로 합치는 이미지 스티칭 기술은 정확한 호모그래피 추정에 의존한다.26 SuperPoint는 반복적인 패턴이 많은 농작물 이미지나 시차가 커서 전통적인 방법이 실패하기 쉬운 까다로운 스티칭 문제에 성공적으로 적용되어 고품질의 파노라마 생성을 가능하게 한다.26</p>
<p><strong>증강 현실(Augmented Reality, AR):</strong> AR은 실제 세계 영상 위에 가상 객체를 정합하는 기술로, 이를 위해서는 카메라의 6자유도(6DoF) 포즈를 실시간으로 정확하게 추정하는 것이 필수적이다.25 SuperPoint의 가볍고 효율적인 아키텍처는 모바일 및 임베디드 기기에서의 실시간 AR 응용에 매우 적합하다. 실제로 SuperPoint는 실시간 객체 추적 및 위치 결정을 위해 Magic Leap AR 플랫폼에 핵심 기술로 통합되었다.13</p>
<h3>5.4  SuperGlue 및 LightGlue와의 결합: 최첨단 정합 파이프라인</h3>
<p>SuperPoint는 그 자체로도 뛰어난 검출기 및 기술자이지만, 그 성능은 학습 기반 매칭기(matcher)와 결합될 때 극대화된다. SuperPoint의 등장 이후, 이를 입력으로 사용하는 강력한 매칭기들이 개발되면서 <code>SuperPoint + Matcher</code> 조합은 희소 특징 정합(sparse feature matching) 분야의 사실상 표준 파이프라인으로 자리 잡았다.</p>
<ul>
<li><strong>SuperGlue:</strong> 그래프 신경망(GNN)을 기반으로 하는 학습 기반 특징점 매칭기로, SuperPoint가 추출한 특징점과 기술자를 입력으로 받아 이미지의 컨텍스트 정보를 활용하여 매우 정확하고 강건한 매칭을 수행한다.25</li>
</ul>
<p><code>SuperPoint + SuperGlue</code> 조합은 현재까지도 가장 강력한 특징점 정합 파이프라인 중 하나로 평가받는다.</p>
<ul>
<li><strong>LightGlue:</strong> SuperGlue보다 더 가볍고 빠른 대안으로, Transformer 아키텍처를 기반으로 한다. <code>SuperPoint + LightGlue</code> 조합은 조명 차이가 매우 큰 이미지 쌍과 같이 SIFT가 완전히 실패하는 극도로 어려운 상황에서도 성공적인 이미지 정합을 보여주며, 실시간 성능과 높은 정확도를 동시에 달성했다.26</li>
</ul>
<p>이러한 생태계의 등장은 개별 알고리즘의 우수성을 넘어, 강력한 시너지를 내는 ’솔루션 스택’이 구축되었음을 의미하며, 이는 다양한 응용 분야에서 SIFT와 같은 전통적 기준선을 대체하는 새로운 표준을 제시했다.</p>
<h2>6.  결론: SuperPoint의 유산과 미래 전망</h2>
<h3>6.1  SuperPoint가 컴퓨터 비전 분야에 미친 영향</h3>
<p>SuperPoint는 특징점 검출 및 기술 분야에서 단순한 성능 향상을 넘어 패러다임의 전환을 이끈 선구적인 연구다. 첫째, 수동 레이블링이 거의 불가능한 문제에 대해 호모그래피 적응이라는 창의적인 자기 지도 학습 해법을 제시함으로써, 유사한 문제들에 대한 딥러닝 접근법의 가능성을 입증했다.</p>
<p>둘째, 검출과 기술을 하나의 공유 인코더 기반의 end-to-end 아키텍처로 통합하여 효율성과 성능 면에서 새로운 표준을 제시했다. 이는 후속 연구들(예: R2D2, DISK)에 큰 영감을 주었으며, 지역 특징점 분야의 연구 방향을 학습 기반으로 전환시키는 데 결정적인 역할을 했다.4</p>
<p>마지막으로, 실용적인 측면에서 SuperPoint와 그 후속 매칭기들은 SLAM, SfM, AR 등 다양한 고수준 비전 응용 프로그램의 강건성과 정확도를 한 단계 끌어올리는 핵심 구성 요소로 자리매김했다. 이는 이론적 성과가 실제 산업계에 미치는 영향을 명확히 보여주는 사례다.</p>
<h3>6.2  현재의 한계점과 향후 연구 방향성</h3>
<p>SuperPoint가 이룬 혁신에도 불구하고, 여전히 한계점과 미래의 연구 과제들이 존재한다.</p>
<ul>
<li>
<p><strong>호모그래피의 한계:</strong> SuperPoint의 자기 지도 학습은 2D 평면 변환인 호모그래피에 크게 의존한다. 그러나 실제 세계는 3D이며, 호모그래피는 평면이 아니거나 시차가 큰 장면의 복잡한 3D 변환을 완벽하게 모델링하지 못한다.3 이는 SuperPoint가 학습하는 불변성이 ’2D 변환 불변성’에 가깝다는 근본적인 한계를 내포한다.</p>
</li>
<li>
<p><strong>NeRF 기반 데이터 생성:</strong> 이러한 한계를 극복하기 위한 다음 단계는 ’진정한 3D 시점 불변성’을 학습하는 것이다. 최근 NeRF(Neural Radiance Fields)와 같은 3D 장면 표현 기술을 사용하여 사실적인 다중 시점 뷰를 합성하고, 이를 훈련 데이터로 활용하는 연구가 진행되고 있다.3 이는 호모그래피의 제약을 넘어 더 일반적이고 강력한 3D 불변성을 학습할 수 있는 새로운 가능성을 열고 있다. 이는 3D 장면 이해 기술의 발전이 특징점 검출과 같은 고전적인 2D 비전 문제의 훈련 방식을 근본적으로 바꾸고 있음을 보여주는 중요한 기술 융합 사례다.</p>
</li>
<li>
<p><strong>효율성과 경량화:</strong> 실시간 임베디드 응용을 위해서는 지속적인 모델 경량화 연구가 필수적이다. MobileNetV3와 같은 경량 백본 네트워크를 사용하거나, 대규모 SuperPoint 모델의 지식을 소형 모델로 이전하는 지식 증류(knowledge distillation) 기법을 통해 성능을 유지하면서 계산 비용을 줄이는 접근법이 활발히 연구되고 있다.4</p>
</li>
</ul>
<p>SuperPoint는 한 시대의 정점을 찍은 연구이자, 동시에 다음 시대를 연 연구로 평가받을 것이다. 자기 지도 학습을 통해 특징점의 본질에 대한 이해를 데이터로부터 이끌어냈으며, 그 유산은 앞으로 더 강건하고, 더 효율적이며, 진정한 3D를 이해하는 차세대 특징점 알고리즘의 개발을 위한 굳건한 발판이 될 것이다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>SuperPoint: Self-Supervised Interest Point Detection and Description - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/DeTone_SuperPoint_Self-Supervised_Interest_CVPR_2018_paper.pdf</li>
<li>SuperPoint: Self-Supervised Interest Point Detection and Description - Saraswathi Mamidala, https://saraswathimamidala30.medium.com/superpoint-self-supervised-interest-point-detection-and-description-7d6b7b0ccf57</li>
<li>nerf-supervised feature point detection and description - arXiv, https://arxiv.org/pdf/2403.08156</li>
<li>FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching - arXiv, https://arxiv.org/html/2507.10770v1</li>
<li>SuperPoint: A Case Study in Self-Supervised Networks for Keypoint …, https://engineering.purdue.edu/kak/computervision/ECE661Folder/KeypointsDetectionWithNN.pdf</li>
<li>What are the advantages and disadvantages of SIFT? - Quora, https://www.quora.com/What-are-the-advantages-and-disadvantages-of-SIFT</li>
<li>Performance and Trade-off Evaluation of SIFT, SURF, FAST, STAR and ORB feature detection algorithms in Visual Odometry - DergiPark, https://dergipark.org.tr/tr/download/article-file/1375257</li>
<li>ALGD-ORB: An improved image feature extraction algorithm with …, https://pmc.ncbi.nlm.nih.gov/articles/PMC10593235/</li>
<li>SIFT vs. ORB: Which Feature Detector Performs Best for Real-Time Applications?, https://eureka.patsnap.com/article/sift-vs-orb-which-feature-detector-performs-best-for-real-time-applications</li>
<li>DAN-SuperPoint: Self-Supervised Feature Point Detection Algorithm with Dual Attention Network - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8914829/</li>
<li>A Visual SLAM Model Based on Lightweight SuperPoint and Depth …, https://www.researchgate.net/publication/369120879_A_Visual_SLAM_Model_Based_on_Lightweight_SuperPoint_and_Depth_Metric_Learning</li>
<li>Augmenting ORB‑SLAM3 with Deep Features, Adaptive NMS, and Learning‑Based Loop Closure - arXiv, https://arxiv.org/html/2506.13089v1</li>
<li>SuperPoint(MagicLeap) as a local feature extractor - Kaggle, https://www.kaggle.com/code/suraj520/superpoint-magicleap-as-a-local-feature-extractor</li>
<li>SuperPoint: Self-Supervised Interest Point Detection and Description, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/superpoint.html</li>
<li>SuperPoint: Self-Supervised Interest Point Detection and Description - ResearchGate, https://www.researchgate.net/publication/321963712_SuperPoint_Self-Supervised_Interest_Point_Detection_and_Description</li>
<li>SuperPoint - Hugging Face, https://huggingface.co/docs/transformers/model_doc/superpoint</li>
<li>eric-yyjau/pytorch-superpoint: Superpoint Implemented in PyTorch: https://arxiv.org/abs/1712.07629 - GitHub, https://github.com/eric-yyjau/pytorch-superpoint</li>
<li>HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors | Request PDF - ResearchGate, https://www.researchgate.net/publication/320968592_HPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptors</li>
<li>rpautrat/SuperPoint: Efficient neural feature detector and descriptor - GitHub, https://github.com/rpautrat/SuperPoint</li>
<li>[1712.07629] SuperPoint: Self-Supervised Interest Point Detection and Description - arXiv, https://arxiv.org/abs/1712.07629</li>
<li>Superpoint · Models - Dataloop, https://dataloop.ai/library/model/magic-leap-community_superpoint/</li>
<li>SupSLAM: A Robust Visual Inertial SLAM System Using SuperPoint for Unmanned Aerial Vehicles | Request PDF - ResearchGate, https://www.researchgate.net/publication/358453891_SupSLAM_A_Robust_Visual_Inertial_SLAM_System_Using_SuperPoint_for_Unmanned_Aerial_Vehicles</li>
<li>D3L-SLAM: A Comprehensive Hybrid Simultaneous Location and Mapping System with Deep Keypoint, Deep Depth, Deep Pose, and Line Detection - MDPI, https://www.mdpi.com/2076-3417/14/21/9748</li>
<li>Building Better Models: Benchmarking Feature Extraction and Matching for Structure from Motion at Construction Sites - MDPI, https://www.mdpi.com/2072-4292/16/16/2974</li>
<li>5): Example of the SuperPoint feature detector and descriptor result in… - ResearchGate, https://www.researchgate.net/figure/Figure-15-Example-of-the-SuperPoint-feature-detector-and-descriptor-result-in-two_fig3_364184629</li>
<li>AgRowStitch: A High-fidelity Image Stitching Pipeline for Ground-based Agricultural Images, https://arxiv.org/html/2503.21990v1</li>
<li>11 -Illustration of SuperPoint training process [29]. | Download Scientific Diagram, https://www.researchgate.net/figure/Illustration-of-SuperPoint-training-process-29_fig26_360514106</li>
<li>PH8411/image-matching: Superpoint and Superglue training and testing code, also some traditional method (SIFT and ORB) - GitHub, https://github.com/PH8411/image-matching</li>
<li>Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation - SciTePress, https://www.scitepress.org/Papers/2025/133382/133382.pdf</li>
<li>Are learned interest point/feature detectors used much in industry for SLAM/ego-motion? : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/1areks7/are_learned_interest_pointfeature_detectors_used/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>