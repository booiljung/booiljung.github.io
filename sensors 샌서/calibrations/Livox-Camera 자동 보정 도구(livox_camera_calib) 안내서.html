<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:Livox-Camera 자동 보정 도구(livox_camera_calib) 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>Livox-Camera 자동 보정 도구(livox_camera_calib) 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">센서 (Sensors)</a> / <a href="index.html">HKU-MaRS</a> / <span>Livox-Camera 자동 보정 도구(livox_camera_calib) 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>Livox-Camera 자동 보정 도구(livox_camera_calib) 안내서</h1>
<h2>1.  <code>livox_camera_calib</code>: 개요 및 핵심 원리</h2>
<h3>1.1  서론: LiDAR-카메라 융합의 중요성</h3>
<p>자율주행, 로보틱스, 3D 장면 재구성 등 현대 지능형 시스템의 핵심은 주변 환경을 정확하게 인식하는 능력에 있다. 이를 위해 서로 다른 특성을 가진 센서들을 융합하는 기술이 필수적이다. 특히, Light Detection and Ranging (LiDAR) 센서는 레이저를 이용하여 주변 환경까지의 거리를 정밀하게 측정함으로써 희소하지만 정확한 3D 포인트 클라우드를 제공한다. 반면, 카메라는 풍부한 색상과 텍스처 정보를 담고 있는 2D 이미지를 제공하여 객체 인식 및 분류에 강점을 보인다.1</p>
<p>이 두 센서의 장점을 결합하면, 즉 3D 포인트 클라우드에 색상 정보를 입히거나(colorization), 2D 이미지 상의 객체에 3D 공간 정보를 부여함으로써, 단일 센서만으로는 불가능했던 고차원의 환경 인식이 가능해진다.3 이러한 센서 융합의 성공 여부는 두 센서 간의 기하학적 관계, 즉 외부 파라미터(Extrinsic Parameter)를 얼마나 정확하게 추정하는지에 달려있다. 외부 파라미터는 한 센서의 좌표계를 다른 센서의 좌표계로 변환하는 회전(Rotation) 및 변위(Translation) 정보로 구성되며, 이 값이 부정확할 경우 융합된 데이터에 심각한 왜곡이 발생하여 시스템 전체의 성능 저하를 초래한다.1</p>
<h3>1.2  <code>livox_camera_calib</code> 패키지 소개</h3>
<p><code>livox_camera_calib</code>은 홍콩대학교 MARS Lab에서 개발한 ROS(Robot Operating System) 패키지로, 고해상도 LiDAR와 카메라 간의 외부 파라미터를 자동으로 보정하기 위한 강력하고 정밀한 도구이다.4 이 패키지는 기존의 번거로운 보정 절차를 혁신적으로 개선한 여러 핵심 특징을 가지고 있다.</p>
<ul>
<li>
<p><strong>Targetless Calibration (타겟리스 보정)</strong>: 전통적인 보정 방식은 체커보드나 특정 패턴이 인쇄된 인공적인 보정 타겟을 필요로 한다. 하지만 이 패키지는 이러한 타겟 없이, 일상적인 자연 환경에서 곧바로 보정을 수행할 수 있다.2 이는 현장 적용성을 극대화하고 준비 과정을 대폭 단축시킨다.</p>
</li>
<li>
<p><strong>Edge-based Approach (엣지 기반 접근법)</strong>: 알고리즘의 핵심은 인공적인 특징점 대신, 건물 모서리, 창틀, 도로 경계선 등 환경에 자연적으로 존재하는 ‘엣지(edge)’ 정보를 활용하는 것이다. LiDAR 포인트 클라우드에서 추출한 3D 엣지와 카메라 이미지에서 추출한 2D 엣지를 정렬함으로써 두 센서 간의 공간적 관계를 추정한다.4</p>
</li>
<li>
<p><strong>High Accuracy (고정밀성)</strong>: 적절한 엣지 정보가 존재하는 환경에서는, 타겟 기반 방식에 필적하거나 심지어 이를 능가하는 픽셀 수준의 높은 보정 정확도를 달성할 수 있다.4</p>
</li>
<li>
<p><strong>Versatility (범용성)</strong>: 실내와 실외 환경을 가리지 않고 안정적으로 작동하며, 특히 Livox LiDAR와 같이 비반복 스캔(non-repetitive scanning) 패턴을 통해 조밀한 포인트 클라우드를 생성하는 고해상도 센서에 최적화되어 있다.4</p>
</li>
</ul>
<h3>1.3  기반 연구: <em>Pixel-level Extrinsic Self Calibration of High Resolution LiDAR and Camera in Targetless Environments</em></h3>
<p>이 패키지의 이론적 토대는 동일한 연구팀이 발표한 논문 <em>Pixel-level Extrinsic Self Calibration of High Resolution LiDAR and Camera in Targetless Environments</em>에 근거한다.4 이 연구는 기존 보정 방법론의 한계를 극복하기 위한 새로운 접근법을 제시하였다.</p>
<p>전통적인 회전형 LiDAR(예: Velodyne)는 스캔 라인 간의 간격이 넓어 희소한(sparse) 포인트 클라우드를 생성하며, 이로 인해 타겟의 특징(예: 체커보드 평면)을 정확히 추출하기 어려운 경우가 많았다.2 본 연구는 Livox LiDAR와 같은 고해상도 센서가 제공하는 밀집된(dense) 포인트 클라우드에 주목했다. 이 밀집된 데이터로부터는 환경의 구조적 특징인 3D 엣지를 매우 안정적이고 정확하게 추출할 수 있다. 논문은 이러한 자연적 엣지 특징을 카메라 이미지의 엣지와 정렬하는 것이 외부 파라미터를 효과적으로 추정할 수 있는 강력한 제약 조건(constraint)이 됨을 이론적으로 분석하고 실험적으로 입증하였다. 구체적으로는 포인트 클라우드를 복셀(voxel) 단위로 분할하고 각 복셀 내에서 평면을 피팅하여 평면 간의 교선을 엣지로 추출하는 효율적인 구현 방법을 제안하였다.5</p>
<h3>1.4  타겟리스 보정의 패러다임과 본 도구의 위상</h3>
<p>LiDAR-카메라 보정 기술은 크게 두 가지 패러다임으로 나뉜다. ‘타겟 기반(Target-based)’ 방식은 체커보드, ArUco 마커 등 사전에 크기와 형태를 아는 타겟을 사용하여 명시적인 대응점(correspondence)을 찾아 문제를 해결한다.1 이 방식은 제어된 환경에서 매우 높은 정확도를 보장하지만, 타겟을 설치하고 여러 각도에서 촬영해야 하는 번거로움이 있으며 현장에서 즉각적인 재보정이 어렵다는 단점이 있다.</p>
<p>반면, <code>livox_camera_calib</code>이 속한 ‘타겟리스(Targetless)’ 방식은 이러한 준비 과정 없이 자연 환경의 특징을 직접 활용한다.2 이 방식의 가장 큰 가치는 ’편의성’에 있다. 그러나 이 편의성은 새로운 형태의 제약, 즉 ’장면 자체’가 보정에 적합해야 한다는 조건과 맞바꾼 결과이다. 타겟을 준비하는 부담이 사라진 대신, 다양한 방향의 엣지가 풍부하게 분포된 ‘특징적인(feature-rich)’ 환경을 찾아야 하는 부담이 생긴 것이다. 이는 사용자가 단순히 도구를 사용하는 것을 넘어, 데이터 취득 환경의 질을 평가하고 선택해야 함을 의미한다. 따라서 <code>livox_camera_calib</code>은 타겟 기반 방식의 물리적 제약을 환경적 제약으로 전환함으로써, 현장 적용성과 신속성을 극대화한 진일보한 도구로 평가할 수 있다.</p>
<h2>2.  시스템 구축 및 설치</h2>
<h3>2.1  필수 의존성</h3>
<p><code>livox_camera_calib</code> 패키지를 성공적으로 빌드하고 실행하기 위해서는 다음의 개발 환경과 라이브러리가 사전에 구축되어 있어야 한다. 명시된 버전은 개발 및 테스트가 검증된 환경이므로, 가급적 이를 준수하는 것이 좋다.4</p>
<ul>
<li>
<p><strong>운영체제</strong>: Ubuntu 16.04 또는 18.04 (64-bit)</p>
</li>
<li>
<p><strong>ROS</strong>: ROS Kinetic 또는 Melodic</p>
</li>
<li>
<p><strong>핵심 라이브러리</strong>:</p>
</li>
<li>
<p><strong>Eigen</strong>: C++용 고성능 선형대수 라이브러리. 대부분의 리눅스 배포판에서 패키지 매니저를 통해 쉽게 설치할 수 있다.</p>
</li>
<li>
<p><code>sudo apt-get install libeigen3-dev</code></p>
</li>
<li>
<p><strong>Ceres Solver</strong>: Google에서 개발한 비선형 최적화 라이브러리. 이 패키지의 핵심인 재투영 오차 최소화에 사용된다. 소스 코드를 직접 빌드하여 설치해야 하며, 버전 호환성이 매우 중요하다. (자세한 내용은 VI. 문제 해결 및 주요 이슈 장에서 다룬다.)</p>
</li>
<li>
<p><strong>PCL (Point Cloud Library)</strong>: 포인트 클라우드 처리를 위한 종합 라이브러리. 이 패키지에서는 1.7 버전으로 테스트되었다.</p>
</li>
<li>
<p><code>sudo apt-get install libpcl-dev</code></p>
</li>
<li>
<p><strong>ROS Packages</strong>: ROS와 다른 라이브러리 간의 데이터 변환을 위한 필수 패키지들이다.</p>
</li>
<li>
<p><code>sudo apt-get install ros-&lt;distro&gt;-cv-bridge ros-&lt;distro&gt;-pcl-conversions</code></p>
</li>
<li>
<p><code>&lt;distro&gt;</code> 부분은 <code>kinetic</code> 또는 <code>melodic</code>으로 대체한다.</p>
</li>
</ul>
<h3>2.2  소스 코드 빌드</h3>
<p>모든 의존성이 설치되었다면, 다음 단계에 따라 <code>catkin</code> 작업 공간에서 소스 코드를 클론하고 빌드한다.4</p>
<ol>
<li>
<p><strong><code>catkin</code> 작업 공간의 <code>src</code> 디렉토리로 이동한다.</strong></p>
<pre><code class="language-Bash">cd ~/catkin_ws/src
</code></pre>
</li>
</ol>
<pre><code>
2. **GitHub 저장소를 클론한다.**

   ```Bash
   git clone https://github.com/hku-mars/livox_camera_calib.git
</code></pre>
<ol start="3">
<li>
<p><strong>작업 공간의 최상위 디렉토리로 이동하여 빌드한다.</strong></p>
<pre><code class="language-Bash">cd..
catkin_make
</code></pre>
</li>
</ol>
<pre><code>
4. **빌드가 완료되면, 환경 설정 스크립트를 `source`하여 현재 터미널 세션에 패키지를 등록한다.**

   ```Bash
   source ~/catkin_ws/devel/setup.bash
</code></pre>
<p>이 명령어는 터미널을 새로 열 때마다 실행하거나, <code>.bashrc</code> 파일에 추가하여 자동으로 실행되도록 할 수 있다.</p>
<h3>2.3  (선택) Docker를 이용한 환경 구축</h3>
<p><code>livox_camera_calib</code> 패키지는 특정 버전의 라이브러리(특히 Ceres Solver)에 강하게 의존하기 때문에, 사용자의 시스템 환경에 따라 빌드 과정에서 예기치 않은 오류가 발생할 수 있다. GitHub 이슈 페이지에는 이러한 의존성 문제로 인한 컴파일 실패 사례가 다수 보고되고 있다.8 이러한 복잡성을 근본적으로 해결하기 위한 매우 효과적인 대안은 Docker를 사용하는 것이다.</p>
<p><code>rosblox/ros-livox-camera-calib</code>는 이 패키지를 즉시 실행할 수 있도록 모든 의존성이 완벽하게 설정된 Docker 이미지를 제공하는 커뮤니티 기반 프로젝트이다.9 Docker 이미지를 사용하면 호스트 시스템의 운영체제나 라이브러리 버전에 구애받지 않고, 격리되고 재현 가능한 환경에서 보정 도구를 실행할 수 있다. 이러한 Docker 화(Dockerization) 프로젝트의 존재 자체가 원본 저장소의 빌드 환경이 얼마나 민감하고 관리하기 어려운지를 방증한다. 이는 단순히 편의를 위한 선택이 아니라, 빌드 과정의 불확실성을 제거하고 신속하게 프로젝트를 시작하고자 하는 사용자에게 가장 먼저 권장되어야 할 해결책이다.</p>
<p>다음 명령어를 통해 미리 빌드된 Docker 이미지를 내려받을 수 있다.9</p>
<pre><code class="language-Bash">docker pull ghcr.io/rosblox/ros-livox-camera-calib:melodic
</code></pre>
<p>이후에는 <code>docker run</code> 명령어를 통해 컨테이너를 실행하고, 컨테이너 내부에서 <code>roslaunch</code> 명령어를 사용하여 보정 작업을 수행할 수 있다.</p>
<h2>3.  보정 실행: 예제 데이터 분석</h2>
<h3>3.1  예제 데이터 다운로드</h3>
<p>패키지의 기본적인 사용법을 익히기 위해, 개발팀에서 제공하는 예제 데이터셋을 사용하는 것이 좋다. 단일 장면(single-scene) 및 다중 장면(multi-scenes) 보정을 위한 데이터셋은 아래 링크에서 다운로드할 수 있다.4</p>
<ul>
<li>
<p><strong>OneDrive</strong>:(https://connecthkuhk-my.sharepoint.com/:f:/g/personal/ycj1_connect_hku_hk/EuZs1x2RHbxFikjsvt9qf80BD8Wjj05ZhVGRgzfzLCQUCQ?e=un8r1y)</p>
</li>
<li>
<p><strong>Baidu Netdisk</strong>:(https://pan.baidu.com/s/1oz3unqsmDnFvBExY5fiBJQ?pwd=i964)</p>
</li>
</ul>
<h3>3.2  단일 장면 보정 실행</h3>
<p>단일 장면 보정은 하나의 포인트 클라우드 파일(.pcd)과 하나의 이미지 파일을 사용하여 외부 파라미터를 추정하는 가장 기본적인 방식이다.</p>
<ol>
<li>
<p>다운로드한 예제 데이터(pcd 파일과 이미지 파일)를 로컬 경로에 저장한다.</p>
</li>
<li>
<p><code>livox_camera_calib/config/calib.yaml</code> 파일을 텍스트 편집기로 연다.</p>
</li>
<li>
<p><code>pcd_path</code>와 <code>image_path</code> 파라미터의 값을 1단계에서 저장한 파일의 절대 경로로 수정한다.</p>
</li>
<li>
<p>터미널에서 아래의 <code>roslaunch</code> 명령어를 실행한다.4</p>
<pre><code class="language-Bash">roslaunch livox_camera_calib calib.launch
</code></pre>
</li>
</ol>
<pre><code>
실행이 시작되면 RViz 창이 열리며, 초기 외부 파라미터로 투영된 포인트 클라우드와 최적화 과정을 거쳐 점차 정렬되는 모습을 시각적으로 확인할 수 있다. 최종 보정 결과는 터미널과 파일로 출력된다.

### 3.3  다중 장면 보정 실행


다중 장면 보정은 여러 위치와 각도에서 촬영한 복수의 데이터 쌍을 사용하여 보정의 정확도와 강인함(robustness)을 향상시키는 기능이다.4 단일 장면에서는 관측되지 않는 자유도(degree of freedom)에 대한 제약을 추가할 수 있어 더욱 신뢰도 높은 결과를 얻을 수 있다.

1. 다중 장면 보정용 예제 데이터를 로컬 경로에 저장한다.

2. 데이터 파일의 이름을 규칙에 맞게 변경해야 한다. 예를 들어, 3개의 데이터 쌍이 있다면 `0.pcd`, `0.png`, `1.pcd`, `1.png`, `2.pcd`, `2.png`와 같이 0부터 시작하는 숫자로 순차적으로 명명한다.4

3. `livox_camera_calib/config/multi_calib.yaml` 파일을 연다.

4. `file_path`를 데이터가 저장된 디렉토리의 절대 경로로 수정하고, `data_num`을 사용할 데이터 쌍의 개수로 설정한다.

5. 터미널에서 아래의 명령어를 실행한다.4

   ```Bash
   roslaunch livox_camera_calib multi_calib.launch
</code></pre>
<h3>3.4  설정 파일 심층 분석 (<code>calib.yaml</code>)</h3>
<p>보정의 성공 여부와 결과의 품질은 <code>calib.yaml</code> 파일의 파라미터 설정에 크게 좌우된다. 각 파라미터의 의미와 역할을 정확히 이해하는 것은 이 도구를 효과적으로 사용하기 위한 필수 과정이다.</p>
<table><thead><tr><th><strong>파라미터 (Parameter)</strong></th><th><strong>타입 (Type)</strong></th><th><strong>설명 (Description)</strong></th><th><strong>중요도 및 튜닝 가이드 (Importance &amp; Tuning Guide)</strong></th></tr></thead><tbody>
<tr><td><code>pcd_path</code></td><td>string</td><td>입력 포인트 클라우드 파일(.pcd)의 절대 경로.</td><td><strong>필수 수정.</strong> 사용자의 데이터 경로로 반드시 변경해야 한다.</td></tr>
<tr><td><code>image_path</code></td><td>string</td><td>입력 이미지 파일의 절대 경로.</td><td><strong>필수 수정.</strong> 사용자의 데이터 경로로 반드시 변경해야 한다.</td></tr>
<tr><td><code>camera_matrix</code></td><td>double array</td><td>카메라 내부 파라미터 행렬 (3x3). <code>[fx, 0, cx, 0, fy, cy, 0, 0, 1]</code> 형식.</td><td><strong>필수.</strong> 사전에 <code>camera_calibration</code> 등 표준 도구로 정확히 보정된 값을 사용해야 한다. 결과 정확도에 치명적인 영향을 미친다.</td></tr>
<tr><td><code>dist_coeffs</code></td><td>double array</td><td>카메라 왜곡 계수. <code>[k1, k2, p1, p2, k3]</code> 형식.</td><td><strong>필수.</strong> 내부 파라미터와 함께 제공되어야 한다. 왜곡이 심한 광각 렌즈일수록 중요하다.</td></tr>
<tr><td><code>extrinsic_T</code></td><td>double array</td><td>LiDAR-카메라 간 외부 파라미터 초기 추정값 (4x4 변환 행렬).</td><td><strong>강력 권장.</strong> 대략적인 값이라도 제공하면 최적화의 수렴 속도와 안정성을 크게 향상시킨다. 모를 경우 단위 행렬(identity matrix)로 설정할 수 있으나, 수렴에 실패할 수 있다.</td></tr>
<tr><td><code>voxel_size</code></td><td>double</td><td>LiDAR 엣지 추출 전, 포인트 클라우드를 다운샘플링하는 복셀의 크기 (단위: m).</td><td>성능과 정밀도의 트레이드오프 관계. 값이 작을수록 세밀한 엣지까지 고려하지만 계산량이 증가하고 노이즈에 민감해진다. 0.1 ~ 0.3 사이에서 시작하여 조정한다.</td></tr>
<tr><td><code>edge_feature_num</code></td><td>int</td><td>이미지에서 추출할 Canny 엣지 특징점의 최대 개수.</td><td>장면에 따라 조절한다. 엣지가 풍부한 복잡한 환경에서는 값을 늘려 더 많은 정보를 활용할 수 있다.</td></tr>
<tr><td><code>max_iter</code></td><td>int</td><td>Ceres Solver를 이용한 비선형 최적화 과정의 최대 반복 횟수.</td><td>일반적으로 기본값을 사용해도 충분하다. 최적화가 조기에 종료되거나 수렴이 잘 안될 경우 값을 늘려볼 수 있다.</td></tr>
<tr><td><code>max_dist</code></td><td>double</td><td>3D 엣지를 2D 이미지에 투영했을 때, 대응되는 2D 엣지를 찾기 위한 최대 탐색 거리 (단위: pixel).</td><td>이 값보다 멀리 떨어진 엣지 쌍은 이상치(outlier)로 간주되어 무시된다. 초기 추정값이 부정확할수록 이 값을 크게 설정해야 한다.</td></tr>
<tr><td><code>min_line_num</code></td><td>int</td><td>최적화에 사용될 최소 엣지 라인의 개수.</td><td>이 값보다 적은 수의 엣지만 추출되면 프로그램이 중단될 수 있다. 엣지가 부족한 환경에서는 값을 낮추어야 할 수 있다.</td></tr>
</tbody></table>
<h2>4.  사용자 데이터 적용 가이드</h2>
<h3>4.1  데이터 수집</h3>
<p>사용자 고유의 센서 시스템에 이 도구를 적용하기 위해서는 양질의 데이터를 수집하는 것이 가장 중요하다.</p>
<ul>
<li>
<p><strong>장면 선택의 중요성</strong>: 알고리즘은 엣지 특징에 전적으로 의존하므로, 데이터 수집 환경이 보정의 성패를 결정한다. 다양한 방향(수직, 수평, 대각선)의 길고 명확한 엣지가 풍부한 장면을 선택해야 한다. 예를 들어, 건물 외벽, 복도, 가구와 벽의 경계 등이 좋은 대상이다. 반면, 허허벌판이나 단조로운 벽면과 같이 엣지 정보가 거의 없는 환경은 피해야 한다.</p>
</li>
<li>
<p><strong>센서 동기화</strong>: LiDAR의 포인트 클라우드와 카메라 이미지는 가급적 동일한 시점에 획득되어야 한다. 가장 이상적인 방법은 하드웨어 트리거를 이용한 물리적 동기화이다. 이것이 불가능할 경우, ROS 타임스탬프를 기반으로 최대한 시간 차이가 적은 데이터 쌍을 선택하는 소프트웨어 동기화 방식을 사용한다.</p>
</li>
<li>
<p><strong>데이터 포맷</strong>: 카메라는 이미지 파일(예: PNG, JPG)로, LiDAR는 포인트 클라우드 파일(PCD)로 저장해야 한다.</p>
</li>
<li>
<p><strong>데이터 기록 (ROS Bag)</strong>: ROS 환경에서는 <code>rosbag</code>을 사용하여 여러 센서 데이터를 시간 정보와 함께 기록하는 것이 표준적인 방법이다. 예를 들어, 카메라 이미지 토픽이 <code>/camera/color/image_raw</code>이고 LiDAR 포인트 클라우드 토픽이 <code>/livox/lidar</code>일 경우, 다음 명령어로 두 토픽을 동시에 <code>my_data.bag</code> 파일에 기록할 수 있다.10</p>
<pre><code class="language-Bash">rosbag record /camera/color/image_raw /livox/lidar -O my_data.bag
</code></pre>
</li>
</ul>
<pre><code>
- **Bag to PCD 변환**: 기록된 `rosbag` 파일에서 특정 시점의 포인트 클라우드 데이터를 `.pcd` 파일로 추출해야 한다. PCL에서 제공하는 `bag_to_pcd` 도구나, 이 프로젝트 내에 포함될 수 있는 변환 스크립트를 활용할 수 있다.4

### 4.2  파라미터 설정


수집한 데이터를 사용하여 보정을 수행하기 전, 설정 파일을 사용자의 시스템에 맞게 수정해야 한다.

- **카메라 내부 파라미터**: `calib.yaml`의 `camera_matrix`와 `dist_coeffs`는 가장 먼저, 그리고 가장 정확하게 수정해야 할 파라미터이다. ROS의 `camera_calibration` 패키지와 체커보드를 사용하여 사전에 정밀하게 보정하는 것을 강력히 권장한다. 이 값이 부정확하면 외부 파라미터 또한 절대 정확할 수 없다.

- **초기 외부 파라미터**: `extrinsic_T`에 입력할 초기 추정값은 보정의 안정성에 큰 영향을 미친다. 센서를 장착할 때 줄자 등으로 대략적인 거리와 각도를 측정하거나, 센서의 CAD 모델을 참고하여 초기 변환 행렬을 계산하여 입력하는 것이 좋다.

- **튜닝 파라미터**: `voxel_size`, `edge_feature_num`, `max_dist`와 같은 파라미터들은 사용자의 센서 해상도와 데이터 취득 환경에 따라 최적값이 달라진다. 예를 들어, LiDAR의 해상도가 매우 높다면 `voxel_size`를 줄여 더 세밀한 특징을 활용할 수 있고, 초기 추정값이 매우 부정확하다면 `max_dist`를 늘려 대응점 탐색 범위를 넓혀야 한다.

## 5.  심층 분석: 알고리즘 및 좌표계 변환


### 5.1  알고리즘 파이프라인


`livox_camera_calib`의 보정 과정은 다음과 같은 단계로 구성된 최적화 파이프라인을 따른다.

1. **데이터 입력**: 사용자가 지정한 PCD 파일과 이미지 파일을 불러온다.

2. **LiDAR 3D 엣지 추출**: 포인트 클라우드 데이터에서 3차원 공간상의 엣지(선분)들을 추출한다.

3. **카메라 2D 엣지 추출**: 이미지 데이터에서 2차원 평면상의 엣지 픽셀들을 추출한다.

4. **데이터 연관 및 오차 계산**: 초기 외부 파라미터를 사용하여 3D 엣지를 이미지 평면에 투영하고, 투영된 엣지와 실제 이미지 엣지 간의 거리(재투영 오차)를 계산한다.

5. **비선형 최적화**: 재투영 오차의 총합을 최소화하는 외부 파라미터(회전 `R`, 변위 `t`)를 찾는다. 이 과정은 Ceres Solver를 통해 반복적으로 수행된다.

### 5.2  LiDAR 3D 엣지 추출


알고리즘의 핵심 중 하나는 노이즈가 포함된 방대한 포인트 클라우드로부터 어떻게 안정적으로 3D 엣지를 추출하는가이다. 논문에서는 '포인트 클라우드 복셀 커팅 및 평면 피팅(point cloud voxel cutting and plane fitting)'이라는 효율적인 방법을 제안했다.5

이 방법의 절차는 다음과 같다. 첫째, 전체 포인트 클라우드 공간을 일정한 크기의 3D 격자, 즉 복셀(voxel)로 분할한다. 둘째, 각 복셀에 포함된 포인트들을 대상으로 RANSAC과 같은 강인한 알고리즘을 사용하여 평면 모델(plane model)을 피팅한다. 셋째, 인접한 두 복셀이 각각 유효한 평면을 가지고 있고, 두 평면의 법선 벡터(normal vector)가 일정 각도 이상 차이가 날 경우, 두 평면이 만나는 교차 지점에 엣지가 존재한다고 판단한다. 이 두 평면의 교선을 계산하여 3D 엣지(선분)로 추출한다. 이 방식은 개별 포인트의 노이즈에 민감하지 않고, 데이터의 국소적인 기하 구조(평면)를 활용하기 때문에 매우 강인하다.

### 5.3  카메라 2D 엣지 추출


카메라 이미지로부터 2D 엣지를 추출하는 데에는 컴퓨터 비전 분야에서 널리 사용되는 표준 기법이 적용된다. 일반적으로 Canny Edge Detector와 같은 알고리즘이 사용된다. 이 알고리즘은 이미지의 그래디언트(gradient) 크기를 계산하고, 비최대 억제(non-maximum suppression)와 이력 스레시홀딩(hysteresis thresholding)을 통해 노이즈를 제거하고 가늘고 명확한 엣지 픽셀들의 집합을 추출한다.

### 5.4  좌표계 변환 및 비용 함수


보정 문제의 수학적 목표는 LiDAR 좌표계 ${L}$과 카메라 좌표계 ${C}$ 사이의 강체 변환(Rigid Body Transformation) 행렬 $T_{CL}$을 찾는 것이다. 이 변환은 3x3 회전 행렬 $R$과 3x1 변위 벡터 $t$로 구성된다. LiDAR 좌표계의 한 점 $P_L$은 다음 수식을 통해 카메라 좌표계의 점 $P_C$로 변환된다.

$$
P_C = R \cdot P_L + t
$$

카메라 좌표계의 3D 점 $P_C =^T$는 카메라 내부 파라미터(초점 거리 $f_x, f_y$, 주점 $c_x, c_y$)를 사용하여 이미지 평면의 2D 픽셀 좌표 $p = [u, v]^T$로 투영된다. 핀홀 카메라 모델(Pinhole Camera Model)에 의한 투영 수식은 다음과 같다.

$$
u = f_x \frac{X_C}{Z_C} + c_x
\\
v = f_y \frac{Y_C}{Z_C} + c_y
$$

알고리즘의 최종 목표는 LiDAR에서 추출한 3D 엣지들을 이미지 평면에 투영했을 때, 이 투영된 엣지들과 이미지에서 실제로 추출된 2D 엣지들 사이의 거리를 최소화하는 것이다. 이를 위해 비용 함수(Cost Function) `E`를 정의한다. i번째 3D 엣지 라인 위의 j번째 점을 $P_{L,i}^{(j)}$라 하고, 이 점과 가장 가까운 이미지 엣지 픽셀을 $p_{img,i}^{(j)}$라고 할 때, 비용 함수는 모든 엣지 포인트 쌍에 대한 재투영 오차의 제곱합으로 정의된다.

$$
\underset{R, t}{\operatorname{argmin}} \sum_{i} \sum_{j} \rho \left( \left\| \pi(R \cdot P_{L,i}^{(j)} + t) - p_{img,i}^{(j)} \right\|^2 \right)
$$

여기서 $\pi()$는 위에서 정의한 카메라 투영 함수를 의미하며, $\rho()$는 이상치(outlier)의 영향을 줄이기 위한 Huber Loss와 같은 강인한 손실 함수(robust loss function)이다. 이 비선형 최소제곱(non-linear least squares) 문제를 풀기 위해 Ceres Solver가 사용되며, 최적화 과정을 통해 오차를 최소화하는 $R$과 $t$ 값을 찾게 된다.

## 6.  문제 해결 및 주요 이슈


### 6.1  개요


`livox_camera_calib`은 강력한 도구이지만, 특정 라이브러리 버전에 대한 강한 의존성 때문에 설치 및 사용 과정에서 여러 문제에 직면할 수 있다. 공식 GitHub 저장소의 이슈(Issues) 페이지는 이러한 문제들을 해결하기 위한 중요한 정보의 보고이다.4 대부분의 문제는 의존성 라이브러리, 특히 Ceres Solver의 버전 불일치에서 기인한다.

### 6.2  주요 문제 및 해결 방안


아래 표는 커뮤니티에 자주 보고되는 문제들과 그에 대한 검증된 해결 방안을 정리한 것이다.

| **문제 현상 (Symptom)**                                                                                      | **이슈 번호 (Issue #)** | **근본 원인 (Root Cause)**                                                                                                                              | **해결 방안 (Solution)**                                                                                                                                                                             |
| -------------------------------------------------------------------------------------------------------- | ------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `catkin_make` 컴파일 에러: `expected type-specifier`, `new ceres::EigenQuaternionParameterization()` 관련 오류 발생 | #119                | Ceres Solver 버전 비호환. Ceres 2.1.0 이상 버전부터 `EigenQuaternionParameterization` API가 `EigenQuaternionManifold`로 변경되었다. 코드는 구버전 API를 사용하고 있어 발생하는 문제이다.8  | **방법 1 (권장): 코드 수정.** `src/lidar_camera_calib.cpp`와 `src/lidar_camera_multi_calib.cpp` 파일에서 `new ceres::EigenQuaternionParameterization()` 구문을 찾아 `new ceres::EigenQuaternionManifold()`로 수정한다.8 |
|                                                                                                          |                     |                                                                                                                                                     | **방법 2: Ceres 다운그레이드.** 시스템에 설치된 최신 Ceres Solver를 완전히 제거하고, 호환성이 검증된 2.0.0 버전을 소스 코드로 다시 빌드하여 설치한다.8                                                                                             |
| OpenCV 버전 관련 컴파일 에러                                                                                      | #124, #103          | 시스템에 설치된 OpenCV 버전(예: OpenCV 4.x)과 코드(주로 OpenCV 3.x 기반으로 작성됨) 간의 API 비호환 문제.                                                                        | 시스템의 기본 OpenCV 버전을 사용하도록 `CMakeLists.txt`의 `find_package(OpenCV...)` 부분을 수정하거나, 코드 내에서 사용된 OpenCV 함수들을 현재 버전에 맞게 변경한다.                                                                           |
| Livox Mid360 모델 사용 시 보정 실패 또는 투영 결과 이상                                                                   | #118, #114          | **원인 1: 왜곡 모델 불일치.** Mid360의 360도에 가까운 넓은 화각과 고유한 스캔 패턴으로 인해 발생하는 포인트 클라우드 왜곡이 코드에 제대로 처리되지 않았을 수 있다. 카메라 왜곡 계수 추가 시 중앙에 광선 형태의 노이즈가 발생한다는 보고가 있다.8 | **해결책:** 우선 `calib.yaml`에서 카메라 왜곡 계수(`dist_coeffs`)를 모두 0으로 설정하고 테스트한다. Livox SDK에서 제공하는 왜곡 보정(motion undistortion) 기능을 먼저 적용한 포인트 클라우드를 입력 데이터로 사용하는 것을 고려한다.                                   |
|                                                                                                          |                     | **원인 2: 최적화 발산.** 초기(Rough) 최적화 단계에서는 결과가 양호해 보이나, 정밀(Fine) 최적화 단계에서 결과가 갑자기 크게 벗어나는 현상이 보고되었다.8                                                    | **해결책:** `extrinsic_T`에 최대한 정확한 초기 외부 파라미터 값을 제공하여 최적화가 안정적인 지점에서 시작하도록 유도한다. 또는 엣지 특징이 더 명확하고 다양한 여러 장면을 사용한 다중 장면 보정을 시도하여 제약 조건을 강화한다.                                                        |
| 실행 시 `Cannot create a KDTree with an empty input cloud!` 에러 발생                                           | #94                 | LiDAR 3D 엣지 추출 단계에서 유의미한 엣지를 하나도 찾지 못하여 빈 포인트 클라우드가 생성된 경우. 장면 내에 엣지가 부족하거나, `voxel_size`와 같은 파라미터 설정이 부적절하여 모든 포인트가 필터링되었을 수 있다.                   | **해결책 1:** 엣지가 풍부한 다른 장면에서 데이터를 다시 수집한다. **해결책 2:** `calib.yaml`의 `voxel_size` 값을 줄여 다운샘플링 과정에서 더 많은 포인트가 유지되도록 설정한다.                                                                            |

## 7.  결론 및 관련 도구 고찰


### 7.1  `livox_camera_calib` 요약


`livox_camera_calib`은 타겟 없이 자연 환경에서 LiDAR와 카메라 간의 외부 파라미터를 픽셀 수준의 정확도로 자동 보정할 수 있는 혁신적인 도구이다. 특히 Livox와 같은 고해상도 LiDAR의 밀집된 포인트 클라우드에서 안정적인 엣지 특징을 추출하는 핵심 아이디어는 이 도구의 높은 성능과 강인함을 뒷받침한다. 현장에서의 신속한 보정 및 재보정이 가능하다는 점은 큰 장점이다.

하지만 이러한 장점 이면에는 몇 가지 한계점도 존재한다. 첫째, 알고리즘의 성능은 엣지가 풍부한 환경이라는 조건에 강하게 의존한다. 둘째, 특정 버전의 라이브러리에 대한 의존성이 높아 초기 환경 설정이 까다로울 수 있으며, 이는 Docker와 같은 가상화 기술의 필요성을 부각시킨다. 셋째, Livox Mid360과 같은 최신 센서 모델에 대해서는 추가적인 파라미터 튜닝이나 코드 수정이 필요할 수 있다.

### 7.2  관련 기술 및 대안 도구


`livox_camera_calib`을 이해하고 그 위치를 파악하기 위해서는 더 넓은 보정 기술의 맥락을 살펴보는 것이 유용하다.

- **타겟 기반 도구**:

- `lidar_camera_calibration`: ROS에서 널리 사용되는 패키지로, ArUco 마커가 부착된 판지를 사용하여 3D-3D 코너점 대응을 통해 보정을 수행한다.1

- MATLAB Lidar Toolbox: 체커보드를 사용하여 선과 평면 대응 관계를 기반으로 보정하는 기능을 제공하며, 상용 소프트웨어로서 안정적인 성능을 보인다.1

- Livox 공식 수동 보정 솔루션: Livox에서 직접 제공하는 매뉴얼로, 보드 코너를 타겟으로 사용하여 수동으로 대응점을 지정하고 외부 파라미터를 계산하는 방법을 안내한다.7

- **기타 타겟리스 도구**:

- 상호 정보량(Mutual Information) 기반 방법: 두 센서 데이터 간의 통계적 의존성을 최대화하는 방향으로 외부 파라미터를 찾는 접근법이다.13

- 딥러닝 기반 방법: 심층 신경망을 학습하여 이미지와 포인트 클라우드에서 특징을 추출하고, 이를 정합하여 보정 파라미터를 직접 추정하는 연구가 진행되고 있다.1

- **Livox 생태계 내 다른 도구**:

- `Livox_automatic_calibration`: Livox SDK에서 제공하는 LiDAR-LiDAR 간 자동 보정 도구로, 환경의 기하학적 구조가 강체라는 가정하에 두 LiDAR의 포인트 클라우드를 정합하여 상대적인 위치와 자세를 추정한다.14

### 7.3  향후 연구 방향


`livox_camera_calib`은 자연 환경의 엣지를 활용하는 타겟리스 보정의 가능성을 성공적으로 보여주었다. 이를 기반으로 한 후속 연구들은 보정의 강인성과 적용 범위를 더욱 확장하는 방향으로 나아가고 있다. 예를 들어, `MFCalib`이라는 연구는 깊이가 연속적인 엣지뿐만 아니라, 깊이가 불연속적인 엣지, LiDAR 반사 강도(intensity)가 불연속적인 엣지 등 여러 종류의 특징을 통합하여 단일 장면에서도 더욱 안정적인 보정 결과를 얻는 방법을 제안했다.3

궁극적으로 이러한 기술은 초기 한 번의 오프라인 보정에 그치지 않고, 시스템이 작동하는 동안 환경 변화나 외부 충격으로 인해 발생할 수 있는 미세한 틀어짐을 실시간으로 감지하고 스스로 보정하는 온라인 자가 보정(Online Self-calibration) 기술로 발전해 나갈 것이다. 이는 자율 시스템의 장기적인 안정성과 신뢰도를 보장하는 데 핵심적인 역할을 할 것으로 기대된다.

## 8. 참고 자료


1. Deephome/Awesome-LiDAR-Camera-Calibration - GitHub, https://github.com/Deephome/Awesome-LiDAR-Camera-Calibration
2. [2103.01627] Pixel-level Extrinsic Self Calibration of High ..., https://ar5iv.labs.arxiv.org/html/2103.01627
3. MFCalib: Single-shot and Automatic Extrinsic Calibration for LiDAR and Camera in Targetless Environments Based on Multi-Feature Edge - arXiv, https://arxiv.org/html/2409.00992v1
4. hku-mars/livox_camera_calib: This repository is used for ... - GitHub, https://github.com/hku-mars/livox_camera_calib
5. [2103.01627] Pixel-level Extrinsic Self Calibration of High Resolution LiDAR and Camera in Targetless Environments - arXiv, https://arxiv.org/abs/2103.01627
6. apricot/livox_camera_calib - Gitee, https://gitee.com/call-me-by-mason/livox_camera_calib
7. Calibrate the extrinsic parameters between Livox LiDAR and camera - GitHub, https://github.com/Livox-SDK/livox_camera_lidar_calibration
8. Issues · hku-mars/livox_camera_calib - GitHub, https://github.com/hku-mars/livox_camera_calib/issues
9. rosblox/ros-livox-camera-calib - GitHub, https://github.com/rosblox/ros-livox-camera-calib
10. Recording data in a bag file with rosbag - ROS Programming: Building Powerful Robots [Book] - O'Reilly Media, https://www.oreilly.com/library/view/ros-programming-building/9781788627436/9fb838a3-61ea-44f3-b523-f4e29ca8fb78.xhtml
11. 19.Topic message recording and playback - Yahboom, [https://www.yahboom.net/public/upload/upload-html/1711360267/19.Topic%20message%20recording%20and%20playback.html](https://www.yahboom.net/public/upload/upload-html/1711360267/19.Topic%20message%20recording%20and%20playback.html)
12. Chapter 9. Recording and replaying messages - A Gentle Introduction to ROS, https://jokane.net/agitr/agitr-letter-bag.pdf
13. Pixel-Level Extrinsic Self Calibration of High Resolution LiDAR and Camera in Targetless Environments - Semantic Scholar, https://www.semanticscholar.org/paper/Pixel-Level-Extrinsic-Self-Calibration-of-High-and-Yuan-Liu/4ebea79c8efa13714472ac6a952bd9adbea4539d
14. An automatic calibration algorithm for livox LiDAR - GitHub, https://github.com/Livox-SDK/Livox_automatic_calibration</code></pre>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>