<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:LCCNet를 이용한 다중 라이다 보정 방법 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>LCCNet를 이용한 다중 라이다 보정 방법 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">센서 (Sensors)</a> / <a href="index.html">HKU-MaRS</a> / <span>LCCNet를 이용한 다중 라이다 보정 방법 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>LCCNet를 이용한 다중 라이다 보정 방법 안내서</h1>
<p>2025-10-21, G25DR</p>
<h2>1.  다중 라이다 시스템의 외재적 보정</h2>
<h3>1.1  다중 라이다 시스템의 필요성 및 목표</h3>
<p>자율주행 시스템의 핵심은 주변 환경을 360도 전방위에 걸쳐 정확하고 끊김 없이 인지하는 능력에 있다. 단일 라이다(LiDAR) 센서는 일반적으로 수평 및 수직 시야각(Field of View, FOV)에 제약이 있어 차량 주변에 필연적으로 사각지대(blind spots)를 발생시킨다.1 이러한 인지 공백은 특히 복잡한 도심 환경에서 보행자나 다른 차량과 같은 동적 객체를 놓치는 원인이 되어 심각한 안전 문제로 이어진다.</p>
<p>이러한 한계를 극복하기 위해 최신 자율주행 차량은 여러 개의 라이다 센서를 차량의 각기 다른 위치에 장착하는 다중 라이다 시스템을 채택한다. 이 시스템은 다음과 같은 핵심적인 이점을 제공한다. 첫째, 각 센서의 FOV를 조합하여 360도 전체를 커버함으로써 사각지대를 효과적으로 제거하고 인지 범위를 극대화한다.1 둘째, 특정 영역을 두 개 이상의 센서가 동시에 감지하게 함으로써 센서 데이터의 이중화(redundancy)를 확보한다. 이는 한 센서에 오작동이 발생하더라도 다른 센서의 정보로 이를 보완하여 시스템 전체의 강건성(robustness)을 크게 향상시킨다.1 셋째, 여러 센서로부터 수집된 포인트 클라우드를 하나의 좌표계로 융합하여 훨씬 더 조밀하고 해상도가 높은 3차원 환경 지도를 생성할 수 있다.4</p>
<p>그러나 이러한 이점을 실현하기 위한 절대적인 전제 조건은 바로 ’정확한 외재적 보정(extrinsic calibration)’이다. 외재적 보정이란 각 센서가 자신만의 독립적인 좌표계를 기준으로 데이터를 수집하기 때문에, 이들을 의미 있는 하나의 정보로 통합하기 위해 모든 센서 데이터를 기준 좌표계(일반적으로 차량의 중심 좌표계)로 변환하는 과정을 의미한다.2 이 변환 관계가 정확하지 않으면, 융합된 포인트 클라우드에서 객체의 형태가 왜곡되거나 이중으로 보이는 현상이 발생하여 후속 인지 알고리즘(객체 탐지, 추적 등)의 성능을 심각하게 저하시킨다.</p>
<h3>1.2  외재적 보정의 정의와 수학적 표현</h3>
<p>외재적 보정은 물리적으로 분리된 두 센서의 좌표계 간의 상대적인 위치와 방향, 즉 강체 변환(rigid-body transformation)을 찾는 과정으로 정의된다. 이 변환은 3차원의 회전(Rotation)과 3차원의 이동(Translation)으로 구성되며, 총 6 자유도(6-DoF)를 가진다.</p>
<p>수학적으로 이 변환 관계는 특수 유클리드 군 <span class="math math-inline">SE(3)</span>에 속하는 <span class="math math-inline">4 \times 4</span> 동차 변환 행렬(homogeneous transformation matrix)로 표현된다. 기준 라이다의 좌표계를 <span class="math math-inline">{L_{ref}}</span>라 하고, 보정 대상이 되는 다른 라이다의 좌표계를 <span class="math math-inline">{L_{target}}</span>이라 할 때, <span class="math math-inline">{L_{target}}</span>에서 <span class="math math-inline">{L_{ref}}</span>로의 변환 행렬 <span class="math math-inline">T_{target}^{ref}</span>는 다음과 같이 정의된다.</p>
<p><span class="math math-display">
T_{target}^{ref} = \begin{pmatrix} R &amp; t \\ 0 &amp; 1 \end{pmatrix} \in SE(3)
</span><br />
여기서 <span class="math math-inline">R</span>은 특수 직교군 <span class="math math-inline">SO(3)</span>에 속하는 <span class="math math-inline">3 \times 3</span> 회전 행렬이며, 세 축에 대한 회전(roll, pitch, yaw)을 나타낸다. <span class="math math-inline">t</span>는 <span class="math math-inline">\mathbb{R}^3</span>에 속하는 <span class="math math-inline">3 \times 1</span> 이동 벡터로, 세 축 방향으로의 변위(<span class="math math-inline">t_x, t_y, t_z</span>)를 나타낸다. 이 변환 행렬을 이용하면, 대상 라이다 좌표계 <span class="math math-inline">{L_{target}}</span>에서 측정된 임의의 3차원 포인트 <span class="math math-inline">p_{target}</span>을 기준 라이다 좌표계 <span class="math math-inline">{L_{ref}}</span>의 포인트 <span class="math math-inline">p_{ref}</span>로 다음과 같이 변환할 수 있다.</p>
<p><span class="math math-display">
p_{ref} = R \cdot p_{target} + t
</span><br />
따라서 다중 라이다 보정의 목표는 시스템에 포함된 모든 라이다 센서 쌍에 대해 이 변환 행렬 <span class="math math-inline">T</span>를 정확하게 추정하는 것이다.</p>
<h3>1.3  다중 라이다 보정의 기술적 난제</h3>
<p>다중 라이다 시스템의 외재적 보정은 여러 가지 기술적 난제로 인해 매우 어려운 문제로 알려져 있다.</p>
<ul>
<li>
<p><strong>시야각 중첩 부족 (Limited FOV Overlap):</strong> 차량의 전방, 후방, 측면에 장착된 라이다들은 사각지대를 최소화하고 최대한 넓은 범위를 감지하도록 의도적으로 배치된다. 이로 인해 센서 간 FOV가 거의 또는 전혀 중첩되지 않는 경우가 많다.1 전통적인 포인트 클라우드 정합(registration) 알고리즘은 두 데이터 간의 공통 영역에 존재하는 특징점을 기반으로 동작하므로, 중첩 영역이 부족하면 안정적인 보정이 불가능해진다.3</p>
</li>
<li>
<p><strong>특징점 희소성 및 비정형성 (Sparsity and Irregularity of Features):</strong> 라이다가 생성하는 포인트 클라우드는 카메라 이미지와 달리 색상이나 질감 정보가 없고, 명확한 코너나 엣지와 같은 기하학적 특징이 상대적으로 부족하다. 데이터 자체가 3차원 공간에 희소하게 분포하기 때문에, 두 포인트 클라우드에서 신뢰할 수 있는 대응점(correspondences)을 충분히 확보하기가 매우 어렵다.4</p>
</li>
<li>
<p><strong>고차원 비선형 최적화 (High-Dimensional, Non-linear Optimization):</strong> 6-DoF 파라미터를 추정하는 것은 <span class="math math-inline">SE(3)</span>라는 고차원 비선형 공간에서의 최적화 문제를 푸는 것과 같다. 이 최적화 문제는 수많은 지역 최솟값(local minima)을 가지고 있어, 초기 추정치가 좋지 않으면 전역 최적해(global optimum)가 아닌 잘못된 값으로 수렴하기 쉽다.2</p>
</li>
<li>
<p><strong>동적 환경 및 주행 중 변위 (Dynamic Environments and Operational Drift):</strong> 차량이 주행하는 실제 환경에는 다른 차량이나 보행자와 같은 동적 객체들이 존재하며, 이는 정적인 특징을 기반으로 하는 보정 알고리즘에 노이즈로 작용한다. 또한, 차량의 지속적인 운행으로 인한 진동이나 충격, 온도 변화 등으로 인해 센서의 장착 상태가 미세하게 틀어질 수 있다. 이러한 변위(drift)는 시간이 지남에 따라 누적되므로, 초기 보정 값의 유효성이 떨어지게 되어 주기적인 재보정 또는 실시간 온라인 보정의 필요성이 대두된다.2</p>
</li>
</ul>
<h3>1.4  전통적 보정 방법론 분류 및 한계</h3>
<p>이러한 난제들을 해결하기 위해 다양한 보정 방법론이 연구되어 왔으며, 크게 타겟 기반 방법과 비타겟 방법으로 나눌 수 있다.</p>
<ul>
<li>타겟 기반 방법 (Target-based Methods):</li>
</ul>
<p>이 방법은 체커보드, 구(sphere), 특정 패턴이 인쇄된 평면 등 사전에 기하학적 정보를 정확히 알고 있는 인공적인 보정 타겟을 사용한다.2 각 라이다 센서가 이 타겟을 스캔하고, 스캔된 포인트 클라우드에서 타겟의 평면이나 모서리를 추출한다. 타겟의 실제 기하 정보와 측정된 기하 정보 간의 관계를 이용하여 변환 행렬을 계산한다. 이 방법은 명확한 제약 조건을 제공하기 때문에 매우 높은 정확도를 달성할 수 있다는 장점이 있다.6 그러나 통제된 환경과 별도의 보정 장비가 반드시 필요하고, 타겟을 설치하고 데이터를 수집하는 과정이 노동 집약적이며 시간이 많이 소요된다. 이러한 단점은 대규모 상용차 플릿(fleet)이나 이미 운행 중인 차량에 적용하기에는 비실용적이어서 확장성에 한계가 있다.2</p>
<ul>
<li>비타겟 방법 (Targetless Methods):</li>
</ul>
<p>비타겟 방법은 인공적인 타겟 없이 주행 환경에 자연적으로 존재하는 구조물(벽, 건물, 기둥, 도로 노면 등)을 특징으로 활용한다.2 이는 별도의 준비 과정 없이 보정을 수행할 수 있어 유연성과 실용성이 높다. 비타겟 방법은 다시 정적 특징 기반 방식과 동작 기반 방식으로 세분화된다.</p>
<ul>
<li>
<p><strong>정적 특징 기반 (Motionless):</strong> 차량이 정지된 상태에서 주변 환경을 스캔하고, 각 라이다의 포인트 클라우드에서 평면, 선, 모서리와 같은 공통된 기하학적 특징을 추출하여 정합한다. 예를 들어, 서로 다른 방향을 바라보는 세 개 이상의 독립적인 평면(예: 방구석)을 동시에 스캔하여 변환 행렬을 계산할 수 있다.3 하지만 이 방식 역시 센서 간에 충분한 FOV 중첩이 필요하다는 근본적인 한계를 가진다.</p>
</li>
<li>
<p><strong>동작 기반 (Motion-based):</strong> 이 방식은 센서 간 FOV 중첩이 부족한 문제를 해결하기 위해 차량의 ‘움직임’ 정보를 활용한다. 차량이 주행하는 동안 각 라이다는 독립적으로 자신의 움직임 궤적(trajectory)을 추정한다(LiDAR Odometry). 두 라이다는 차량이라는 강체에 고정되어 함께 움직이므로, 각자가 추정한 궤적은 동일한 움직임을 서로 다른 좌표계에서 기술한 것이다. 따라서 두 궤적을 정렬함으로써 두 센서 간의 상대적인 변환 관계를 유추할 수 있다. 이 문제는 로봇 팔 끝에 달린 카메라의 위치를 추정하는 ‘핸드-아이 보정(Hand-Eye Calibration)’ 문제와 수학적으로 동일한 구조를 가진다.1 이 방법은 FOV 중첩이 필요 없다는 큰 장점이 있지만, 각 라이다의 주행 거리계(odometry)가 누적 오차(drift)를 포함하고 있어 보정 정확도가 이에 민감하게 반응한다는 단점이 있다.4</p>
</li>
</ul>
<p>이처럼 전통적인 방법론의 발전 과정은 현장의 운영 요구사항에 의해 추동된 필연적인 결과로 볼 수 있다. 타겟 기반 방법의 비실용성 2은 비타겟 방법의 등장을 이끌었다. 그러나 정적 비타겟 방법이 센서 간 FOV 중첩 부족 문제 6에 부딪히자, 시간에 따른 궤적을 정렬하는 동작 기반 접근법이 대안으로 부상했다. 이는 결국 보정 방법의 선택이 단순히 여러 옵션 중 하나를 고르는 것이 아니라, 차량에 장착된 센서의 물리적 구성과 운영 환경이라는 제약 조건 하에서 가장 적합한 해법을 찾는 과정임을 시사한다.</p>
<h2>2.  LCCNet의 원리: 라이다-카메라 보정을 위한 심층 학습 접근법</h2>
<h3>2.1  학습 기반 보정의 패러다임 전환</h3>
<p>전통적인 보정 방법들이 수작업으로 설계된 특징 추출 알고리즘과 반복적인 최적화 기법에 의존하는 반면, 최근에는 심층 학습(Deep Learning)을 이용한 새로운 패러다임이 부상하고 있다. 학습 기반 보정 방법은 라이다와 카메라와 같은 이종(heterogeneous) 센서 데이터로부터 특징과 그들 사이의 복잡한 상관관계를 심층 신경망이 직접 학습하도록 한다. 이를 통해 명시적인 대응점을 찾는 어려운 과정을 생략하고, 센서 데이터를 입력받아 외재적 보정 파라미터를 직접 회귀(regress)하는 종단간(end-to-end) 모델을 구축한다.8</p>
<p>LCCNet(LiDAR-Camera Self-Calibration Network)은 이러한 학습 기반 접근법의 대표적인 예시로, 라이다 포인트 클라우드와 카메라 RGB 이미지 간의 외재적 보정을 위해 특별히 설계된 네트워크다.7 LCCNet은 두 이종 데이터 간의 기하학적, 시각적 대응 관계를 학습하여 6-DoF 변환 행렬을 높은 정확도로 실시간 추정하는 것을 목표로 한다.</p>
<h3>2.2  입력 데이터 처리: 포인트 클라우드의 2D 이미지 투영</h3>
<p>LCCNet은 두 개의 2D 이미지를 입력으로 사용한다. 하나는 카메라로부터 직접 얻는 RGB 이미지이고, 다른 하나는 라이다 포인트 클라우드를 가상의 이미지 평면에 투영하여 생성한 깊이 이미지(Depth Image)이다.7 이 투영 과정은 라이다의 3차원 기하 정보를 카메라의 2차원 이미지 공간으로 가져와 두 데이터의 비교를 가능하게 하는 핵심적인 전처리 단계다.</p>
<p>깊이 이미지 생성 과정은 다음과 같은 수학적 절차를 따른다. 먼저, 라이다 좌표계 <span class="math math-inline">{L}</span>에 있는 3차원 포인트 <span class="math math-inline">P_L =^T</span>를 카메라 좌표계 <span class="math math-inline">{C}</span>로 변환해야 한다. 이를 위해서는 초기 추정 변환 행렬 <span class="math math-inline">T_{init}</span>가 필요하다. 이 초기값은 대략적인 수동 측정값이나 차량 설계도(CAD 모델)로부터 얻을 수 있다.</p>
<p><span class="math math-display">
P_C = T_{init} \cdot P_L = R_{init} \cdot P_L + t_{init}
</span><br />
다음으로, 카메라 좌표계로 변환된 3차원 포인트 <span class="math math-inline">P_C</span>를 카메라의 내재적 파라미터(intrinsic parameters) 행렬 <span class="math math-inline">K</span>를 사용하여 2차원 이미지 평면으로 투영한다.</p>
<p><span class="math math-display">
p&#39; = K \cdot P_C
</span><br />
여기서 <span class="math math-inline">p&#39; = [u&#39;, v&#39;, w&#39;]^T</span>는 동차 좌표(homogeneous coordinates)로 표현된 2차원 픽셀 좌표이다. 이를 정규화하여 최종 픽셀 좌표 <span class="math math-inline">(u, v) = (u&#39;/w&#39;, v&#39;/w&#39;)</span>를 얻는다. 이 계산된 <span class="math math-inline">(u, v)</span> 위치의 픽셀에 해당 포인트의 깊이 값, 즉 카메라 원점으로부터의 거리인 <span class="math math-inline">w&#39;</span>를 저장하면 깊이 이미지가 완성된다.8 이렇게 생성된 깊이 이미지는 RGB 이미지와 동일한 시점에서 바라본 장면의 기하학적 구조를 담고 있게 된다.</p>
<h3>2.3  LCCNet 네트워크 아키텍처 상세 분석</h3>
<p>LCCNet은 특징 추출, 특징 정합, 파라미터 회귀의 세 가지 주요 단계로 구성된 종단간 학습 네트워크다.8</p>
<ul>
<li>특징 추출 인코더 (Feature Extraction Encoder):</li>
</ul>
<p>네트워크의 첫 부분은 입력된 RGB 이미지와 깊이 이미지로부터 의미 있는 특징을 추출하는 두 개의 대칭적인 인코더로 구성된다. 각 인코더는 ResNet-18 11이나 Network-in-Network (NiN) 10 블록과 같은 컨볼루션 신경망(CNN) 구조를 기반으로 한다. 이 인코더들은 여러 계층을 거치면서 입력 이미지로부터 다양한 스케일의 특징 맵(multi-scale feature maps)을 생성한다. RGB 이미지는 풍부한 색상과 질감 정보를, 깊이 이미지는 객체의 윤곽과 기하학적 구조 정보를 주로 담고 있으므로, 두 인코더는 서로 다른 종류의 특징을 학습해야 한다. 따라서 두 브랜치는 동일한 구조를 가지더라도 가중치(weights)를 공유하지 않는다.11</p>
<ul>
<li>비용 볼륨 계층 (Cost Volume Layer):</li>
</ul>
<p>이 계층은 LCCNet 아키텍처의 가장 핵심적인 구성 요소로, 두 센서 모달리티 간의 특징 상관관계를 명시적으로 계산하고 저장하는 역할을 한다.7 이는 전통적인 스테레오 비전에서 disparity를 계산하는 방식에서 영감을 얻었다.</p>
<p>작동 원리는 다음과 같다. RGB 이미지에서 추출된 특징 맵의 한 픽셀 위치에 있는 특징 벡터를 기준으로, 깊이 이미지의 특징 맵에서 해당 픽셀 주변의 미리 정의된 탐색 범위(search range) 내에 있는 모든 특징 벡터들과의 유사도(예: 내적)를 계산한다. 이 계산된 유사도 값들을 모두 모아 새로운 차원의 텐서를 구성하는데, 이것이 바로 ’비용 볼륨(Cost Volume)’이다.</p>
<p>결과적으로 비용 볼륨은 (높이, 너비, 탐색범위_x, 탐색범위_y) 형태의 4D 텐서가 되며, 각 픽셀이 다른 이미지에서 얼마나 이동했는지(disparity)에 대한 ‘비용’ 또는 ’일치 확률’을 인코딩한다. 이 과정을 통해 네트워크는 이미지 전체에 걸쳐 픽셀 수준의 미세한 대응 관계를 학습할 수 있게 된다.</p>
<ul>
<li>파라미터 회귀 디코더 (Parameter Regression Decoder):</li>
</ul>
<p>생성된 4D 비용 볼륨은 여러 개의 3D 컨볼루션 레이어로 구성된 디코더에 입력된다. 이 디코더는 비용 볼륨에 인코딩된 지역적인(local) 대응 정보를 종합하여 전역적인(global) 변환 관계를 추론하는 역할을 한다.8 최종적으로 디코더는 6-DoF 외재적 파라미터의 편차(deviation) <span class="math math-inline">\Delta T</span>를 예측한다.</p>
<p>여기서 중요한 점은 LCCNet이 절대적인 변환 행렬 <span class="math math-inline">T</span>를 직접 예측하는 것이 아니라, 주어진 초기 추정치 <span class="math math-inline">T_{init}</span>로부터 실제 정답(ground truth)까지의 ‘편차’ <span class="math math-inline">\Delta T</span>를 예측한다는 것이다.7 이 접근법은 전체 <span class="math math-inline">SE(3)</span> 공간이라는 거대하고 비선형적인 탐색 공간을 탐색하는 대신, 초기 추정치 주변의 훨씬 작고 선형에 가까운 공간에서 미세 조정을 수행하는 문제로 변환시켜준다. 이는 학습을 훨씬 안정적으로 만들고, 네트워크가 더 높은 정밀도를 달성할 수 있도록 돕는 핵심적인 설계 전략이다.</p>
<h3>2.4  손실 함수와 학습 전략</h3>
<p>LCCNet은 효과적인 학습을 위해 여러 손실 함수와 특별한 학습 전략을 사용한다.</p>
<ul>
<li>지도 손실 (Supervised Loss):</li>
</ul>
<p>학습의 주된 감독 신호는 예측된 파라미터 편차 <span class="math math-inline">\Delta T_{pred}</span>와 실제 편차 <span class="math math-inline">\Delta T_{gt}</span> 사이의 오차를 최소화하는 것이다. 일반적으로 이동 벡터와 회전 벡터(쿼터니언으로 표현)에 대해 각각 Smooth L1 Loss를 적용한다. 두 손실에는 서로 다른 가중치를 부여하여 학습의 균형을 맞춘다.7</p>
<ul>
<li>자기 지도 손실 (Self-supervised Loss):</li>
</ul>
<p>지도 학습을 위한 정답 데이터가 부족한 경우를 대비하여, LCCNet은 자기 지도 방식의 손실을 추가적으로 사용할 수 있다. 예측된 변환 <span class="math math-inline">T_{pred} = \Delta T_{pred} \cdot T_{init}</span>를 사용하여 원본 라이다 포인트 클라우드를 이미지 평면에 재투영한다. 이 재투영된 포인트들과 정답 변환을 통해 투영된 포인트들 간의 거리(예: Chamfer Distance)를 계산하여 손실 함수에 추가한다. 이는 정답 라벨 없이도 네트워크가 기하학적 일관성을 학습하도록 유도하는 역할을 한다.8</p>
<ul>
<li>반복적 정제 (Iterative Refinement):</li>
</ul>
<p>LCCNet은 추정의 정확도를 점진적으로 향상시키기 위해 반복적 정제 기법을 사용한다.7 첫 번째 추론 단계에서 예측된 편차 <span class="math math-inline">\Delta T_1</span>을 초기값 <span class="math math-inline">T_{init}</span>에 적용하여 1차적으로 개선된 변환 <span class="math math-inline">T_1 = \Delta T_1 \cdot T_{init}</span>을 얻는다. 그 다음, 이 <span class="math math-inline">T_1</span>을 새로운 초기값으로 사용하여 라이다 포인트 클라우드를 다시 투영하고, 생성된 새로운 깊이 이미지를 네트워크에 다시 입력한다. 이 과정을 통해 더 미세한 편차 <span class="math math-inline">\Delta T_2</span>를 예측하고, 이를 다시 적용한다. 이러한 과정을 일반적으로 5회 정도 반복하면, 큰 초기 오차부터 시작하여 점차 정밀한 값으로 수렴해 나갈 수 있다.8 이 전략은 “오차를 예측하는” 기본 설계 철학과 맞물려, 네트워크가 다양한 크기의 오차에 강건하게 대응할 수 있도록 만든다.</p>
<h2>3.  LCCNet의 다중 라이다 보정 문제 적용 방안</h2>
<h3>3.1  핵심 과제: 모달리티 불일치 문제 해결</h3>
<p>사용자의 질의는 LCCNet을 다중 라이다 보정에 사용하는 것이지만, 앞서 분석했듯이 LCCNet은 근본적으로 라이다와 카메라라는 두 가지 다른 종류의 센서, 즉 이종 모달리티(heterogeneous modalities)를 입력으로 가정하고 설계되었다. LCCNet의 성공은 RGB 이미지가 제공하는 밀도 높은 2D 텍스처 정보와 깊이 이미지가 제공하는 희소한 3D 기하 정보 간의 상호 보완적인 특징을 학습하는 데 기반한다.</p>
<p>반면, 라이다-라이다 보정 문제는 두 입력이 모두 동일한 모달리티, 즉 라이다 센서로부터 파생된 희소한 3D 기하 정보라는 점에서 근본적인 차이가 있다. 따라서 기존 LCCNet 아키텍처에 두 라이다의 포인트 클라우드를 직접 입력하는 것은 불가능하다. 이 문제를 해결하기 위해서는 라이다-라이다 보정 문제를 LCCNet이 처리할 수 있는 형태로 변환하는, 일종의 <strong>모달리티 변환(Modality Transformation)</strong> 과정이 반드시 필요하다.</p>
<h3>3.2  제안 방법론: 의사 이미지 생성을 통한 문제 재구성</h3>
<p>본 보고서에서 제안하는 핵심 아이디어는 라이다-라이다 보정 문제를 LCCNet이 해결할 수 있는 <strong>‘의사 이미지-깊이 이미지(Pseudo-Image vs. Depth Image)’</strong> 보정 문제로 재구성하는 것이다. 즉, 두 라이다 중 하나를 가상의 ’카메라’처럼 취급하여 LCCNet의 RGB 이미지 입력 역할을 할 수 있는 2D 의사 이미지를 생성하는 전략이다.</p>
<ul>
<li>
<p><strong>기준 라이다 (<span class="math math-inline">L_1</span>):</strong> 이 라이다의 포인트 클라우드는 LCCNet의 ‘RGB 이미지’ 입력을 대체할 <strong>의사 이미지</strong>를 생성하는 데 사용된다. 이 이미지는 텍스처와 유사한 정보를 담아야 한다.</p>
</li>
<li>
<p><strong>대상 라이다 (<span class="math math-inline">L_2</span>):</strong> 이 라이다의 포인트 클라우드는 기존 방식과 동일하게 기준 라이다의 시점으로 투영되어 LCCNet의 ‘깊이 이미지’ 입력을 생성하는 데 사용된다.</p>
</li>
</ul>
<p>이러한 재구성을 통해, 우리는 두 라이다 데이터 간의 보정 문제를 LCCNet의 기존 프레임워크 내에서 해결할 수 있는 가능성을 열 수 있다.</p>
<h3>3.3  의사 이미지 생성 기법 상세</h3>
<p>의사 이미지를 생성하기 위해서는 3차원 포인트 클라우드를 2차원 그리드로 변환하는 투영 과정이 필요하다. 가장 일반적인 방법은 구면 투영(Spherical Projection)이다. 각 3D 포인트 <span class="math math-inline">P = (x, y, z)</span>는 센서 원점을 기준으로 방위각(azimuth) <span class="math math-inline">\theta</span>와 고도각(elevation) <span class="math math-inline">\phi</span>로 변환될 수 있다.</p>
<p><span class="math math-display">
\theta = \text{atan2}(y, x)
</span></p>
<p><span class="math math-display">
\phi = \text{asin}(z / \sqrt{x^2 + y^2 + z^2})
</span></p>
<p>계산된 <span class="math math-inline">(\theta, \phi)</span> 값은 이미지의 열(u)과 행(v) 인덱스로 정규화되어 매핑된다. 이 2D 그리드의 각 픽셀에 어떤 정보를 채워 넣느냐에 따라 다양한 종류의 의사 이미지를 생성할 수 있다.</p>
<ul>
<li>방법 1: 반사 강도 이미지 (Intensity Image):</li>
</ul>
<p>가장 유력하고 효과적인 후보는 라이다 포인트가 수집 시 함께 기록하는 반사 강도(reflectivity/intensity) 값을 활용하는 것이다. 반사 강도는 레이저 빔이 부딪힌 표면의 재질, 색상, 거칠기에 따라 달라지는 값으로, 아스팔트, 차선, 표지판, 건물 벽 등이 서로 다른 강도 값을 가진다. 이 강도 값을 픽셀의 밝기 값으로 사용하여 이미지를 생성하면, 이는 카메라 이미지의 텍스처 정보와 매우 유사한 역할을 수행할 수 있다. 예를 들어, 도로 위의 흰색 차선은 주변 아스팔트보다 훨씬 높은 강도 값을 가지므로 이미지 상에서 뚜렷한 선 특징으로 나타날 것이다.</p>
<ul>
<li>방법 2: 거리 이미지 (Range Image):</li>
</ul>
<p>각 픽셀에 센서 원점으로부터의 유클리드 거리 <span class="math math-inline">r = \sqrt{x^2 + y^2 + z^2}</span> 값을 저장하는 방식이다. 이는 순수한 기하 정보만을 담고 있다. 하지만 이 방법은 대상 라이다(<span class="math math-inline">L_2</span>)로부터 생성될 깊이 이미지와 정보의 종류가 거의 동일해져, 두 입력 간의 특징 변별력이 크게 떨어질 위험이 있다. LCCNet의 비용 볼륨이 유의미한 상관관계를 계산하기 어려워질 수 있다.</p>
<ul>
<li>방법 3: 다중 채널 의사 이미지 (Multi-channel Pseudo-Image):</li>
</ul>
<p>더 풍부하고 차별화된 정보를 네트워크에 제공하기 위해 여러 종류의 정보를 결합하여 다중 채널 이미지를 생성하는 방법이다. 예를 들어, 3채널 이미지를 생성하여 첫 번째 채널(R)에는 거리(Range), 두 번째 채널(G)에는 반사 강도(Intensity), 세 번째 채널(B)에는 높이(z-값)를 각각 정규화하여 저장할 수 있다. 이 방식은 LCCNet의 3채널 RGB 인코더 구조를 변경 없이 그대로 활용할 수 있다는 장점이 있으며, 네트워크가 더 복잡하고 미세한 특징을 학습할 수 있도록 유도할 수 있다.</p>
<h3>3.4  적용 시나리오 및 파이프라인</h3>
<p>제안된 방법론을 적용하기 위한 전체적인 파이프라인은 다음과 같다.</p>
<ol>
<li>
<p><strong>데이터 수집:</strong> 기준 라이다 <span class="math math-inline">L_1</span>과 대상 라이다 <span class="math math-inline">L_2</span>가 장착된 차량을 다양한 환경에서 주행하여 동기화된 포인트 클라우드 시퀀스를 수집한다.</p>
</li>
<li>
<p><strong>초기 추정치 설정 (<span class="math math-inline">T_{init}</span>):</strong> 차량의 CAD 모델이나 간단한 수동 측정을 통해 두 라이다 간의 대략적인 초기 변환 행렬 <span class="math math-inline">T_{2}^{1}</span>을 설정한다. 이 값은 어느 정도의 오차를 포함해도 무방하다.</p>
</li>
<li>
<p><strong>입력 생성:</strong> 각 타임스탬프의 데이터 쌍에 대해 다음을 수행한다.</p>
</li>
</ol>
<ul>
<li>
<p><span class="math math-inline">L_1</span>의 포인트 클라우드로부터 구면 투영을 통해 <strong>반사 강도 이미지</strong>(또는 다중 채널 의사 이미지)를 생성한다. 이것이 LCCNet의 첫 번째 입력이 된다.</p>
</li>
<li>
<p><span class="math math-inline">L_2</span>의 포인트 클라우드를 초기 추정치 <span class="math math-inline">T_{init}</span>를 이용해 <span class="math math-inline">L_1</span>의 좌표계로 변환한다. 이 변환된 포인트 클라우드를 <span class="math math-inline">L_1</span>의 시점에서 구면 투영하여 <strong>깊이 이미지</strong>를 생성한다. 이것이 LCCNet의 두 번째 입력이 된다.</p>
</li>
</ul>
<ol start="4">
<li>
<p><strong>LCCNet 추론:</strong> 생성된 두 이미지(강도 이미지, 깊이 이미지)를 사전 학습된 LCCNet 모델에 입력하여 두 이미지 간의 정렬 오차에 해당하는 변환 편차 <span class="math math-inline">\Delta T</span>를 예측한다.</p>
</li>
<li>
<p><strong>최종 변환 계산:</strong> 예측된 편차를 초기 추정치에 적용하여 최종 보정 파라미터 <span class="math math-inline">T_{final} = \Delta T \cdot T_{init}</span>를 계산한다. 필요에 따라 이 <span class="math math-inline">T_{final}</span>을 새로운 초기값으로 사용하여 3-5단계를 반복하는 반복적 정제를 수행하여 정확도를 더욱 향상시킬 수 있다.</p>
</li>
</ol>
<h3>3.5  예상되는 문제점 및 극복 방안</h3>
<p>이 제안 방법론은 원리적으로 타당하지만, 실제 적용 시 몇 가지 예상되는 문제점이 존재한다.</p>
<ul>
<li>
<p><strong>특징의 동질성 (Homogeneity of Features):</strong> 비록 반사 강도와 깊이가 서로 다른 물리량을 나타내지만, 두 정보 모두 동일한 라이다 센싱 원리에서 파생되었기 때문에, 이질적인 물리 현상(빛의 반사 vs 레이저 비행시간)을 측정하는 RGB-깊이 쌍에 비해 특징의 변별력이 낮을 수 있다. 이는 비용 볼륨의 상관관계 계산을 어렵게 만들어 보정 성능을 저하시킬 수 있다.</p>
</li>
<li>
<p><strong>극복 방안:</strong> 다중 채널 의사 이미지를 사용하여 입력 정보의 풍부함을 높이거나, 특징 추출 인코더를 ResNet-50과 같이 더 깊고 표현력이 강한 모델로 교체하여 미세한 특징 차이를 더 잘 학습하도록 유도할 수 있다.</p>
</li>
<li>
<p><strong>데이터 희소성 (Sparsity):</strong> 라이다 데이터는 본질적으로 희소하므로, 투영을 통해 생성된 이미지는 많은 빈 픽셀(hole)을 포함하게 된다. 표준적인 컨볼루션 연산은 이러한 빈 영역을 0으로 처리하여 정보 왜곡을 유발할 수 있다.</p>
</li>
<li>
<p><strong>극복 방안:</strong> 학습 데이터 생성 시 간단한 인페인팅(inpainting) 기법으로 빈 영역을 주변 값으로 채우는 전처리를 적용하거나, 유효한 데이터가 있는 픽셀에만 연산을 적용하는 희소 컨볼루션(sparse convolution) 또는 마스크 컨볼루션(masked convolution)을 네트워크에 도입하는 것을 고려할 수 있다.</p>
</li>
<li>
<p><strong>전이 학습의 한계 (Limitation of Transfer Learning):</strong> KITTI와 같은 라이다-카메라 데이터셋으로 사전 학습된 LCCNet의 가중치는 라이다-라이다 데이터의 고유한 특징 분포를 잘 표현하지 못할 수 있다.</p>
</li>
<li>
<p><strong>극복 방안:</strong> 이 문제를 해결하기 위해서는 대규모의 라이다-라이다 보정 데이터셋을 구축하는 것이 필수적이다. CARLA나 AirSim과 같은 고충실도 시뮬레이터를 사용하여 다양한 차량과 센서 배치에 대한 데이터를 자동으로 생성하고, 이를 이용해 모델을 처음부터 학습(train from scratch)시키거나 기존 모델을 미세 조정(fine-tuning)하는 과정이 필요하다.</p>
</li>
</ul>
<p>이러한 적응 과정의 성공 여부는 LCCNet의 핵심 메커니즘인 비용 볼륨의 일반화 능력을 시험하는 중요한 척도가 된다. 만약 이 방법이 성공한다면, 이는 비용 볼륨이 특정 센서 조합에 국한되지 않고, 공간적으로 상관관계가 있는 두 개의 ‘이미지 형태’ 표현 간의 기하학적 대응 관계를 학습할 수 있는 강력하고 범용적인 메커니즘임을 입증하는 것이다. 이는 나아가 라이다-레이더, 레이더-카메라 등 다른 이종 센서 조합의 보정 문제에도 동일한 프레임워크를 적용할 수 있는 길을 열어줄 수 있다.</p>
<h2>4.  성능 평가 및 벤치마크</h2>
<h3>4.1  평가 데이터셋: KITTI Odometry</h3>
<p>LCCNet을 포함한 대부분의 학습 기반 라이다-카메라 보정 방법의 성능은 KITTI Vision Benchmark Suite, 특히 KITTI Odometry 데이터셋을 사용하여 검증되었다.7 이 데이터셋은 독일 카를스루에 시내와 주변 도로를 주행하며 수집한 방대한 양의 실제 도로 환경 데이터를 제공한다.14 차량에는 Velodyne HDL-64E 라이다, 여러 대의 고해상도 카메라, 그리고 센티미터 수준의 정확도를 제공하는 고정밀 GPS/IMU 시스템이 장착되어 있다. 특히, 모든 센서 간의 정확한 외재적 보정 파라미터가 Ground Truth로 제공되기 때문에, 지도 학습 기반 방법의 학습 및 정량적 평가에 이상적인 환경을 제공한다.</p>
<h3>4.2  평가 지표</h3>
<p>보정 알고리즘의 성능은 일반적으로 예측된 6-DoF 파라미터가 실제 Ground Truth 값과 얼마나 차이 나는지를 측정하는 이동 오차와 회전 오차로 정량화된다.</p>
<ul>
<li>이동 오차 (Translation Error):</li>
</ul>
<p>예측된 이동 벡터 <span class="math math-inline">t_{pred}</span>와 실제 이동 벡터 <span class="math math-inline">t_{gt}</span> 사이의 유클리드 거리(Euclidean distance)로 계산된다. 단위는 주로 센티미터(cm)가 사용된다.</p>
<p><span class="math math-display">
  \text{Error}_{trans} = \Vert t_{gt} - t_{pred} \Vert_2
</span></p>
<ul>
<li>회전 오차 (Rotation Error):</li>
</ul>
<p>예측된 회전 행렬 <span class="math math-inline">R_{pred}</span>와 실제 회전 행렬 <span class="math math-inline">R_{gt}</span> 사이의 각도 차이로 계산된다. 두 행렬 간의 상대 회전 행렬 <span class="math math-inline">\Delta R = R_{gt}^T R_{pred}</span>을 계산한 뒤, 이 행렬의 축-각 표현(axis-angle representation)에서 회전 각도를 추출한다. 단위는 도(degree, °)가 사용된다.</p>
<p><span class="math math-display">
  \text{Error}_{rot} = \text{acos}\left(\frac{\text{trace}(\Delta R) - 1}{2}\right)
</span></p>
<p>최종 성능은 보통 데이터셋의 모든 테스트 프레임에 대해 각 오차를 계산한 후, 그 평균 절대 오차(Mean Absolute Error, MAE)를 보고하는 방식으로 제시된다.7</p>
<h3>4.3  LCCNet의 기존 성능</h3>
<p>제안된 다중 라이다 보정 적용 방식의 성능 목표를 설정하고 그 성공 여부를 판단하기 위해서는, 원본 LCCNet이 본래의 라이다-카메라 보정 문제에서 어느 정도의 성능을 보이는지 파악하는 것이 중요하다. 이는 제안 방법론의 성능을 비교 평가할 수 있는 중요한 기준선(baseline)을 제공한다.</p>
<p><strong>표 1: LCCNet 및 유사 모델의 라이다-카메라 보정 성능 (KITTI 데이터셋 기준)</strong></p>
<table><thead><tr><th><strong>방법론 (Method)</strong></th><th><strong>평균 이동 오차 (cm)</strong></th><th><strong>평균 회전 오차 (°)</strong></th><th><strong>추론 시간 (ms)</strong></th><th><strong>출처 (Source)</strong></th></tr></thead><tbody>
<tr><td>LCCNet</td><td>0.297</td><td>0.017</td><td>-</td><td>7</td></tr>
<tr><td>LCCNet (개선)</td><td>0.278</td><td>0.020</td><td>23</td><td>15</td></tr>
<tr><td>제안 알고리즘</td><td>0.26</td><td>0.02</td><td>-</td><td>10</td></tr>
</tbody></table>
<p>이 표는 LCCNet과 유사한 최신 학습 기반 방법들이 라이다-카메라 보정에서 매우 높은 정확도를 달성하고 있음을 보여준다. 평균 이동 오차는 0.3cm 미만, 평균 회전 오차는 0.02° 수준으로, 이는 수작업 보정에 필적하거나 이를 능가하는 정밀도이다. 따라서, 본 보고서에서 제안한 라이다-라이다 보정 방법론이 성공적이라고 평가받기 위해서는, 유사한 수준의 오차 범위(예: 이동 오차 &lt; 0.5 cm, 회전 오차 &lt; 0.05°)를 달성하는 것을 목표로 해야 한다. 이 정량적 목표는 추상적인 연구 제안을 측정 가능한 엔지니어링 과제로 전환시킨다.</p>
<h3>4.4  다중 라이다 보정 시나리오의 평가 방안</h3>
<p>제안된 방법론의 성능을 체계적으로 검증하기 위해서는 Ground Truth가 존재하는 다중 라이다 데이터셋이 필수적이다. 현재 공개된 대규모 실제 데이터셋은 드물기 때문에, 다음과 같은 두 가지 접근 방식을 병행할 수 있다.</p>
<ul>
<li>
<p><strong>시뮬레이션 데이터 활용:</strong> CARLA, AirSim과 같은 고충실도 자율주행 시뮬레이터를 활용하는 것이 가장 효과적인 방법이다. 시뮬레이션 환경에서는 가상의 차량에 원하는 수와 종류의 라이다 센서를 원하는 위치에 자유롭게 장착할 수 있다. 센서 간의 상대 위치와 방향은 시뮬레이터 내에서 정확하게 제어되므로, 완벽한 Ground Truth를 가진 대규모 학습 및 평가 데이터셋을 자동으로 생성할 수 있다. 이를 통해 다양한 센서 배치와 주행 시나리오에 대한 모델의 강건성을 체계적으로 평가할 수 있다.</p>
</li>
<li>
<p><strong>실제 데이터 활용:</strong> 실제 차량에 여러 개의 라이다를 장착하여 데이터를 수집하고, 이를 평가에 활용할 수도 있다. 이 경우 정확한 Ground Truth를 얻기 어렵다는 문제가 있지만, 타겟 기반 방법과 같은 고정밀 오프라인 보정 기법을 사용하여 ’의사 Ground Truth(pseudo ground truth)’를 생성할 수 있다. 즉, 먼저 매우 정밀하지만 번거로운 전통적 방법으로 보정을 수행하여 기준 값을 설정하고, 제안된 온라인 방법의 결과를 이 기준 값과 비교하여 그 정확도를 간접적으로 평가하는 것이다. 이는 제안 방법이 실제 환경의 노이즈와 불확실성 속에서도 얼마나 잘 작동하는지를 검증하는 데 중요한 역할을 한다.</p>
</li>
</ul>
<h2>5.  결론 및 전망</h2>
<h3>5.1  연구 요약 및 기여</h3>
<p>본 보고서는 자율주행 시스템의 핵심 기술인 다중 라이다 시스템의 외재적 보정 문제에 대해 심도 있게 분석했다. 먼저 다중 라이다 시스템의 필요성과 보정의 기술적 난제를 살펴보고, 전통적인 보정 방법론들의 장단점을 고찰했다. 이어서, 최신 심층 학습 기반 라이다-카메라 보정 모델인 LCCNet의 아키텍처와 비용 볼륨이라는 핵심 원리를 상세히 설명했다.</p>
<p>본 보고서의 핵심적인 기여는 LCCNet의 원리를 라이다-라이다 보정이라는 새로운 문제에 적용하기 위한 구체적이고 원리적인 방법론을 제안한 데 있다. 이 방법론은 한 라이다의 포인트 클라우드로부터 반사 강도 이미지와 같은 의사 이미지를 생성하여, 문제를 LCCNet이 해결할 수 있는 형태로 변환하는 모달리티 변환 전략에 기반한다. 이는 기존의 강력한 모델을 새로운 도메인에 창의적으로 적용하는 방안을 제시했다는 점에서 의의가 있다.</p>
<h3>5.2  한계 및 향후 연구 방향</h3>
<p>제안된 방법론은 가능성을 제시하지만, 몇 가지 근본적인 한계 또한 내포하고 있다.</p>
<ul>
<li><strong>2D 투영의 정보 손실:</strong> 3차원의 풍부한 기하 정보를 가진 포인트 클라우드를 2차원 이미지로 투영하는 과정에서는 시점과 해상도에 따른 정보의 손실이 필연적으로 발생한다. 특히, 서로 겹쳐 보이는 포인트들의 정보가 유실될 수 있다. 포인트 클라우드 자체의 3D 구조를 직접 활용하는 방식이 이론적으로 더 높은 성능을 달성할 잠재력을 가지고 있다.</li>
</ul>
<p>이러한 한계를 극복하고 연구를 발전시키기 위한 향후 연구 방향은 다음과 같다.</p>
<ul>
<li>
<p><strong>향후 연구 방향 1: 3D 심층 학습 모델 도입:</strong> 2D 투영을 완전히 배제하고, PointNet++, DGCNN과 같이 포인트 클라우드 데이터를 직접 입력으로 받는 3D 심층 학습 모델을 도입하는 연구가 필요하다. 두 라이다의 포인트 클라우드에서 직접 3D 특징점을 추출하고, 이들 간의 대응 관계를 학습하여 변환을 추정하는 새로운 네트워크 아키텍처를 설계할 수 있다. 3D 희소 컨볼루션(3D sparse convolution)을 사용하면 포인트 클라우드의 희소성을 효율적으로 처리하면서 3D 공간에서의 특징을 효과적으로 학습할 수 있을 것이다.</p>
</li>
<li>
<p><strong>향후 연구 방향 2: 완전한 온라인 자가 보정 시스템:</strong> 본 보고서에서 제안한 단일 프레임 기반의 보정 방식을 넘어, 주행 중 실시간으로 발생하는 센서의 미세한 변위(drift)를 지속적으로 감지하고 보정하는 완전한 온라인 자가 보정 시스템을 구축하는 연구가 필요하다.2 이는 여러 프레임에 걸친 데이터의 시간적 일관성을 모델링하는 순환 신경망(RNN) 구조를 도입하거나, 칼만 필터(Kalman Filter)와 같은 시간적 필터링 기법을 딥러닝 모델의 출력에 결합하여 안정적이고 부드러운 보정 값 업데이트를 수행함으로써 달성할 수 있다.</p>
</li>
</ul>
<h3>5.3  산업적 파급 효과 및 전망</h3>
<p>정확하고, 빠르며, 완전 자동화된 다중 라이다 보정 기술은 자율주행 차량의 대량 생산, 운영, 및 유지보수 패러다임을 바꿀 수 있는 핵심 기술이다. 현재는 차량 출고 시 또는 정기적으로 숙련된 엔지니어가 수동으로 보정을 수행해야 하므로 막대한 시간과 비용이 발생한다.</p>
<p>본 보고서에서 제안한 것과 같은 학습 기반 온라인 보정 기술이 성숙 단계에 이르면, 차량은 더 이상 정비소에 입고될 필요 없이 운행 중에 스스로 센서의 정렬 상태를 진단하고 실시간으로 보정할 수 있게 된다. 이는 수동 보정에 필요한 인력과 비용을 획기적으로 절감할 뿐만 아니라, 주행 중 발생하는 예기치 못한 충격이나 변위에도 즉각적으로 대응하여 자율주행 시스템의 인지 성능을 최상의 상태로 유지할 수 있게 한다. 궁극적으로 이는 자율주행 기술의 전반적인 안전성과 신뢰성을 한 단계 끌어올리는 데 결정적인 기여를 할 것으로 전망된다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>Multi-Lidar Calibration - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/lidar/ug/multi-lidar-calibration-workflow.html</li>
<li>A Multi-LiDAR Self-Calibration System Based on Natural Environments and Motion Constraints - MDPI, https://www.mdpi.com/2227-7390/13/19/3181</li>
<li>Extrinsic Calibration of Multiple 3D LiDAR Sensors by the Use of Planar Objects - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9571506/</li>
<li>A Novel Dual-Lidar Calibration Algorithm Using Planar Surfaces - Jianhao Jiao, https://gogojjh.github.io/assets/pdf/jiao2019novel.pdf</li>
<li>Automatic Calibration of Multiple 3D LiDARs in Urban Environments - Rui (Ranger) FAN, https://www.ruirangerfan.com/pdf/iros2019_jiao.pdf</li>
<li>Multi-LiCa: A Motion- and Targetless Multi - LiDAR-to-LiDAR Calibration Framework - arXiv, https://arxiv.org/html/2501.11088v1</li>
<li>LCCNet: LiDAR and Camera Self-Calibration using Cost Volume Network, https://www.computer.org/csdl/proceedings-article/cvprw/2021/489900c888/1yVzZPu4VVK</li>
<li>LCCNet: LiDAR and Camera Self-Calibration Using Cost Volume Network - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Lv_LCCNet_LiDAR_and_Camera_Self-Calibration_Using_Cost_Volume_Network_CVPRW_2021_paper.pdf</li>
<li>What Really Matters for Learning-based LiDAR-Camera Calibration - arXiv, https://arxiv.org/html/2501.16969v1</li>
<li>A LiDAR-Camera Joint Calibration Algorithm Based on Deep Learning - MDPI, https://www.mdpi.com/1424-8220/24/18/6033</li>
<li>CFNet: LiDAR-Camera Registration Using Calibration Flow Network - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8662422/</li>
<li>LCCNet: LiDAR and Camera Self-Calibration using Cost Volume Network - ResearchGate, https://www.researchgate.net/publication/354304016_LCCNet_LiDAR_and_Camera_Self-Calibration_using_Cost_Volume_Network</li>
<li>Official PyTorch implementation of the paper “LCCNet: Lidar and Camera Self-Calibration usingCost Volume Network”. - GitHub, https://github.com/IIPCVLAB/LCCNet</li>
<li>The KITTI Vision Benchmark Suite - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/</li>
<li>Online Calibration Method of LiDAR and Camera Based on Fusion of Multi-Scale Cost Volume - MDPI, https://www.mdpi.com/2078-2489/16/3/223</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>