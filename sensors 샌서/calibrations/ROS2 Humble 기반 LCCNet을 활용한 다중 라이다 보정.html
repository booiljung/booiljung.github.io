<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:ROS2 Humble 기반 LCCNet을 활용한 다중 라이다 보정 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>ROS2 Humble 기반 LCCNet을 활용한 다중 라이다 보정 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">센서 (Sensors)</a> / <a href="index.html">HKU-MaRS</a> / <span>ROS2 Humble 기반 LCCNet을 활용한 다중 라이다 보정 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>ROS2 Humble 기반 LCCNet을 활용한 다중 라이다 보정 안내서</h1>
<h2>1. 서론: 왜 새로운 방식이 필요한가?</h2>
<p>자율주행차나 로봇이 주변 환경을 정확하게 인식하려면 여러 센서의 정보를 하나로 합치는 ’센서 퓨전’이 필수적이다. 특히, 여러 개의 라이다 센서를 사용할 때, 각 센서가 바라보는 세상의 좌표를 완벽하게 일치시키는 **외부 파라미터 보정(Extrinsic Calibration)**은 전체 시스템의 성능을 좌우하는 핵심 과제이다.1</p>
<p>기존에는 주로 ICP(Iterative Closest Point)와 같은 기하학적 알고리즘을 사용했다.3 이 방법은 두 라이다 데이터의 형태를 직접 비교하여 맞추는 방식으로, 마치 두 개의 퍼즐 조각을 맞춰보는 것과 같다. 하지만 주변에 벽이나 기둥처럼 특징적인 구조물이 부족한 긴 복도나 넓은 공터에서는 퍼즐 조각의 모양이 뚜렷하지 않아 맞추기 어렵고, 아주 약간만 어긋난 상태에서 시작하면 엉뚱한 곳에 끼워 맞추는 실수를 하기도 한다.3</p>
<p>이러한 문제를 해결하기 위해, 본 안내서는 최신 딥러닝 기술인 **LCCNet(“LCCNet: LiDAR and Camera Self-Calibration using Cost Volume Network”)**을 다중 라이다 보정에 창의적으로 응용하는 방법을 제안한다.1 LCCNet은 본래 라이다와 카메라 보정을 위해 개발되었지만, 우리는 시스템에 장착된 <strong>단일 카메라를 ’다리(Bridge)’처럼 활용</strong>하여 각 라이다를 먼저 카메라에 정밀하게 맞춘 뒤, 이 결과를 조합해 라이다 간의 관계를 알아내는 새로운 접근법을 사용할 것이다. 이 방식은 LCCNet의 높은 정확도와 강인함을 빌려와, 기존 방식으로는 어려웠던 환경에서도 신뢰도 높은 보정 결과를 얻을 수 있는 강력한 해법을 제시한다.</p>
<h2>2.  다중 라이다 보정을 위한 LCCNet 활용 전략: ‘브릿지 보정’</h2>
<h3>2.1  문제 정의: 왜 라이다끼리 직접 보정할 수 없는가?</h3>
<p>우리의 최종 목표는 로봇에 달린 <code>LiDAR_1</code>과 <code>LiDAR_2</code>가 서로에 대해 어디에 위치하고 어떤 방향을 바라보는지, 즉 상대 변환 행렬 <span class="math math-inline">T_{L2\_L1}</span>을 알아내는 것이다. 하지만 LCCNet은 태생적으로 컬러 이미지(RGB)와 3D 포인트 클라우드(라이다)라는 서로 다른 종류의 데이터를 입력받아 그 관계를 학습하도록 설계되었다.2 즉, LCCNet은 ‘라이다-카메라’ 언어는 이해하지만, ‘라이다-라이다’ 언어는 이해하지 못한다. 이것이 우리가 LCCNet을 라이다 간 보정에 직접 사용할 수 없는 근본적인 이유이다.</p>
<h3>2.2  제안 전략: 카메라를 공통의 ’다리’로 삼기</h3>
<p>이 한계를 극복하기 위해, 우리는 시스템의 중앙에 있는 카메라를 **공통 참조 프레임(Common Reference Frame), 즉 ‘다리’**로 사용하는 독창적인 전략을 사용한다. 아이디어는 간단하다.</p>
<ol>
<li><code>LiDAR_1</code>에서 ’다리’인 카메라까지 건너가는 길(<span class="math math-inline">T_{C\_L1}</span>)을 찾는다.</li>
<li><code>LiDAR_2</code>에서 ’다리’인 카메라까지 건너가는 또 다른 길(<span class="math math-inline">T_{C\_L2}</span>)을 찾는다.</li>
<li>두 개의 길 정보를 조합하면, <code>LiDAR_1</code>에서 <code>LiDAR_2</code>로 직접 가는 경로(<span class="math math-inline">T_{L2\_L1}</span>)를 계산해낼 수 있다.</li>
</ol>
<p>이처럼 직접 해결이 어려운 문제를 두 개의 쉬운 문제로 나누어 푸는 ‘분할 정복’ 접근법을 통해, 우리는 LCCNet의 강력한 성능을 다중 라이다 보정 문제에 적용할 수 있게 된다.</p>
<h3>2.3  보정 워크플로우 및 수학적 원리 상세 해설</h3>
<p>이 ‘브릿지 보정’ 전략은 다음과 같은 3단계로 진행된다. 각 단계의 수학적 의미를 쉽게 풀어보겠다.</p>
<p><strong>1단계: <code>LiDAR_1</code> → <code>Camera</code> 보정</strong></p>
<ul>
<li><strong>무엇을 하나요?</strong> LCCNet에 <code>LiDAR_1</code>의 포인트 클라우드와 카메라 이미지를 입력한다.</li>
<li><strong>결과물은 무엇인가요?</strong> <span class="math math-inline">T_{C\_L1}</span>라는 4x4 행렬을 얻는다. 이 행렬은 <code>LiDAR_1</code>의 좌표 세상에 있는 어떤 점이든 카메라의 좌표 세상에서는 어디에 보이는지 알려주는 ’마법의 레시피’와 같다.</li>
</ul>
<p><strong>2단계: <code>LiDAR_2</code> → <code>Camera</code> 보정</strong></p>
<ul>
<li><strong>무엇을 하나요?</strong> 이번에는 LCCNet에 <code>LiDAR_2</code>의 포인트 클라우드와 카메라 이미지를 입력한다.</li>
<li><strong>결과물은 무엇인가요?</strong> <span class="math math-inline">T_{C\_L2}</span>라는 또 다른 4x4 행렬을 얻는다. 이것은 <code>LiDAR_2</code>의 세상을 카메라의 세상으로 바꿔주는 두 번째 ’마법의 레시피’이다.</li>
</ul>
<p>3단계: LiDAR_1 → LiDAR_2 변환 유도 (두 레시피 조합하기)</p>
<p>이제 두 개의 레시피($T_{C_L1}, <span class="math math-inline">T_{C\_L2}</span>)를 가지고 최종 목표인 <span class="math math-inline">T_{L2\_L1}</span>을 만들어 보겠다.</p>
<ul>
<li>
<p><strong>첫 번째 조합:</strong> 우리는 <code>LiDAR_1</code>에서 <code>LiDAR_2</code>로 가고 싶다. 우리가 아는 길은 카메라를 거쳐 가는 길뿐이다. 즉, <code>LiDAR_1</code> → <code>Camera</code> → <code>LiDAR_2</code> 경로를 만들어야 한다.</p>
</li>
<li>
<p><code>LiDAR_1</code>에서 카메라로 가는 길은 <span class="math math-inline">T_{C\_L1}</span>이다.</p>
</li>
<li>
<p>카메라에서 <code>LiDAR_2</code>로 가는 길은, 우리가 가진 <span class="math math-inline">T_{C\_L2}</span>(<code>LiDAR_2</code> → <code>Camera</code>)를 거꾸로 되돌아가는 것과 같다. 수학에서는 이를 **역행렬(Inverse Matrix)**이라고 하며, <span class="math math-inline">(T_{C\_L2})^{-1}</span>로 표현한다.</p>
</li>
<li>
<p>따라서, <code>LiDAR_1</code>에서 <code>LiDAR_2</code>로 가는 변환 <span class="math math-inline">T_{L2\_L1}</span>은 두 변환을 순서대로 곱해주면 된다.</p>
<p><span class="math math-display">
T_{L2\_L1} = (T_{C\_L2})^{-1} \cdot T_{C\_L1}
</span></p>
</li>
<li>
<p>주의사항: 오차의 누적</p>
</li>
</ul>
<p>이 전략은 매우 효과적이지만, 한 가지 명심해야 할 점이 있다. 마치 ’소문 전달 게임’과 같다. 1단계에서 발생한 아주 작은 오차와 2단계에서 발생한 또 다른 작은 오차가 3단계에서 합쳐지면서 더 큰 오차로 증폭될 수 있다. 따라서 각 라이다-카메라 보정 단계에서 LCCNet의 성능을 최대한 활용하여 오차를 최소화하는 것이 최종 결과의 품질을 결정하는 가장 중요한 요소이다.</p>
<h2>3.  ROS2 Humble 환경 구축 및 LCCNet 노드 구현: 차근차근 따라하기</h2>
<h3>3.1  개발 환경 구성: 충돌 없는 깨끗한 작업 공간 만들기</h3>
<ul>
<li>
<p><strong>기본 환경</strong>: Ubuntu 22.04 운영체제와 ROS2 Humble을 기준으로 설명한다.</p>
</li>
<li>
<p><strong>가장 큰 난관: Python 버전 충돌</strong>: LCCNet 원본 코드는 오래된 Python 3.6과 PyTorch 버전을 요구하지만 , ROS2 Humble은 최신 Python 3.10을 사용한다.2 이는 마치 오래된 기계 부품을 최신 자동차에 끼우려는 것과 같아서 심각한 충돌을 일으킨다.</p>
</li>
<li>
<p><strong>해결책: 파이썬 가상 환경(Virtual Environment)</strong>: 이 ’의존성 지옥’을 피하기 위해, 우리는 LCCNet만을 위한 독립적인 작업 공간인 ’가상 환경’을 만들 것이다. 이렇게 하면 시스템의 다른 ROS2 패키지들과 완전히 분리되어 서로 영향을 주지 않는 깨끗한 환경에서 LCCNet을 설치하고 실행할 수 있다.</p>
<pre><code class="language-Bash"># 1. ROS2 워크스페이스의 소스(src) 폴더로 이동한다.
cd ~/ros2_ws/src

# 2. 'lccnet_env'라는 이름의 가상 환경을 생성한다.
python3 -m venv lccnet_env

# 3. 생성된 가상 환경을 활성화한다. 이제부터 설치하는 모든 패키지는 이 안에만 설치된다.
source lccnet_env/bin/activate

# 4. 가상 환경 안에서 LCCNet에 필요한 라이브러리들을 설치한다.
# (GPU 사용을 가정, CUDA 11.8 기준)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install opencv-python-headless ros2-numpy
#... LCCNet이 요구하는 기타 패키지들을 여기에 설치한다.
</code></pre>
</li>
</ul>
<pre><code>
### 3.2  LCCNet ROS2 래퍼 노드(Wrapper Node) 설계: LCCNet에게 ROS2 가르치기


원본 LCCNet 코드는 ROS2를 전혀 알지 못한다. 그래서 우리는 LCCNet의 핵심 기능을 감싸서 ROS2와 소통할 수 있게 해주는 **'래퍼(Wrapper) 노드'**를 만들어야 한다. 이 노드는 LCCNet과 ROS2 사이의 '통역사' 역할을 한다.

- **노드 이름**: `lccnet_calibrator`
- **역할**:
1. **입력(구독)**: ROS2 토픽을 통해 카메라 이미지(`/camera/color/image_raw`)와 라이다 포인트 클라우드(`/lidar_1/points`) 데이터를 실시간으로 받아온다.
2. **처리**: 받아온 데이터를 LCCNet이 이해할 수 있는 형태로 가공하여 모델에 입력하고, 보정 결과를 얻어낸다.
3. **출력(발행)**: 계산된 최종 변환 값을 ROS2의 표준 좌표계 관리 시스템인 `/tf_static` 토픽으로 발행한다. 이렇게 하면 Rviz2 같은 시각화 도구나 다른 노드들이 즉시 이 보정 결과를 사용할 수 있게 된다.
- **설정 관리(파라미터)**: 노드를 실행할 때마다 코드를 수정하는 것은 비효율적이다. 따라서 이미지 토픽 이름, 라이다 토픽 이름, LCCNet 모델 파일 경로 등 자주 바뀌는 설정들은 별도의 `config.yaml` 파일로 분리하여 관리한다. 이는 노드의 재사용성과 유연성을 크게 높여준다.

### 3.3  데이터 전처리 파이프라인 구현: 모델을 위한 데이터 요리하기


LCCNet 모델은 특정 형식으로 '요리된' 데이터만 먹을 수 있다. ROS2 토픽으로 들어온 날것의 데이터를 모델의 입맛에 맞게 가공하는 과정이 필요하다.

1. **PointCloud2 → 깊이 이미지 변환**: LCCNet은 3차원 포인트 클라우드를 직접 처리하지 못하고, 2차원 이미지 형태로 변환된 데이터를 입력으로 받는다. 이 과정은 3D 영화를 2D 스크린으로 보는 것과 유사하다. 각 3D 포인트를 카메라의 시점에서 바라본 2D 이미지 평면에 투영하고, 각 픽셀에 카메라로부터의 거리(깊이) 정보를 색상이나 밝기 값으로 저장하여 '깊이 이미지(Depth Image)'를 만듭니다.
2. **ROS 이미지 → 텐서 변환**: 카메라로부터 받은 이미지 데이터도 모델이 바로 사용할 수 없다. 이미지를 PyTorch가 이해하는 기본 데이터 단위인 '텐서(Tensor)'로 변환해야 한다. 또한, LCCNet 모델이 학습될 때 사용했던 것과 똑같은 방식으로 이미지의 크기를 조정하고 색상 값의 범위를 정규화(normalization)하는 과정을 반드시 거쳐야 한다. 모델이 특정 규격의 음식에만 익숙해져 있기 때문이다.

### 3.4  LCCNet 추론 및 변환 발행 로직: 똑똑하게 계산하고 결과 공유하기


1. **모델 로딩**: 노드가 시작될 때, 무거운 LCCNet 모델을 딱 한 번만 메모리에 올려놓는다. 매번 데이터를 받을 때마다 모델을 로드하면 매우 비효율적이기 때문이다.
2. **추론 실행**: 카메라와 라이다 데이터가 모두 준비되면, 전처리된 데이터를 LCCNet 모델에 입력하여 '추론(Inference)'을 실행한다. 모델은 현재 변환 값의 오차가 얼마나 되는지를 `delta_T` (6축 자유도: x, y, z 이동, roll, pitch, yaw 회전) 형태로 출력한다. 즉, 정답을 바로 알려주는 것이 아니라 '오른쪽으로 3cm, 위로 2도 돌리면 더 정확해져' 와 같은 '수정 제안'을 해주는 것이다.
3. **변환 업데이트 및 발행**: 모델이 제안한 수정 값(`delta_T`)을 현재 변환 행렬에 적용하여 더 정확한 변환 행렬로 업데이트한다. 이 과정을 여러 번 반복하면(Iterative Refinement), 점진적으로 오차가 줄어들어 매우 정밀한 결과를 얻을 수 있다. 최종적으로 완성된 변환 행렬은 ROS2의 `StaticTransformBroadcaster`를 통해 `/tf_static` 토픽에 발행되어, 로봇 시스템 전체가 이 정확한 센서 위치 정보를 공유하게 된다.

## 4.  데이터 준비 및 동기화: 보정의 첫 단추


### 4.1  시간 동기화의 중요성


LCCNet 기반 보정의 성공은 양질의 데이터에 달려있다. 여기서 '양질'이란, **정확히 동일한 순간에** 촬영된 카메라 이미지와 라이다 포인트 클라우드 쌍을 의미한다. 이는 움직이는 기차의 특정 창문 밖 풍경을 사진으로 찍는 것과 같다. 셔터를 누르는 순간과 창문 앞을 풍경이 지나가는 순간이 정확히 일치해야 원하는 사진을 얻을 수 있다.

만약 시간 동기화가 제대로 이루어지지 않으면, LCCNet은 서로 다른 시간, 즉 서로 다른 공간 상태를 담은 데이터를 비교하게 된다. 이는 움직이는 물체에 대해 치명적인 오차를 유발하며, 정적인 환경에서도 로봇의 미세한 움직임만으로도 잘못된 보정 결과를 도출할 수 있다. 밀리초(ms) 단위의 시간 불일치가 수 센티미터(cm)의 인식 오류로 이어질 수 있으며, 이는 고속 주행 환경에서 심각한 안전 문제의 원인이 된다. 따라서 `rosbag`을 기록하기 전에, 센서 데이터의 타임스탬프를 정밀하게 동기화하는 것은 선택이 아닌 필수 과정이다.

### 4.2  ROS2에서의 시간 동기화 방법


ROS2 환경에서 여러 센서의 데이터를 동기화하는 방법은 크게 하드웨어 방식과 소프트웨어 방식으로 나눌 수 있다.

- **하드웨어 동기화 (이상적인 방법)**:
- **PTP (Precision Time Protocol)**: 네트워크를 통해 여러 장치의 시계를 마이크로초(µs) 이하 단위로 정밀하게 동기화하는 프로토콜이다. PTP를 지원하는 센서와 네트워크 스위치를 사용하면 가장 이상적인 동기화 환경을 구축할 수 있다.
- **하드웨어 트리거 (PPS)**: GPS 등에서 나오는 1PPS(Pulse Per Second) 신호를 모든 센서의 트리거 입력에 연결하여, 매초 동일한 순간에 데이터 수집을 시작하도록 강제하는 방식이다. 이는 가장 결정론적이고 정확한 동기화를 보장한다.
- **장단점**: 가장 높은 정확도를 제공하지만, 이를 지원하는 고가의 하드웨어가 필요하고 시스템 구성이 복잡하다.
- **소프트웨어 동기화 (현실적인 접근법)**:
- **NTP (Network Time Protocol)**: 로봇의 온보드 컴퓨터를 포함한 네트워크상의 모든 컴퓨터 시계를 공통 서버 시간으로 동기화한다. 이는 모든 데이터에 타임스탬프를 찍는 기준 시각을 일치시키는 가장 기본적인 단계이다.
- **ROS2 `message_filters`**: 하드웨어 동기화가 불가능할 때 가장 널리 사용되는 강력한 소프트웨어적 해법이다. 특히 `ApproximateTimeSynchronizer`는 각 센서 토픽에서 발행되는 메시지들의 헤더(header)에 기록된 타임스탬프를 비교하여, 지정된 시간 오차(slop) 범위 내에 있는 메시지 묶음을 찾아 콜백 함수로 전달해준다. LCCNet 노드 내부에서 이 필터를 사용하여, `rosbag`에서 재생되는 데이터 스트림으로부터 동기화된 데이터 쌍을 실시간으로 찾아내어 처리하게 된다.
- **장단점**: 추가 하드웨어 없이 구현 가능하여 접근성이 높지만, 정확도는 하드웨어 방식보다 낮으며 네트워크 지연(latency)이나 시스템 부하에 영향을 받을 수 있다. 하지만 대부분의 애플리케이션에서 충분히 신뢰할 만한 성능을 제공한다.

### 4.3  `rosbag` 기록을 위한 준비 절차


1. **NTP 설정 확인**: 보정에 사용할 모든 컴퓨터(온보드 PC, 센서 제어 PC 등)가 `chrony` 등의 NTP 클라이언트를 통해 동일한 시간 서버에 동기화되어 있는지 반드시 확인한다.
2. **타임스탬프 확인**: `rosbag` 기록 전, `ros2 topic echo /&lt;sensor_topic&gt;` 명령으로 각 센서 데이터의 `header.stamp` 필드를 확인한다. 타임스탬프가 현재 시간과 근사하게 잘 찍히고 있는지, 모든 센서가 동일한 시간 소스(예: ROS Time)를 사용하고 있는지 점검한다.
3. **데이터 기록**: 위 사항이 확인되면, `ros2 bag record` 명령으로 보정에 필요한 모든 센서 토픽을 동시에 기록한다.

## 5.  다중 라이다 보정 실행 및 검증: 실전 가이드


정확하게 동기화된 데이터가 준비되었다면, 다음 절차에 따라 보정을 실행한다.

### 5.1  단계별 실행 절차


1. **1단계: 1차 보정 (`LiDAR_1` → `Camera`)**
- 준비된 `rosbag` 파일을 `ros2 bag play` 명령으로 재생한다.
- `lccnet_calibrator` 노드를 실행하되, 설정 파일에서 입력 토픽을 `LiDAR_1`과 카메라로 지정한다.
- 노드가 계산을 마치고 `/tf_static`으로 발행한 최종 변환 행렬 $T_{C\_L1}$을 복사하여 텍스트 파일이나 YAML 파일에 안전하게 저장한다.
2. **2단계: 2차 보정 (`LiDAR_2` → `Camera`)**
- 동일한 `rosbag` 파일을 다시 재생한다.
- 이번에는 설정 파일의 입력 토픽을 `LiDAR_2`와 카메라로 변경하여 `lccnet_calibrator` 노드를 다시 실행한다.
- 노드가 발행한 최종 변환 행렬 $T_{C\_L2}$를 다른 파일에 저장한다.
3. **3단계: 최종 변환 계산 및 적용**
- 1, 2단계에서 저장한 두 개의 변환 행렬($T_{C\_L1}`, $T_{C\_L2}$)을 입력으로 받아, I-1.3절에서 설명한 수학 공식을 계산하는 간단한 파이썬 스크립트를 실행한다.
- 이 스크립트의 결과물인 최종 변환 행렬 $T_{L2\_L1}$을 로봇의 구조를 정의하는 URDF 파일에 영구적으로 기록하거나, 로봇이 시작될 때마다 이 변환을 발행하는 static_transform_publisher 노드에 적용한다.

### 5.2  결과 검증 방법: 눈으로 직접 확인하기


보정이 얼마나 잘 되었는지는 시각화 도구인 **Rviz2**를 통해 가장 확실하게 확인할 수 있다.

- **정성적 검증 (눈으로 확인)**:
- Rviz2를 실행하고, 기준 좌표계(`Fixed Frame`)를 `LiDAR_2`의 좌표계(예: `lidar_2_link`)로 설정한다.
- `LiDAR_1`과 `LiDAR_2`의 포인트 클라우드를 화면에 동시에 표시한다.
- **성공의 증거**: 만약 보정이 완벽하게 되었다면, 두 라이다의 포인트 클라우드가 하나의 센서에서 나온 것처럼 공간적으로 완벽하게 합쳐져 보일 것이다. 주변 환경의 벽은 흐릿한 이중상이 아닌 날카로운 단일 평면으로, 기둥은 뚱뚱한 두 개가 아닌 선명한 하나의 원기둥으로 보여야 한다. 여러 각도에서 돌려보며 어긋나는 부분이 없는지 꼼꼼히 확인한다.
- **정량적 검증 (숫자로 확인)**:
- 정확한 정답(Ground Truth)이 없는 한 최종 결과를 숫자로 평가하기는 어렵다. 하지만 각 라이다-카메라 보정 단계의 신뢰도를 통해 간접적으로 평가할 수 있다. LCCNet 논문에 따르면, 이 모델은 KITTI 데이터셋에서 평균 0.297cm의 이동 오차와 0.017°의 회전 오차라는 놀라운 정확도를 보여준다. 우리 시스템에서 각 보정 단계의 결과가 이와 유사한 수준의 정밀도를 보이는지, 또는 반복적인 보정 과정에서 오차가 안정적으로 수렴하는지를 확인함으로써 최종 결과의 신뢰도를 가늠할 수 있다.

## 6.  보정 성능 비교 및 분석: 어떤 도구를 선택할 것인가?


우리가 제안한 LCCNet 기반 방식과 전통적인 ICP/NDT 기반 방식을 비교하면 다음과 같다. 어떤 도구가 여러분의 프로젝트에 더 적합한지 판단하는 데 도움이 될 것이다.

| **특징 (Feature)**   | **LCCNet 기반 (본 안내서 제안)**            | **ICP/GICP/NDT 기반 (예: Multi_LiCa)**           |
| -------------------- | ------------------------------------------- | ------------------------------------------------ |
| **핵심 원리**        | 딥러닝으로 데이터 간의 '의미'를 학습        | 기하학적 '형태'를 직접 비교                      |
| **필요 센서**        | 라이다(들), **카메라 1대 (필수)**           | 라이다(들)만 있으면 됨                           |
| **정확도**           | **매우 높음** (cm, 0.01° 단위)              | 환경과 초기 추정값에 따라 크게 달라짐            |
| **초기 오차 강인성** | **매우 강인함** (크게 어긋나도 잘 찾아냄)   | 초기 추정값에 민감함 (조금만 틀려도 실패 가능) 3 |
| **환경 의존성**      | 특징 없는 환경에서도 **안정적**             | 명확한 구조물이 있어야 성능이 좋음               |
| **계산 자원**        | **GPU 가속이 거의 필수적**                  | CPU만으로도 충분히 가능                          |
| **구현 난이도**      | **높음** (딥러닝 모델 통합, 브릿지 전략 등) | 중간 (이미 만들어진 ROS2 패키지 활용 가능)       |

**결론적으로,**

- **LCCNet 기반 방식은** 최고의 정밀도와 신뢰성이 필요하고, 시스템에 카메라가 이미 포함되어 있으며, GPU 자원을 활용할 수 있는 고성능 자율주행 시스템에 가장 적합하다. 초기 설정의 복잡함을 감수할 만큼 월등한 성능을 제공한다.
- **ICP/NDT 기반 방식은** 개발 일정이 촉박하거나, 카메라 센서가 없거나, 요구되는 정확도 수준이 상대적으로 높지 않은 프로젝트에서 더 빠르고 합리적인 선택이 될 수 있다.

## 7. 최종 결론


본 안내서는 최신 딥러닝 모델인 LCCNet을 다중 라이다 보정 문제에 창의적으로 적용하는 새로운 방법론을 상세히 제시했다. 특히, 보정의 성공을 위한 가장 근본적인 전제 조건인 **센서 데이터의 시간 동기화**의 중요성을 강조하고 ROS2 환경에서의 해결책을 제시했다. 단일 카메라를 '다리'로 사용하는 '브릿지 보정' 전략을 통해 LCCNet의 한계를 극복하고, 실제 구현을 위한 구체적인 개발 환경 설정, 노드 설계, 단계별 실행 및 검증 절차까지 포괄적으로 다루었다.

이 방법론은 기존 방식들이 어려움을 겪었던 환경에서도 **SOTA(State-of-the-art) 수준의 높은 정확도와 강인성**을 제공하며, 수동으로 정밀한 초기값을 입력해야 하는 번거로움을 해결해준다는 명확한 장점을 가진다. 물론 이를 위해 정확한 데이터 준비 과정과 딥러닝 모델을 통합하는 구현상의 노력이 필요하다.

궁극적으로, 본 안내서에서 제안한 방법은 단순히 주어진 도구를 사용하는 것을 넘어, 기술의 핵심 원리를 깊이 이해하고 그것을 새로운 문제에 창의적으로 적용하는 공학적 문제 해결의 좋은 본보기라 할 수 있다. 이 가이드가 여러분의 로봇 시스템의 '눈'을 더욱 날카롭게 만드는 데 기여하기를 바란다.

## 8. 참고 자료


1. LCCNet: LiDAR and Camera Self-Calibration using Cost Volume Network - ResearchGate, https://www.researchgate.net/publication/354304016_LCCNet_LiDAR_and_Camera_Self-Calibration_using_Cost_Volume_Network
2. LCCNet: LiDAR and Camera Self-Calibration ... - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021W/WAD/papers/Lv_LCCNet_LiDAR_and_Camera_Self-Calibration_Using_Cost_Volume_Network_CVPRW_2021_paper.pdf
3. TUMFTM/Multi_LiCa: Multi - LiDAR-to-LiDAR calibration ... - GitHub, https://github.com/TUMFTM/Multi_LiCa
4. pixmoving-moveit/multi_lidar_calibration_ros2: This repo contains a ROS2 package that implements NDT &amp; ICP registration of PCL for multiple LiDAR extrinsics calibration. - GitHub, https://github.com/pixmoving-moveit/multi_lidar_calibration_ros2
5. LCCNet: LiDAR and Camera Self-Calibration Using Cost Volume Network - CVPR 2021 Open Access Repository - The Computer Vision Foundation, https://openaccess.thecvf.com/content/CVPR2021W/WAD/html/Lv_LCCNet_LiDAR_and_Camera_Self-Calibration_Using_Cost_Volume_Network_CVPRW_2021_paper.html
6. [2012.13901] LCCNet: LiDAR and Camera Self-Calibration using Cost Volume Network, https://arxiv.org/abs/2012.13901
7. Official PyTorch implementation of the paper “LCCNet: Lidar and Camera Self-Calibration usingCost Volume Network”. - GitHub, https://github.com/IIPCVLAB/LCCNet
8. Online Calibration Method of LiDAR and Camera Based on Fusion of Multi-Scale Cost Volume - MDPI, https://www.mdpi.com/2078-2489/16/3/223
</code></pre>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>