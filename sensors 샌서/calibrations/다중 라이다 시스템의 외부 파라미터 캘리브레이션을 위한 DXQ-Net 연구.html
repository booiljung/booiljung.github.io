<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:다중 라이다 시스템의 외부 파라미터 캘리브레이션을 위한 DXQ-Net 연구</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>다중 라이다 시스템의 외부 파라미터 캘리브레이션을 위한 DXQ-Net 연구</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">센서 (Sensors)</a> / <a href="index.html">HKU-MaRS</a> / <span>다중 라이다 시스템의 외부 파라미터 캘리브레이션을 위한 DXQ-Net 연구</span></nav>
                </div>
            </header>
            <article>
                <h1>다중 라이다 시스템의 외부 파라미터 캘리브레이션을 위한 DXQ-Net 연구</h1>
<h2>1.  다중 라이다 시스템과 캘리브레이션의 중요성</h2>
<h3>1.1  자율주행 시스템의 3D 환경 인지 센서로서의 라이다</h3>
<p>자율주행 기술의 발전은 첨단 센서 기술의 통합을 통해 이루어지고 있으며, 그 중심에는 라이다(LiDAR, Light Detection and Ranging)가 있다. 라이다는 레이저 펄스를 방출하고, 물체에 반사되어 돌아오는 시간을 측정하여 거리를 계산하는 능동형 센서다(Time-of-Flight, ToF).1 이 원리를 통해 센서 주변 환경에 대한 수백만 개의 3차원 측정값 집합, 즉 포인트 클라우드(Point Cloud)를 생성한다. 라이다가 생성하는 고밀도의 3D 포인트 클라우드는 자율주행 차량이 주변 환경의 형태와 구조를 밀리미터 단위의 정밀도로 인지할 수 있게 하는 핵심 정보를 제공한다.3</p>
<p>카메라가 색상과 질감 정보를 제공하고 레이더가 악천후 속에서 객체의 속도를 감지하는 데 강점이 있다면, 라이다는 주야간이나 조명 조건의 변화에 거의 영향을 받지 않고 일관된 3D 공간 정보를 제공한다는 점에서 독보적인 위치를 차지한다.1 이러한 특성 덕분에 라이다는 자율주행 시스템의 핵심 기능인 ▲정밀 지도 생성(High-Definition Mapping) ▲장애물 탐지 및 회피(Obstacle Detection and Avoidance) ▲차선 유지 및 적응형 순항 제어(Lane Keeping and Adaptive Cruise Control) 등을 구현하는 데 필수적인 센서로 자리매김했다.3 차량, 보행자, 도로 표지판, 연석 등 주행 환경을 구성하는 모든 요소를 3차원 공간상에서 정확히 식별하고 위치를 파악하는 능력은 안전한 자율주행을 위한 전제 조건이며, 라이다는 이 전제 조건을 충족시키는 가장 신뢰도 높은 데이터를 제공한다.</p>
<h3>1.2  단일 라이다의 한계와 다중 라이다 시스템의 필요성</h3>
<p>초기 자율주행 연구에서는 차량 지붕에 장착된 고가의 360도 회전형 기계식 라이다 하나에 의존하는 경우가 많았다.1 이러한 센서는 단일 장치로 차량 주변 전체를 스캔할 수 있다는 장점이 있지만, 높은 비용, 큰 부피, 그리고 기계적 구동부의 내구성 문제로 인해 양산 차량에 적용하기에는 한계가 명확했다.5</p>
<p>이러한 한계를 극복하기 위해 최근에는 마이크로미러(MEMS)나 광학 위상 배열(OPA) 기술을 사용하는 소형의 고정형(Solid-state) 라이다가 주목받고 있다.1 고정형 라이다는 비용이 저렴하고 크기가 작으며 내구성이 뛰어나 양산차 적용에 유리하지만, 개별 센서가 감지할 수 있는 시야각(Field of View, FoV)이 제한적이라는 단점을 가진다. 따라서 차량 주변 360도 전체를 끊김 없이 인지하고, 차량 자체 구조물이나 다른 도로 사용자로 인해 발생하는 사각지대(blind spot)를 최소화하기 위해서는 여러 개의 고정형 라이다를 차량의 전후측면에 분산 배치하는 다중 라이다 시스템(Multi-LiDAR System) 구성이 필수가 되었다.7</p>
<p>다중 라이다 시스템은 단순히 시야각을 넓히는 것 이상의 이점을 제공한다. 여러 센서의 측정 영역이 중첩되는 구간에서는 포인트 클라우드의 밀도가 높아져 더 작거나 멀리 있는 객체에 대한 탐지 성능이 향상된다.7 또한, 일부 센서가 오작동하거나 특정 방향이 일시적으로 가려지더라도 다른 센서가 해당 영역을 보완해주는 데이터 이중화(redundancy)를 통해 전체 인지 시스템의 강건성(robustness)과 안전성을 크게 높일 수 있다.8 결국, 상용 자율주행 시스템의 현실적인 요구사항인 비용, 성능, 안전성을 모두 만족시키기 위한 하드웨어 구성의 발전 방향은 자연스럽게 다중 라이다 시스템으로 귀결되었다.</p>
<h3>1.3  정밀한 외부 파라미터 캘리브레이션의 핵심적 역할과 기술적 난제</h3>
<p>다중 라이다 시스템이 성공적으로 작동하기 위한 가장 중요한 전제 조건은 바로 정밀한 외부 파라미터 캘리브레이션(Extrinsic Parameter Calibration)이다. 시스템에 장착된 각 라이다 센서는 자신만의 고유한 3차원 좌표계를 기준으로 포인트 클라우드를 생성한다. 따라서 여러 센서로부터 수집된 개별적인 포인트 클라우드들을 하나의 의미 있는 전체 장면으로 융합(fusion)하기 위해서는, 각 센서 좌표계 간의 상대적인 기하학적 관계를 정확히 알아야 한다.6 이 관계는 3차원 회전(Rotation)과 3차원 이동(Translation)으로 구성된 6자유도(6-DoF) 변환 행렬로 정의되며, 이를 외부 파라미터라고 한다.</p>
<p>만약 이 외부 파라미터에 미세한 오차라도 존재한다면, 각 센서의 포인트 클라우드를 하나의 공통 좌표계(예: 차량 중심 좌표계)로 변환했을 때 데이터가 서로 어긋나게 정렬된다. 이러한 오정렬은 마치 여러 장의 투명 필름에 그려진 그림을 잘못 겹쳐 놓은 것처럼, 동일한 물체가 여러 겹으로 보이거나(ghosting effect) 왜곡된 형태로 나타나는 결과를 초래한다. 이는 객체의 크기, 속도, 위치를 잘못 인식하게 만들어 인지 시스템 전체의 신뢰도를 무너뜨리는 치명적인 오류로 이어진다.10</p>
<p>다중 라이다 캘리브레이션은 여러 기술적 난제를 안고 있다. 특히 고정형 라이다를 사용하는 현대적 시스템에서는 센서 간 시야각이 전혀 겹치지 않거나(non-overlapping FoV) 아주 일부만 겹치는 경우가 많아, 전통적인 정합(registration) 기반의 방법을 적용하기 어렵다.5 또한, 서로 다른 제조사의 라이다를 함께 사용하는 경우 해상도, 스캔 패턴, 측정 노이즈 특성이 달라 데이터 간의 직접적인 비교가 까다롭다.11 마지막으로, 차량이 장기간 주행하면서 겪는 진동이나 온도 변화는 센서의 장착 위치를 미세하게 변화시켜 초기 캘리브레이션 값을 무효화할 수 있다. 따라서 일회성 오프라인 캘리브레이션뿐만 아니라, 주행 중에도 파라미터 변화를 감지하고 보정할 수 있는 온라인 캘리브레이션 기술의 필요성이 대두되고 있다.10</p>
<h2>2.  라이다 캘리브레이션 기술 패러다임 분석</h2>
<p>라이다 캘리브레이션 기술은 크게 전통적 기법과 딥러닝 기반 기법으로 나눌 수 있다. 전통적 기법은 다시 인공적인 타겟 사용 여부에 따라 타겟 기반과 비-타겟 방식으로 분류된다. 이러한 기술 패러다임의 변화는 정확도와 신뢰도를 유지하면서도 캘리브레이션 과정의 자동화와 유연성을 높이려는 방향으로 진행되어 왔다.</p>
<h3>2.1  전통적 캘리브레이션 기법</h3>
<h4>2.1.1  타겟 기반(Target-based) 기법</h4>
<p>타겟 기반 기법은 캘리브레이션 과정에서 알려진 크기와 형태를 가진 인공적인 타겟을 사용하는 가장 고전적인 접근법이다.6 주로 체커보드(checkerboard) 패턴이나 원형 패턴이 인쇄된 평면 보드를 사용하며, 이 타겟을 여러 라이다의 시야각이 중첩되는 영역에 배치한다.9 캘리브레이션 절차는 각 라이다 센서의 포인트 클라우드 데이터에서 타겟의 명확한 기하학적 특징(평면의 법선 벡터, 모서리, 꼭짓점 등)을 추출하는 것에서 시작한다.11 이후, 각 센서 좌표계에서 추출된 동일한 특징들이 하나의 좌표계에서 일치하도록 만드는 변환 행렬, 즉 외부 파라미터를 최적화 문제를 풀어 계산한다.</p>
<p>이 방식의 가장 큰 장점은 명확한 기하학적 제약 조건을 사용하기 때문에 통제된 환경 하에서 매우 높은 정확도를 달성할 수 있다는 점이다.6 하지만, 실용적인 측면에서 여러 한계를 가진다. 우선, 캘리브레이션을 위해 별도의 타겟을 정밀하게 제작하고 현장에 설치해야 하는 번거로움이 있다. 특히, 여러 센서의 FoV가 모두 겹치는 위치에 타겟을 배치하는 것이 물리적으로 어려운 경우가 많다.5 또한, 이 과정은 대부분 수동으로 진행되어 시간이 많이 소요되고, 대규모 차량 플릿(fleet)에 적용하기에는 확장성이 떨어진다. 가장 결정적으로, 이는 공장이나 연구실 환경에서 수행되는 오프라인(offline) 방식이므로, 차량 운행 중에 발생하는 외부 파라미터의 미세한 변화(drift)에 대응할 수 없다는 근본적인 한계를 지닌다.9</p>
<h4>2.1.2  비-타겟(Targetless) 기법</h4>
<p>비-타겟 기법은 인공적인 타겟 없이 주변 환경의 자연스러운 특징이나 센서의 움직임 자체를 정보로 활용하여 캘리브레이션을 수행한다. 이는 타겟 설치의 번거로움을 없애고 온라인 캘리브레이션의 가능성을 열었다는 점에서 중요한 진전으로 평가받는다.10</p>
<p><strong>모션 기반(Motion-based) 기법</strong>: 이 접근법은 ‘핸드-아이 캘리브레이션(Hand-eye calibration)’ 문제와 동일한 원리를 적용한다.8 차량이 주행하는 동안 각 라이다 센서는 독립적으로 자신의 이동 궤적(trajectory)을 추정한다(LiDAR Odometry). 차량이라는 강체(rigid body)에 모든 센서가 고정되어 있으므로, 각 센서가 계산한 이동 궤적 사이에는 외부 파라미터에 의해 정의되는 일정한 기하학적 관계가 성립한다. 모션 기반 기법은 이 관계를 나타내는 <code>AX=XB</code> 형태의 행렬 방정식을 풀어 외부 파라미터 <code>X</code>를 구한다.8 이 방식의 가장 큰 장점은 센서 간 시야각이 전혀 겹치지 않는 경우에도 캘리브레이션이 가능하다는 것이다. 하지만, 오도메트리 추정 과정에서 필연적으로 발생하는 누적 오차(drift)가 캘리브레이션 정확도에 직접적인 영향을 미치며, 이를 보정하기 위한 추가적인 기법이 필요하다.5</p>
<p><strong>정적 환경 기반(Static Environment-based) 기법</strong>: 이 기법은 차량을 움직이지 않은 상태에서, 정적인 환경으로부터 수집한 포인트 클라우드를 직접 정합(registration)하여 외부 파라미터를 추정한다. 대표적인 알고리즘으로는 ICP(Iterative Closest Point)와 NDT(Normal Distributions Transform)가 있다.6 ICP는 한 포인트 클라우드의 점들을 다른 포인트 클라우드에서 가장 가까운 점에 대응시켜 두 데이터 간의 거리를 최소화하는 변환을 반복적으로 찾아가는 방식이다. NDT는 포인트 클라우드를 정규 분포의 집합으로 표현하고, 두 확률 분포 간의 유사도를 최대화하는 변환을 찾는다. 이러한 정합 기반 방식은 시간 동기화나 동적 객체 제거와 같은 복잡한 전처리 과정이 필요 없어 상대적으로 간결하지만, 정합 성공을 위해서는 두 포인트 클라우드 간에 충분한 중첩 영역과 구별 가능한 구조적 특징이 존재해야 한다는 제약이 있다.6</p>
<h3>2.2  딥러닝 기반 캘리브레이션 기법의 대두</h3>
<p>최근 딥러닝 기술이 컴퓨터 비전 분야에서 괄목할 만한 성공을 거두면서, 이를 라이다 캘리브레이션 문제에 적용하려는 연구가 활발히 진행되고 있다. 딥러닝 기반 기법은 데이터로부터 특징과 그 관계를 스스로 학습함으로써, 기존 방법들의 한계였던 수동 작업과 환경 제약을 극복하고, 더 높은 수준의 자동화와 강건성을 달성하는 것을 목표로 한다.12</p>
<h4>2.2.1  직접 회귀(Direct Regression) 방식과 매칭(Matching) 기반 방식</h4>
<p>딥러닝 기반 캘리브레이션은 크게 두 가지 접근법으로 나뉜다.12</p>
<p><strong>직접 회귀 방식</strong>: 이 방식은 두 센서(예: 라이다와 카메라)의 데이터를 입력으로 받아 6-DoF 외부 파라미터(회전 벡터 3개, 이동 벡터 3개)를 직접 예측하는 종단간(end-to-end) 심층 신경망을 구성한다. 네트워크는 입력 데이터 간의 미세한 불일치를 감지하고, 이를 보정하기 위한 변환 파라미터 값을 직접 회귀하도록 학습된다.16 구조가 단순하고 추론 속도가 빠르다는 장점이 있지만, 기하학적 제약 조건을 명시적으로 활용하지 않기 때문에 학습 데이터에 존재하지 않는 큰 오차 범위에 대해서는 일반화 성능이 떨어질 수 있다.</p>
<p><strong>매칭 기반 방식</strong>: 이 방식은 캘리브레이션 문제를 두 센서 데이터 간의 조밀한 대응점(correspondence)을 찾는 문제로 재정의한다. 네트워크는 두 입력 데이터에서 각각 특징 맵을 추출한 뒤, 이 특징 맵들 간의 유사도를 기반으로 각 지점의 대응 관계를 추정한다. 이후, 찾아낸 신뢰도 높은 대응점 쌍들을 이용해 기하학적 최적화(예: PnP, ICP)를 통해 외부 파라미터를 계산한다.12 이 접근법은 기하학적 원리를 명시적으로 활용하므로 해석 가능성이 높고, 더 넓은 범위의 오차에 대해 강건한 성능을 보이는 경향이 있다. 본 보고서에서 심층적으로 분석할 DXQ-Net이 바로 이 매칭 기반 방식의 대표적인 예다.</p>
<h4>2.2.2  온라인, 자동화 캘리브레이션의 가능성</h4>
<p>딥러닝 기반 방법의 가장 큰 잠재력은 온라인 자동화 캘리브레이션의 실현에 있다.10 한 번 학습된 네트워크는 별도의 타겟이나 특정 환경 없이, 주행 중에 실시간으로 들어오는 센서 데이터만으로 현재의 캘리브레이션 상태를 평가하고 오차를 보정할 수 있다. 차량 운행 중 발생하는 진동이나 충격으로 인한 외부 파라미터의 점진적인 변화를 지속적으로 추적하고 수정함으로써, 다중 센서 시스템의 인지 성능을 최상의 상태로 유지할 수 있게 된다. 이는 기존의 오프라인 방식으로는 불가능했던 수준의 동적인 강건성을 시스템에 부여한다.12</p>
<p>캘리브레이션 기술 패러다임의 발전 과정을 종합해 보면, 과거 정밀한 타겟 제작과 통제된 환경 설정 등 물리적 세계에 존재했던 복잡성이 점차 알고리즘의 정교함, 즉 계산적 영역으로 이전되는 뚜렷한 경향을 보인다.6 타겟 기반 방식은 물리적 제약에 크게 의존하는 반면, 비-타겟 방식은 알고리즘이 환경의 특징을 해석하도록 복잡성을 이전시켰다. 딥러닝 기반 방식은 이러한 경향의 정점으로, 캘리브레이션에 필요한 거의 모든 지능을 대규모 데이터셋과 복잡한 네트워크 아키텍처를 통해 학습하는 계산 집약적 패러다임을 채택한다. 이러한 전환은 더 큰 유연성과 자동화를 가능하게 했지만, 동시에 데이터 편향이나 모델 일반화 실패와 같은 새로운 형태의 잠재적 오류를 야기하기도 했다.12</p>
<table><thead><tr><th><strong>구분</strong></th><th><strong>타겟 기반 (Target-based)</strong></th><th><strong>비-타겟 (Targetless) - 모션 기반</strong></th><th><strong>비-타겟 (Targetless) - 정적/정합 기반</strong></th><th><strong>딥러닝 기반 (Deep Learning-based)</strong></th></tr></thead><tbody>
<tr><td><strong>기본 원리</strong></td><td>알려진 기하학적 타겟의 특징점/평면을 이용한 제약 조건 최적화</td><td>각 센서의 이동 궤적 간의 기하학적 상관관계(<code>AX=XB</code>) 해석</td><td>중첩된 포인트 클라우드 간의 기하학적 정합(ICP, NDT 등)</td><td>데이터로부터 특징 대응 관계를 학습하여 파라미터 추정</td></tr>
<tr><td><strong>정확도</strong></td><td>높음 (통제 환경)</td><td>중간 (오도메트리 드리프트에 영향 받음)</td><td>높음 (중첩 영역이 충분할 경우)</td><td>매우 높음 (학습 데이터 범위 내)</td></tr>
<tr><td><strong>자동화 수준</strong></td><td>낮음 (수동 개입 필요)</td><td>높음</td><td>높음</td><td>매우 높음 (완전 자동화 가능)</td></tr>
<tr><td><strong>FoV 중첩 요구사항</strong></td><td>필수</td><td>불필요</td><td>필수 (높은 중첩률 요구)</td><td>유연함 (학습 방식에 따라 다름)</td></tr>
<tr><td><strong>주요 장점</strong></td><td>높은 정확도와 신뢰성</td><td>FoV 중첩 불필요, 유연한 적용</td><td>시간 동기화 불필요, 단순한 데이터 수집</td><td>온라인/실시간 캘리브레이션 가능, 타겟 불필요</td></tr>
<tr><td><strong>주요 한계</strong></td><td>번거로운 수동 작업, 확장성 부족, 오프라인 전용</td><td>오도메트리 오차에 민감, 복잡한 움직임 필요</td><td>초기 추정치에 민감, 구조적 특징이 없는 환경에서 실패</td><td>대규모 학습 데이터 필요, 도메인 일반화 문제</td></tr>
</tbody></table>
<h2>3.  심층 분석: <em>DXQ-Net: Differentiable LiDAR-Camera Extrinsic Calibration Using Quality-aware Flow</em></h2>
<p>DXQ-Net은 2022년 IROS 학회에서 발표된 딥러닝 기반의 라이다-카메라 외부 파라미터 캘리브레이션 방법론이다.19 이 모델의 명칭에서 알 수 있듯이, 본래의 설계 목적은 이종(heterogeneous) 센서인 라이다와 카메라 간의 캘리브레이션이다. DXQ-Net은 파라미터를 직접 회귀하는 대신, 캘리브레이션 오차를 픽셀 공간에서의 변위 벡터 필드, 즉 ’플로우(flow)’로 정의하고 이를 추정하는 독창적인 접근법을 취한다. 이 섹션에서는 DXQ-Net의 핵심 개념과 네트워크 구조, 그리고 학습 과정을 상세히 분석한다.</p>
<h3>3.1  핵심 개념: 캘리브레이션 문제를 대응점(Correspondence) 문제로의 재정의</h3>
<p>DXQ-Net의 가장 핵심적인 아이디어는 6-DoF 외부 파라미터 추정이라는 기하학적 최적화 문제를, 조밀한 2D-3D 대응점을 찾는 문제로 변환한 것이다. 이는 최신 광학 흐름(Optical Flow) 추정 연구, 특히 RAFT(Recurrent All-Pairs Field Transforms for Optical Flow) 네트워크에서 큰 영감을 받았다.16</p>
<p>캘리브레이션이 틀어진 상태에서 초기 외부 파라미터를 사용해 라이다의 3D 포인트 <code>$P_L$</code>을 카메라 이미지 평면에 투영하면, 픽셀 좌표 <code>$p_{init}$</code>에 맺히게 된다. 하지만 실제 정답 파라미터를 사용했다면 이 포인트는 올바른 위치인 <code>$p_{gt}$</code>에 맺혔을 것이다. DXQ-Net은 이 두 픽셀 위치 간의 차이, 즉 변위 벡터 <code>$\Delta p = p_{gt} - p_{init}$</code>를 ’캘리브레이션 플로우(Calibration Flow)’로 정의한다.17 네트워크의 목표는 이미지와 (투영된) 라이다 깊이 맵을 입력받아, 모든 유효한 라이다 포인트에 대한 캘리브레이션 플로우 <code>$\Delta p$</code>를 예측하는 것이다. 일단 정확한 플로우가 예측되면, 수정된 2D 대응점 <code>$p_{gt} = p_{init} + \Delta p$</code>와 원래의 3D 라이다 포인트 <code>$P_L$</code> 간의 대응 관계가 확립된다. 이 다수의 2D-3D 대응점 쌍을 이용하면, PnP(Perspective-n-Point)와 같은 고전적인 기하학 알고리즘을 통해 6-DoF 외부 파라미터를 견고하게 계산할 수 있다.19</p>
<h3>3.2  네트워크 아키텍처 상세 분석</h3>
<p>DXQ-Net은 특징 추출기, 4D 상관관계 볼륨, 그리고 반복적 업데이트 모듈의 세 가지 주요 구성 요소로 이루어진다. 이는 RAFT의 구조를 캘리브레이션 문제에 맞게 변형한 것이다.16</p>
<h4>3.2.1  특징 추출기(Feature Extractor)</h4>
<p>네트워크는 두 개의 독립적인 특징 추출 인코더를 가진 듀얼 브랜치(dual-branch) 구조로 시작한다.16 하나는 카메라의 RGB 이미지를, 다른 하나는 라이다 포인트 클라우드를 2D 이미지 평면에 투영하여 생성한 깊이 맵(depth map)을 입력으로 받는다. 각 인코더는 여러 개의 잔차 블록(residual block)으로 구성된 컨볼루션 신경망(CNN)으로, 입력 데이터로부터 다중 스케일의 특징 맵(feature map)을 추출한다. 이 구조는 서로 다른 모달리티(색상/질감 정보와 기하학적 깊이 정보)에 특화된 표현을 각각 효과적으로 학습하도록 설계되었다.22</p>
<h4>3.2.2  4D 상관관계 볼륨(4D Correlation Volume)의 구성과 역할</h4>
<p>두 특징 추출기에서 나온 특징 맵 <code>$f_{cam}$</code>과 <code>$f_{lidar}$</code> 사이의 모든 가능한 픽셀 쌍에 대한 유사도를 계산하여 4D 상관관계 볼륨 <code>$C$</code>를 구축한다. 이미지 픽셀 <code>$p_1 = (u_1, v_1)$</code>과 깊이 맵 픽셀 <code>$p_2 = (u_2, v_2)$</code> 사이의 상관관계 값은 두 위치에서의 특징 벡터의 내적(dot product)으로 계산된다.</p>
<p>코드 스니펫</p>
<pre><code>$$C(p_1, p_2) = f_{cam}(p_1)^T \cdot f_{lidar}(p_2)$$
</code></pre>
<p>이 4D 볼륨은 <code>$H \times W \times H \times W$</code>의 거대한 크기를 가지므로, 실제 구현에서는 RAFT와 마찬가지로 지역적 상관관계 볼륨을 사용하거나 다중 스케일 피라미드 구조를 통해 계산 효율성을 높인다.16 이 상관관계 볼륨은 특정 픽셀에 대응하는 점을 찾기 위한 모든 가능한 후보와의 매칭 점수를 담고 있는 ‘조회 테이블(lookup table)’ 역할을 수행하며, 후속 업데이트 단계에서 플로우를 추정하는 데 핵심적인 정보를 제공한다.</p>
<h4>3.2.3. 반복적 업데이트를 위한 GRU 기반 업데이트 연산자</h4>
<p>DXQ-Net은 단 한 번의 순방향 패스(forward pass)로 플로우를 계산하는 대신, 반복적인 정제(iterative refinement) 방식을 사용한다. 이 과정의 핵심은 GRU(Gated Recurrent Unit) 셀을 기반으로 하는 업데이트 연산자(update operator)이다.16</p>
<p>초기 캘리브레이션 플로우는 0으로 초기화된다. 각 반복 단계 <code>$k$</code>에서, 업데이트 연산자는 현재의 플로우 추정치 <code>$f_k$</code>, 상관관계 볼륨 <code>$C$</code>, 그리고 컨텍스트 네트워크(context network)에서 추출한 모션 특징을 입력으로 받는다. 현재 플로우 <code>$f_k$</code>를 이용해 상관관계 볼륨에서 관련 영역의 특징을 조회(lookup)하고, 이를 다른 입력들과 결합하여 플로우의 수정량 <code>$\Delta f$</code>를 예측한다. 그 후, 새로운 플로우 <code>$f_{k+1} = f_k + \Delta f$</code>를 계산하여 다음 단계로 전달한다. 이 과정은 고정된 횟수(예: 12회)만큼 반복되며, 매 단계마다 플로우 추정치는 점차 실제 값에 수렴하게 된다. 이러한 반복적 구조는 큰 오차 범위에서도 점진적으로 해를 찾아갈 수 있게 하여 모델의 수렴성과 정확도를 높인다.</p>
<h3>3.3. 캘리브레이션 플로우(Calibration Flow)와 품질 인식(Quality-aware) 추정</h3>
<h4>3.3.1. 픽셀 공간에서의 변위 벡터로서의 캘리브레이션 플로우 정의</h4>
<p>앞서 설명했듯이, 네트워크의 최종 출력은 각 픽셀에 대한 2D 변위 벡터 필드, 즉 캘리브레이션 플로우 <code>$\mathbf{f} = (\Delta u, \Delta v)$</code>이다.17 이 플로우는 초기 외부 파라미터 <code>$\mathbf{T}_{init} =$</code>의 오차로 인해 발생한 이미지 평면상의 투영 오차를 보상하는 역할을 한다. 즉, 이 플로우를 통해 얻은 정확한 2D-3D 대응점들은 캘리브레이션 오차를 바로잡기 위한 기하학적 정보를 내포하고 있다.</p>
<h4>3.3.2. 불확실성(Uncertainty) 예측을 통한 신뢰도 높은 대응점 필터링</h4>
<p>DXQ-Net의 가장 중요한 독창성은 ‘품질 인식(Quality-aware)’ 메커니즘에 있다.19 이는 조밀한 대응점 추정 시 발생하는 본질적인 문제, 즉 모든 픽셀이 캘리브레이션에 동일하게 유용한 정보를 제공하지 않는다는 점을 해결하기 위한 장치다. 예를 들어, 하늘이나 도로처럼 질감이 없는 영역, 반사되는 표면, 폐색(occlusion) 경계 등은 모호하거나 잘못된 매칭 신호를 제공할 가능성이 높다.</p>
<p>이 문제를 해결하기 위해 DXQ-Net은 캘리브레이션 플로우와 함께 각 대응점의 신뢰도를 나타내는 불확실성(uncertainty) 맵을 예측한다. 이는 플로우 추정을 확률적 모델로 공식화하여, 예측된 플로우 벡터의 분산(variance)을 불확실성의 척도로 사용하는 방식으로 구현된다. 후속 포즈 추정 단계에서는 이 불확실성 값을 이용해 신뢰도가 낮은(불확실성이 높은) 대응점 쌍의 영향력을 줄이거나(가중 최소 제곱법), 아예 제거하는(outlier rejection) 과정을 거친다. 이 품질 인식 메커니즘은 데이터에 내재된 노이즈와 모호성에 강건하게 대처할 수 있게 하여, 실제 주행 환경에서의 캘리브레이션 성능을 크게 향상시키는 핵심 요소로 작용한다. 이는 단순한 이상치 제거 기법인 RANSAC보다 더 정교한, 데이터 기반의 학습된 주의(attention) 메커니즘으로 볼 수 있다.</p>
<h3>3.4. 손실 함수(Loss Function) 및 미분 가능한 포즈 추정기</h3>
<h4>3.4.1. 지도 학습(Supervised Learning)을 위한 손실 함수 구성</h4>
<p>DXQ-Net은 지도 학습(supervised learning) 방식으로 훈련된다. 학습 데이터셋은 이미지, 포인트 클라우드, 그리고 정답 외부 파라미터 <code>$\mathbf{T}_{gt}$</code>로 구성된다. 훈련 시에는 <code>$\mathbf{T}_{gt}$</code>에 임의의 오차를 가해 초기 파라미터 <code>$\mathbf{T}_{init}$</code>를 생성한다. 정답 캘리브레이션 플로우 <code>$\mathbf{f}_{gt}$</code>는 <code>$\mathbf{T}_{gt}$</code>와 <code>$\mathbf{T}_{init}$</code>를 이용해 기하학적으로 계산할 수 있다.</p>
<p>손실 함수 <code>$\mathcal{L}$</code>는 GRU 업데이트 과정에서 생성되는 모든 중간 플로우 예측치 <code>$\mathbf{f}^{(i)}_{pred}$</code>와 정답 플로우 <code>$\mathbf{f}_{gt}$</code> 간의 L1 거리(distance)의 가중 합으로 정의된다.</p>
<p>코드 스니펫</p>
<pre><code>$$
\mathcal{L} = \sum_{i=1}^{N} \gamma^{N-i} \left\| \mathbf{f}_{gt} - \mathbf{f}_{pred}^{(i)} \right\|_1
$$
</code></pre>
<p>여기서 <code>$N$</code>은 총 반복 횟수, <code>$\gamma$</code>는 후반부 예측에 더 큰 가중치를 부여하기 위한 할인 계수(discount factor, 예: 0.8)이다. 이 손실 함수는 네트워크가 점진적으로 더 정확한 플로우를 예측하도록 유도한다.</p>
<h4>3.4.2. 종단간(End-to-End) 학습을 가능하게 하는 미분 가능한 PnP 솔버</h4>
<p>네트워크가 예측한 플로우로부터 최종 외부 파라미터를 계산하는 과정까지 학습에 포함시키기 위해, DXQ-Net은 미분 가능한(differentiable) PnP 솔버를 사용한다.17 전통적인 PnP 알고리즘은 반복적인 최적화 과정을 포함하여 미분이 불가능하지만, 최근 연구들을 통해 이를 근사하여 역전파가 가능한 형태로 구현할 수 있게 되었다.</p>
<p>미분 가능한 PnP 모듈은 품질 인식을 통해 필터링된 2D-3D 대응점 세트를 입력받아, 재투영 오차(reprojection error)를 최소화하는 외부 파라미터 <code>$\mathbf{T}_{pred}$</code>를 출력한다. 이 과정 전체가 미분 가능하므로, 최종적으로 예측된 파라미터 <code>$\mathbf{T}_{pred}$</code>와 정답 파라미터 <code>$\mathbf{T}_{gt}$</code> 간의 오차(예: 회전 행렬 간의 각도 차이, 이동 벡터 간의 거리)를 추가적인 손실 항으로 사용하여 네트워크를 훈련시킬 수 있다. 이는 플로우 예측의 정확성뿐만 아니라, 최종 파라미터 추정의 정확성까지 직접적으로 최적화하여 전체 시스템의 성능을 극대화하는 종단간 학습을 가능하게 한다.</p>
<h2>4. DXQ-Net 원리의 다중 라이다 캘리브레이션 적용 방안</h2>
<p>DXQ-Net은 라이다-카메라라는 이종 센서 간 캘리브레이션을 위해 설계되었지만, 그 핵심 철학인 ’기하학적 변환 문제를 조밀한 플로우 추정 문제로 변환’하는 아이디어는 동종(homogeneous) 센서인 다중 라이다 간 캘리브레이션에도 적용할 수 있는 강력한 잠재력을 가지고 있다. 이를 위해서는 문제 정의, 데이터 표현, 네트워크 구조 등 여러 측면에서 수정이 필요하다. 이 섹션에서는 DXQ-Net의 원리를 다중 라이다 캘리브레이션에 적용하기 위한 구체적인 방안을 제안한다.</p>
<h3>4.1. 문제 재구성: 이종(Heterogeneous) 센서에서 동종(Homogeneous) 센서로의 전환</h3>
<p>가장 먼저 해결해야 할 과제는 입력 데이터의 모달리티 차이를 극복하는 것이다. 기존 DXQ-Net은 한쪽은 RGB 이미지(2D, 질감 정보), 다른 한쪽은 깊이 맵(2D, 기하 정보)을 처리했다. 다중 라이다 캘리브레이션에서는 두 입력 모두 동일한 3D 포인트 클라우드이다. 따라서 DXQ-Net의 플로우 기반 접근법을 사용하려면, 먼저 두 라이다의 3D 포인트 클라우드를 네트워크가 비교하고 처리할 수 있는 공통된 2D 표현(이미지 형태)으로 변환하는 과정이 선행되어야 한다.</p>
<h3>4.2. 입력 데이터 표현 방식 제안</h3>
<p>3D 포인트 클라우드를 2D 이미지로 변환하는 데는 여러 투영(projection) 기법이 있으며, 각각 장단점을 가진다. 어떤 투영 방식을 선택하느냐에 따라 네트워크가 학습하는 특징의 종류와 캘리브레이션 오차의 민감도가 달라진다.</p>
<h4>4.2.1. 포인트 클라우드의 2D 투영</h4>
<p><strong>구면 투영(Spherical Projection)</strong>: 이 방식은 라이다 센서의 원점(ego-centric) 관점에서 포인트 클라우드를 2D 이미지로 펼치는 방법이다. 각 3D 포인트 <code>$(x, y, z)$</code>는 구면 좌표계의 방위각(<code>$\theta$</code>)과 고도각(<code>$\phi$</code>)으로 변환되고, 이 각도 값들이 이미지의 가로(<code>$u$</code>) 및 세로(<code>$v$</code>) 좌표에 비례하도록 매핑된다. 이미지의 각 픽셀 값으로는 해당 각도 방향으로 측정된 거리(range), 반사 강도(intensity), 높이(<code>$z$</code>) 등의 정보를 1개 또는 다중 채널로 저장할 수 있다. 이 방법은 라이다의 고유한 스캔 패턴을 보존하고, 회전 오차 중 특히 roll과 pitch 각도 변화에 따른 이미지 왜곡이 뚜렷하게 나타나므로 해당 파라미터 추정에 유리하다.</p>
<p><strong>조감도(Bird’s-Eye-View, BEV) 투영</strong>: 이 방식은 포인트 클라우드를 위에서 내려다본 시점으로 2D 그리드 맵에 투영한다. 3D 공간의 <code>$(x, y)$</code> 평면을 일정한 크기의 셀로 나누고, 각 셀에 속하는 포인트들의 통계적 특징(예: 최대 높이, 밀도, 평균 강도)을 계산하여 픽셀 값으로 인코딩한다. BEV 표현은 높이(<code>$z$</code>) 축에 대한 변화에 불변(invariant)하므로 차량의 상하 움직임에 강건하며, 수평 방향 이동(<code>$t_x, t_y$</code>)과 수직축 중심의 회전(yaw) 오차를 감지하는 데 매우 효과적이다.</p>
<h4>4.2.2. 투영 이미지 간의 플로우 추정 문제로의 변환</h4>
<p>캘리브레이션하고자 하는 두 라이다(소스 라이다 <code>$L_S$</code>, 타겟 라이다 <code>$L_T$</code>)의 포인트 클라우드를 각각 동일한 투영 방식(예: 구면 투영)을 사용하여 2D 이미지 <code>$I_S$</code>와 <code>$I_T$</code>로 변환한다. 이제 문제는, 소스 이미지 <code>$I_S$</code>의 각 픽셀이 타겟 이미지 <code>$I_T$</code>의 어느 픽셀에 대응되는지를 나타내는 2D 변위 벡터 필드, 즉 ’라이다-라이다 플로우(LiDAR-LiDAR Flow)’를 추정하는 문제로 완전히 변환된다. 이 플로우는 두 라이다 간의 외부 파라미터 오차로 인해 발생하는 투영 이미지 상의 불일치를 나타낸다.</p>
<h3>4.3. 제안하는 네트워크 아키텍처 수정안</h3>
<p>다중 라이다 캘리브레이션 문제를 위해 DXQ-Net을 수정하는 아키텍처를 가칭 ’LXL-Net(LiDAR-eXtrinsic-LiDAR Net)’으로 명명하고, 다음과 같은 수정을 제안한다.</p>
<table><thead><tr><th><strong>모듈 (Module)</strong></th><th><strong>기존 DXQ-Net (라이다-카메라)</strong></th><th><strong>제안 LXL-Net (라이다-라이다)</strong></th><th><strong>주요 변경 사유</strong></th></tr></thead><tbody>
<tr><td><strong>입력 (Input)</strong></td><td>RGB 이미지 + 라이다 깊이 맵</td><td>소스 라이다 투영 이미지 + 타겟 라이다 투영 이미지</td><td>입력 데이터 모달리티가 3D-2D에서 3D-3D로 변경됨에 따라, 이를 2D-2D 문제로 변환하기 위함</td></tr>
<tr><td><strong>특징 추출기</strong></td><td>이종(Dual-branch) 인코더</td><td>동일 가중치를 공유하는 샴(Siamese) 인코더</td><td>두 입력의 모달리티가 동일하므로, 가중치 공유를 통해 일관된 특징 표현을 학습하고 파라미터 효율성을 높임</td></tr>
<tr><td><strong>대응점 추정</strong></td><td>캘리브레이션 플로우 (LiDAR → Camera)</td><td>라이다-라이다 플로우 (LiDAR_S → LiDAR_T)</td><td>문제 정의가 라이다-카메라 간의 픽셀 변위에서 라이다-라이다 간의 픽셀 변위 추정으로 변경됨</td></tr>
<tr><td><strong>포즈 추정기</strong></td><td>미분 가능한 PnP (3D-2D)</td><td>미분 가능한 3D-3D 정합 모듈 (예: ICP 변형)</td><td>대응점 관계가 3D-2D에서 3D-3D로 변경되므로, 이에 맞는 기하학적 솔버가 필요함</td></tr>
</tbody></table>
<h4>4.3.1. 동일 구조의 샴(Siamese) 특징 추출기 활용</h4>
<p>LXL-Net의 특징 추출기는 두 개의 동일한 구조를 가지며 파라미터 가중치를 공유하는 샴(Siamese) 네트워크로 구성하는 것이 효과적이다. 두 입력 데이터(소스 및 타겟 라이다의 투영 이미지)가 동일한 모달리티와 통계적 특성을 가지므로, 가중치 공유는 네트워크가 더 일반적이고 일관된 특징 표현을 학습하도록 유도하며, 동시에 학습 파라미터 수를 줄여 효율성을 높인다.</p>
<h4>4.3.2. 라이다-라이다 플로우(LiDAR-LiDAR Flow) 정의 및 손실 함수 재설계</h4>
<p>네트워크는 소스 라이다 투영 이미지 <code>$I_S$</code>에서 타겟 라이다 투영 이미지 <code>$I_T$</code>로의 플로우 <code>$\mathbf{f}_{S \to T}$</code>를 예측한다. 이 플로우를 이용해 <code>$I_S$</code>를 변형(warping)하여 <code>$I_S'$</code>를 생성할 수 있다. 만약 플로우가 정확하다면, 변형된 이미지 <code>$I_S'$</code>는 타겟 이미지 <code>$I_T$</code>와 매우 유사해야 한다.</p>
<p>이를 바탕으로, 자기 지도 학습(self-supervised learning) 방식의 손실 함수를 설계할 수 있다. 가장 대표적인 것은 광도측정 오차(photometric loss)로, 두 이미지 간의 픽셀 값 차이를 최소화하는 것이다.</p>
<p>코드 스니펫</p>
<pre><code>$$\mathcal{L}_{photo} = \sum_{p} \rho(I_T(p) - I_S'(p))$$
</code></pre>
<p>여기서 <code>$p$</code>는 픽셀 위치, <code>$\rho$</code>는 이상치(outlier)에 강건한 손실 함수(예: Charbonnier loss)이다. 추가적으로, 이미지의 그라디언트(gradient) 일관성을 유지하는 스무딩 손실(smoothing loss)을 더해 플로우가 국부적으로 부드럽게 변하도록 규제할 수 있다.</p>
<h3>3.3  예상되는 기술적 과제 및 해결 방향</h3>
<p><strong>투영 왜곡 및 정보 손실</strong>: 3D 포인트 클라우드를 2D로 투영하는 과정은 필연적으로 정보 손실과 왜곡을 수반한다. 예를 들어, 구면 투영에서는 거리가 먼 물체가 작게 표현되고, BEV에서는 수직 구조물 정보가 압축된다. 이 문제를 완화하기 위해, 거리, 강도, 높이, 법선 벡터 등 다양한 3D 정보를 다중 채널(multi-channel) 2D 이미지로 인코딩하여 네트워크에 더 풍부한 정보를 제공하는 전략이 유효하다. 또한, 구면 투영 이미지와 BEV 이미지를 모두 입력으로 사용하는 듀얼 뷰(dual-view) 아키텍처를 고려하여 각 투영 방식의 장점을 상호 보완할 수도 있다.</p>
<p><strong>Non-Overlapping FoV 문제</strong>: 두 라이다 간 중첩 영역이 전혀 없는 경우, 2D 투영 이미지 간의 직접적인 플로우 추정은 불가능하다. 이 문제는 딥러닝만으로 해결하기 어렵고, 전통적인 모션 기반 기법의 아이디어를 차용해야 한다. 즉, 차량의 움직임을 통해 시간적으로 누적된 포인트 클라우드 맵을 각 라이다에 대해 생성하고, 이 맵들 간의 정합 문제로 확장하여 해결하는 접근이 필요하다. 이 경우, 딥러닝 모델은 포인트 클라우드에서 강건한 특징점을 추출하거나, 맵 정합 과정의 초기 추정치를 제공하는 역할을 수행할 수 있다.23</p>
<h2>4.  성능 평가 및 비교 분석</h2>
<p>딥러닝 기반 캘리브레이션 모델의 성능은 표준화된 공개 벤치마크 데이터셋을 통해 객관적으로 평가되고 비교된다. 자율주행 연구 분야에서는 독일의 KITTI 데이터셋과 미국의 nuScenes 데이터셋이 가장 널리 사용된다.</p>
<h3>4.1  표준 벤치마크 데이터셋: KITTI 및 nuScenes</h3>
<p><strong>KITTI 데이터셋</strong>: 2012년에 공개된 KITTI는 자율주행 연구의 고전적인 벤치마크로, 스테레오 카메라, 64채널 Velodyne 라이다, GPS/IMU 센서 데이터를 제공한다.24 비교적 잘 정돈된 독일 카를스루에 시내와 고속도로 환경을 담고 있으며, 많은 초기 딥러닝 기반 캘리브레이션 모델들이 이 데이터셋을 사용하여 성능을 검증했다.18 그러나 상대적으로 환경의 복잡성이 낮고 객체 밀도가 적어, 최신 모델들의 일반화 성능을 평가하기에는 한계가 있다는 지적도 있다.</p>
<p><strong>nuScenes 데이터셋</strong>: 2019년에 공개된 nuScenes는 6대의 카메라, 1대의 32채널 360도 라이다, 5대의 레이더, GPS/IMU 등 훨씬 복잡하고 풍부한 센서 구성을 갖추고 있다.25 미국 보스턴과 싱가포르의 복잡한 도심 환경에서 수집된 데이터로, 다양한 날씨와 조명 조건, 높은 객체 밀도를 특징으로 한다. 따라서 nuScenes는 모델이 얼마나 다양한 실제 주행 시나리오에 강건하게 대처할 수 있는지를 평가하는 데 더 적합한 벤치마크로 여겨진다.26</p>
<h3>4.2  DXQ-Net의 라이다-카메라 캘리브레이션 성능 분석 (기존 연구 기반)</h3>
<p>DXQ-Net 원 논문에서는 KITTI 데이터셋을 사용하여 모델의 성능을 평가했다. 그 결과, 특히 회전(rotation) 오차 측면에서 당시의 다른 최신 딥러닝 기반 방법들을 능가하는 SOTA(State-of-the-Art) 성능을 달성했으며, 이동(translation) 오차 측면에서도 매우 경쟁력 있는 결과를 보였다고 보고했다.19 이는 캘리브레이션 플로우 추정과 품질 인식 메커니즘이 효과적으로 작동했음을 시사한다.</p>
<h3>4.3  타 딥러닝 기반 모델과의 정량적 성능 비교</h3>
<p>최근 발표된 “RobustCalib” 논문에서는 여러 딥러닝 기반 라이다-카메라 캘리브레이션 모델들의 성능을 KITTI와 nuScenes 데이터셋에서 공정하게 비교한 결과를 제시했다.18 이 결과를 통해 DXQ-Net과 다른 주요 모델들의 성능을 정량적으로 비교 분석할 수 있다.</p>
<p>Table 2: KITTI 데이터셋 기반 라이다-카메라 캘리브레이션 성능 비교</p>
<p>(오차는 평균 절대 오차(Mean Absolute Error) 기준)</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>Translation Error (cm)</strong></th><th><strong>Rotation Error (°)</strong></th></tr></thead><tbody>
<tr><td></td><td><strong>mean</strong></td><td>x</td></tr>
<tr><td>LCCNet 27</td><td>2.112</td><td>2.910</td></tr>
<tr><td>DXQ-Net 19</td><td>1.542</td><td>1.830</td></tr>
<tr><td>RobustCalib 18</td><td><strong>0.806</strong></td><td><strong>0.820</strong></td></tr>
</tbody></table>
<p>KITTI 데이터셋에서의 성능 비교 결과를 보면, DXQ-Net은 LCCNet에 비해 이동 및 회전 오차 모두에서 상당한 성능 향상을 보였다. 이는 플로우 기반의 반복적 정제 방식과 품질 인식 메커니즘이 유효했음을 보여준다. 한편, 가장 최신 모델인 RobustCalib은 일관성 학습(consistency learning)이라는 새로운 접근법을 통해 DXQ-Net보다도 약 50% 가까이 오차를 줄이며 가장 높은 정확도를 기록했다.18</p>
<p>Table 3: nuScenes 데이터셋 기반 라이다-카메라 캘리브레이션 성능 비교</p>
<p>(오차는 평균 절대 오차(Mean Absolute Error) 기준)</p>
<table><thead><tr><th><strong>모델</strong></th><th><strong>Translation Error (cm)</strong></th><th><strong>Rotation Error (°)</strong></th></tr></thead><tbody>
<tr><td></td><td><strong>mean</strong></td><td>x</td></tr>
<tr><td>LCCNet 27</td><td>2.218</td><td>2.720</td></tr>
<tr><td>RobustCalib 18</td><td><strong>1.600</strong></td><td><strong>2.000</strong></td></tr>
</tbody></table>
<p>nuScenes 데이터셋에서의 결과는 모델의 일반화 성능에 대한 중요한 시사점을 제공한다. RobustCalib 논문에서는 DXQ-Net의 nuScenes 결과를 제공하지 않았으나, LCCNet과 RobustCalib의 성능을 비교해 볼 수 있다. 두 모델 모두 KITTI에 비해 nuScenes에서 전반적으로 오차가 증가하는 경향을 보인다. 예를 들어, RobustCalib의 평균 이동 오차는 0.806cm에서 1.600cm로 거의 두 배 증가했다.</p>
<p>이러한 성능 저하는 단순히 데이터셋의 난이도 차이 때문만은 아니다. 이는 현재의 딥러닝 모델들이 특정 데이터셋의 센서 특성(예: KITTI의 64채널 라이다 vs. nuScenes의 32채널 라이다)과 환경적 사전 정보(prior)에 과적합(overfitting)되는 경향이 있음을 암시한다.28 KITTI의 더 조밀한 64채널 라이다 데이터로 학습된 모델은, nuScenes의 상대적으로 희소한 32채널 라이다 데이터에서 동일한 수준의 미세한 기하학적 특징을 찾기 어려울 수 있다. 이는 현재의 종단간 모델들이 아직 진정으로 센서에 구애받지 않는(sensor-agnostic) 일반화된 특징을 학습하지 못하고 있음을 보여주는 증거다. 따라서, 서로 다른 도메인 간의 성능 격차를 줄이는 것이 향후 딥러닝 기반 캘리브레이션 연구의 핵심 과제임을 알 수 있다.</p>
<h2>5.  결론 및 향후 연구 방향</h2>
<h3>5.1  DXQ-Net의 핵심 기여와 다중 라이다 캘리브레이션에 대한 시사점 요약</h3>
<p>본 보고서는 DXQ-Net 모델의 원리를 심층적으로 분석하고, 이를 다중 라이다 캘리브레이션 문제에 적용하기 위한 방안을 탐구했다. DXQ-Net은 라이다-카메라 캘리브레이션 문제를 광학 흐름 추정의 관점에서 재해석하여, 조밀한 대응점 기반의 새로운 패러다임을 성공적으로 제시했다. 특히, 상관관계 볼륨을 활용한 반복적 플로우 정제, 품질 인식(불확실성 예측)을 통한 강건성 확보, 그리고 미분 가능한 PnP 솔버를 통한 종단간 학습의 구현은 이 분야의 기술적 진보에 크게 기여한 부분이다.19</p>
<p>비록 DXQ-Net이 본래 라이다-카메라용으로 설계되었지만, 그 핵심 아이디어인 ’3D 데이터를 2D로 투영한 후, 이미지 간의 플로우를 추정하여 기하학적 불일치를 보상한다’는 원리는 동종 센서인 다중 라이다 캘리브레이션 문제에도 충분히 적용 가능한 강력한 잠재력을 가지고 있음을 확인했다. 이를 위해 포인트 클라우드를 구면 투영이나 BEV와 같은 2D 이미지로 표현하고, 샴 네트워크 구조와 3D-3D 정합에 맞는 손실 함수를 설계하는 구체적인 수정 방안을 제안했다.</p>
<h3>5.2  플로우 기반 딥러닝 모델의 발전 가능성 및 연구 방향 제언</h3>
<p>DXQ-Net과 같은 플로우 기반 캘리브레이션 모델은 향후 다음과 같은 방향으로 더욱 발전할 수 있을 것이다.</p>
<p><strong>자기 지도 학습(Self-supervised Learning)으로의 전환</strong>: 현재 대부분의 고성능 딥러닝 캘리브레이션 모델은 정확한 정답 외부 파라미터가 레이블링된 대규모 데이터셋을 필요로 한다. 이러한 데이터셋을 구축하는 것은 막대한 비용과 시간이 소요된다. 향후 연구는 레이블 없이, 센서 데이터 자체의 일관성(consistency)만을 이용해 학습하는 자기 지도 학습 방식으로 나아가야 한다.29 예를 들어, 제안된 LXL-Net 구조에서 예측된 플로우로 변형시킨 소스 라이다 이미지와 타겟 라이다 이미지 간의 광도측정 오차나 특징 일관성 손실을 최소화하는 방식으로 네트워크를 훈련시킬 수 있다. 이는 데이터 수집의 제약을 극복하고 모델의 확장성을 크게 높일 것이다.</p>
<p><strong>다중 모달리티 투영 및 특징 융합</strong>: 3D 포인트 클라우드를 단일 채널(예: 거리) 이미지로 투영하는 것은 상당한 정보 손실을 야기한다. 거리, 반사 강도, 높이, 법선 벡터 등 다양한 기하학적, 물리적 정보를 다중 채널 2D 이미지로 인코딩하여 네트워크가 더 풍부하고 식별력 있는 특징을 학습하도록 유도하는 연구가 필요하다. 더 나아가, 구면 투영과 BEV 투영의 장점을 모두 활용하기 위해 두 가지 표현을 동시에 처리하고 그 특징을 융합하는 멀티뷰(multi-view) 아키텍처를 개발하는 것도 유망한 연구 방향이다.</p>
<p><strong>도메인 일반화 성능 향상</strong>: 현재 모델들이 KITTI, nuScenes 등 특정 벤치마크 데이터셋에 과적합되는 경향은 실용화를 가로막는 가장 큰 장애물이다.18 실제 상용차에는 매우 다양한 종류와 사양의 라이다 센서가 탑재될 것이다. 따라서 특정 센서나 환경에 국한되지 않고, 보지 못한 종류의 라이다와 주행 환경에서도 안정적인 성능을 내는 일반화 능력을 확보하는 것이 매우 중요하다. 이를 위해 도메인 적응(Domain Adaptation) 및 도메인 일반화(Domain Generalization)와 같은 기계 학습 기법들을 캘리브레이션 네트워크에 적극적으로 도입하여, 학습 데이터와 실제 적용 환경 간의 간극을 줄이려는 노력이 시급하다.</p>
<h2>6. 참고 자료</h2>
<ol>
<li>LiDAR’s Role in Autonomous Driving: Laser Vision for Cars - NI - National Instruments, https://www.ni.com/en/solutions/transportation/adas-and-autonomous-driving-testing/adas-and-autonomous-driving-sensors-and-electronics-test/lidar-role-autonomous-driving-laser-vision-cars.html</li>
<li>LiDAR technology in autonomous driving - State, future, simulation - Anyverse, https://anyverse.ai/lidar-technology-autonomous-driving-state-future-simulation/</li>
<li>Research on Application of LIDAR in Auto Driving: A Review | Applied and Computational Engineering, https://www.ewadirect.com/proceedings/ace/article/view/21646</li>
<li>How Lidar Technology Helps Build Next-Gen Autonomous Vehicles - rinf.tech, https://www.rinf.tech/how-lidar-technology-helps-build-next-gen-autonomous-vehicles/</li>
<li>Extrinsic calibration for multi-LiDAR systems involving heterogeneous laser scanning models - Optica Publishing Group, https://opg.optica.org/abstract.cfm?uri=oe-31-26-44754</li>
<li>Multi-LiCa: A Motion- and Targetless Multi - LiDAR-to-LiDAR Calibration Framework - arXiv, https://arxiv.org/html/2501.11088v1</li>
<li>Multi-LiDAR placement, calibration, and co-registration for off-road autonomous vehicle operation - Scholars Junction - Mississippi State University, https://scholarsjunction.msstate.edu/cgi/viewcontent.cgi?article=4229&amp;context=td</li>
<li>Multi-Lidar Calibration - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/lidar/ug/multi-lidar-calibration-workflow.html</li>
<li>A Novel Dual-Lidar Calibration Algorithm Using Planar Surfaces - Jianhao Jiao, https://gogojjh.github.io/assets/pdf/jiao2019novel.pdf</li>
<li>A Multi-LiDAR Self-Calibration System Based on Natural Environments and Motion Constraints - MDPI, https://www.mdpi.com/2227-7390/13/19/3181</li>
<li>Extrinsic Calibration of Multiple 3D LiDAR Sensors by the Use of Planar Objects - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9571506/</li>
<li>What Really Matters for Learning-based LiDAR-Camera Calibration - arXiv, https://arxiv.org/html/2501.16969v1</li>
<li>What Is Lidar-Camera Calibration? - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/lidar/ug/lidar-camera-calibration.html</li>
<li>Automatic Extrinsic Calibration Method for LiDAR and Camera Sensor Setups, https://www.researchgate.net/publication/359181926_Automatic_Extrinsic_Calibration_Method_for_LiDAR_and_Camera_Sensor_Setups</li>
<li>TUMFTM/Multi_LiCa: Multi - LiDAR-to-LiDAR calibration framework for ROS2 and non-ROS applications - GitHub, https://github.com/TUMFTM/Multi_LiCa</li>
<li>A Review of Deep Learning-Based LiDAR and Camera Extrinsic …, https://pmc.ncbi.nlm.nih.gov/articles/PMC11207430/</li>
<li>Improvement on LiDAR-Camera Calibration Using Square Targets - arXiv, https://arxiv.org/html/2506.18294v1</li>
<li>RobustCalib: Robust Lidar-Camera Extrinsic Calibration with Consistency Learning - arXiv, https://arxiv.org/html/2312.01085v1</li>
<li>[2203.09385] DXQ-Net: Differentiable LiDAR-Camera Extrinsic Calibration Using Quality-aware Flow - arXiv, https://arxiv.org/abs/2203.09385</li>
<li>CFNet: LiDAR-Camera Registration Using Calibration Flow Network - MDPI, https://www.mdpi.com/1424-8220/21/23/8112</li>
<li>CFNet: LiDAR-Camera Registration Using Calibration Flow Network - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC8662422/</li>
<li>A LiDAR-Camera Joint Calibration Algorithm Based on Deep Learning - MDPI, https://www.mdpi.com/1424-8220/24/18/6033</li>
<li>DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA, https://arxiv.org/html/2507.09176v1</li>
<li>The KITTI Vision Benchmark Suite - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/</li>
<li>Scene planning - nuScenes, https://www.nuscenes.org/nuscenes</li>
<li>nuScenes tracking task, https://www.nuscenes.org/tracking</li>
<li>LCCNet: LiDAR and Camera Self-Calibration using Cost Volume Network - ResearchGate, https://www.researchgate.net/publication/354304016_LCCNet_LiDAR_and_Camera_Self-Calibration_using_Cost_Volume_Network</li>
<li>Nuscenes trained models performs poorly but kitti pretrained relaesed models generallize well on custom pointclouds · Issue #221 · open-mmlab/OpenPCDet - GitHub, https://github.com/open-mmlab/OpenPCDet/issues/221</li>
<li>Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural Calibration - arXiv, https://arxiv.org/abs/2401.12452</li>
<li>Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural Calibration - arXiv, https://arxiv.org/html/2401.12452v4</li>
<li>Deep Learning-Based Online Automatic Targetless LiDAR–Camera Calibration with Iterative and Attention-Driven Post - arXiv, <a href="https://arxiv.org/pdf/2502.17648">https://arxiv.org/pdf/2502.17648?</a></li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>