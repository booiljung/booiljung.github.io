<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:포인트 클라우드 고속 스트리밍 기술</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>포인트 클라우드 고속 스트리밍 기술</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">센서 (Sensors)</a> / <a href="index.html">포인트 클라우드</a> / <span>포인트 클라우드 고속 스트리밍 기술</span></nav>
                </div>
            </header>
            <article>
                <h1>포인트 클라우드 고속 스트리밍 기술</h1>
<p>2025-11-11, G25DR</p>
<h2>1.  서론: 3D 데이터 스트리밍의 패러다임 전환</h2>
<h3>1.1  포인트 클라우드의 정의 및 데이터 특성</h3>
<p>포인트 클라우드(Point Cloud)는 3차원 공간을 기술하는 핵심적인 데이터 표현 방식이다. 이는 3차원 좌표계(X, Y, Z) 내의 수많은 점(point)의 집합으로 정의되며, 각 점은 기하학적 위치 외에도 색상(RGB), 반사 강도(Intensity) 등 다양한 속성(attribute)을 가질 수 있다.1 이러한 데이터는 주로 LiDAR(Light Detection and Ranging) 2, 사진측량(Photogrammetry) 4, 또는 구조광 스캐닝(Structured Light Scanning) 4과 같은 3D 스캐닝 기술을 통해 수집된다.5</p>
<p>포인트 클라우드의 본질적 특성은 2D 이미지 데이터와 근본적인 차이를 보인다. 2D 이미지가 행렬(matrix)과 같은 정형화된 그리드(grid) 구조를 가지는 반면 7, 포인트 클라우드는 다음과 같은 고유한 특성을 지닌다.</p>
<ol>
<li><strong>대규모성 (Large-scale):</strong> 현실 세계의 객체나 환경을 정밀하게 표현하기 위해 수백만 개에서 수십억 개의 점으로 구성되어, 막대한 원본 데이터 크기 및 저장 공간을 요구한다.1</li>
<li><strong>비정형성 (Unstructured):</strong> 데이터 포인트 간의 위상적 연결성(connectivity) 정보가 명시적으로 부재하며, 점들의 밀도가 공간적으로 불규칙하다.1</li>
<li><strong>비순차성 (Unordered):</strong> 데이터셋 내 점들의 순서(ordering)가 특정 의미를 갖지 않으며, 동일한 씬을 표현하는 데이터라도 순서가 다를 수 있다. 따라서 처리는 순서에 무관(permutation-invariant)해야 한다.1</li>
</ol>
<p>이러한 데이터의 ’비정형성’과 ‘비순차성’ 1은 고속 스트리밍 기술의 모든 난제를 야기하는 근본 원인이다. 2D 비디오는 픽셀 간의 공간적 인접성이 명확한 ‘그리드’ 구조 7를 가지므로, 인접 블록 또는 이전 프레임을 기반으로 한 고효율의 예측 압축(예: HEVC)이 가능하다. 반면 포인트 클라우드는 이러한 예측 가능한 구조가 부재한다.1 데이터 목록의 5번째 점과 6번째 점은 공간적으로 아무 관계가 없을 수 있다.</p>
<p>결과적으로, 기존 2D 비디오 압축 기술을 포인트 클라우드에 직접 적용하는 것은 원천적으로 불가능하다. 포인트 클라우드 압축(PCC)의 모든 접근 방식은 본질적으로 이 ‘비정형’ 데이터에 ’정형성’을 인위적으로 부여하는 과정이다. MPEG의 G-PCC는 옥트리(Octree)라는 계층적 3D 그리드를 생성하며 11, V-PCC는 3D 데이터를 2D 평면에 투영(projection)하여 2D 그리드를 생성한다.11 이 구조화 작업이 압축의 핵심 전제 조건이 된다.</p>
<h3>1.2  고속 스트리밍의 필요성: 정적 보관에서 동적 상호작용으로</h3>
<p>초기 포인트 클라우드 기술의 활용은 주로 정적(static) 데이터의 오프라인(offline) 분석에 중점을 두었다. 이는 주로 고정밀 측량 12, 자산의 디지털 기록 보존 및 문서화 5, 인프라 유지보수 14 등 비실시간 분석 작업에 해당했다.</p>
<p>그러나 기술의 발전은 ’데이터-앳-레스트(data-at-rest)’에서 ’데이터-인-모션(data-in-motion)’으로의 패러다임 전환을 강력하게 요구하고 있다. 대규모 포인트 클라우드를 로컬에 다운로드하여 처리하는 방식은 더 이상 실시간 동적 애플리케이션의 요구사항을 충족할 수 없다. 고속 스트리밍은 이러한 데이터를 실시간으로 전송하고 소비하기 위한 필수 기술로 부상했으며, 다음과 같은 핵심 응용 분야가 이를 견인하고 있다.</p>
<ul>
<li><strong>자율주행 (Autonomous Driving):</strong> 차량에 탑재된 LiDAR 센서 15가 실시간으로 생성하는 동적 포인트 클라우드를 즉각적으로 처리하거나, V2X(Vehicle-to-Everything) 통신을 통해 엣지 서버 또는 다른 차량과 공유해야 한다.16 이는 장애물 감지 및 경로 계획에 치명적이다.19</li>
<li><strong>디지털 트윈 (Digital Twin) 및 스마트 시티:</strong> 물리적 자산이나 도시 전체의 가상 복제본(Digital Twin) 14을 구축하고, 센서 데이터를 통해 실시간으로 상태를 모니터링하며 시뮬레이션한다.12</li>
<li><strong>메타버스 및 몰입형 미디어 (VR/AR):</strong> 사용자가 6자유도(6-DoF, Six Degrees of Freedom) 22를 가지며 자유롭게 탐색할 수 있는 포토 리얼리스틱한 가상 환경 24을 구현하기 위해, 동적인 3D 비디오나 포인트 클라우드 시퀀스의 실시간 전송이 필수적이다.16</li>
</ul>
<p>이러한 스트리밍의 필요성은 상기 언급된 ’구조화-압축-전송-디코딩-렌더링’의 전체 파이프라인이 실시간(real-time) 18으로, 특히 VR/AR 환경에서는 사용자의 멀미를 방지하기 위해 10밀리초(ms) 미만의 극도로 짧은 지연 시간 26 내에 완료되어야 함을 의미한다.</p>
<h2>2.  스트리밍의 핵심 난제: 대역폭, 지연 시간, 및 렌더링 부하</h2>
<p>포인트 클라우드 데이터를 고속으로 스트리밍하는 것은 세 가지 핵심적인 기술적 난제(Bandwidth, Latency, Computation)에 직면한다.</p>
<h3>2.1  대역폭 한계 (Bandwidth Constraint)</h3>
<p>포토 리얼리스틱한 품질의 포인트 클라우드는 프레임당 막대한 데이터 볼륨을 생성한다.27 이는 “bulky and severely bandwidth intensive“28로 묘사되며, 현재의 네트워크 인프라, 특히 “bandwidth-constrained networks“18로 규정되는 모바일 또는 무선 환경에서는 심각한 병목 현상을 야기한다.28 수십억 개의 점으로 구성된 데이터를 압축 없이 전송하는 것은 사실상 불가능하다.</p>
<h3>2.2  지연 시간 요구사항 (Latency Requirement)</h3>
<p>대부분의 핵심 응용 분야는 “low latency“18 또는 “strict latency“23를 요구한다. 자율주행 18이나 원격 수술과 같은 애플리케이션에서는 지연이 치명적인 결과를 초래할 수 있다.</p>
<p>특히 VR/AR과 같은 몰입형 통신 23에서는 사용자의 머리 움직임이 실제 화면에 반영되기까지의 시간, 즉 “Motion-to-Photon (MTP) latency“가 사용자 경험을 좌우한다. MTP 지연 시간이 10ms를 초과할 경우, 사용자는 시각적 불편함(visual discomfort)이나 멀미를 경험하게 된다.26 이는 스트리밍 파이프라인 전체가 극도로 엄격한 시간 제약하에 동작해야 함을 의미한다.</p>
<h3>2.3  클라이언트 연산 부하 (Computational Load)</h3>
<p>데이터의 최종 소비자인 클라이언트는 종종 “resource-constrained devices“18, 즉 모바일 기기, AR/VR 헤드셋, 또는 차량 내 임베디드 시스템이다. 이러한 기기들은 배터리, 전력, 발열에 민감하며 연산 능력에 한계가 있다.</p>
<p>대용량 포인트 클라우드 스트림을 수신하여 실시간으로 디코딩하고, 수십억 개의 포인트를 3D 렌더링하는 것은 “heavy computation“26을 유발한다. 이는 클라이언트의 리소스를 고갈시켜 사실상 실시간 상호작용을 불가능하게 만들 수 있다.</p>
<h3>2.4  프로토콜의 한계 (Protocol Limitations)</h3>
<p>전통적인 비디오 스트리밍 표준인 DASH(Dynamic Adaptive Streaming over Http) 29는 대부분 HTTP/TCP를 기반으로 한다.30 TCP는 데이터 전송의 신뢰성(reliability)을 보장하기 위해 설계되었으며, 패킷 손실이 발생하면 해당 세그먼트를 재전송(retransmission)한다.30</p>
<p>그러나 실시간 스트리밍 31에서는 ’데이터의 완전성(Completeness)’보다 ’데이터의 적시성(Timeliness)’이 압도적으로 중요하다. TCP의 재전송 메커니즘은 과거의 데이터(손실된 패킷)를 복구하기 위해 현재 데이터의 전송을 지연시키며, 이는 전체 “segment retrieval latency“를 증가시켜 “playback stalls”(재생 멈춤)를 유발한다.30</p>
<p>특히 옥트리(Octree)와 같이 계층적으로 구조화된 데이터 30에서 일부 패킷 손실이 전체 프레임 재구성을 지연시킬 수 있으므로, TCP는 실시간 포인트 클라우드 스트리밍에 근본적으로 “ill-suited”(부적합)하다.30 이러한 한계는 신뢰성을 보장하지 않는 대신 저지연성을 확보하는 UDP 기반 프로토콜로의 전환을 강제한다. 이것이 WebRTC (RTP/UDP 사용) 32 또는 HTTP/3의 QUIC 33이 차세대 스트리밍 프로토콜로 주목받는 이유이며, 이들은 TCP의 재전송 지연 문제없이 패킷 손실에 훨씬 탄력적으로 대응할 수 있다.33</p>
<h2>3.  핵심 기술 1: 포인트 클라우드 압축(PCC) 표준</h2>
<p>대역폭 한계를 극복하기 위한 첫 번째 단계는 효율적인 데이터 압축이다. MPEG(Moving Picture Experts Group)는 상호운용성을 보장하기 위해 두 가지 핵심적인 포인트 클라우드 압축(PCC) 표준을 개발했다.</p>
<h3>3.1  MPEG G-PCC (ISO/IEC 23090-9) 기술 심층 분석</h3>
<p>G-PCC는 “Geometry-based Point Cloud Compression”(기하 기반 포인트 클라우드 압축)의 약자이다.34 이 표준의 핵심 철학은 3D 데이터를 2D로 변환하지 않고, 3D 공간 자체에서 직접 압축하는 것이다.11</p>
<ul>
<li><strong>핵심 기술 (옥트리):</strong> G-PCC는 3D 공간을 복셀화(voxelization) 19한 후, 이를 옥트리(Octree)라는 8진 트리 데이터 구조를 사용하여 계층적으로 분할한다.11 옥트리는 데이터가 점유(occupancy)된 공간만 효율적으로 표현하고 희소(sparse)한 빈 공간은 무시함으로써 압축의 기반을 마련한다.11</li>
<li><strong>압축 메커니즘:</strong> 옥트리 구조 내에서 인접 노드 간의 예측(OctreePrediction) 11 또는 RAHT(Region-Adaptive Hierarchical Transform) 38와 같은 고급 변환 기법을 사용하여 기하 정보(geometry)와 속성(attribute)을 효율적으로 인코딩한다.</li>
<li><strong>표준화 및 전송:</strong> G-PCC는 ISO/IEC 23090-9 표준으로 명시되어 있으며 39, TMC13(Test Model 13)이라는 참조 소프트웨어 모델 11을 기반으로 한다. 압축된 비트스트림은 ISO/IEC 23090-18 표준 42에 따라 ISOBMFF(ISO-based Media File Format) 42로 캡슐화되어 전송될 수 있다.</li>
<li><strong>성능:</strong> G-PCC는 무손실 압축에서 최대 10:1, 손실 압축에서 최대 35:1의 압축률을 제공할 수 있다.43</li>
<li><strong>주요 적용 분야:</strong> 이 기술은 데이터가 “희소(sparse) 또는 불규칙(irregular)” 11할 때 가장 효율적이다.</li>
<li><strong>자율주행 LiDAR:</strong> LiDAR 센서 데이터는 본질적으로 희소하며 비정형적이다.11 G-PCC는 이러한 희소 시퀀스를 효율적으로 압축하여 차량 내 데이터 흐름을 개선한다.15</li>
<li><strong>정적 스캔 및 매핑:</strong> 문화유산 아카이빙 13이나 UAV/모바일 매핑 6을 통해 수집된 대규모 정적 데이터 압축에 적합하다.</li>
</ul>
<h3>3.2  MPEG V-PCC (ISO/IEC 23090-5) 기술 심층 분석</h3>
<p>V-PCC는 “Video-based Point Cloud Compression”(비디오 기반 포인트 클라우드 압축)의 약자이다.19 이 표준의 철학은 G-PCC와 정반대이다. 3D 데이터를 직접 압축하는 대신, 이를 2D 이미지의 시퀀스로 변환(투영)한 후, 이미 성숙된 2D 비디오 코덱을 재활용하여 압축한다.11</p>
<ul>
<li><strong>핵심 기술 (프로젝션):</strong></li>
</ul>
<ol>
<li><strong>패치 생성 (Patch Generation):</strong> 3D 포인트 클라우드를 여러 각도에서 2D 평면으로 투영하여 작은 2D 표면 조각(patch)들로 분할한다.11</li>
<li><strong>아틀라스 패킹 (Atlas Packing):</strong> 생성된 2D 패치들을 비디오 프레임(아틀라스) 내에 빈 공간이 최소화되도록 효율적으로 채워 넣는다.11</li>
<li><strong>비디오 인코딩:</strong> 이렇게 생성된 2D 비디오 시퀀스(기하 정보 맵, 색상 정보 맵)를 HEVC(High Efficiency Video Coding)와 같은 표준 비디오 코덱으로 압축한다.11</li>
</ol>
<ul>
<li><strong>표준화 및 장점:</strong> V-PCC는 ISO/IEC 23090-5 표준 44으로 명시되며, TMC2(Test Model 2) 19를 기반으로 한다. 가장 큰 장점은 기존 비디오 인코더/디코더 하드웨어(예: GPU, 모바일 AP)를 수정 없이 재사용할 수 있다는 점이다.11 이는 “Fast market deployability”(빠른 시장 배포) 19를 가능하게 한다.</li>
<li><strong>주요 적용 분야:</strong> 이 기술은 데이터가 “조밀한(dense) 동적(dynamic)” 11 특성을 가질 때 최적의 성능을 보인다.</li>
<li><strong>VR/AR 및 텔레프레즌스:</strong> 조밀한 사람(human)이나 객체(object) 11의 동적 캡처 데이터 38를 실시간 전송하는 데 탁월한 효율을 보인다.19</li>
<li><strong>몰입형 3D 비디오:</strong> 6-DoF를 지원하는 3D 비디오 콘텐츠 19 스트리밍에 사용된다.</li>
</ul>
<h3>3.3  G-PCC 대 V-PCC: 핵심 비교 및 활용 사례</h3>
<p>MPEG는 표준화 초기부터 “dense/projectable” 콘텐츠(V-PCC용)와 “sparse/irregular” 콘텐츠(G-PCC용)라는 “basic split”(기본적 구분) 11을 명확히 인지하고 두 개의 표준을 병행 개발했다. 두 기술은 상호 대체 관계가 아닌, 상호 보완적 관계에 있다.</p>
<p>성능 비교 연구에 따르면, V-PCC는 조밀한 객체 38와 동적 데이터 37 압축에서 현재의 SOTA(State-of-the-Art) 성능을 보이며 G-PCC를 능가한다. 반면, 희소 데이터셋 38에서는 V-PCC가 2D 투영 시 “empty pixels”(빈 픽셀) 11로 인한 심각한 대역폭 낭비를 초래하므로, 3D 공간에서 직접 처리하는 G-PCC가 훨씬 우수한 성능을 보인다.</p>
<p>따라서 아키텍트는 자신의 애플리케이션이 다루는 데이터의 특성(조밀성 vs. 희소성)에 따라 적합한 표준을 선택해야 한다.</p>
<p><strong>표 1: G-PCC (ISO/IEC 23090-9) vs. V-PCC (ISO/IEC 23090-5) 기술 비교</strong></p>
<table><thead><tr><th><strong>특성</strong></th><th><strong>G-PCC (Geometry-based)</strong></th><th><strong>V-PCC (Video-based)</strong></th></tr></thead><tbody>
<tr><td><strong>표준 번호</strong></td><td>ISO/IEC 23090-9 36</td><td>ISO/IEC 23090-5 19</td></tr>
<tr><td><strong>핵심 원리</strong></td><td>3D 공간에서 직접 압축 11</td><td>3D를 2D 평면으로 투영 후 압축 11</td></tr>
<tr><td><strong>데이터 구조</strong></td><td>옥트리(Octree) 11</td><td>2D 패치(Patches), 아틀라스(Atlas) 11</td></tr>
<tr><td><strong>기반 코덱</strong></td><td>3D 고유 알고리즘 (산술 부호화) 19</td><td>기존 2D 비디오 코덱 (HEVC 등) 19</td></tr>
<tr><td><strong>하드웨어 의존성</strong></td><td>전용 디코더 필요 (느린 배포) 19</td><td>기존 비디오 하드웨어(GPU) 가속 가능 11</td></tr>
<tr><td><strong>최적 데이터 유형</strong></td><td>희소(Sparse), 불규칙(Irregular) 11</td><td>조밀(Dense), 동적(Dynamic) 11</td></tr>
<tr><td><strong>비효율적 데이터</strong></td><td>조밀하고 연속적인 표면 (V-PCC 대비)</td><td>희소한 데이터 (Empty pixels 낭비) 11</td></tr>
<tr><td><strong>주요 활용 사례</strong></td><td>자율주행 LiDAR 41, 모바일 매핑 6</td><td>VR/AR 19, 텔레프레즌스 41</td></tr>
</tbody></table>
<h2>4.  핵심 기술 2: 차세대 압축: 딥러닝 기반 코덱</h2>
<h3>4.1  AI 기반 압축의 부상과 필요성</h3>
<p>G-PCC와 V-PCC는 “handcrafted”(수동 설계) 47 방식, 즉 옥트리나 2D 투영이라는 정해진 규칙에 기반한다. 이러한 방식은 예측 가능한 성능을 제공하지만, 데이터의 복잡한 3D 공간적 상관관계(spatial correlations)를 완벽하게 모델링하는 데 한계가 있다.47</p>
<p>반면, 딥러닝(Deep Learning) 48은 대규모 데이터셋으로부터 3D 구조의 복잡한 패턴과 잠재 특징(latent features)을 직접 학습 49할 수 있다. 오토인코더(Autoencoder) 50나 3D 컨볼루션 신경망(3D CNN) 52과 같은 딥러닝 모델은 기존 표준 코덱의 압축 효율을 능가하는 새로운 잠재력을 제공한다.22</p>
<h3>4.2  G-PCC 대비 딥러닝 모델의 성능 벤치마크</h3>
<p>다수의 최근 연구 52는 딥러닝 기반 압축 모델이 G-PCC의 성능을 <em>상당히</em> 능가함을 정량적으로 입증했다.</p>
<ul>
<li><strong>정량적 성과:</strong></li>
<li>Voxel-based(복셀 기반) 딥러닝 압축 프레임워크인 “PCGCv2” 52는 G-PCC 대비 70%의 BD-Rate 이득(동일 품질 대비 비트레이트 절감)을 달성했다.52</li>
<li>한 연구에서는 G-PCC v23 대비 평균 46.64%의 비트레이트 감소를 달성했다고 보고했다.54</li>
<li>LiDAR 데이터셋(SemanticKITTI)을 대상으로 한 연구에서도 G-PCC 대비 37.95%, 조밀한 객체 데이터셋(MPEG 8i)에서는 48.98%의 비트레이트 감소를 달성했다.56</li>
</ul>
<p>이러한 모델들은 어텐션 메커니즘(Attention) 54, 희소 컨볼루션(Sparse Convolutions) 50 등 최신 AI 기술을 활용하여 점유 정보(occupancy)나 기하 정보(geometry)를 효율적으로 학습한다.</p>
<h3>4.3  실시간 스트리밍을 위한 난제: 연산 복잡도</h3>
<p>딥러닝 모델의 압도적인 압축률에도 불구하고, 실시간 스트리밍 적용에는 치명적인 장벽이 존재한다. 바로 “additional computational complexity”(추가적인 연산 복잡도) 55이다. 딥러닝 모델, 특히 3D CNN은 “computationally expensive”(연산 비용이 매우 높다).50</p>
<p>한 벤치마크에 따르면, 딥러닝 모델은 GPU 가속 시 G-PCC(CPU 실행)보다 빠를 수 있으나, 동일한 CPU 환경에서는 G-PCC보다 디코딩 속도가 37.1배 느릴 수 있다.55 이러한 과도한 디코딩 시간은 MTP 10ms 26 같은 실시간 요구사항을 충족할 수 없게 만든다.</p>
<p>이 문제를 해결하기 위해 다음과 같은 경량화 연구가 진행 중이다.</p>
<ol>
<li><strong>희소 컨볼루션 (Sparse Convolutions):</strong> 3D CNN의 복잡도는 복셀 그리드 크기에 세제곱으로 증가한다.52 이는 고해상도 처리를 불가능하게 만든다. 희소 컨볼루션은 데이터가 <em>점유된</em> 복셀에서만 연산을 수행하여 50, 연산량을 획기적으로 줄이면서 G-PCC보다 우수한 성능을 달성한다.52</li>
<li><strong>경량 모델:</strong> 53의 RENO 모델은 1MB 크기의 “lightweight model“을 제안하며 G-PCC보다 우수한 성능과 “real-time manner“의 압축을 달성했다고 주장한다.</li>
<li><strong>하이브리드 접근 (V-PCC + AI):</strong> V-PCC의 복잡한 2D 프로젝션 단계(예: CU 분할)에 AI(CNN)를 적용하여 최적화함으로써, 전체 인코딩 시간을 54.75% 단축하는 하이브리드 접근법도 존재한다.58</li>
</ol>
<h3>4.4  표준화 동향 및 패러다임의 분기</h3>
<p>MPEG는 이러한 딥러닝의 성과를 공식적으로 인지하고 있다. 2024년 4월, MPEG는 G-PCC/V-PCC의 한계를 넘어 “AI-based Point Cloud Coding”(AI 기반 포인트 클라우드 코딩) 59에 대한 공식적인 제안 요청(Call for Proposals)을 발표했다. 이는 차세대 표준이 딥러닝 기반이 될 것임을 강력히 시사한다.59</p>
<p><strong>표 2: 대표적 압축 기술 성능 비교 (G-PCC v.s. Deep Learning)</strong></p>
<table><thead><tr><th><strong>비교 대상</strong></th><th><strong>기준 코덱</strong></th><th><strong>방법론</strong></th><th><strong>비트레이트 절감 (BD-Rate)</strong></th><th><strong>핵심 한계</strong></th></tr></thead><tbody>
<tr><td>PCGCv2 52</td><td>G-PCC</td><td>Voxel-based (Sparse Conv.)</td><td>70% 이득 52</td><td>높은 연산 복잡도 55</td></tr>
<tr><td>54 연구</td><td>G-PCC v23</td><td>Cross-Attention</td><td>-46.64% 54</td><td>54</td></tr>
<tr><td>53 연구 (RENO)</td><td>G-PCC, Draco</td><td>Neural Model (1MB)</td><td>G-PCC/Draco 능가 53</td><td>53</td></tr>
<tr><td>56 연구</td><td>G-PCC</td><td>Hybrid Context (Octree-based)</td><td>-37.95% (LiDAR) / -48.98% (Dense)</td><td>-</td></tr>
<tr><td>58 연구</td><td>V-PCC (TMC2)</td><td>V-PCC + CNN (CU 분할)</td><td>인코딩 시간 -54.75% 58</td><td>BD-Rate 약간 증가 58</td></tr>
</tbody></table>
<p>여기서 우리는 압축의 목표가 ’인간의 시각(Human Vision)’인지 ’기계의 분석(Machine Analysis)’인지에 따라 기술이 분기되는 중요한 지점을 목격한다.</p>
<p>대부분의 딥러닝 압축 연구 52는 G-PCC와 마찬가지로 ‘인간의 시각적 품질’(PSNR 54) 또는 ‘기하학적 정확도’(BD-Rate 52)를 극대화하는 것을 목표로 한다. 이는 VR/AR, 메타버스 22에 적합하다.</p>
<p>그러나 포인트 클라우드의 가장 큰 시장 중 하나인 자율주행 15에서, 데이터의 최종 소비자는 인간이 아닌 ‘기계’(객체 탐지 AI)이다.6161와 62의 연구는 충격적인 결과를 제시한다. “low-to-medium levels of compression do not have a major impact on object detection performance.” 즉, 기계는 중간 수준의 압축 손실에 인간의 눈만큼 민감하지 않으며, 기하학적 ’정확성’보다 ‘의미론적(semantic)’ 구조(이것이 자동차 형상인지)를 더 중요하게 여길 수 있다.</p>
<p>이는 ’인간 시각’에 맞춰진 PSNR이나 BD-Rate 같은 벤치마크가 ’기계 분석’용 압축에는 *틀린 지표(wrong metric)*일 수 있음을 시사한다. 따라서 미래의 AI 압축은 두 갈래로 진화할 것이다.</p>
<ol>
<li><strong>HVC (Human Visual Coding):</strong> VR/AR을 위한 시각 품질(PSNR) 최적화 코덱.22</li>
<li><strong>MVC (Machine Vision Coding):</strong> 자율주행/로보틱스를 위한 기계 태스크(예: 객체 탐지 F1-score) 최적화 코덱.63</li>
</ol>
<h2>5.  핵심 기술 3: 스트리밍 아키텍처</h2>
<p>압축된 데이터를 효율적으로 전송하기 위해서는 지능적인 스트리밍 아키텍처가 필수적이다.</p>
<h3>5.1  적응형 스트리밍 (Adaptive Streaming) 전략</h3>
<p>제한된 대역폭 28과 가변적인 네트워크 환경 30 하에서 전체 고해상도 데이터를 전송하는 것은 불가능하다. 따라서 사용자 및 환경에 맞춰 전송 데이터를 동적으로 조절하는 적응형 스트리밍 전략이 요구된다.18</p>
<ul>
<li><strong>타일링 (Tiling):</strong> 전체 포인트 클라우드 씬을 3D 공간 64 또는 2D 프로젝션 27 기반의 작은 “타일(tiles)” 또는 “셀(cells)“로 분할한다. 이는 적응형 스트리밍의 기본 전제 조건이다.</li>
<li><strong>시점 예측 (Viewport Prediction):</strong> 사용자의 머리 움직임(head motion) 65이나 시선(gaze) 67을 실시간으로 추적하고 예측한다. 서버는 이 예측을 기반으로 사용자가 <em>즉시 볼 것으로 예상되는</em> 타일만 고화질로 우선 전송한다.27</li>
<li><strong>콘텐츠 인지 (Content-Awareness):</strong> 시맨틱 분할(semantic segmentation) 딥러닝 18을 사용하여, 씬에 포함된 객체(예: 자동차, 보행자)의 중요도를 실시간으로 파악한다. 시스템은 사용자의 안전이나 상호작용에 더 중요한 객체에 더 많은 비트레이트를 할당한다.18</li>
</ul>
<h3>5.2  엣지 컴퓨팅 아키텍처 (MEC)</h3>
<p>초저지연(low latency) 요구사항을 충족하기 위해, 중앙 집중식 클라우드 아키텍처는 한계가 명확하다. 이에 대한 해결책이 엣지 컴퓨팅이다.</p>
<p>MEC(Multi-access Edge Computing) 69는 중앙 클라우드 서버 72가 아닌, 사용자와 물리적으로 가까운 네트워크의 “엣지”(Edge) 73, 예를 들어 5G 기지국 70이나 도로변 장치(RSU) 17에 컴퓨팅 및 저장 자원을 배치하는 분산 아키텍처이다.73</p>
<p>포인트 클라우드 스트리밍에서 MEC는 다음 세 가지 핵심 역할을 수행한다.</p>
<ol>
<li><strong>지연 시간 최소화:</strong> 데이터 처리가 사용자 근처에서 73 일어나므로, 중앙 클라우드까지의 물리적 왕복 시간(RTT)이 사실상 사라진다. 이는 자율주행 17 및 AR/VR 75의 MTP 10ms 26와 같은 초저지연 요구사항 75을 충족하는 데 필수적이다.</li>
<li><strong>지능형 캐싱 (Intelligent Caching):</strong> 엣지 서버는 단순한 캐시가 아니다. 엣지 서버는 사용자의 시점을 능동적으로 예측하고 66, 예측된 타일 64이나 자주 요청되는 콘텐츠 67를 <em>미리 캐싱</em>하여, 사용자 요청 시 즉각적인 전송을 지원한다.</li>
<li><strong>연산 오프로딩 (Computation Offloading):</strong> 엣지 서버는 리소스가 제한된 클라이언트 기기 18의 무거운 연산(디코딩 55, 렌더링 77, 객체 탐지 61)을 대신 수행(offloading)할 수 있다.73</li>
</ol>
<p>이러한 적응형 스트리밍 전략(타일링, 시점 예측)과 엣지 컴퓨팅(캐싱)은 개별적인 기술이 아니라, ’초저지연 적응형 스트리밍’이라는 단일 목표를 달성하기 위한 <em>필수적이고 상호 의존적인 아키텍처 구성 요소</em>이다. 타일링 64은 적응형 전송의 단위를 정의하고, 시점 예측 66은 전송할 타일을 선별하며, 엣지 캐싱 64은 선별된 타일을 가장 빠른 경로로 전달한다.</p>
<p>더 나아가, 엣지 아키텍처는 단순한 캐시 서버(Phase 3)에서 ‘분산 렌더링 플랫폼’(Phase 4)으로 진화하고 있다. 미래의 엣지 서버는 GPU를 탑재하여 75, 압축된 데이터를 전송하는 대신 <em>직접 렌더링</em>을 수행할 수 있다.77 이 모델에서 클라이언트는 자신의 카메라 자세(Pose) 정보(수 Kbyte)만 엣지로 전송하고, 엣지 서버는 해당 시점의 렌더링 결과물인 <em>2D 비디오 스트림</em>을 클라이언트로 전송한다. 이 ‘픽셀 스트리밍’ 모델은 클라이언트의 연산 부하 18를 0에 가깝게 만들지만, 전체 왕복 과정(클라이언트-&gt;엣지-&gt;클라이언트)이 MTP 10ms 26 내에 완료되어야 하는 극단적인 네트워크 엔지니어링을 요구한다.</p>
<h3>5.3  다중 코덱 (Multi-Codec) 적응형 스트리밍</h3>
<p>현실 세계의 포인트 클라우드 씬은 조밀한 객체(V-PCC 적합)와 희소한 배경(G-PCC 적합)이 혼재되어 있다.11 또한 Draco 78와 같은 경량 코덱도 존재한다. 단일 코덱으로는 이 모든 상황에 최적으로 대응할 수 없다.</p>
<p>이에 대한 해결책은 “Multi-codec rate adaptive point cloud streaming”(다중 코덱 비율 적응형 포인트 클라우드 스트리밍) 23이다.</p>
<p>이 아키텍처에서 스트리밍 시스템(주로 엣지 서버)은 G-PCC, V-PCC, Draco 등 가용한 모든 코덱의 성능(예상 비트레이트, 디코딩 시간, 품질)을 예측하는 모델 78을 보유한다. 그리고 현재 네트워크 상태 79, 클라이언트의 연산 능력 79, 그리고 전송할 <em>콘텐츠의 특성</em>(예: 조밀도, 복잡도) 37을 실시간으로 분석하여, “최소의 지연 시간을 제공할 수 있는 코덱과 압축 파라미터를” 78 <em>동적으로 선택</em>하고 즉시 전환한다.</p>
<h2>6.  핵심 기술 4: 실시간 렌더링 파이프라인</h2>
<p>스트림의 마지막 단계는 수신된 데이터를 클라이언트의 화면에 시각화하는 렌더링이다.</p>
<h3>6.1  전통적 렌더링: LOD (Level of Detail) 기법</h3>
<p>스트리밍의 목표가 수십억, 수조 개의 점 81으로 구성된 씬을 렌더링하는 것이라면, 이 데이터를 GPU 메모리 83에 한 번에 로드하는 것은 불가능하다.</p>
<p>LOD(Level of Detail)는 이 문제를 해결하기 위한 고전적이고 필수적인 기법이다. LOD는 옥트리 81나 LPC(Layered Point Cloud) 81와 같은 계층적 데이터 구조를 사용하여, 원본 데이터를 다양한 해상도 레벨로 미리 구성한다.</p>
<p>렌더링 시, 시스템은 카메라의 위치를 기준으로 85 가까운 영역은 고해상도(LOD 높음) 노드를, 먼 영역은 저해상도(LOD 낮음) 노드를 선택적으로 로드하고 렌더링한다.85</p>
<p>그러나 LOD 기법은 두 가지 고질적인 문제를 안고 있다.</p>
<ol>
<li><strong>LOD 구성 시간:</strong> 이 계층적 LOD 구조를 생성하는 것 자체가 “takes time”(시간이 소요된다).81 특히 스트리밍 중 87에 실시간으로 이 구조를 생성하는 것은 매우 어렵다. 이에 대한 해결책으로 CPU 기반이 아닌 GPU(CUDA)를 통해 LOD 구성을 가속하는 연구 81가 진행되었으며, RTX 3090에서 초당 10억 개의 점으로 LOD를 구성할 수 있다는 보고도 있다.82</li>
<li><strong>시각적 품질 (Holes):</strong> LOD는 점(point) 기반 렌더링이므로, 점과 점 사이의 “empty space”(빈 공간) 88 또는 “holes”(구멍) 26 문제가 필연적으로 발생한다. 이를 매우기 위해 각 점을 작은 원반이나 사각형으로 확장하여 렌더링하는 ‘스플래팅(splatting)’ 86 기법이 필요하다.</li>
</ol>
<h2>7.  차세대 렌더링: 3D Gaussian Splatting (3DGS)</h2>
<h3>7.1  3D Gaussian Splatting의 정의와 원리</h3>
<p>3D Gaussian Splatting (3DGS) 89은 2023년 SIGGRAPH에서 발표된 이후 3D 렌더링 분야를 뒤흔든 “game-changing”(판도를 바꾸는) 90 기술이다. 이는 NeRF(Neural Radiance Fields)와 달리 렌더링 시점에 무거운 신경망을 사용하지 않고 91, 씬을 수백만 개의 명시적인(explicit) 3D 가우시안(Gaussian)으로 표현한다.92</p>
<p>각각의 3D 가우시안은 3D 공간상의 타원체(ellipsoid)로, 위치(Position), 형태(Shape, 즉 공분산 행렬), 색상(Color, 구면 조화 함수(SH)로 표현), 그리고 투명도(Transparency) 93 속성을 가진다.</p>
<p>렌더링 시, 3DGS는 이 수백만 개의 가우시안을 “visibility-aware sorting”(가시성 인지 정렬) 94이라는 고도로 최적화된 알고리즘을 통해 GPU에서 초고속 래스터라이제이션(rasterization)을 수행한다.91 그 결과, 100 FPS 이상의 “superior real-time performance”(우월한 실시간 성능) 90로 포토 리얼리스틱한 씬을 렌더링할 수 있다.</p>
<h3>7.2  3DGS와 스트리밍 적용</h3>
<p>3DGS는 그 본질이 “volumetric, point cloud-based nature”(체적, 포인트 클라우드 기반 본성) 91을 가지므로, 포인트 클라우드 렌더링의 직계 후속 기술로 간주된다.</p>
<p>3DGS는 본래 정적(static) 95, 소규모 씬 96을 위해 설계되었으나, 이를 대규모 스트리밍에 적용하려는 연구가 폭발적으로 증가하고 있다.</p>
<ul>
<li>“Compressed 3D Gaussian Splatting” 97은 3DGS 데이터를 압축하여 전송 대역폭을 줄이려는 시도이다.</li>
<li>“LoD of Gaussians” 96는 3DGS에 계층적 LOD 구조를 명시적으로 도입하여 “ultra-large scale”(초거대 스케일) 96 씬을 동적으로 스트리밍하는 프레임워크를 제안한다.</li>
</ul>
<p>3DGS는 포인트 클라우드 렌더링의 오랜 난제였던 LOD와 스플래팅(VI.A)을 하나의 우아한 방식으로 통합하여 해결한다.</p>
<ol>
<li><strong>스플래팅 문제 해결:</strong> 3DGS는 ’점’이 아닌 ‘가우시안’(타원체) 93을 기본 단위로 사용한다. 가우시안은 크기와 투명도를 가지므로 ’스플래팅’이 <em>내재</em>되어 있어 ‘구멍’ 26 문제가 원천적으로 해결된다.</li>
<li><strong>LOD 문제 해결:</strong> 가우시안은 계층적 표현에 적합하다. “LoD of Gaussians” 96 연구가 증명하듯, 먼 거리의 가우시안 100개를 더 큰 가우시안 1개로 대체하는 계층적 표현이 가능하다.98</li>
</ol>
<p>이는 스트리밍 파이프라인의 대변혁을 의미한다. 과거의 파이프라인(G-PCC 압축 해제 -&gt; 실시간 LOD 구조 생성 82 -&gt; 실시간 렌더링 88)에서, “LOD 생성“이라는 가장 복잡하고 시간 소모적인 런타임 단계가 <em>사라지고</em> 데이터 구조(LoD of Gaussians 96)에 통합된다. 즉, 3DGS는 단순한 ’렌더러’가 아니라, ‘압축’, ‘데이터 구조’, ’렌더러’를 모두 포함하는 <em>새로운 3D 표현 방식</em> 그 자체이다. 스트리밍의 대상이 ’점’에서 ’가우시안’으로 바뀌는 것이다.</p>
<h3>7.3  3DGS의 한계와 LiDAR 데이터 적용 난제</h3>
<p>3DGS는 만능이 아니며, 특히 포인트 클라우드의 핵심 사용처인 LiDAR 데이터와 관련하여 명확한 한계를 가진다.</p>
<ul>
<li><strong>입력 데이터의 한계:</strong> 3DGS의 표준 파이프라인 89은 <em>여러 장의 고품질 사진</em>과 <em>SfM(Structure from Motion)</em> 99을 입력으로 요구한다.</li>
<li><strong>LiDAR 적용의 어려움:</strong> 자율주행 15의 핵심인 LiDAR 데이터는 사진이 아니다. LiDAR 데이터는 희소하고 11, 텍스처가 부족(low-texture) 100하며, 360도 스캔 데이터이다. 표준 3DGS는 이러한 데이터에 직접 적용하기 매우 어렵다.101</li>
<li><strong>해결 시도:</strong> “Li-GS” 102, “LiDAR-GS” 103 등 최신 연구들은 이 문제를 해결하려 시도한다. 이들은 3DGS를 밑바닥부터 학습하는 대신, LiDAR 포인트 클라우드를 3DGS의 <em>초기값(initialization)</em> 101으로 사용하거나, LiDAR의 정확한 기하 정보를 학습 가속 102 및 정확도 향상에 활용하는 하이브리드 접근 방식을 취하고 있다.</li>
</ul>
<h2>8.  주요 구현 플랫폼 및 라이브러리 분석</h2>
<h3>8.1  웹 기반 렌더링: Potree와 Cesium 3D Tiles 비교</h3>
<p>대규모 포인트 클라우드를 웹 브라우저에서 스트리밍하고 렌더링하는 데는 두 가지 주요 오픈소스 생태계가 있다.</p>
<ul>
<li><strong>Potree:</strong></li>
<li>Potree는 “free open-source WebGL based point cloud renderer” 105로서, “대규모 포인트 클라우드 렌더링“이라는 <em>한 가지</em> 목적에 고도로 특화되어 있다 (“Does one thing and it does it well”).108</li>
<li>PotreeConverter 84를 통해 고유의 효율적인 옥트리 LOD 형식을 생성한다. 일부 전문가는 포인트 클라우드 <em>자체</em>의 렌더링 효율성만 놓고 보면 Potree 형식이 3D Tiles보다 “superior”(우월하다)고 주장한다.108</li>
<li>Cesium과 연동하여 맵 위에 Potree 뷰어를 올리는 하이브리드 사용이 가능하며 109, 정밀 측정 111과 같은 고급 상호작용 기능을 제공한다.</li>
<li><strong>Cesium 3D Tiles:</strong></li>
<li>Cesium 3D Tiles는 OGC(Open Geospatial Consortium)의 개방형 표준이다.112</li>
<li><strong>핵심 차이:</strong> Cesium의 목표는 포인트 클라우드 전문 렌더링이 아닌, “massive heterogeneous 3D geospatial datasets”(대규모 이종 3D 지리공간 데이터셋) 112의 융합 및 스트리밍이다.</li>
<li>즉, 포인트 클라우드 115뿐만 아니라 BIM/CAD, 3D 빌딩, 사진측량 모델, 벡터 데이터 113 등 <em>모든 유형</em>의 3D 데이터를 <em>하나의 씬</em>에 융합하는 것이 목적이다. 이는 디지털 트윈과 스마트 시티 117에 필수적인 기능이다.</li>
<li>Cesium Ion 118이라는 클라우드 서비스를 통해 LAS/LAZ 118 같은 원본 데이터를 3D Tiles로 변환 및 압축(Draco 압축 사용) 118하여 스트리밍한다.</li>
<li>Cesium은 Potree 형식을 직접 지원하지 않으며, 3D Tiles로의 변환을 요구한다.119</li>
</ul>
<p>플랫폼 선택은 ’전문성’과 ‘확장성’ 사이의 트레이드오프이다. 사용자의 요구사항이 “고정밀 포인트 클라우드 <em>만</em> 본다“라면 Potree 108가 효율적인 선택이다. 하지만 요구사항이 “포인트 클라우드 <em>와 함께</em> 빌딩 모델(BIM), 도로망(벡터)을 융합해서 본다”(디지털 트윈) 113라면, OGC 표준 112인 Cesium이 유일한 솔루션이다.</p>
<p>장기적으로는 융합과 상호운용성 113이 중요하며, “3D Tiles Next” 117가 glTF 114를 기반으로 생태계를 확장함에 따라, 3DGS와 같은 신기술(Potree 커뮤니티에서도 논의 중 120)을 흡수하며 Cesium 생태계가 산업 표준 플랫폼으로 자리 잡을 가능성이 높다.</p>
<p><strong>표 3: Potree와 Cesium 3D Tiles 스트리밍 플랫폼 비교</strong></p>
<table><thead><tr><th><strong>비교 항목</strong></th><th><strong>Potree</strong></th><th><strong>Cesium 3D Tiles</strong></th></tr></thead><tbody>
<tr><td><strong>핵심 철학</strong></td><td>대규모 포인트 클라우드 전문 렌더러 108</td><td>이종(Heterogeneous) 3D 지리공간 데이터 융합 플랫폼 112</td></tr>
<tr><td><strong>데이터 형식</strong></td><td>Potree 고유 형식 (Octree-based) 84</td><td>OGC 3D Tiles 112 (glTF 기반 117)</td></tr>
<tr><td><strong>주요 강점</strong></td><td>포인트 클라우드 <em>단일</em> 데이터 렌더링 효율성 108</td><td>이종 데이터 융합 (BIM, Vector, 3D Models) 113</td></tr>
<tr><td><strong>데이터 변환</strong></td><td>PotreeConverter (오프라인) 106</td><td>Cesium Ion 클라우드 서비스 118</td></tr>
<tr><td><strong>타 형식 지원</strong></td><td>제한적. (3D Tiles 미지원) 119</td><td>Potree 형식 미지원, 변환 필요 119</td></tr>
<tr><td><strong>주요 사용 사례</strong></td><td>고정밀 포인트 클라우드 분석/측정 111</td><td>디지털 트윈, 스마트 시티, 이종 데이터 융합 115</td></tr>
</tbody></table>
<h3>8.2  백엔드 라이브러리: PCL (Point Cloud Library)</h3>
<p>PCL 121은 Potree/Cesium과 같은 웹 <em>렌더러</em>가 아니다. PCL은 C++ 기반의 거대하고 포괄적인 3D 데이터 <em>처리(processing)</em> 라이브러리이다.122</p>
<p>PCL의 역할은 스트리밍 파이프라인의 <em>서버 측</em> 또는 <em>오프라인 전처리</em> 단계에 있다. LiDAR 124 등에서 수집된 원본(raw) 데이터는 노이즈가 많고 정렬되지 않았을 수 있다.121 PCL은 이러한 원본 데이터를 받아 필터링(filtering) 121, 노이즈 제거(denoising) 121, 정합(registration, 여러 스캔을 하나로 합침) 121, 분할(segmentation) 123 등 데이터 ’가공’을 수행한다.</p>
<p>이렇게 PCL로 가공된 깨끗한 데이터가 PotreeConverter 106나 Cesium Ion 118의 입력으로 들어가 스트리밍용 LOD를 생성하게 된다. PCL 자체에도 실시간 시각화 121 기능이 있으나, 이는 주로 개발 및 분석용이며 대규모 웹 스트리밍용은 아니다.</p>
<h3>8.3  3DGS 생태계: 폭발적 성장</h3>
<p>3DGS는 발표된 지 얼마 되지 않았음에도 불구하고 폭발적인 생태계를 구축하고 있다.</p>
<ul>
<li><strong>오픈소스 라이브러리:</strong> 3DGS 원 논문의 공식 구현체 89 외에도, PyTorch 기반의 모듈형 API를 제공하는 <code>gsplat</code> 126, THREE.js와 쉽게 통합 가능한 <code>Spark</code> 127 등 다양한 라이브러리가 등장하고 있다.</li>
<li><strong>상용 플랫폼 및 하드웨어:</strong> AWS 91는 3DGS 모델의 생성, 저장(S3), 배포(ECS)를 위한 클라우드 파이프라인을 제공한다. Polycam 128, 3DVista 129, vid2scene 130 등은 3DGS 모델을 쉽게 생성하고 호스팅하는 상용 서비스를 제공한다. Qualcomm 131은 스냅드래곤 XR2, 8 Elite 등 모바일/XR 플랫폼에서 3DGS를 실시간(8.85ms 렌더링) 131으로 구동하기 위한 하드웨어 최적화를 수행하고 있다.</li>
</ul>
<h2>9.  종합 및 미래 전망</h2>
<p>포인트 클라우드 고속 스트리밍 기술은 압축, 아키텍처, 렌더링의 세 축이 동시에 급격하게 진화하는 변곡점에 있다.</p>
<h3>9.1  AI 기반 압축과 3DGS 렌더링의 융합</h3>
<p>미래의 스트리밍 파이프라인은 ’압축’과 ’렌더링’이 분리되지 않고 하나로 융합될 것이다. 3DGS 91는 그 자체로 효율적인 렌더링이 가능한 ’표현(representation)’이다. 따라서 미래의 압축 기술(MPEG의 차세대 표준 59)은 G-PCC처럼 ’점’을 압축하는 것이 아니라, “Compressed 3D Gaussian Splatting” 97처럼 3DGS <em>자체</em>를 압축하거나, 혹은 오토인코더 50의 잠재 벡터(latent vector)에서 3DGS 97로 직접 디코딩하는 ’신경망 표현(Neural Representation)’을 스트리밍하게 될 것이다. 이러한 신경망 표현은 “LoD of Gaussians” 96처럼 계층적(hierarchical)이며 스트리밍에 친화적인 구조를 갖출 것이다.</p>
<h3>9.2  엣지-클라우드 연합(Federated) 아키텍처의 진화</h3>
<p>중앙 클라우드 72는 수 페타바이트(PB)에 달하는 디지털 트윈 마스터 데이터를 저장하고 AI 모델(시점 예측, 콘텐츠 인지, 압축 모델)을 학습시키는 역할을 맡는다. MEC 75는 ’추론(Inference)’과 ’실시간 상호작용’을 담당한다. 즉, 중앙에서 학습된 모델을 엣지에서 실행하여 73 지능형 캐싱 64을 수행한다. 궁극적으로 엣지는 V.B에서 분석했듯이, 캐싱을 넘어 실시간 렌더링 77까지 수행하여, 클라이언트 18는 5G 70망을 통해 최종 픽셀 스트림만 수신하는 ‘초경량 클라이언트’ 모델로 진화할 것이다.</p>
<h3>9.3  MPEG의 차세대 표준화 동향</h3>
<p>MPEG는 G-PCC 34와 V-PCC 44의 성공에 안주하지 않고, AI 기반 압축 59의 압도적인 성능 52을 수용하여 차세대 표준을 모색하고 있다. 이는 단일 코덱이 아닌, 다양한 유형의 포인트 클라우드 59와 콘텐츠 특성 37에 대응하는 “unified codec”(통합 코덱) 또는 “specialized codecs”(특화 코덱) 60의 등장을 예고한다. 이러한 미래 표준은 ‘다중 코덱 적응형 스트리밍’(V.C) 78 아키텍처와 결합하여, G-PCC, V-PCC, 그리고 새로운 AI 코덱이 실시간으로 상호 전환되는 고도로 지능화된 스트리밍 시스템의 기반이 될 것이다.</p>
<h2>10. 참고 자료</h2>
<ol>
<li>A Beginner’s Guide to 3D Data: Understanding Point Clouds, Meshes, and Voxels - Medium, https://medium.com/@sanjivjha/a-beginners-guide-to-3d-data-understanding-point-clouds-meshes-and-voxels-385e02108141</li>
<li>Everything you need to know about point cloud data and point cloud scanning - NavVis, https://www.navvis.com/blog/everything-you-need-to-know-about-point-clouds-navvis</li>
<li>3D Point Cloud 데이터의 이해 - jiho, <a href="https://jih0.medium.com/3d-point-cloud-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-db6a75316645">https://jih0.medium.com/3d-point-cloud-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-db6a75316645</a></li>
<li>Point Cloud Rendering: Key Methods and Applications - RebusFarm, https://rebusfarm.net/blog/what-is-a-point-cloud</li>
<li>포인트 클라우드 | 정확도 및 효율성 | 오토데스크 - Autodesk, https://www.autodesk.com/kr/solutions/point-clouds</li>
<li>Point Cloud - 포인트 클라우드란 무엇입니까? - SBG Systems, https://www.sbg-systems.com/ko/glossary/point-cloud/</li>
<li>Foundational Models for 3D Point Clouds: A Survey and Outlook - arXiv, https://arxiv.org/html/2501.18594v1</li>
<li>Blind Quality Assessment of Dense 3D Point Clouds with Structure Guided Resampling - -ORCA - Cardiff University, https://orca.cardiff.ac.uk/id/eprint/168856/1/SGR_PCQA.pdf</li>
<li>Three-Dimensional Point Cloud Reconstruction of Unstructured Terrain for Autonomous Robots - MDPI, https://www.mdpi.com/1424-8220/25/16/4890</li>
<li>Understanding the difference between structured, unified, and unstructured scan data with Prevu3D, https://www.prevu3d.com/news/differences-between-structured-unified-and-unstructured-scan-data/</li>
<li>MPEG-I — Point Cloud Compression (V-PCC, G-PCC &amp; branches), https://mpeg.expert/v-g-pcc/index.html</li>
<li>What is Point Cloud ? Uses &amp; Benefits | Egnyte, https://www.egnyte.com/guides/aec/what-is-point-cloud</li>
<li>An overview of ongoing point cloud compression standardization activities: video-based (V-PCC) and geometry-based (G-PCC) | APSIPA Transactions on Signal and Information Processing | Cambridge Core, https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/an-overview-of-ongoing-point-cloud-compression-standardization-activities-videobased-vpcc-and-geometrybased-gpcc/56FCAF660DD44348BCB1BCA9B5EC56CF</li>
<li>Understanding Point Cloud: Applications, Benefits, and How It Works - Capnor, https://www.capnor.com/en/blog/what-is-point-cloud-and-why-is-it-important</li>
<li>A Survey on Data Compression Techniques for Automotive LiDAR Point Clouds - MDPI, https://www.mdpi.com/1424-8220/24/10/3185</li>
<li>Digital Twin and Artificial Intelligence-Empowered Panoramic Video Streaming - Nxtbook Media, https://read.nxtbook.com/ieee/vehicular_technology/vehiculartechnology_dec_2023/digital_twin_and_artificial_i.html</li>
<li>Digital Twins for Autonomous Driving: A Comprehensive Implementation and Demonstration, https://arxiv.org/html/2401.08653v1</li>
<li>Content-Aware Adaptive Point Cloud Delivery - IMDEA Networks Principal, https://dspace.networks.imdea.org/bitstream/handle/20.500.12761/1699/Content-Aware_Adaptive_Point_Cloud_Delivery.pdf?sequence=1</li>
<li>An Introduction to Point Cloud Compression Standards - Auburn University, https://www.eng.auburn.edu/~szm0001/papers/ChenGetMobile23.pdf</li>
<li>Digital Twin Metaverse: A Quick Guide - Idea Usher, https://ideausher.com/blog/digital-twin-metaverse/</li>
<li>Towards Metaverse: Utilizing Extended Reality and Digital Twins to Control Robotic Systems, https://www.mdpi.com/2076-0825/12/6/219</li>
<li>Bits-to-Photon: End-to-End Learned Scalable Point Cloud Compression for Direct Rendering - arXiv, https://arxiv.org/html/2406.05915v1</li>
<li>Multi-Codec Rate Adaptive Point Cloud Streaming - OhioLINK, http://rave.ohiolink.edu/etdc/view?acc_num=ucin1703171379005799</li>
<li>Digital Twins and Big Data in the Metaverse: Addressing Privacy, Scalability, and Interoperability with AI and Blockchain - MDPI, https://www.mdpi.com/2220-9964/14/8/318</li>
<li>Emerging MPEG Standards for Point Cloud Compression - CORE, https://core.ac.uk/download/pdf/301635383.pdf</li>
<li>Low Latency Point Cloud Rendering with Learned Splatting - arXiv, https://arxiv.org/html/2409.16504v1</li>
<li>Evaluating the Impact of Tiled User-Adaptive Real-Time Point Cloud Streaming on VR Remote Communication, https://ir.cwi.nl/pub/32512/32512D.pdf</li>
<li>[1804.10878] Dynamic Adaptive Point Cloud Streaming - arXiv, https://arxiv.org/abs/1804.10878</li>
<li>비디오 기반 포인트 클라우드 압축(V-PCC) 콘텐츠의 스트리밍 플랫폼, https://www.kibme.org/resources/journal/20230807171538940.pdf</li>
<li>INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video - arXiv, https://arxiv.org/html/2508.13756v1</li>
<li>(PDF) Point Cloud Video Streaming: Challenges and Solutions - ResearchGate, https://www.researchgate.net/publication/356128040_Point_Cloud_Video_Streaming_Challenges_and_Solutions</li>
<li>초보 개발자 WebRTC 시동걸기 - by Jaeyeoul Ahn - Medium, <a href="https://medium.com/monday-9-pm/%EC%B4%88%EB%B3%B4-%EA%B0%9C%EB%B0%9C%EC%9E%90-webrtc-%EC%8B%9C%EB%8F%99%EA%B1%B8%EA%B8%B0-ebefe6feadf7">https://medium.com/monday-9-pm/%EC%B4%88%EB%B3%B4-%EA%B0%9C%EB%B0%9C%EC%9E%90-webrtc-%EC%8B%9C%EB%8F%99%EA%B1%B8%EA%B8%B0-ebefe6feadf7</a></li>
<li>Point Cloud Streaming with Latency-Driven Implicit Adaptation using MoQ - arXiv, https://arxiv.org/html/2507.15673</li>
<li>11월 11, 2025에 액세스, [https://register.metaverse-standards.org/spps/254#:<sub>:text=MPEG%20G%2DPCC%20is%20an,processing%20of%20point%20cloud%20data.](https://register.metaverse-standards.org/spps/254#:</sub>:text=MPEG G-PCC is an, <a href="https://register.metaverse-standards.org/spps/254#:~:text=MPEG%20G-PCC%20is%20an,processing%20of%20point%20cloud%20data.">https://register.metaverse-standards.org/spps/254#:~:text=MPEG%20G%2DPCC%20is%20an,processing%20of%20point%20cloud%20data.</a></li>
<li>Geometry-based Point Cloud Compression | MPEG, https://mpeg.chiariglione.org/standards/mpeg-i/geometry-based-point-cloud-compression.html</li>
<li>An overview of ongoing point cloud compression standardization activities: video-based (V-PCC) and geometry-based (G-PCC) - Now Publishers, https://www.nowpublishers.com/article/Details/SIP-149</li>
<li>Improved Video-Based Point Cloud Compression via Segmentation - MDPI, https://www.mdpi.com/1424-8220/24/13/4285</li>
<li>Presenting a Novel Pipeline for Performance Comparison of V-PCC and G-PCC Point Cloud Compression Methods on Datasets with Varyi - SciTePress, https://www.scitepress.org/Papers/2022/108202/108202.pdf</li>
<li>Geometry-based point cloud compression (G-PCC) - Metaverse Standards Register, https://register.metaverse-standards.org/spps/254</li>
<li>MPEG-I: Geometry-based Point Cloud Compression, https://www.mpeg.org/standards/MPEG-I/9/</li>
<li>An overview of ongoing point cloud compression standardization activities: video-based (V-PCC) and geometry-based (G-PCC), https://mpeg-pcc.org/wp-content/uploads/2020/04/an_overview_of_ongoing_point_cloud_compression_standardization_activities_videobased_vpcc_and_geometrybased_gpcc.pdf</li>
<li>S4-211171_pCR_r01_EncapsulationVolumetricMedia.docx - 3GPP, https://www.3gpp.org/ftp/TSG_SA/WG4_CODEC/TSGS4_115-e/Inbox/Drafts/Video/S4-211171_pCR_r01_EncapsulationVolumetricMedia.docx</li>
<li>Point Cloud Compression in MPEG - ICIP 2020, https://2020.ieeeicip.org/program/tutorials/point-cloud-compression-in-mpeg/index.html</li>
<li>ISO/IEC 23090-5:2023 - iTeh Standards, https://cdn.standards.iteh.ai/samples/83535/e59cd7c408a3451e882f5a5d593e9897/ISO-IEC-23090-5-2023.pdf</li>
<li>MPEG-I: Visual Volumetric Video-based Coding (V3C) and Video-based Point Cloud Compression (V-PCC) - Standards – MPEG, https://www.mpeg.org/standards/MPEG-I/5/</li>
<li>MPEG 127, https://www.mpeg.org/meetings/mpeg-127/</li>
<li>Deep Learning-Based Point Cloud Compression: An In-Depth Survey and Benchmark, https://pubmed.ncbi.nlm.nih.gov/40742848/</li>
<li>[2409.08376] Learned Compression for Images and Point Clouds - arXiv, https://arxiv.org/abs/2409.08376</li>
<li>[2106.01504] DeepCompress: Efficient Point Cloud Geometry Compression - arXiv, https://arxiv.org/abs/2106.01504</li>
<li>deep learning based point cloud processing and compression, https://mospace.umsystem.edu/xmlui/bitstream/handle/10355/91327/Akhtar_umkc_0134D_11888.pdf?sequence=1</li>
<li>Point Cloud Geometry Compression Based on Multi-Layer Residual Structure - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC9689459/</li>
<li>Point Cloud Compression and Objective Quality Assessment: A Survey - arXiv, https://arxiv.org/html/2506.22902v1</li>
<li>RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds - arXiv, https://arxiv.org/html/2503.12382v1</li>
<li>Efficient Large-Scale Point Cloud Geometry Compression - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC11902766/</li>
<li>Survey on Deep Learning-Based Point Cloud Compression - Frontiers, https://www.frontiersin.org/journals/signal-processing/articles/10.3389/frsip.2022.846972/full</li>
<li>PVContext: Hybrid Context Model for Point Cloud Compression - arXiv, https://arxiv.org/html/2409.12724v1</li>
<li>kaiwangm/awesome-learned-point-cloud-compression - GitHub, https://github.com/kaiwangm/awesome-deep-point-cloud-compression</li>
<li>Fast Coding Unit Partitioning Method for Video-Based Point Cloud Compression: Combining Convolutional Neural Networks and Bayesian Optimization - MDPI, https://www.mdpi.com/2079-9292/14/7/1295</li>
<li>146th meeting of MPEG, https://www.mpeg.org/146th-meeting-of-mpeg/</li>
<li>MPEG 146, https://www.mpeg.org/meetings/mpeg-146/</li>
<li>Point Cloud Compression: Impact on Object Detection in Outdoor Contexts - PubMed, https://pubmed.ncbi.nlm.nih.gov/35957323/</li>
<li>Point Cloud Compression: Impact on Object Detection in Outdoor Contexts - MDPI, https://www.mdpi.com/1424-8220/22/15/5767</li>
<li>[2308.05959] Learned Point Cloud Compression for Classification - arXiv, https://arxiv.org/abs/2308.05959</li>
<li>Spatial Visibility and Temporal Dynamics: Revolutionizing Field of View Prediction in Adaptive Point Cloud Video Streaming - arXiv, https://arxiv.org/html/2409.18236v2</li>
<li>Viewport Prediction for Volumetric Video Streaming by Exploring Video Saliency and User Trajectory Information - arXiv, https://arxiv.org/html/2311.16462v3</li>
<li>Content-Aware Proactive VR Video Caching for Cache-Enabled AP over Edge Networks, https://www.mdpi.com/2079-9292/11/18/2824</li>
<li>CaV3: Cache-assisted Viewport Adaptive Volumetric Video Streaming - eScholarship, https://escholarship.org/content/qt3fp7020v/qt3fp7020v.pdf?t=sizbbh</li>
<li>Edge caching for live 360 • video streaming. | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Edge-caching-for-live-360-video-streaming_fig4_341703327</li>
<li>Multi-access Edge Computing (MEC) - ETSI, https://www.etsi.org/technologies/multi-access-edge-computing</li>
<li>What is multi-access edge computing (MEC)? - Red Hat, https://www.redhat.com/en/topics/edge-computing/what-is-multi-access-edge-computing</li>
<li>Edge Computing - 3GPP, https://www.3gpp.org/technologies/edge-computing</li>
<li>What Is Edge Computing? | Microsoft Azure, https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-edge-computing</li>
<li>What Is Edge Computing (Versus Cloud Computing)? – Arm®, https://www.arm.com/glossary/edge-computing-vs-cloud-computing</li>
<li>What Is Edge Computing? 8 Examples and Architecture You Should Know - FSP Group, https://www.fsp-group.com/en/knowledge-app-42.html</li>
<li>What is multi access edge computing | Glossary - HPE, https://www.hpe.com/us/en/what-is/multi-access-edge-computing.html</li>
<li>Multi-Access Edge Computing: How It Can Accelerate Transformation - AT&amp;T Business, https://www.business.att.com/learn/articles/multi-access-edge-computing-how-it-can-accelerate-transformation.html</li>
<li>00Cloud Rendering vs Edge Processing: When Users Complain About Lag — Which Scales Better for Digital-Twin Platforms? - AlterSquare, https://altersquare.medium.com/cloud-rendering-vs-edge-processing-when-users-complain-about-lag-which-scales-better-for-5d69f9628e94</li>
<li>Multi-codec rate adaptive point cloud streaming for holographic-type communication - ITU, https://www.itu.int/pub/S-JNL-VOL4.ISSUE4-2023-A42</li>
<li>MULTI‑CODEC RATE ADAPTIVE POINT CLOUD STREAMING FOR HOLOGRAPHIC‑TYPE COMMUNICATION - ITU, https://www.itu.int/dms_pub/itu-s/opb/jnl/S-JNL-VOL4.ISSUE4-2023-A42-PDF-E.pdf</li>
<li>Content-Adaptive Level of Detail for Lossless Point Cloud Compression - Now Publishers, https://www.nowpublishers.com/article/Details/SIP-2022-0004</li>
<li>m-schuetz/CudaLOD: CUDA-accelerated LOD construction for point clouds - GitHub, https://github.com/m-schuetz/CudaLOD</li>
<li>GPU-Accelerated LOD Generation for Point Clouds - Eurographics Association, https://diglib.eg.org/bitstream/handle/10.1111/cgf14877/v42i8_07_14877.pdf</li>
<li>(PDF) Virtualized Point Cloud Rendering - ResearchGate, https://www.researchgate.net/publication/382801142_Virtualized_Point_Cloud_Rendering</li>
<li>potree/PotreeConverter: Create multi res point cloud to use with potree - GitHub, https://github.com/potree/PotreeConverter</li>
<li>Real-Time Continuous Level of Detail Rendering of Point Clouds - Research Unit of Computer Graphics | TU Wien, https://www.cg.tuwien.ac.at/research/publications/2019/schuetz-2019-CLOD/schuetz-2019-CLOD-paper_preprint.pdf</li>
<li>How we render extremely large point clouds - Magnopus, https://www.magnopus.com/blog/how-we-render-extremely-large-point-clouds</li>
<li>SimLOD: Simultaneous LOD Generation and Rendering for Point Clouds - Research Unit of Computer Graphics | TU Wien, https://www.cg.tuwien.ac.at/research/publications/2024/SCHUETZ-2024-SIMLOD/SCHUETZ-2024-SIMLOD-paper.pdf</li>
<li>Real-Time Interpolated Rendering of Terrain Point Cloud Data - PMC - NIH, https://pmc.ncbi.nlm.nih.gov/articles/PMC9824316/</li>
<li>Original reference implementation of “3D Gaussian Splatting for Real-Time Radiance Field Rendering” - GitHub, https://github.com/graphdeco-inria/gaussian-splatting</li>
<li>On the Impacts of Spherical Harmonics and Gaussian Counts in WebGL-Based 3-D Gaussian Splatting - IEEE Computer Society, https://www.computer.org/csdl/magazine/ic/2025/03/11029135/27pwKaa6Mve</li>
<li>3D Gaussian Splatting: Performant 3D Scene Reconstruction at Scale - Amazon AWS, https://aws.amazon.com/blogs/spatial/3d-gaussian-splatting-performant-3d-scene-reconstruction-at-scale/</li>
<li>Trends and Techniques in 3D Reconstruction and Rendering: A Survey with Emphasis on Gaussian Splatting - PMC - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC12197226/</li>
<li>3D Gaussian Splatting: A new frontier in rendering - The Chaos Blog, https://blog.chaos.com/3d-gaussian-splatting-new-frontier-in-rendering</li>
<li>Large-Scale 3D Gaussian Splatting - USC Institute for Creative Technologies, https://ict.usc.edu/research/projects/large-scale-3d-gaussian-splatting/</li>
<li>Adaptive 3D Gaussian Splatting Video Streaming - arXiv, https://arxiv.org/html/2507.14432v1</li>
<li>LoD of Gaussians: Hierarchical 3D Rendering - Emergent Mind, https://www.emergentmind.com/topics/a-lod-of-gaussians</li>
<li>The Impact and Outlook of 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2510.26694v1</li>
<li>CLoD-GS: Continuous Level-of-Detail via 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2510.09997v1</li>
<li>Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting - arXiv, https://arxiv.org/html/2403.09413v1</li>
<li>Full article: Li-GS: a fast 3D Gaussian reconstruction method assisted by LiDAR point clouds, https://www.tandfonline.com/doi/full/10.1080/20964471.2025.2479428?scroll=top&amp;needAccess=true</li>
<li>SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving - arXiv, https://arxiv.org/html/2411.16816v3</li>
<li>Full article: Li-GS: a fast 3D Gaussian reconstruction method assisted by LiDAR point clouds, https://www.tandfonline.com/doi/full/10.1080/20964471.2025.2479428?af=R</li>
<li>LiDAR-GS: Real-time LiDAR Re-Simulation using Gaussian Splatting - arXiv, https://arxiv.org/html/2410.05111v3</li>
<li>Creating 3DGS using LiDAR + photos : r/GaussianSplatting - Reddit, https://www.reddit.com/r/GaussianSplatting/comments/1m804qc/creating_3dgs_using_lidar_photos/</li>
<li>Potree, https://potree.github.io/</li>
<li>potree/potree: WebGL point cloud viewer for large datasets - GitHub, https://github.com/potree/potree</li>
<li>3D Tiles: the next big step for Cesium and 3D geospatial - Google Groups, https://groups.google.com/g/cesium-dev/c/tCCooBxpZFU</li>
<li>Better clouds in the browser - by Martin Heidegger - Medium, https://medium.com/@leichtgewicht/better-clouds-in-the-browser-ef8ef3a2fcb8</li>
<li>Cesium Retz - Potree Viewer, http://potree.org/potree/examples/cesium_retz.html</li>
<li>Hi. I want to load point cloud with cesium 3d tileset. · Issue #600 - GitHub, https://github.com/potree/potree/issues/600</li>
<li>3D measurements on point cloud - CesiumJS - Cesium Community, https://community.cesium.com/t/3d-measurements-on-point-cloud/8955</li>
<li>3D Tiles – Cesium, https://cesium.com/why-cesium/3d-tiles/</li>
<li>CesiumGS/3d-tiles: Specification for streaming massive heterogeneous 3D geospatial datasets :earth_americas - GitHub, https://github.com/CesiumGS/3d-tiles</li>
<li>3D Tiles: the next big step for Cesium and 3D geospatial, https://community.cesium.com/t/3d-tiles-the-next-big-step-for-cesium-and-3d-geospatial/2906</li>
<li>3D Tiles: Point Cloud Filtering - YouTube, https://www.youtube.com/watch?v=CZSo8lLpy64</li>
<li>Taking City Visualization into the Third Dimension with Point Clouds, 3D Tiles, and deck.gl, https://www.uber.com/blog/3d-tiles-loadersgl/</li>
<li>Introducing 3D Tiles Next, Streaming Geospatial to the Metaverse - Cesium, https://cesium.com/blog/2021/11/10/introducing-3d-tiles-next/</li>
<li>Point Clouds Tiler - Cesium, https://cesium.com/platform/cesium-ion/3d-tiling-pipeline/point-clouds/</li>
<li>Comparative Analysis of Web-Based Point Cloud Visualization Tools: Cesium versus Deck.gl - MATOM.AI, https://matom.ai/insights/cesium-vs-deck-gl/</li>
<li>3D Gaussian Splatting Support · Issue #1382 · potree/potree - GitHub, https://github.com/potree/potree/issues/1382</li>
<li>Point Cloud Library - Wikipedia, https://en.wikipedia.org/wiki/Point_Cloud_Library</li>
<li>PointCloudLibrary/pcl: Point Cloud Library (PCL) - GitHub, https://github.com/PointCloudLibrary/pcl</li>
<li>Point Cloud Library | The Point Cloud Library (PCL) is a standalone, large scale, open project for 2D/3D image and point cloud processing., https://pointclouds.org/</li>
<li>Tutorial - Point Cloud Library – pmdtechnologies gmbh - 3D Cameras, https://3d.pmdtec.com/en/tutorial-point-cloud-library/</li>
<li>Stream of Cloud Point Visualization using PCL - Stack Overflow, https://stackoverflow.com/questions/9003239/stream-of-cloud-point-visualization-using-pcl</li>
<li>gsplat: An Open-Source Library for Gaussian Splatting - arXiv, https://arxiv.org/html/2409.06765v1</li>
<li>Show HN: Spark, An advanced 3D Gaussian Splatting renderer for Three.js | Hacker News, https://news.ycombinator.com/item?id=44249565</li>
<li>3D Gaussian Splatting | Polycam, https://poly.cam/tools/gaussian-splatting</li>
<li>Update 2025.0: Support for 3D Gaussian Splatting models and more… - 3DVista, https://www.3dvista.com/en/blog/update-2025-0-support-for-3d-gaussian-splatting-models-and-more/</li>
<li>vid2scene - a free, end-to-end video to gaussian splat web platform : r/GaussianSplatting - Reddit, https://www.reddit.com/r/GaussianSplatting/comments/1i7ixsh/vid2scene_a_free_endtoend_video_to_gaussian_splat/</li>
<li>Driving photorealistic 3D avatars in real time with on-device 3D Gaussian splatting., https://www.qualcomm.com/developer/blog/2024/12/driving-photorealistic03d-avatars-in-real-time-on-device-3d-gaussian-splatting</li>
<li>Point As Gaussian: Forward Point Cloud Rendering - Zibo Zhao, https://maikouuu.github.io/data/pag/pag.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>