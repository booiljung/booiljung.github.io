<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:포인트클라우드 정합</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>포인트클라우드 정합</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">센서 (Sensors)</a> / <a href="../index.html">포인트 클라우드</a> / <a href="index.html">포인트 클라우드 정합</a> / <span>포인트클라우드 정합</span></nav>
                </div>
            </header>
            <article>
                <h1>포인트클라우드 정합</h1>
<h2>1.  3차원 공간의 정렬 문제</h2>
<h3>1.1  포인트클라우드 데이터의 본질과 획득</h3>
<p>포인트클라우드는 3차원 공간상에 분포하는 데이터 점들의 집합으로 정의된다.1 각 점은 일반적으로 3차원 데카르트 좌표계의 좌표값 <span class="math math-inline">(X, Y, Z)</span>를 가지며, 필요에 따라 추가적인 속성 정보를 포함할 수 있다. 이러한 속성에는 RGB 색상 정보, 표면 법선 벡터(surface normal vector), 레이저 반사 강도(intensity), 타임스탬프 등이 포함될 수 있다.2 이 데이터는 현실 세계의 객체나 환경의 외부 표면을 이산적인 점들로 샘플링하여 표현한 결과물이다.</p>
<p>포인트클라우드 데이터의 주요 획득 기술은 매우 다양하며, 대표적으로 라이다(LiDAR, Light Detection and Ranging), RGB-D(Depth) 카메라, 사진 측량(Photogrammetry), 구조광(Structured Light) 스캐너 등이 있다.2 라이다는 레이저 펄스를 발사하여 대상까지의 거리를 정밀하게 측정함으로써 광범위한 영역에 걸쳐 정확하고 밀도 높은 포인트클라우드를 생성하며, 특히 자율주행 및 지형 매핑 분야에서 핵심적인 역할을 한다. RGB-D 카메라는 깊이 정보와 색상 정보를 동시에 획득하여 저비용으로 3차원 데이터를 생성할 수 있게 한다. 사진 측량은 여러 각도에서 촬영한 2D 이미지들로부터 SfM(Structure from Motion)과 같은 컴퓨터 비전 알고리즘을 통해 3차원 점군을 재구성하는 기술이다.2</p>
<p>이러한 방식으로 획득된 포인트클라우드 데이터는 몇 가지 고유한 특성을 지닌다. 첫째, **비정형성(unstructured)**이다. 픽셀이나 복셀처럼 규칙적인 그리드 구조를 갖지 않고, 점들이 공간상에 불규칙하게 분포한다. 둘째, **순서 불변성(permutation invariance)**이다. 점들의 집합으로서 데이터의 순서가 바뀌어도 기하학적 의미는 변하지 않는다. 셋째, **밀도 불균일성(non-uniform density)**이다. 스캔 거리나 표면의 각도, 재질 등에 따라 점의 밀도가 공간적으로 균일하지 않다. 이러한 특성들은 포인트클라우드 처리를 위한 알고리즘, 특히 딥러닝 모델을 설계할 때 반드시 고려되어야 하는 중요한 요소들이다.5</p>
<h3>1.2  정합의 정의와 목표: 공통 좌표계로의 변환</h3>
<p>포인트클라우드 정합(Point Cloud Registration), 또는 스캔 매칭(Scan Matching)은 서로 다른 시점, 시간, 혹은 상이한 센서로부터 획득된 두 개 이상의 포인트클라우드 데이터를 하나의 일관된 공통 좌표계(common coordinate system)로 정렬하는 과정을 의미한다.1 단일 스캔으로는 객체나 환경의 전체 모습을 포착하기 어렵기 때문에, 여러 각도에서 얻은 부분적인 데이터 조각들을 합쳐 완전한 3D 모델을 생성해야 할 필요가 있다. 정합은 바로 이 데이터 조각들을 기하학적으로 올바르게拼接하는 핵심적인 기술이다.8</p>
<p>정합의 핵심 목표는 소스 포인트클라우드(Source Point Cloud)를 타겟 포인트클라우드(Target Point Cloud)에 최적으로 정렬하기 위한 공간 변환(spatial transformation)을 찾는 것이다.3 이 변환은 일반적으로 회전(rotation)과 이동(translation)으로 구성된 강체 변환(rigid transformation)이다. 이 과정을 통해 분절된 데이터들은 하나의 좌표계로 통합되며, 이를 통해 비로소 완전하고 포괄적인 3D 모델을 구축할 수 있다.1</p>
<p>포인트클라우드 정합은 현대 3D 컴퓨터 비전 및 로보틱스 기술의 근간을 이루며, 그 응용 분야는 매우 광범위하다.</p>
<ul>
<li><strong>자율주행 및 로보틱스:</strong> 라이다 스캔을 연속적으로 정합하여 차량이나 로봇의 위치를 추정하고(LiDAR Odometry), 주변 환경의 3D 지도를 생성하는 SLAM(Simultaneous Localization and Mapping) 기술의 핵심이다.3</li>
<li><strong>3D 모델링 및 재구성:</strong> 여러 각도에서 스캔한 데이터를 정합하여 건물, 산업 부품, 문화유산 등의 정밀한 3D 디지털 모델을 생성한다.2</li>
<li><strong>의료 영상:</strong> 서로 다른 시점이나 다른 장비(예: CT, MRI)로 촬영한 환자의 의료 영상을 정합하여 진단, 수술 계획, 치료 경과 추적 등에 활용한다.3</li>
<li><strong>제조 및 품질 검사:</strong> 제조된 부품을 스캔한 포인트클라우드를 원본 CAD 모델과 정합하여 설계와의 오차를 측정하고 품질을 검사한다.2</li>
<li><strong>가상 및 증강 현실(VR/AR):</strong> 현실 공간을 3D로 스캔하고 정합하여 가상 객체를 현실에 자연스럽게 배치하거나 몰입감 있는 가상 환경을 구축한다.3</li>
</ul>
<p>정합은 단순히 두 점 집합을 기계적으로 맞추는 작업을 넘어, 분절되고 노이즈가 섞인 관측 데이터로부터 원래의 완전한 형상과 센서의 상대적 위치 및 자세를 ’추론’하는 근본적인 기하학적 문제로 이해해야 한다.1 이는 데이터의 불완전성(부분 중첩, 폐색)과 측정 오차(노이즈, 이상치)라는 현실적 제약 조건 하에서 최적의 해를 찾는 과정이다. 따라서 정합 알고리즘의 발전은 이러한 불확실성을 얼마나 강건하게 다룰 수 있느냐에 달려 있다.</p>
<h3>1.3  정합의 수학적 공식화</h3>
<p>포인트클라우드 정합은 수학적으로 최적화 문제로 공식화될 수 있다. 소스 포인트클라우드를 <span class="math math-inline">P = \{p_i\}_{i=1}^{N_p}</span>, 타겟 포인트클라우드를 <span class="math math-inline">Q = \{q_j\}_{j=1}^{N_q}</span>라 할 때, 정합의 목표는 <span class="math math-inline">P</span>를 <span class="math math-inline">Q</span>에 가장 잘 정렬하는 변환 <span class="math math-inline">T</span>를 찾는 것이다.</p>
<h4>1.3.1 강체 변환 (Rigid Transformation)</h4>
<p>대부분의 정합 문제에서 사용되는 변환은 강체 변환이다. 강체 변환은 객체의 형태나 크기를 변경하지 않고, 점들 간의 거리를 보존하는 변환을 의미한다.3 3차원 공간에서 강체 변환은 6 자유도(Degrees of Freedom, DoF)를 가지며, 이는 3개의 회전축에 대한 회전과 3개의 좌표축에 대한 이동으로 구성된다. 이는 3x3 회전 행렬 <span class="math math-inline">R \in SO(3)</span>과 3x1 이동 벡터 <span class="math math-inline">t \in \mathbb{R}^3</span>로 표현된다.10 여기서 <span class="math math-inline">SO(3)</span>는 특수 직교 그룹(Special Orthogonal Group)으로, 행렬식이 1인 직교 행렬의 집합을 의미한다 (<span class="math math-inline">R^T R = I</span>, <span class="math math-inline">\det(R)=1</span>). 소스 포인트클라우드 <span class="math math-inline">P</span>의 임의의 점 <span class="math math-inline">p_i</span>는 이 변환에 의해 다음과 같이 새로운 위치 <span class="math math-inline">p_i&#39;</span>로 이동한다.15<br />
<span class="math math-display">
p_i&#39; = R p_i + t
</span><br />
이 변환은 동차 좌표계(Homogeneous Coordinates)를 사용하여 단일 4x4 행렬 <span class="math math-inline">T</span>로 간결하게 표현할 수 있다. 점 <span class="math math-inline">p_i</span>를 동차 좌표 <span class="math math-inline">^T</span>로 표현하면, 변환은 행렬 곱으로 나타낼 수 있다.17<br />
<span class="math math-display">
T = \begin{bmatrix} R &amp; t \\ 0^T &amp; 1 \end{bmatrix}, \quad \begin{bmatrix} p_i&#39; \\ 1 \end{bmatrix} = T \begin{bmatrix} p_i \\ 1 \end{bmatrix}
</span></p>
<h4>1.3.2 비강체 변환 (Non-Rigid Transformation)</h4>
<p>객체가 변형되는 경우, 강체 변환으로는 정합이 불가능하다. 이때는 비강체 변환 또는 변형 가능 정합(deformable registration)이 필요하다.3 비강체 변환은 점들 간의 거리를 보존하지 않으며, 아핀 변환(크기 조절, 전단 포함)이나 더 복잡한 비선형 함수로 모델링된다.3 예를 들어, 의료 영상에서 호흡이나 장기 움직임으로 인한 조직의 변형을 추적하거나, 컴퓨터 그래픽스에서 얼굴 표정 변화를 모델링하는 데 사용된다.20 이는 훨씬 더 높은 자유도를 가지므로, 변형이 물리적으로 타당하도록 부드러움(smoothness)과 같은 제약 조건을 추가하여 최적화 문제를 풀어야 한다.</p>
<h4>1.3.3 목적 함수 (Objective Function)</h4>
<p>정합의 목표는 변환된 소스 포인트와 그에 대응하는 타겟 포인트 간의 거리 오차를 최소화하는 최적의 변환 <span class="math math-inline">T^*</span>를 찾는 것이다. 이를 위해 목적 함수(objective function) 또는 비용 함수(cost function)를 정의한다. 가장 널리 사용되는 것은 대응점 쌍 <span class="math math-inline">(p_i, q_i)</span> 간의 유클리드 거리 제곱의 합을 최소화하는 최소제곱법(Least Squares) 기반의 목적 함수이다.3<br />
<span class="math math-display">
T^* = \arg\min_{T} \sum_{(p_i, q_i) \in C} w_i \lVert T(p_i) - q_i \rVert^2
</span><br />
여기서 <span class="math math-inline">C</span>는 대응점 쌍의 집합, <span class="math math-inline">T(p_i)</span>는 변환이 적용된 소스 포인트, <span class="math math-inline">q_i</span>는 그에 대응하는 타겟 포인트, 그리고 <span class="math math-inline">w_i</span>는 각 대응점 쌍의 신뢰도를 나타내는 가중치이다.15 이 공식에서 핵심적인 두 가지 과제는 (1) 어떻게 정확한 대응점 집합 <span class="math math-inline">C</span>를 찾을 것인가, 그리고 (2) 주어진 <span class="math math-inline">C</span>에 대해 어떻게 목적 함수를 효율적으로 최소화할 것인가이다.</p>
<p>이처럼 어떤 변환 모델을 선택하느냐는 해당 응용 분야의 물리적, 기하학적 특성을 반영하며, 이는 알고리즘 설계의 가장 근본적인 출발점이 된다. 강체 변환은 SLAM이나 정적 객체 인식 문제에 적합하지만 3, 비강체 변환은 의료 영상이나 동적 객체 모델링과 같이 더 복잡하고 자유도가 높은 문제를 다루기 위해 필수적이다.20</p>
<h2>2.  고전적 정합 방법론 분석</h2>
<p>포인트클라우드 정합 기술의 발전은 고전적 방법론의 토대 위에서 이루어졌다. 이 방법론들은 기하학적 원리와 최적화 이론에 깊이 뿌리내리고 있으며, 오늘날에도 여전히 많은 시스템에서 핵심적인 역할을 수행한다. 특히 Iterative Closest Point (ICP) 알고리즘과 특징점 기반의 Coarse-to-Fine 전략은 이 분야의 근간을 이룬다.</p>
<h3>2.1  Iterative Closest Point (ICP) 알고리즘 심층 분석</h3>
<p>ICP 알고리즘은 1992년 Besl과 McKay, 그리고 Chen과 Medioni에 의해 거의 동시에 독립적으로 제안된 이래로, 포인트클라우드 정밀 정합(fine registration)의 표준으로 자리 잡았다.25 이 알고리즘의 핵심 철학은 두 포인트클라우드가 이미 어느 정도 근접하게 정렬되어 있다는 가정 하에, 반복적인 최적화를 통해 정렬 오차를 점진적으로 최소화하는 것이다.</p>
<h4>2.1.1 핵심 원리 및 단계</h4>
<p>ICP는 두 가지 핵심 단계를 반복적으로 수행하는 구조를 가진다.18</p>
<ol>
<li>
<p><strong>대응점 탐색 (Correspondence Search):</strong> 소스 포인트클라우드 <span class="math math-inline">P</span>의 각 점 <span class="math math-inline">p_i</span>에 대해, 현재의 변환을 적용한 위치 <span class="math math-inline">T_k(p_i)</span>에서 가장 가까운 타겟 포인트클라우드 <span class="math math-inline">Q</span>의 점 <span class="math math-inline">q_i</span>를 찾는다. 이 과정은 전체 점들에 대해 수행될 경우 계산 비용이 높기 때문에, 일반적으로 k-d tree와 같은 공간 분할 자료구조를 사용하여 최근접 이웃 탐색(Nearest Neighbor Search)을 가속화한다.28</p>
</li>
<li>
<p><strong>변환 추정 (Transformation Estimation):</strong> 1단계에서 찾은 모든 대응점 쌍 <span class="math math-inline">(p_i, q_i)</span>에 대해, 이들 간의 평균 제곱 오차(Mean Squared Error, MSE)를 최소화하는 최적의 강체 변환(<span class="math math-inline">R_{k+1}, t_{k+1}</span>)을 계산한다. 이 최적화 문제는 다음과 같이 정의된다.<br />
<span class="math math-display">
(R_{k+1}, t_{k+1}) = \arg\min_{R, t} \sum_{i} \lVert (R p_i + t) - q_i \rVert^2
</span><br />
이 문제는 특이값 분해(Singular Value Decomposition, SVD)나 쿼터니언(Quaternion) 기반 방법을 통해 해석적 해(closed-form solution)를 효율적으로 구할 수 있다.14</p>
</li>
<li>
<p><strong>변환 적용 및 수렴 확인 (Update and Convergence Check):</strong> 새로 계산된 변환 <span class="math math-inline">T_{k+1}</span>을 소스 포인트클라우드에 적용하여 위치를 갱신한다. 그 후, 이전 단계와의 변환 행렬 변화량이나 MSE 값의 변화량이 미리 설정된 임계값보다 작아지거나 최대 반복 횟수에 도달하면 알고리즘을 종료한다. 그렇지 않으면 1단계로 돌아가 과정을 반복한다.25</p>
</li>
</ol>
<h4>2.1.2 주요 변형 알고리즘 비교</h4>
<p>기본적인 ICP(Point-to-Point)는 특정 상황에서 한계를 보이며, 이를 개선하기 위한 다양한 변형 알고리즘이 제안되었다.</p>
<ul>
<li>
<p><strong>Point-to-Point ICP:</strong> 가장 기본적인 형태로, 두 대응점 사이의 유클리드 거리를 직접 최소화한다.25 구현이 간단하지만, 대응점 탐색이 표면의 기하학적 구조를 고려하지 않기 때문에 평평하거나 특징이 없는 표면에서는 점들이 표면을 따라 미끄러지는 현상이 발생하여 수렴이 느리거나 잘못된 지역 최적해에 빠질 수 있다.33</p>
</li>
<li>
<p><strong>Point-to-Plane ICP:</strong> 이 변형은 대응점 탐색의 한계를 극복하기 위해 제안되었다. 소스 포인트 <span class="math math-inline">p_i</span>와 타겟 포인트 <span class="math math-inline">q_i</span> 사이의 거리를 최소화하는 대신, 변환된 소스 포인트 <span class="math math-inline">T(p_i)</span>와 타겟 포인트 <span class="math math-inline">q_i</span>에서의 접평면(tangent plane) 사이의 거리를 최소화한다.26 이 접평면은 <span class="math math-inline">q_i</span>의 법선 벡터 <span class="math math-inline">n_i</span>에 의해 정의된다. 목적 함수는 다음과 같다.<br />
<span class="math math-display">
(R, t) = \arg\min_{R, t} \sum_{i} ( (R p_i + t - q_i) \cdot n_i )^2
</span><br />
이 방식은 점들이 평면을 따라 자유롭게 미끄러질 수 있도록 허용하여, 구조적인 환경(예: 건물, 실내)에서 Point-to-Point 방식보다 훨씬 빠른 수렴 속도와 높은 정확도를 보인다.33</p>
</li>
<li>
<p><strong>Generalized-ICP (G-ICP):</strong> G-ICP는 Point-to-Point와 Point-to-Plane을 확률론적 프레임워크로 통합한 알고리즘이다.38 이 방법은 소스 및 타겟 포인트클라우드 양쪽 모두에서 점 주변의 표면 구조를 공분산 행렬로 모델링하고, 두 점이 동일한 평면에서 샘플링되었다는 가정 하에 ‘plane-to-plane’ 오차를 최소화한다.<br />
<span class="math math-display">
d_i^T (C_i^Q + T C_i^P T^T)^{-1} d_i \quad \text{where} \quad d_i = q_i - T p_i
</span><br />
여기서 <span class="math math-inline">C_i^P</span>와 <span class="math math-inline">C_i^Q</span>는 각각 <span class="math math-inline">p_i</span>와 <span class="math math-inline">q_i</span> 주변 점들의 공분산 행렬이다. 이 방식은 두 스캔 데이터의 불확실성을 모두 고려하므로 노이즈가 많거나 희소한 데이터에 대해 더 강건한 성능을 제공한다.38</p>
</li>
</ul>
<h4>2.1.3 ICP의 본질적 한계와 개선 방안</h4>
<p>ICP는 강력한 알고리즘이지만 몇 가지 근본적인 한계를 내포하고 있다.</p>
<ul>
<li><strong>지역 최적해 (Local Minima):</strong> ICP의 목적 함수는 비볼록(non-convex) 형태이므로, 경사 하강법 기반의 최적화는 전역 최적해를 보장하지 못한다. 초기 정렬 상태가 전역 최적해의 수렴 영역(basin of convergence)에서 멀리 떨어져 있으면, 알고리즘은 잘못된 지역 최적해에 수렴하게 된다.27</li>
<li><strong>초기 정렬 의존성:</strong> 위와 같은 이유로, ICP는 성공적인 정합을 위해 두 포인트클라우드가 사전에 어느 정도 정렬되어 있어야 한다는 강한 제약을 가진다.44 이 문제를 해결하기 위해 ICP 적용 전에 대략적인 정렬을 수행하는 ‘Coarse-to-Fine’ 전략이 필수적으로 사용된다.</li>
<li><strong>이상치 및 부분 중첩 민감도:</strong> 대응점 탐색 시, 이상치(outliers)나 두 클라우드가 겹치지 않는 영역의 점들은 잘못된 대응 관계를 형성하여 변환 추정 과정을 심각하게 왜곡시킨다.27 이를 완화하기 위해 대응점 쌍 중 거리가 먼 일부를 제거하는 Trimmed ICP 25나, 대응점 쌍의 품질을 평가하여 가중치를 부여하는 방법, 또는 RANSAC과 같은 강건한 추정 기법을 결합하는 방식이 제안되었다.48</li>
<li><strong>계산 비용:</strong> 포인트의 수가 수백만 개에 이르는 대규모 데이터셋의 경우, 모든 점에 대해 최근접 이웃을 탐색하는 것은 막대한 계산 비용을 초래한다. 이를 해결하기 위해 전체 포인트의 일부를 무작위 또는 균일하게 샘플링하거나 48, k-d tree와 같은 효율적인 자료구조를 최적화하여 사용한다.45</li>
</ul>
<h3>2.2  특징점 기반 정합 파이프라인</h3>
<p>ICP의 초기 정렬 의존성 문제를 해결하기 위해, 대부분의 실용적인 정합 시스템은 <strong>Coarse-to-Fine (대략적 정합에서 정밀 정합으로)</strong> 전략을 채택한다.1 이 전략은 먼저 계산 비용이 높더라도 전역적인 관점에서 대략적인 정렬(coarse registration)을 수행하여 좋은 초기 변환을 찾고, 이어서 ICP와 같은 알고리즘을 사용하여 정밀하게 미세 조정(fine registration)하는 2단계 접근법이다.53 이 중 Coarse Registration은 주로 특징점 기반 파이프라인을 통해 이루어진다.</p>
<p>이러한 접근 방식은 탐색의 ’전역성(globality)’과 ‘정밀성(precision)’ 사이의 균형을 체계적으로 해결하는 전략적 프레임워크로 볼 수 있다. Coarse 단계는 모든 가능한 변환이라는 거대한 탐색 공간에서 가능성이 높은 작은 영역으로 문제를 좁히는 과정이며, 일단 탐색 공간이 충분히 좁혀지면 Fine 단계에서 효율적인 지역 최적화 알고리즘으로 정밀한 해를 찾는다. 이 프레임워크의 성공은 각 단계가 후속 단계의 실패 확률을 얼마나 효과적으로 줄여주는지에 달려 있다.</p>
<h4>2.2.1 단계: 핵심점 검출 (Keypoint Detection)</h4>
<p>전체 포인트클라우드에서 정합의 기준이 될 만한 소수의 안정적이고 독특한 점, 즉 핵심점(keypoint)을 식별하는 단계이다. 모든 점을 사용하는 대신 소수의 핵심점만을 사용함으로써 계산 효율성을 크게 높일 수 있다. 핵심점은 주변의 기하학적 구조가 복잡하여 반복적으로 검출 가능하고 위치가 명확해야 한다. 대표적인 알고리즘으로는 3D Harris 코너 검출기, ISS(Intrinsic Shape Signatures) 등이 있다.56</p>
<h4>2.2.2 단계: 3D 지역 특징 기술자 (3D Local Feature Descriptors)</h4>
<p>각 핵심점 주변의 지역적인 기하학적 정보를 고유한 수치 벡터, 즉 기술자(descriptor)로 인코딩하는 단계이다. 좋은 기술자는 회전 및 이동과 같은 강체 변환에 대해 불변(invariant)해야 하며, 다른 위치의 기하학적 구조와는 명확히 구분되는 변별력(descriptiveness)을 가져야 한다.59</p>
<ul>
<li><strong>FPFH (Fast Point Feature Histograms):</strong> Point Feature Histogram (PFH)의 계산 복잡도를 <span class="math math-inline">O(nk^2)</span>에서 <span class="math math-inline">O(nk)</span>로 크게 개선한 기술자이다.62 먼저 각 점</li>
</ul>
<p><span class="math math-inline">p</span>에 대해, 그 이웃 점들과의 관계(법선 벡터 간의 각도, 거리 등)를 계산하여 단순화된 히스토그램(SPFH)을 만든다. 그 후, <span class="math math-inline">p</span>의 이웃 점들의 SPFH를 가중 평균하여 최종 FPFH를 계산한다. 이 2단계 구조는 정보 손실을 최소화하면서 계산 속도를 크게 향상시킨다.59 PCL, Open3D 등 주요 라이브러리에서 표준 기술자로 널리 사용된다.</p>
<ul>
<li><strong>SHOT (Signatures of Histograms of OrienTations):</strong> FPFH보다 더 높은 변별력과 강인성을 목표로 설계된 기술자이다.65 핵심점 주변에 지역 기준 좌표계(Local Reference Frame, LRF)를 설정하여 회전 불변성을 확보한다. 그 후, 이 좌표계를 기준으로 공간을 여러 구역으로 나누고, 각 구역에 속한 점들의 법선 벡터와 중심점 법선 벡터 간의 각도 분포를 히스토그램으로 만들어 모두 이어 붙인다. 이 구조는 시그니처(공간 구조)와 히스토그램(통계적 강인성)의 장점을 결합한 형태이다.59</li>
</ul>
<p>이러한 고전적 방법론의 발전 과정은 정합 문제에 사용되는 ’정보의 질’을 점진적으로 높이는 방향으로 이루어졌다. Point-to-Point ICP는 단순히 ‘거리’ 정보만을 사용했지만 25, Point-to-Plane ICP는 ‘표면의 방향’(법선 벡터) 정보를 추가하여 제약 조건을 강화했다.26 G-ICP는 양쪽 클라우드의 ’표면 불확실성’까지 고려했으며 38, 특징점 기반 방법은 한 걸음 더 나아가 ’지역 기하학의 고유성’을 FPFH나 SHOT 같은 기술자로 인코딩하여 정보의 질을 극대화했다.64 이는 정합 문제의 본질이 더 풍부하고 변별력 있는 정보를 사용하여 대응점 탐색의 모호성을 줄이는 과정임을 명확히 보여준다.</p>
<h4>2.2.3 단계: 특징 매칭 및 이상치 제거 (Feature Matching and Outlier Rejection)</h4>
<p>계산된 기술자들을 기반으로 소스 포인트클라우드의 핵심점과 타겟 포인트클라우드의 핵심점 간의 잠정적인 대응 관계(putative correspondences)를 설정한다. 이는 보통 기술자 벡터 공간에서 최근접 이웃을 찾는 방식으로 이루어진다. 하지만 기술자의 변별력 한계와 장면의 반복적인 구조 때문에 이 단계에서 생성된 대응 관계에는 수많은 잘못된 매칭, 즉 이상치(outliers)가 포함된다.68</p>
<ul>
<li><strong>RANSAC (Random Sample Consensus):</strong> 이 문제를 해결하기 위한 가장 대표적인 강건한 추정(robust estimation) 알고리즘이다.45 RANSAC의 작동 원리는 ’대부분의 데이터는 소수의 정상치(inliers)와 다수의 이상치로 구성된다’는 가정에 기반한다.70</li>
</ul>
<ol>
<li>
<p>잠정적인 대응점 집합에서 변환을 계산하는 데 필요한 최소한의 샘플(강체 변환의 경우 3쌍)을 무작위로 선택한다.</p>
</li>
<li>
<p>이 샘플을 이용해 변환 행렬 가설(hypothesis)을 계산한다.</p>
</li>
<li>
<p>전체 대응점 집합에 이 변환 가설을 적용하여, 가설을 지지하는 대응점(즉, 변환 후 오차가 특정 임계값 이내인 쌍), 즉 인라이어(inliers)의 수를 센다.</p>
</li>
<li>
<p>위 과정을 여러 번 반복하여 가장 많은 인라이어를 확보한 변환 가설을 최적의 모델로 채택한다.</p>
</li>
</ol>
<p>RANSAC은 이상치 비율이 90%를 넘는 극한 상황에서도 안정적으로 동작할 수 있는 강력한 방법이지만, 확률에 기반하므로 최적해를 보장하지는 않으며, 반복 횟수와 인라이어 임계값 설정에 따라 성능과 계산 비용이 크게 달라진다.57</p>
<h3>2.3  확률론적 접근법: Coherent Point Drift (CPD)</h3>
<p>ICP와 특징점 기반 방법 외에, 정합 문제를 다른 관점에서 접근하는 확률론적 방법론도 존재한다. 그중 가장 대표적인 것이 Coherent Point Drift (CPD)이다.27</p>
<p>CPD는 정합을 확률 밀도 추정 문제로 재정의한다. 한 포인트클라우드(예: 타겟 <span class="math math-inline">Q</span>)를 데이터 포인트로 보고, 다른 포인트클라우드(예: 소스 <span class="math math-inline">P</span>)를 가우시안 혼합 모델(Gaussian Mixture Model, GMM)의 중심(centroid)으로 간주한다. 정합 과정은 데이터 포인트 <span class="math math-inline">Q</span>가 주어졌을 때 GMM의 우도(likelihood)를 최대화하도록 GMM의 중심 위치를 이동시키는 과정으로 모델링된다.27</p>
<p>ICP가 각 소스 포인트에 대해 가장 가까운 단 하나의 타겟 포인트를 대응시키는 ‘하드 할당(hard assignment)’ 방식을 사용하는 반면, CPD는 각 데이터 포인트가 모든 GMM 중심에 속할 확률을 계산하는 ‘소프트 할당(soft assignment)’ 방식을 사용한다. 이 확률론적 접근법 덕분에 CPD는 노이즈, 이상치, 그리고 데이터 누락에 대해 ICP보다 본질적으로 더 강건한 특성을 보인다.27</p>
<p>또한 CPD는 GMM 중심들이 하나의 그룹으로서 위상 구조를 유지하며 일관성 있게 움직이도록 하는 ’모션 일관성 제약(motion coherence constraint)’을 도입한다. 이 제약을 통해 부드러운 비강체 변환까지 자연스럽게 모델링할 수 있어, 변형 가능한 객체의 정합에도 효과적으로 적용될 수 있다.74</p>
<table><thead><tr><th>특징 기술자 (Descriptor)</th><th>핵심 원리</th><th>기술자 차원</th><th>계산 복잡도</th><th>강인성 (Robustness)</th><th>변별력 (Descriptiveness)</th></tr></thead><tbody>
<tr><td><strong>Spin Images</strong> 59</td><td>점의 법선 벡터를 축으로 하여 주변 점들을 2D 원통 좌표계에 투영, 2D 히스토그램(이미지) 생성</td><td>높음 (예: 153)</td><td>높음</td><td>노이즈에 강건하나, 점 밀도 변화에 민감</td><td>중간</td></tr>
<tr><td><strong>FPFH</strong> 59</td><td>점과 이웃 간의 기하학적 관계(3개 각도)를 SPFH로 계산 후, 이웃의 SPFH를 가중 평균하여 최종 히스토그램 생성</td><td>낮음 (33)</td><td>낮음 (<span class="math math-inline">O(nk)</span>)</td><td>점 밀도 변화와 노이즈에 다소 민감</td><td>중간</td></tr>
<tr><td><strong>SHOT</strong> 59</td><td>LRF 설정 후, 구형 그리드 내 각 구역의 법선 벡터 방향 히스토그램을 결합</td><td>높음 (예: 352)</td><td>중간</td><td>노이즈, 점 밀도 변화, 폐색에 강건</td><td>높음</td></tr>
</tbody></table>
<p><em>Table 1: 고전적 3D 지역 특징 기술자 비교</em></p>
<h2>3.  딥러닝 기반 정합 방법론의 부상</h2>
<p>지난 10년간 딥러닝 기술이 컴퓨터 비전 분야에 혁명을 일으키면서, 포인트클라우드 정합 분야 역시 근본적인 패러다임 전환을 맞이했다. 고전적인 방법론이 기하학적 원리와 수동으로 설계된 특징에 의존했던 반면, 딥러닝 기반 접근법은 대규모 데이터로부터 정합에 유용한 특징과 패턴을 직접 학습한다.</p>
<h3>3.1  학습 기반 접근법의 패러다임 전환</h3>
<p>딥러닝 기반 정합 방법론은 기존의 한계를 극복하기 위한 새로운 가능성을 제시하며 빠르게 주류로 부상했다.</p>
<h4>3.1.1 장점</h4>
<ol>
<li><strong>데이터 기반 특징 학습 (Data-Driven Feature Learning):</strong> 딥러닝 모델, 특히 3D 컨볼루션 신경망(CNN)이나 PointNet과 같은 아키텍처는 포인트클라우드의 원시 데이터로부터 직접적으로 강건하고 변별력 있는 특징을 학습할 수 있다.6 이는 FPFH나 SHOT과 같이 수동으로 설계된 특징(hand-crafted features)이 특정 상황이나 데이터 유형에서 보이는 성능적 한계를 극복하게 해준다.59</li>
<li><strong>End-to-End 학습 (End-to-End Learning):</strong> 전통적인 파이프라인이 특징 추출, 매칭, 이상치 제거, 변환 추정 등 여러 독립적인 단계로 구성된 반면, 많은 딥러닝 모델은 이 모든 과정을 하나의 네트워크 안에서 통합하여 종단간(end-to-end)으로 학습한다.79 이를 통해 최종 목표인 정확한 변환 추정에 모든 구성 요소가 최적화되어 상호 보완적인 성능 향상을 이끌어낼 수 있다.</li>
<li><strong>강인성 및 속도 (Robustness and Speed):</strong> 딥러닝 모델은 대규모 데이터를 통해 노이즈, 이상치, 낮은 중첩률과 같은 다양한 열악한 조건에 대응하는 법을 학습한다. 그 결과, 많은 벤치마크에서 고전적 방법보다 뛰어난 강인성을 보인다.81 또한, 학습이 완료된 후 추론(inference) 단계에서는 GPU 가속을 통해 매우 빠른 속도로 변환을 추정할 수 있어 실시간 응용에 유리하다.83</li>
</ol>
<h4>3.1.2 단점</h4>
<ol>
<li><strong>일반화 성능 (Generalization Performance):</strong> 딥러닝 모델의 가장 큰 약점은 학습 데이터에 과적합(overfitting)될 수 있다는 점이다. 특정 데이터셋(예: 실내 가구)으로 학습된 모델은 완전히 다른 도메인(예: 실외 자율주행)의 데이터나 다른 종류의 센서로 취득된 데이터(cross-source)에 대해서는 성능이 급격히 저하될 수 있다.84</li>
<li><strong>데이터 의존성 (Data Dependency):</strong> 모델을 효과적으로 학습시키기 위해서는 정확한 정답 변환(ground truth transformation)이 레이블링된 대규모의 고품질 데이터셋이 필요하다. 이러한 데이터셋을 구축하는 것은 막대한 시간과 비용을 요구하는 어려운 작업이다.84</li>
<li><strong>해석 가능성 부족 (Lack of Interpretability):</strong> 대부분의 딥러닝 모델은 “블랙박스“처럼 동작하여, 왜 특정 정합 결과가 도출되었는지 또는 왜 실패했는지를 직관적으로 이해하기 어렵다. 이는 시스템의 신뢰성 검증이나 디버깅을 어렵게 만드는 요인이 된다.</li>
</ol>
<p>이러한 패러다임의 변화는 정합 문제를 ’특징 표현 학습’에서 ‘관계 추론’ 문제로 재정의하는 과정으로 볼 수 있다. 초기 딥러닝 접근법은 고전적 파이프라인의 ‘특징 기술자’ 부분을 더 나은 학습 기반 기술자로 대체하는 데 중점을 두었지만 88, 최신 모델들은 두 형상 간의 구조적 유사성과 전역적인 기하학적 ’관계’를 직접 추론하는 방향으로 발전하고 있다.</p>
<h3>3.2  주요 딥러닝 아키텍처 분석</h3>
<p>딥러닝 기반 정합 방법은 크게 대응점 탐색 여부에 따라 두 가지 범주로 나눌 수 있다.</p>
<h4>3.2.1 대응점 없는 (Correspondence-Free) 접근법</h4>
<p>이 접근법은 두 포인트클라우드 간의 명시적인 점대점 대응 관계를 찾는 과정을 생략하고, 전체 포인트클라우드를 표현하는 전역 특징(global feature)을 직접 정렬한다.</p>
<ul>
<li><strong>PointNetLK:</strong> 이 방법은 딥러닝과 고전 컴퓨터 비전 알고리즘의 영리한 결합을 보여준다. PointNet 90을 사용하여 소스 및 타겟 포인트클라우드를 각각 고정된 차원(예: 1024차원)의 전역 특징 벡터로 인코딩한다. 그 후, 두 특징 벡터 간의 차이를 최소화하는 강체 변환을 고전적인 Lucas-Kanade (LK) 알고리즘 91을 통해 반복적으로 추정한다. 점대점 대응 관계를 계산할 필요가 없어 계산적으로 매우 효율적이며, 학습된 특징 표현 덕분에 노이즈에 강건하다.59</li>
</ul>
<h4>3.2.2 대응점 기반 (Correspondence-Based) 접근법</h4>
<p>이 접근법은 딥러닝을 사용하여 더 정확하고 강건한 대응 관계를 찾은 후, 이를 기반으로 변환을 추정한다.</p>
<ul>
<li><strong>Deep Closest Point (DCP):</strong> DCP는 ICP 파이프라인의 각 단계를 딥러닝 모듈로 대체한 End-to-End 아키텍처이다.94 먼저 PointNet이나 DGCNN과 같은 네트워크를 사용하여 각 점의 지역적 기하학 정보를 포함하는 고차원 특징 벡터를 추출한다. 그 후, Transformer에서 영감을 받은 어텐션(attention) 모듈을 사용하여 두 포인트클라우드의 특징 벡터들 간의 유사도를 계산하고, 이를 통해 부드러운 대응 확률 행렬(soft correspondence matrix)을 생성한다. 마지막으로, 이 확률 행렬을 가중치로 사용하여 대응점 쌍의 좌표를 가중 평균하고, 미분 가능한 SVD 계층(differentiable SVD layer)을 통해 최적의 강체 변환을 계산한다.59 이 모든 과정이 미분 가능하여 종단간 학습이 가능하다는 것이 핵심적인 장점이다.</li>
</ul>
<h4>3.2.3 Transformer 기반 모델</h4>
<p>최근에는 자연어 처리 분야에서 큰 성공을 거둔 Transformer 아키텍처가 포인트클라우드 정합에도 활발히 도입되고 있다. 어텐션 메커니즘을 통해 점들 간의 장거리 의존성 및 전역적인 컨텍스트를 효과적으로 모델링할 수 있기 때문이다.</p>
<ul>
<li>
<p><strong>Predator:</strong> 이 모델은 저중첩(low-overlap) 시나리오에 초점을 맞춘다. 핵심 아이디어는 두 포인트클라우드 간의 중첩 영역을 예측하는 잠재적 중첩 어텐션(latent overlap attention) 모듈을 도입한 것이다. 이 모듈을 통해 네트워크는 중첩될 가능성이 높은 영역의 점들에 더 집중하여 특징을 학습하고 대응점을 찾는다. 그 결과, 중첩률이 10-30%에 불과한 매우 어려운 3DLoMatch 벤치마크에서 기존 방법들을 압도하는 성능을 보였다.97</p>
</li>
<li>
<p><strong>REGTR (Registration Transformer):</strong> REGTR은 RANSAC과 같은 후처리 과정 없이 Transformer를 사용하여 직접적으로 깨끗한 대응점 집합을 예측하는 것을 목표로 한다.80 KPConv를 백본으로 사용하여 특징을 추출한 후, 여러 층의 Transformer Cross-Encoder를 통과시킨다. 이 과정에서 Self-Attention과 Cross-Attention을 통해 두 포인트클라우드 간의 정보가 교환되고 정제된다. 최종적으로 네트워크는 각 점의 대응점 위치와 중첩 확률을 직접 예측하며, 이를 기반으로 SVD를 통해 변환을 계산한다.101</p>
</li>
<li>
<p>GeoTransformer: GeoTransformer는 Transformer가 포인트클라우드의 기하학적 구조를 명시적으로 활용하지 못한다는 점에 착안했다. 이 모델은 점 쌍 간의 거리(distance)와 세 점이 이루는 각도(angle)와 같이 강체 변환에 불변인 기하학적 정보를 Transformer의 어텐션 메커</p>
</li>
</ul>
<p>니즘에 직접 주입한다. 이를 통해 네트워크는 변환에 불변인 기하학적 구조를 학습하게 되어, 초기 정렬 상태가 매우 다르거나 중첩률이 낮은 경우에도 매우 강건하고 정확한 매칭을 수행할 수 있다. GeoTransformer가 생성하는 대응 관계는 매우 정확하여 RANSAC 없이도 바로 변환을 추정할 수 있어, 추론 속도가 매우 빠르다.103</p>
<h3>3.3  고전적 방법론과 딥러닝의 성능 비교</h3>
<p>딥러닝 기반 방법론의 등장은 정합 성능을 새로운 차원으로 끌어올렸다.</p>
<ul>
<li><strong>정확도 및 강인성:</strong> 표준 벤치마크 데이터셋, 특히 저중첩 시나리오인 3DLoMatch에서 Predator, GeoTransformer와 같은 최신 딥러닝 모델들은 FPFH+RANSAC과 같은 고전적 베이스라인의 등록 성공률(Registration Recall)을 20-30% 이상 크게 상회하는 SOTA(State-of-the-Art) 성능을 기록했다.85 이는 딥러닝 모델이 데이터로부터 학습한 사전 지식(prior)을 통해 어려운 상황에서도 강건하게 대응 관계를 추론할 수 있음을 보여준다.</li>
<li><strong>속도:</strong> 학습된 딥러닝 모델은 추론 시, 특히 RANSAC과 같은 반복적인 샘플링 과정을 제거한 End-to-End 모델의 경우, 수십 밀리초 내에 정합을 완료할 수 있다.104 이는 수 초에서 수 분이 소요될 수 있는 고전적인 전역 정합 방법에 비해 월등히 빠른 속도로, 실시간 응용에 결정적인 이점을 제공한다.83</li>
<li><strong>일반화:</strong> 그럼에도 불구하고, 딥러닝 모델의 일반화 능력은 여전히 중요한 과제이다. 학습 데이터와 통계적 분포가 크게 다른 미지의 환경이나 센서 데이터에 대해서는 성능이 저하될 수 있다.84 반면, 고전적 방법은 특정 데이터에 대한 가정이 없으므로 이러한 상황에서 더 예측 가능하고 안정적인 성능을 보일 수 있다. 이 때문에 실제 산업 현장에서는 여전히 고전적 방법과 딥러닝 방법을 결합한 하이브리드 접근법이 선호되기도 한다.91</li>
</ul>
<p>결론적으로, 딥러닝과 고전적 방법은 대립 관계가 아닌 상호 보완적인 관계에 있다. 딥러닝은 데이터로부터 강력한 사전 지식을 학습하여 좋은 초기 해를 빠르게 찾는 데 뛰어나지만 일반화에 한계가 있을 수 있다. 반면, 고전적 방법(특히 ICP)은 기하학적 원리에 기반하여 주어진 초기 해로부터 정밀한 최적화를 수행하는 데 강하지만 좋은 초기 해가 없으면 실패한다. 따라서 가장 강력한 시스템은 딥러닝으로 Coarse Registration을 수행하여 강건하고 빠른 초기 정렬을 달성한 후, ICP로 Fine Registration을 수행하여 최종 정밀도를 보장하는 하이브리드 형태일 가능성이 높다. 이는 딥러닝의 ’학습 능력’과 고전적 방법의 ’기하학적 정밀성’을 모두 활용하는 최적의 전략이다.</p>
<table><thead><tr><th>알고리즘</th><th>패러다임</th><th>핵심 아이디어</th><th>장점</th><th>단점</th><th>주요 적용 분야</th></tr></thead><tbody>
<tr><td><strong>ICP (Point-to-Plane)</strong> 26</td><td>고전적 (최적화)</td><td>대응점과 타겟 표면의 접평면 간 거리를 반복적으로 최소화</td><td>높은 정밀도, 수학적 원리 명확</td><td>지역 최적해, 초기 정렬에 매우 민감, 이상치에 취약</td><td>정밀 정합 (Fine Registration), 3D 모델링</td></tr>
<tr><td><strong>FPFH + RANSAC</strong> 64</td><td>고전적 (특징 기반)</td><td>FPFH 특징으로 대응점 탐색 후, RANSAC으로 이상치를 제거하며 변환 추정</td><td>초기 정렬 불필요, 이상치에 강건</td><td>계산 비용 높음, 특징 변별력 한계, 저중첩에 취약</td><td>초기 정합 (Coarse Registration), SLAM</td></tr>
<tr><td><strong>PointNetLK</strong> 91</td><td>딥러닝 (대응점 없음)</td><td>PointNet으로 전역 특징 추출 후, LK 알고리즘으로 특징 공간에서 정렬</td><td>매우 빠름, End-to-End, 대응점 탐색 불필요</td><td>큰 변환에 취약, 정밀도 한계</td><td>빠른 초기 정렬, 로봇 비전</td></tr>
<tr><td><strong>DCP</strong> 94</td><td>딥러닝 (대응점 기반)</td><td>어텐션으로 소프트 대응 관계 학습 후, 미분 가능한 SVD로 변환 추정</td><td>End-to-End, 노이즈에 강건</td><td>저중첩 성능 저하, 복잡한 모델 구조</td><td>객체 포즈 추정, 3D 재구성</td></tr>
<tr><td><strong>GeoTransformer</strong> 103</td><td>딥러닝 (대응점 기반)</td><td>Transformer에 변환 불변 기하학적 정보(거리, 각도)를 인코딩하여 매칭</td><td>저중첩에 매우 강건, 높은 정확도, RANSAC 불필요</td><td>학습 데이터 의존성, 대규모 메모리 요구</td><td>자율주행, 저중첩 장면 재구성</td></tr>
</tbody></table>
<p><em>Table 2: 주요 정합 알고리즘 비교</em></p>
<h2>4.  고급 정합 문제와 해결 과제</h2>
<p>강체(rigid) 가정이 성립하고 데이터 품질이 좋은 이상적인 환경을 넘어, 실제 세계의 정합 문제는 훨씬 더 복잡하고 다양한 도전을 제기한다. 객체가 변형되거나, 환경 내에 움직이는 요소가 존재하거나, 데이터 자체에 심각한 결함이 있는 경우가 이에 해당한다. 이러한 고급 정합 문제를 해결하기 위한 연구는 현재 가장 활발하게 진행되는 분야 중 하나이다.</p>
<h3>4.1  비강체 및 변형 가능 객체 정합</h3>
<p>비강체 정합(Non-rigid registration)은 객체의 형태가 변형(deformation)되는 상황에서 두 포인트클라우드를 정렬하는 문제이다.108 강체 변환이 6개의 파라미터(6-DoF)로 전체 클라우드를 한번에 움직이는 것과 달리, 비강체 정합은 각 점 또는 점의 국소 영역이 독립적인 움직임을 가질 수 있어 이론적으로 무한한 자유도를 가진다. 이로 인해 문제는 훨씬 더 복잡하고 ill-posed(정의가 불안정한) 특성을 띤다.20 따라서 모든 비강체 정합 알고리즘의 성공은 ‘물리적으로 타당한’ 변형을 어떻게 수학적으로 제약하는가에 달려있다.</p>
<h4>4.1.1 고전적 접근법</h4>
<ul>
<li><strong>Coherent Point Drift (CPD):</strong> 앞서 언급했듯이, CPD는 GMM 중심들이 그룹으로서 일관성 있게 움직이도록 하는 모션 일관성 제약을 통해 부드러운 변형을 모델링하는 데 강점을 보인다.73 이는 변형을 표현하는 모델 자체가 강력한 정규화(regularization) 역할을 수행하는 대표적인 예이다.</li>
<li><strong>Embedded Deformation Graph:</strong> 객체를 노드와 엣지로 구성된 그래프로 표현하고, 각 노드에 아핀 변환을 할당하여 전체적인 변형을 제어한다. 주변 노드 간의 변환이 유사하도록 제약을 가함으로써 국소적인 강성(local rigidity)을 유지하며 부드러운 변형을 유도한다. 이 모델은 Non-rigid ICP (N-ICP) 알고리즘을 통해 효율적으로 최적화될 수 있다.110</li>
</ul>
<h4>4.1.2 딥러닝 기반 접근법</h4>
<ul>
<li><strong>Scene Flow Estimation:</strong> 두 시점의 포인트클라우드 간에 각 점이 어떻게 이동했는지를 나타내는 3D 모션 벡터 필드, 즉 Scene Flow를 추정하는 방식이다.111 이는 강체 운동과 비강체 변형을 모두 포함하는 가장 일반적인 움직임 표현 방식이다. FlowNet3D 113와 같은 딥러닝 모델들은 두 포인트클라우드를 입력받아 각 점의 변위 벡터를 직접 예측한다. 이 방식은 매우 유연하지만, 점대점 대응 관계를 정확히 추적하기 어렵고, 폐색(occlusion)이나 밀도 변화에 민감하며, 계산 비용이 높다는 단점이 있다.111</li>
<li><strong>Neural Deformation Fields:</strong> MLP(다층 퍼셉트론)와 같은 신경망을 사용하여 3D 공간 자체를 변형시키는 연속적인 함수, 즉 변형장(deformation field)을 학습하는 접근법이다. 임의의 3D 좌표를 입력하면 해당 위치의 변위 벡터를 출력하는 함수를 학습함으로써, 포인트클라우드를 연속적인 표면처럼 다룰 수 있다. Neural Deformation Pyramid (NDP)는 이러한 변형을 전역적인 강체 움직임부터 국소적인 미세 변형까지 계층적으로 분해하여 모델링함으로써, 복잡한 변형을 효율적이고 안정적으로 학습한다.20</li>
</ul>
<p>이러한 비강체 정합 기술은 의료 영상 분야에서 수술 전후의 장기 형태 변화를 추적하거나 117, 컴퓨터 그래픽스에서 실제 배우의 얼굴 표정을 3D 아바타에 옮기는 퍼포먼스 캡처 등에서 핵심적인 역할을 한다.21</p>
<h3>4.2  동적 환경에서의 정합</h3>
<p>자율주행이나 모바일 로보틱스와 같이 주변 환경이 정적이지 않은 경우, 움직이는 객체(다른 차량, 보행자 등)는 정합 과정에서 심각한 오류를 유발하는 주요 원인이 된다.13 정적인 배경을 기준으로 센서의 움직임을 추정해야 하는데, 동적 객체들이 잘못된 대응 관계를 형성하여 전체 정합 결과를 왜곡시키기 때문이다.</p>
<h4>4.2.1 해결 방안: 의미론적 정보 활용 (Semantic-ICP)</h4>
<p>이 문제를 해결하기 위해, 정합은 순수한 기하학적 문제를 넘어 장면을 이해하는 인식(perception) 문제로 확장된다. 딥러닝 기반의 의미론적 분할(semantic segmentation) 기술을 통해 포인트클라우드의 각 점에 ‘도로’, ‘건물’, ‘차량’, ’보행자’와 같은 의미론적 레이블을 부여한다.13 그 후, ’차량’이나 ’보행자’와 같이 움직일 가능성이 높은 카테고리에 속하는 점들을 정합 과정에서 제외하거나 가중치를 낮춘다. 반면, ’건물’이나 ’도로’와 같이 정적일 가능성이 높은 객체들의 점들만을 사용하여 강건하게 변환을 추정한다. 이 Semantic-ICP 접근법은 동적 환경에서의 SLAM 및 위치 추정 성능을 획기적으로 향상시킨다.122</p>
<h4>4.2.2 다중 센서 융합 (Multi-Sensor Fusion)</h4>
<p>단일 센서의 한계를 극복하기 위해 여러 종류의 센서 데이터를 융합하는 것 또한 동적 환경에서의 강인성을 높이는 중요한 전략이다.</p>
<ul>
<li><strong>IMU (관성 측정 장치) 융합:</strong> IMU는 매우 높은 주파수(100Hz 이상)로 각속도와 가속도를 측정한다. 라이다 스캔의 낮은 주파수(예: 10Hz) 사이의 시간적 공백 동안의 움직임을 IMU 데이터로 보간(interpolate)하여 모션 왜곡(motion distortion)을 보정할 수 있다. 또한, IMU는 정합을 위한 매우 정확한 초기 변환 값을 제공하여 ICP와 같은 지역 최적화 알고리즘의 수렴을 돕는다.124</li>
<li><strong>카메라 융합:</strong> 카메라는 라이다가 제공하기 어려운 풍부한 색상과 텍스처 정보를 제공한다. 평평한 벽이나 도로와 같이 기하학적 특징이 거의 없는 영역에서는 라이다만으로는 정합이 어렵지만, 카메라 이미지의 시각적 특징점을 활용하면 대응 관계를 더 명확하게 찾을 수 있다.9</li>
</ul>
<p>센서 융합은 각 센서의 강점을 결합하여 약점을 보완하는 전략이다. IMU는 라이다의 ‘시간적 해상도’ 한계를, 카메라는 ’기하학적 모호성’의 한계를 극복하게 해주며, 이를 통해 단일 센서로는 달성하기 어려운 높은 수준의 강인성과 정확도를 달성할 수 있다.</p>
<h3>4.3  실세계 데이터의 난제</h3>
<p>실제 환경에서 수집된 데이터는 이상적인 벤치마크 데이터셋과 달리 여러 가지 어려운 문제들을 포함한다.</p>
<ul>
<li><strong>저중첩 (Low Overlap):</strong> 두 스캔이 공유하는 영역이 매우 작을 경우(예: 30% 미만), 신뢰할 수 있는 대응점을 충분히 찾기 어려워 대부분의 정합 알고리즘이 실패한다. 이는 좁은 복도를 지나거나, 큰 루프를 닫는 SLAM 시나리오에서 빈번하게 발생한다. 이 문제를 해결하는 것은 현재 딥러닝 기반 정합 연구의 주요 목표 중 하나이다.87</li>
<li><strong>대규모 데이터 (Large-Scale Data):</strong> 도시 전체나 넓은 지형을 스캔한 포인트클라우드는 수억 개 이상의 점을 포함할 수 있다. 이러한 대규모 데이터를 직접 처리하는 것은 메모리 및 계산 시간 측면에서 비현실적이다. 따라서 효율적인 다운샘플링, 핵심점 기반 접근, 또는 데이터를 계층적으로 요약하여 처리하는 방법이 필수적이다.13</li>
<li><strong>이종 센서 데이터 (Cross-Source Data):</strong> 라이다, 사진 측량, RGB-D 카메라 등 서로 다른 원천에서 생성된 포인트클라우드를 정합하는 것은 매우 어려운 과제이다. 각 데이터는 점의 밀도, 노이즈 패턴, 누락된 데이터의 특성 등이 모두 다르기 때문이다. 이종 데이터 간의 도메인 갭(domain gap)을 극복하고 일관된 특징 표현을 학습하는 것은 향후 연구의 중요한 방향이다.13</li>
</ul>
<h2>5.  실용적 구현 및 성능 평가</h2>
<p>포인트클라우드 정합 알고리즘의 이론적 이해를 넘어, 실제 응용을 위해서는 이를 구현할 수 있는 소프트웨어 라이브러리와 알고리즘의 성능을 객관적으로 평가할 수 있는 표준 벤치마크에 대한 이해가 필수적이다.</p>
<h3>5.1  주요 라이브러리 비교: PCL vs. Open3D</h3>
<p>포인트클라우드 처리를 위한 대표적인 오픈소스 라이브러리로는 PCL과 Open3D가 있다.</p>
<ul>
<li><strong>PCL (Point Cloud Library):</strong> PCL은 C++로 작성된 포괄적이고 성숙한 라이브러리로, 오랫동안 학계와 산업계에서 포인트클라우드 처리의 표준으로 여겨져 왔다.3 필터링, 특징 추출, 정합, 분할, 표면 재구성 등 방대한 범위의 알고리즘을 제공한다. C++ 기반이므로 성능이 매우 뛰어나지만, API가 다소 복잡하고 방대하여 학습 곡선이 가파르다는 단점이 있다.130 성능을 극한까지 최적화해야 하는 상용 제품이나 특정 하드웨어에 배포할 때 큰 유연성을 제공한다.</li>
<li><strong>Open3D:</strong> Open3D는 비교적 최근에 등장한 라이브러리로, 현대적이고 사용하기 쉬운 API를 지향한다. 특히 Python 바인딩이 매우 잘 되어 있어, 연구 및 빠른 프로토타이핑에 매우 유리하다.134 강력한 시각화 기능과 최신 딥러닝 프레임워크와의 쉬운 통합을 장점으로 내세운다. PCL에 비해 제공하는 알고리즘의 종류는 적지만, ICP, FPFH, RANSAC 등 핵심적인 정합 기능들은 효율적으로 구현되어 있으며, 성능 또한 우수하다.130</li>
</ul>
<p>라이브러리의 선택은 ’생산성’과 ‘제어 수준’ 사이의 트레이드오프 문제로 귀결된다. Open3D는 빠른 개발과 실험을 중시하는 연구 환경에 적합하며, PCL은 저수준 제어와 성능 최적화가 중요한 상용 개발 환경에 더 적합할 수 있다.</p>
<h4>5.1.1 구현 예시: Open3D를 이용한 FPFH+RANSAC+ICP 파이프라인</h4>
<p>Open3D를 사용하면 Coarse-to-Fine 정합 파이프라인을 매우 간결하게 구현할 수 있다.</p>
<ol>
<li><strong>전처리 및 특징 추출:</strong> 소스 및 타겟 포인트클라우드를 로드한 후, <code>voxel_down_sample</code> 함수를 사용하여 균일하게 다운샘플링한다. 이는 계산량을 줄이고 점 밀도를 균일하게 만드는 효과가 있다. 그 후, <code>estimate_normals</code>로 각 점의 법선 벡터를 계산하고, 이를 입력으로 <code>compute_fpfh_feature</code> 함수를 호출하여 각 점에 대한 FPFH 기술자를 계산한다.138</li>
<li><strong>초기 정합 (RANSAC):</strong> <code>registration_ransac_based_on_feature_matching</code> 함수를 사용하여 FPFH 기술자 간의 대응 관계를 기반으로 RANSAC을 수행한다. 이 함수는 소스 및 타겟의 다운샘플링된 포인트클라우드와 FPFH 특징, 그리고 거리 임계값을 입력받아 강건한 초기 변환 행렬을 반환한다.138</li>
<li><strong>정밀 정합 (ICP):</strong> RANSAC으로 구한 초기 변환 행렬을 <code>registration_icp</code> 함수의 <code>init</code> 파라미터로 전달한다. 이때 원본(다운샘플링되지 않은) 포인트클라우드를 사용하고, <code>TransformationEstimationPointToPlane</code>을 오차 측정 방식으로 지정하여 Point-to-Plane ICP를 수행한다. 이를 통해 최종적으로 정밀하게 정렬된 변환 행렬을 얻는다.136</li>
</ol>
<h3>5.2  표준 벤치마크 데이터셋 분석</h3>
<p>알고리즘의 성능을 객관적으로 비교하고 평가하기 위해 여러 표준 벤치마크 데이터셋이 사용된다.</p>
<ul>
<li><strong>3DMatch &amp; 3DLoMatch:</strong> 실내 장면 재구성을 목표로 하는 RGB-D 스캔 데이터셋이다.88 여러 장면에서 캡처된 수많은 3D 데이터 조각(fragment)과 그들 간의 실제 변환 행렬을 제공한다. 특히</li>
</ul>
<p><strong>3DMatch</strong>는 중첩률이 30% 이상인 비교적 쉬운 쌍들로 구성되어 있고, <strong>3DLoMatch</strong>는 중첩률이 10%에서 30% 사이인 매우 어려운 쌍들로 구성되어 있다. 3DLoMatch는 저중첩(low-overlap) 시나리오에서 알고리즘의 강인성을 평가하는 데 핵심적인 벤치마크로 사용된다.97</p>
<ul>
<li><strong>KITTI Odometry:</strong> 자율주행 연구를 위한 대표적인 실외 대규모 LiDAR 데이터셋이다.142 차량이 독일 카를스루에 시내와 주변 도로를 주행하며 수집한 연속적인 Velodyne LiDAR 스캔과, GPS/IMU 센서로 정밀하게 측정한 차량의 실제 주행 경로(ground truth trajectory)를 제공한다. 이를 통해 LiDAR Odometry 및 SLAM 알고리즘의 정확도를 평가할 수 있다.144</li>
</ul>
<p>이러한 벤치마크는 알고리즘의 ’강점’을 드러내는 특정 ’문제’를 정의한다. 3DMatch는 실내 객체의 복잡한 국소 기하학 매칭 능력을 테스트하는 반면, KITTI는 대규모 공간에서의 구조적 일관성과 동적 환경 대처 능력을 평가한다. 어떤 알고리즘이 특정 벤치마크에서 뛰어난 성능을 보인다는 것은, 그 알고리즘이 해당 벤치마크가 제시하는 특정 종류의 도전에 더 적합하게 설계되었음을 의미한다.</p>
<h4>5.2.1 주요 평가 지표</h4>
<ul>
<li><strong>Registration Recall (RR):</strong> 등록 성공률. 전체 테스트 쌍 중에서 추정된 변환 오차가 미리 정의된 임계값(예: 회전 오차 5도, 이동 오차 2m) 이내인 쌍의 비율을 나타낸다. 알고리즘의 전반적인 강인성과 성공률을 평가하는 가장 중요한 지표이다.97</li>
<li><strong>Relative Rotation Error (RRE) / Relative Translation Error (RTE):</strong> 상대 회전/이동 오차. 성공적으로 등록된 쌍에 한하여, 추정된 회전 행렬 및 이동 벡터가 실제 값과 얼마나 차이 나는지를 측정한다. 알고리즘의 정밀도를 나타낸다.144</li>
<li><strong>Feature Match Recall (FMR):</strong> 특징 매칭 성공률. 두 포인트클라우드 간에 설정된 잠정적인 대응점들 중, 실제 인라이어(inlier)의 비율이 RANSAC과 같은 강건한 추정기가 성공적으로 작동할 수 있는 임계값(예: 5%)을 넘는 쌍의 비율을 의미한다. 이는 특징 기술자의 변별력과 매칭 단계의 성능을 독립적으로 평가하는 지표이다.147</li>
</ul>
<h3>5.3  주요 알고리즘 벤치마크 성능 요약</h3>
<p>벤치마크 데이터셋에서의 성능 비교는 알고리즘의 실제적인 우수성을 가늠하는 중요한 척도이다.</p>
<ul>
<li><strong>3DMatch / 3DLoMatch 성능:</strong></li>
<li><strong>고전적 방법:</strong> FPFH+RANSAC과 같은 고전적 방법은 중첩률이 높은 3DMatch에서는 준수한 성능을 보이지만, 저중첩 시나리오인 3DLoMatch에서는 RR이 급격히 감소하여 한계를 드러낸다.</li>
<li><strong>딥러닝 방법:</strong> Predator, GeoTransformer와 같은 최신 딥러닝 모델들은 3DLoMatch에서 고전적 방법을 압도하는 높은 RR을 기록하며 저중첩 문제 해결에 대한 획기적인 발전을 보여주었다. 예를 들어, Predator는 3DLoMatch에서 기존 SOTA 대비 RR을 15% 이상 향상시켰으며 98, GeoTransformer는 7% 이상 추가로 개선했다.105 이는 딥러닝 모델이 데이터로부터 중첩 영역을 예측하거나 변환 불변의 기하학적 구조를 학습하는 능력이 뛰어남을 입증한다.</li>
<li><strong>KITTI 성능:</strong></li>
<li>대규모 실외 환경인 KITTI 데이터셋에서는 구조적 정보를 효과적으로 활용하는 것이 중요하다. GeoTransformer와 같은 Transformer 기반 모델은 점들 간의 전역적인 기하학적 관계를 학습함으로써 매우 높은 RR(99% 이상)을 달성했다.106 이는 복잡하고 구조적인 실외 환경에서 딥러닝 모델의 우수성을 보여주는 결과이다.</li>
</ul>
<p>아래 표는 주요 알고리즘들의 대표적인 벤치마크 성능을 요약한 것이다.</p>
<table><thead><tr><th>데이터셋</th><th>메트릭</th><th>FPFH + RANSAC</th><th>Predator 98</th><th>GeoTransformer 104</th><th>REGTR 80</th></tr></thead><tbody>
<tr><td><strong>3DMatch</strong> (&gt;30% overlap)</td><td>RR (%)</td><td>~80</td><td>89.0</td><td>92.5</td><td>92.0</td></tr>
<tr><td></td><td>RRE (deg)</td><td>-</td><td>1.14</td><td>0.49</td><td>0.54</td></tr>
<tr><td></td><td>RTE (cm)</td><td>-</td><td>4.3</td><td>1.8</td><td>2.0</td></tr>
<tr><td><strong>3DLoMatch</strong> (10-30% overlap)</td><td>RR (%)</td><td>~35</td><td>62.4</td><td>74.2</td><td>71.3</td></tr>
<tr><td></td><td>RRE (deg)</td><td>-</td><td>2.50</td><td>1.10</td><td>1.25</td></tr>
<tr><td></td><td>RTE (cm)</td><td>-</td><td>9.0</td><td>4.3</td><td>4.9</td></tr>
<tr><td><strong>KITTI Odometry</strong></td><td>RR (%)</td><td>34.2 146</td><td>99.1</td><td>99.8 106</td><td>99.2</td></tr>
<tr><td></td><td>RRE (deg)</td><td>-</td><td>0.35</td><td>0.23</td><td>0.38</td></tr>
<tr><td></td><td>RTE (cm)</td><td>-</td><td>8.8</td><td>6.2</td><td>10.3</td></tr>
</tbody></table>
<p><em>Table 3: 3DMatch 및 KITTI 벤치마크 성능 요약 (대표적인 SOTA 모델 중심)</em></p>
<p><em>(주: 수치는 각 논문에서 보고된 최상의 결과에 기반하며, 실험 설정에 따라 다소 차이가 있을 수 있음)</em></p>
<p>이러한 정량적 비교는 딥러닝, 특히 Transformer 기반 아키텍처가 고전적 방법론의 한계를 극복하고 포인트클라우드 정합 기술을 새로운 수준으로 끌어올렸음을 명확하게 보여준다.</p>
<h2>6.  결론 및 향후 연구 방향</h2>
<h3>6.1  현재 기술 수준의 요약 및 핵심적 통찰</h3>
<p>포인트클라우드 정합 기술은 지난 수십 년간 괄목할 만한 발전을 이루었다. 고전적인 ICP 기반의 반복적 최적화 방법론에서 출발하여, FPFH와 RANSAC을 활용하는 강건한 특징점 기반 파이프라인으로 진화했으며, 현재는 Transformer 아키텍처를 필두로 한 End-to-End 딥러닝 방법론이 기술 발전을 주도하고 있다.1</p>
<p>딥러닝 기술의 도입은 특히 저중첩, 노이즈, 이상치와 같은 실세계의 어려운 조건 하에서 기존 방법론의 한계를 극복하는 데 결정적인 역할을 했다.78 데이터로부터 직접 특징과 대응 관계를 학습함으로써, 수동 설계의 제약을 벗어나 특정 작업에 최적화된 고성능 정합 모델을 구축할 수 있게 되었다. 그러나 이러한 발전에도 불구하고, 학습된 모델의 일반화 성능과 대규모 레이블링 데이터에 대한 의존성은 여전히 해결해야 할 중요한 과제로 남아있다.84</p>
<p>현재 강체 정합 기술은 상당한 성숙도에 도달했지만, 비강체 객체의 변형을 다루거나, 동적 요소가 혼재된 환경을 이해하거나, 서로 다른 센서로부터 얻은 이종 데이터를 정합하는 것과 같은 복잡한 시나리오는 여전히 활발한 연구가 진행 중인 미개척 분야로 남아있다.129</p>
<h3>6.2  미해결 과제와 향후 연구 방향</h3>
<p>포인트클라우드 정합 기술의 미래는 현재의 한계를 극복하고 더욱 복잡하며 현실적인 문제들을 해결하는 방향으로 나아갈 것이다.</p>
<h4>6.2.1 강인성과 일반화 성능의 향상</h4>
<ul>
<li><strong>이종 및 교차 소스 정합 (Cross-Source Registration):</strong> LiDAR, 사진 측량, 구조광, RGB-D 카메라 등 서로 다른 물리적 원리로 동작하는 센서로부터 얻은 포인트클라우드를 정합하는 것은 매우 중요한 과제이다. 각 데이터는 고유한 밀도 분포, 노이즈 패턴, 결측 특성을 가지므로, 이러한 ’도메인 갭’을 극복할 수 있는 새로운 표현 학습 및 정합 방법론이 필요하다.13</li>
<li><strong>비지도 및 자기지도 학습 (Unsupervised and Self-Supervised Learning):</strong> 현재의 고성능 딥러닝 모델은 대부분 대규모의 정답 레이블(ground truth)을 요구한다. 이러한 데이터 구축 비용을 절감하고 모델의 일반화 성능을 향상시키기 위해, 레이블 없이 데이터 자체의 내재적 구조나 일관성(예: 사이클 일관성)을 손실 함수로 활용하는 비지도 및 자기지도 학습 방법론에 대한 연구가 더욱 중요해질 것이다.109</li>
</ul>
<h4>6.2.2 복잡한 동적 및 비강체 시나리오 모델링</h4>
<ul>
<li><strong>물리 기반 딥러닝 (Physics-Informed Deep Learning):</strong> 비강체 변형이나 동적 객체의 움직임을 모델링할 때, 신경망이 물리적으로 타당하지 않은 결과를 생성할 수 있다. 이를 방지하기 위해 강체 동역학, 연속체 역학 등 물리 법칙을 미분 가능한 형태로 신경망의 손실 함수나 구조에 제약 조건으로 통합하는 연구가 유망하다. 이는 모델이 더 현실적이고 예측 가능한 결과를 생성하도록 유도할 수 있다.108</li>
<li><strong>4D 포인트클라우드 처리:</strong> 시간 축이 포함된 4D 포인트클라우드(연속적인 3D 스캔 시퀀스)를 하나의 단위로 처리하여 시공간적 맥락을 통합적으로 이해하는 연구가 필요하다. 이를 통해 객체의 일관된 추적, 움직임 예측, 그리고 더 정확한 Scene Flow 추정이 가능해질 것이다.20</li>
</ul>
<h4>6.2.3 실시간 및 대규모 처리</h4>
<ul>
<li><strong>효율적인 네트워크 아키텍처:</strong> AR/VR 기기, 모바일 로봇, 드론과 같이 컴퓨팅 자원이 제한된 엣지 디바이스에서 실시간으로 정합을 수행하기 위해서는 모델 경량화, 양자화(quantization), 프루닝(pruning) 등 효율적인 네트워크 아키텍처에 대한 연구가 필수적이다.156</li>
<li><strong>계층적/점진적 대규모 정합:</strong> 도시 전체와 같은 수억, 수십억 개의 점으로 구성된 대규모 포인트클라우드를 효율적으로 처리하기 위해, 데이터를 여러 해상도의 계층으로 요약하거나, 새로운 데이터를 기존 지도에 점진적으로 통합하는 알고리즘 연구가 지속적으로 필요하다.158</li>
</ul>
<h4>6.2.4 딥러닝과 고전적 방법의 융합</h4>
<p>미래의 정합 시스템은 딥러닝과 고전적 방법론의 장점을 결합하는 하이브리드 형태로 발전할 가능성이 높다. 딥러닝의 강력한 특징 표현 능력과 데이터 기반 추론을 통해 강건한 초기 정렬을 신속하게 달성하고, 이후 ICP와 같은 고전적 최적화 기법을 통해 기하학적 정밀도를 수학적으로 보장하며 미세 조정하는 방식이다.1</p>
<p>미래의 정합 기술은 단순히 ’데이터’에서 패턴을 학습하는 것을 넘어, 물리 법칙, 기하학적 불변성, 의미론적 맥락과 같은 사전 ’지식’을 모델에 명시적으로 통합하는 방향으로 발전해야 한다. 이는 모델이 데이터 분포를 암기하는 것을 넘어, 세상이 어떻게 동작하는지에 대한 근본적인 이해를 바탕으로 추론하도록 만드는 중요한 전환점이 될 것이다.</p>
<p>또한, 정합은 더 이상 독립된 문제로 취급되지 않고, 객체 탐지, 의미론적 분할, 추적, 장면 이해 등 다른 인식 작업들과 긴밀하게 상호작용하는 거대한 ’인식 파이프라인’의 한 부분으로 통합될 것이다.78 미래의 연구는 이러한 작업들을 공동으로 최적화하여, 각 모듈이 서로의 성능을 향상시키는 선순환 구조를 만드는 방향으로 나아갈 것이다. 이는 정합 기술이 더 큰 시스템의 일부로서 어떻게 기여하고 시너지를 낼 수 있는지에 대한 고민이 중요해짐을 의미한다.</p>
<h2>7. 참고 자료</h2>
<ol>
<li>Point cloud registration: a mini-review of current state, challenging issues and future directions - AIMS Press, https://www.aimspress.com/article/doi/10.3934/geosci.2023005?viewType=HTML</li>
<li>Point cloud - Wikipedia, https://en.wikipedia.org/wiki/Point_cloud</li>
<li>Point-set registration - Wikipedia, https://en.wikipedia.org/wiki/Point-set_registration</li>
<li>[논문]리프로젝션 이미지를 이용한 3차원 포인트 클라우드 정합, https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=DIKO0013692659</li>
<li>Deep Learning on Point Clouds and Its Application: A Survey - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC6806315/</li>
<li>Review: Deep Learning on 3D Point Clouds - MDPI, https://www.mdpi.com/2072-4292/12/11/1729</li>
<li>Mastering Point Cloud Registration - Number Analytics, https://www.numberanalytics.com/blog/ultimate-guide-point-cloud-registration-metrology-inspection</li>
<li>포인트 클라우드 모델을 편집하기 - Mech-Mind Documentation, https://docs.mech-mind.net/ko/suite-software-manual/1.8.3/vision-tools/edit-point-cloud-model.html</li>
<li>특징점을 사용한 포인트 클라우드 정합 - Korea Science, https://www.koreascience.kr/article/CFKO201904533894621.pdf</li>
<li>Point Cloud Registration Based on Multiparameter Functional - MDPI, https://www.mdpi.com/2227-7390/9/20/2589</li>
<li>Point Cloud Registration in Action - Number Analytics, https://www.numberanalytics.com/blog/point-cloud-registration-action-cognitive-robotics</li>
<li>Applications of point cloud registration (figures taken from the Internet). (a) SLAM; (b) Autonomous driving;(c) 3D modeling. - ResearchGate, https://www.researchgate.net/figure/Applications-of-point-cloud-registration-figures-taken-from-the-Internet-a-SLAM-b_fig1_342575636</li>
<li>Automatic Point Cloud Registration for Large Outdoor Scenes Using a Priori Semantic Information - MDPI, https://www.mdpi.com/2072-4292/13/17/3474</li>
<li>Finding optimal rotation and translation between corresponding 3D points - Nghia Ho, https://nghiaho.com/?page_id=671</li>
<li>[논문 리뷰] Deep Learning-Based Point Cloud Registration: A Comprehensive Survey and Taxonomy - Moonlight, https://www.themoonlight.io/ko/review/deep-learning-based-point-cloud-registration-a-comprehensive-survey-and-taxonomy</li>
<li>Inverse Composition Discriminative Optimization for Point Cloud Registration - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2018/papers/Vongkulbhisal_Inverse_Composition_Discriminative_CVPR_2018_paper.pdf</li>
<li>How to move a point cloud using a rigid transformation? - libpointmatcher, https://libpointmatcher.readthedocs.io/en/latest/Transformations/</li>
<li>(ICP)Iterative Closest Point - Repository - 티스토리, https://define.tistory.com/entry/Iterative-Closest-Point</li>
<li>[논문]포인트 클라우드 콘텐츠 해상도 향상을 위한 점진적 렌더링 방법, https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=JAKO202117651617613</li>
<li>Non-rigid Point Cloud Registration with Neural Deformation Pyramid, https://proceedings.neurips.cc/paper_files/paper/2022/file/b2077e6d66da612fcb701589efa9ce88-Paper-Conference.pdf</li>
<li>Image Registration - Quantitative Bioimaging Laboratory, https://fei-lab.org/image-registration/</li>
<li>Deformable / Non-Rigid Registration - Carnegie Mellon University, https://www.cs.cmu.edu/~galeotti/methods_course/DeformableRegistration.pdf</li>
<li>(PDF) Linear Least-Squares Optimization for Point-to-Plane ICP Surface Registration, https://www.researchgate.net/publication/228571031_Linear_Least-Squares_Optimization_for_Point-to-Plane_ICP_Surface_Registration</li>
<li>Solving a least squares problem to align two point clouds. - Mathematics Stack Exchange, https://math.stackexchange.com/questions/3843479/solving-a-least-squares-problem-to-align-two-point-clouds</li>
<li>Iterative closest point - Wikipedia, https://en.wikipedia.org/wiki/Iterative_closest_point</li>
<li>Linear Least-Squares Optimization for Point-to-Plane ICP Surface Registration ∑ - NUS Computing, https://www.comp.nus.edu.sg/~lowkl/publications/lowk_point-to-plane_icp_techrep.pdf</li>
<li>Point Cloud Registration - Classic Approaches - Towards Data Science, https://towardsdatascience.com/point-cloud-registration-classic-approaches-d6191302b0b2/</li>
<li>Understanding Iterative Closest Point (ICP) Algorithm with Code - LearnOpenCV, https://learnopencv.com/iterative-closest-point-icp-explained/</li>
<li>Pset #2 part 1 of 3: Iterative Closest Point (ICP) - Robotic Manipulation, https://manipulation.csail.mit.edu/Fall2020/pset2/pset2_ICP.html</li>
<li>ICP 계산속도 향상을 위한 빠른 Correspondence 매칭 방법, https://koreascience.kr/article/JAKO202225751947644.pdf</li>
<li>Point cloud to point cloud rigid transformations Minimizing Rigid Registration Errors, https://www.cs.jhu.edu/cista/455/Lectures/Rigid3D3DCalculations.pdf</li>
<li>[Registration Algorithm] Point-to-Point, Point-to-Plane ICP - velog, https://velog.io/@harms/Registration-Algorithm-Point-to-Point-Point-to-Plane-ICP</li>
<li>Point Cloud Alignment Alignment of 3D Data Points Scan Alignment in Mapping Iterative Closest Point (I, https://www.ipb.uni-bonn.de/html/teaching/msr2-2020/sse2-03-icp.pdf</li>
<li>ICP(Iterative closest point) Point-To-Plane | by Daekwanko - Medium, https://medium.com/@daekwanko123/icp-iterative-closest-point-plane-to-plane-2eb518653c4c</li>
<li>An Automatic 3D Point Cloud Registration Method Based on Biological Vision - MDPI, https://www.mdpi.com/2076-3417/11/10/4538</li>
<li>ICP point-to-plane odometry algorithm - OpenCV Documentation, https://docs.opencv.org/4.x/d7/dbe/kinfu_icp.html</li>
<li>pcregistericp - Register two point clouds using ICP algorithm - MATLAB - MathWorks, https://www.mathworks.com/help/vision/ref/pcregistericp.html</li>
<li>www.roboticsproceedings.org, https://www.roboticsproceedings.org/rss05/p21.pdf</li>
<li>[PDF] Generalized-ICP - Semantic Scholar, <a href="https://www.semanticscholar.org/paper/Generalized-ICP-Segal-H%C3%A4hnel/b352b3a7f1068b2d562ba12a446628397dfe8a77">https://www.semanticscholar.org/paper/Generalized-ICP-Segal-H%C3%A4hnel/b352b3a7f1068b2d562ba12a446628397dfe8a77</a></li>
<li>Generalized ICP - Wiki, https://wiki.hanzheteng.com/algorithm/slam/generalized-icp</li>
<li>Visually Bootstrapped Generalized ICP - University of Michigan, https://robots.engin.umich.edu/publications/gpandey-2011b.pdf</li>
<li>Hybrid algorithm ideology. ICP step by step comes to local minima…. | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Hybrid-algorithm-ideology-ICP-step-by-step-comes-to-local-minima-After-local-minima_fig4_281412803</li>
<li>A Globally Optimal Solution to 3D ICP Point-Set Registration - arXiv, http://arxiv.org/pdf/1605.03344</li>
<li>An Iterative Closest Points Algorithm for Registration of 3D Laser Scanner Point Clouds with Geometric Features, https://pmc.ncbi.nlm.nih.gov/articles/PMC5580094/</li>
<li>RANSAC vs. ICP: 두 강력한 알고리즘의 비교와 활용 - 직관적인느낌 - 티스토리, https://lbj142632.tistory.com/139</li>
<li>(PDF) Robust symmetric iterative closest point - ResearchGate, https://www.researchgate.net/publication/358425006_Robust_symmetric_iterative_closest_point</li>
<li>Efficient Variants of the ICP Algorithm - cs.princeton.edu, https://www.cs.princeton.edu/~smr/papers/fasticp/fasticp_paper.pdf</li>
<li>Iterative Closest Point Algorithm Introduction to Mobile Robotics - GMU CS Department, https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf</li>
<li>A Global Structure and Adaptive Weight Aware ICP Algorithm for Image Registration - MDPI, https://www.mdpi.com/2072-4292/15/12/3185</li>
<li>Slam 3-2강 (ICP algorithm &amp; Unknown Data Association) 요약 - GitHub, https://taeyoung96.github.io/slam/SLAM_03_2/</li>
<li>ICP 알고리즘 이용 3D 거리 영상 정합 Reading &amp; Summary - Just the other night, - 티스토리, https://saint-swithins-day.tistory.com/21</li>
<li>Multi-source point cloud registration for urban areas using a coarse-to-fine approach, https://www.tandfonline.com/doi/full/10.1080/15481603.2024.2341557</li>
<li>3D Point Cloud Registration for Localization Using a Deep Neural Network Auto-Encoder - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2017/papers/Elbaz_3D_Point_Cloud_CVPR_2017_paper.pdf</li>
<li>KR20170020629A - 점군 정합 장치 - Google Patents, https://patents.google.com/patent/KR20170020629A/ko</li>
<li>Journal Archive - 한국생산제조학회 학술지 영문 홈페이지, http://journal.ksmte.kr/_common/do.php?a=full&amp;b=22&amp;bidx=3500&amp;aidx=38849</li>
<li>Keypoint Matching for Point Cloud Registration using Multiplex Dynamic Graph Attention Networks, https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/shi2021ral.pdf</li>
<li>XuyangBai/awesome-point-cloud-registration - GitHub, https://github.com/XuyangBai/awesome-point-cloud-registration</li>
<li>Point cloud registration from local feature correspondences-Evaluation on challenging datasets | PLOS One - Research journals, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0187943</li>
<li>Point Cloud Registration Essentials - Number Analytics, https://www.numberanalytics.com/blog/point-cloud-registration-essentials</li>
<li>A fast and robust local descriptor for 3D point cloud registration - ResearchGate, https://www.researchgate.net/publication/293330421_A_fast_and_robust_local_descriptor_for_3D_point_cloud_registration</li>
<li>SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Ao_SpinNet_Learning_a_General_Surface_Descriptor_for_3D_Point_Cloud_CVPR_2021_paper.pdf</li>
<li>Fast Point Feature Histograms (FPFH) descriptors - Point Cloud Library 0.0 documentation, https://pcl.readthedocs.io/projects/tutorials/en/master/fpfh_estimation.html</li>
<li>The New Fast Point Feature Histograms Algorithm Based on Adaptive Selection - Journal of Applied Science and Engineering, http://jase.tku.edu.tw/articles/jase-202006-23-2-0006.pdf</li>
<li>Fast Point Feature Histograms (FPFH) for 3D Registration - 3D Vision Laboratory, https://www.cvl.iis.u-tokyo.ac.jp/class2016/2016w/papers/6.3DdataProcessing/Rusu_FPFH_ICRA2009.pdf</li>
<li>Unique Signatures of Histograms for Local Surface Description - ResearchGate, https://www.researchgate.net/publication/221304551_Unique_Signatures_of_Histograms_for_Local_Surface_Description</li>
<li>Signature structure for SHOT | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Signature-structure-for-SHOT_fig9_221304551</li>
<li>fedassa/SHOT: C++ implementation of the SHOT 3D descriptor - GitHub, https://github.com/fedassa/SHOT</li>
<li>SANDRO: a Robust Solver with a Splitting Strategy for Point Cloud Registration - arXiv, https://arxiv.org/html/2503.07743v1</li>
<li>GS-Matching: Reconsidering Feature Matching task in Point Cloud Registration - arXiv, https://arxiv.org/html/2412.04855v1</li>
<li>RANSAC for Computer Vision - Number Analytics, https://www.numberanalytics.com/blog/ransac-for-computer-vision</li>
<li>점군 데이터를 활용한 옹벽의 단면 수치 정보 자동화 도출 - Korea Science, https://koreascience.kr/article/JAKO202420152111103.pdf</li>
<li>[별첨] 바닥제거 (RANSAC) (70%) - Tutorial, https://pcl.gitbook.io/tutorial/part-2/part01-chapter05</li>
<li>filters.cpd - Point Data Abstraction Library (PDAL), https://pdal.io/en/stable/stages/filters.cpd.html</li>
<li>Non-rigid point set registration: Coherent Point Drift - NIPS, https://papers.nips.cc/paper/2962-non-rigid-point-set-registration-coherent-point-drift</li>
<li>(PDF) Point Set Registration: Coherent Point Drift - ResearchGate, https://www.researchgate.net/publication/47544556_Point_Set_Registration_Coherent_Point_Drift</li>
<li>Point set registration: Coherent point drifts - Oregon Health &amp; Science University, https://ohsu.elsevierpure.com/en/publications/point-set-registration-coherent-point-drifts-2</li>
<li>Non-rigid point set registration: Coherent Point Drift - Stanford Computer Graphics Laboratory, https://graphics.stanford.edu/courses/cs468-07-winter/Papers/nips2006_0613.pdf</li>
<li>Deep Learning-Based Point Cloud Registration: A Comprehensive Survey and Taxonomy, https://arxiv.org/html/2404.13830v3</li>
<li>(PDF) Deep Closest Point: Learning Representations for Point Cloud Registration (2019) | Yue Wang | 789 Citations - SciSpace, https://scispace.com/papers/deep-closest-point-learning-representations-for-point-cloud-kq94dtgkhn</li>
<li>REGTR: End-to-End Point Cloud Correspondences With Transformers - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Yew_REGTR_End-to-End_Point_Cloud_Correspondences_With_Transformers_CVPR_2022_paper.pdf</li>
<li>A review of rigid point cloud registration based on deep learning - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1281332/full</li>
<li>A review of rigid point cloud registration based on deep learning - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10794353/</li>
<li>Research on the Improved ICP Algorithm for LiDAR Point Cloud Registration - MDPI, https://www.mdpi.com/1424-8220/25/15/4748</li>
<li>Deep Learning-based Point Cloud Registration for Augmented Reality-guided Surgery, https://arxiv.org/html/2405.03314v1</li>
<li>Comparison of Point Cloud Registration Techniques on Scanned Physical Objects - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11014384/</li>
<li>Comparison of Point Cloud Registration Techniques on Scanned Physical Objects, https://www.researchgate.net/publication/379347882_Comparison_of_Point_Cloud_Registration_Techniques_on_Scanned_Physical_Objects</li>
<li>A BRIEF OVERVIEW OF THE CURRENT STATE, CHALLENGING ISSUES AND FUTURE DIRECTIONS OF POINT CLOUD REGISTRATION, https://isprs-annals.copernicus.org/articles/X-3-W1-2022/17/2022/isprs-annals-X-3-W1-2022-17-2022.pdf</li>
<li>3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions, https://3dmatch.cs.princeton.edu/</li>
<li>3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions - Princeton Vision &amp; Robotics Group, https://3dvision.princeton.edu/projects/2016/3DMatch/paper_v2.pdf</li>
<li>(PDF) PointNetLK: Robust &amp; Efficient Point Cloud Registration Using PointNet - ResearchGate, https://www.researchgate.net/publication/338509136_PointNetLK_Robust_Efficient_Point_Cloud_Registration_Using_PointNet</li>
<li>PointNetLK Revisited - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Li_PointNetLK_Revisited_CVPR_2021_paper.pdf</li>
<li>PointNetLK: Robust &amp; Efficient Point Cloud Registration Using PointNet - CVF Open Access, https://openaccess.thecvf.com/content_CVPR_2019/papers/Aoki_PointNetLK_Robust__Efficient_Point_Cloud_Registration_Using_PointNet_CVPR_2019_paper.pdf</li>
<li>FPGA-Accelerated Correspondence-free Point Cloud Registration with PointNet Features - arXiv, https://arxiv.org/html/2404.01237v1</li>
<li>Deep Closest Point: Learning Representations for Point Cloud Registration - CVF Open Access, https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.pdf</li>
<li>Deep Closest Point: Learning Representations for Point Cloud Registration - ResearchGate, https://www.researchgate.net/publication/332977482_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration</li>
<li>Deep Closest Point: Learning Representations for Point Cloud Registration - ResearchGate, https://www.researchgate.net/publication/339555589_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration</li>
<li>PREDATOR: Registration of 3D Point Clouds with Low Overlap - ResearchGate, https://www.researchgate.net/publication/355869599_PREDATOR_Registration_of_3D_Point_Clouds_with_Low_Overlap</li>
<li>[2011.13005] Predator: Registration of 3D Point Clouds with Low Overlap - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/2011.13005</li>
<li>Predator: Registration of 3D Point Clouds With Low Overlap - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Predator_Registration_of_3D_Point_Clouds_With_Low_Overlap_CVPR_2021_paper.pdf</li>
<li>REGTR: End-to-end Point Cloud Correspondences with Transformers | alphaXiv, https://www.alphaxiv.org/overview/2203.14517v1</li>
<li>REGTR: End-to-end Point Cloud Correspondences with Transformers | Request PDF, https://www.researchgate.net/publication/363887344_REGTR_End-to-end_Point_Cloud_Correspondences_with_Transformers</li>
<li>[2203.14517] REGTR: End-to-end Point Cloud Correspondences with Transformers - arXiv, https://arxiv.org/abs/2203.14517</li>
<li>GeoTransformer: Fast and Robust Point Cloud Registration With Geometric Transformer, https://www.computer.org/csdl/journal/tp/2023/08/10076895/1LFQ0GcHene</li>
<li>Geometric Transformer for Fast and Robust Point Cloud Registration - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.pdf</li>
<li>Geometric Transformer for Fast and Robust Point Cloud Registration - ResearchGate, https://www.researchgate.net/publication/358603361_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration</li>
<li>qinzheng93/GeoTransformer: [CVPR2022] Geometric Transformer for Fast and Robust Point Cloud Registration - GitHub, https://github.com/qinzheng93/GeoTransformer</li>
<li>Point Cloud Registration on 3DMatch (at least 30% overlapped - sample 5k interest points), https://paperswithcode.com/sota/point-cloud-registration-on-3dmatch-at-least-2</li>
<li>Nonrigid Point Cloud Registration Using Piecewise Tricubic Polynomials as Transformation Model - MDPI, https://www.mdpi.com/2072-4292/15/22/5348</li>
<li>Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised Clustering Analysis - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Correspondence-Free_Non-Rigid_Point_Set_Registration_Using_Unsupervised_Clustering_Analysis_CVPR_2024_paper.pdf</li>
<li>Deep Graph-Based Spatial Consistency for Robust Non-Rigid Point Cloud Registration - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Deep_Graph-Based_Spatial_Consistency_for_Robust_Non-Rigid_Point_Cloud_Registration_CVPR_2023_paper.pdf</li>
<li>No Pain, Big Gain: Classify Dynamic Point Cloud Sequences With Static Models by Fitting Feature-Level Space-Time Surfaces - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_No_Pain_Big_Gain_Classify_Dynamic_Point_Cloud_Sequences_With_CVPR_2022_paper.pdf</li>
<li>A non‐rigid point cloud registration method based on scene flow estimation - ResearchGate, https://www.researchgate.net/publication/387560169_A_non-rigid_point_cloud_registration_method_based_on_scene_flow_estimation</li>
<li>FlowNet3D: Learning Scene Flow in 3D Point Clouds | Request PDF - ResearchGate, https://www.researchgate.net/publication/338510004_FlowNet3D_Learning_Scene_Flow_in_3D_Point_Clouds</li>
<li>(PDF) Deep Learning for Scene Flow Estimation on Point Clouds: A Survey and Prospective Trends - ResearchGate, https://www.researchgate.net/publication/369779661_Deep_Learning_for_Scene_Flow_Estimation_on_Point_Clouds_A_Survey_and_Prospective_Trends</li>
<li>Non-rigid point cloud registration with dynamic computing neural deformation pyramid, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13709/137092O/Non-rigid-point-cloud-registration-with-dynamic-computing-neural-deformation/10.1117/12.3075009.full</li>
<li>Non-rigid point cloud registration with dynamic computing neural deformation pyramid - SPIE Digital Library, <a href="https://www.spiedigitallibrary.org/proceedings/Download?urlId=10.1117/12.3075009">https://www.spiedigitallibrary.org/proceedings/Download?urlId=10.1117%2F12.3075009</a></li>
<li>Non-rigid point cloud registration for middle ear diagnostics with endoscopic optical coherence tomography - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10769937/</li>
<li>DEEP LEARNING REGISTRATION OF NON-RIGID OBJECTS IN MEDICAL AR APPLICATIONS – OPTIMIZATION OF METHODS AND DATA - OPUS, https://opus4.kobv.de/opus4-haw-landshut/files/395/BAGreipel2024.pdf</li>
<li>Object-Level Semantic Map Construction for Dynamic Scenes, https://pdfs.semanticscholar.org/bb89/3dd553b63f38e27a2e377d9da7362af20041.pdf</li>
<li>Semantic Segmentation-Driven Integration of Point Clouds from Mobile Scanning Platforms in Urban Environments - MDPI, https://www.mdpi.com/2072-4292/16/18/3434</li>
<li>[1703.01661] SegICP: Integrated Deep Semantic Segmentation and Pose Estimation - ar5iv - arXiv, https://ar5iv.labs.arxiv.org/html/1703.01661</li>
<li>LiDAR Localization by Removing Moveable Objects - MDPI, https://www.mdpi.com/2079-9292/12/22/4659</li>
<li>Semantic Iterative Closest Point through Expectation-Maximization - Perceptual Robotics Laboratory (PeRL), http://robots.engin.umich.edu/publications/sparkison-2018a.pdf</li>
<li>Research on Sensor Fusion-Based Calibration and Real-Time Point Cloud Mapping Methods for Laser LiDAR and IMU, https://isprs-archives.copernicus.org/articles/XLVIII-1-W2-2023/1199/2023/isprs-archives-XLVIII-1-W2-2023-1199-2023.pdf</li>
<li>LiDAR point cloud positioning using sensor fusion - ELTE Geoinformatics Laboratory, https://gis.inf.elte.hu/wordpress/wp-content/uploads/2022/05/farkaspeter_jambordominik_tdk_compressed.pdf</li>
<li>IMU-Aided Registration of MLS Point Clouds Using Inertial Trajectory Error Model and Least Squares Optimization - MDPI, https://www.mdpi.com/2072-4292/14/6/1365</li>
<li>Low Overlapping Point Cloud Registration Using Line Features Detection - MDPI, https://www.mdpi.com/2072-4292/12/1/61</li>
<li>Low Overlapping Point Cloud Registration Using Line Features Detection - ResearchGate, https://www.researchgate.net/publication/338125302_Low_Overlapping_Point_Cloud_Registration_Using_Line_Features_Detection</li>
<li>[2103.02690] A comprehensive survey on point cloud registration - arXiv, https://arxiv.org/abs/2103.02690</li>
<li>Point Cloud Libraries: PCL vs. Open3D for Processing Efficiency - Patsnap Eureka, https://eureka.patsnap.com/article/point-cloud-libraries-pcl-vs-open3d-for-processing-efficiency</li>
<li>Module registration - Point Cloud Library (PCL), https://pointclouds.org/documentation/group__registration.html</li>
<li>PCL Walkthrough - Point Cloud Library 0.0 documentation - Read the Docs, https://pcl.readthedocs.io/projects/tutorials/en/master/walkthrough.html</li>
<li>PCL Tutorial: - The Point Cloud Library By Example, https://cse.buffalo.edu/~jryde/cse673/files/pcl_tutorial.pdf</li>
<li>Point Cloud Processing Essentials - Number Analytics, https://www.numberanalytics.com/blog/point-cloud-processing-essentials</li>
<li>Alternative to PCL on Python for Processing and Visualization - Stack Overflow, https://stackoverflow.com/questions/54761250/alternative-to-pcl-on-python-for-processing-and-visualization</li>
<li>ICP registration - Open3D primary (unknown) documentation, https://www.open3d.org/docs/latest/tutorial/pipelines/icp_registration.html</li>
<li>Pipelines - Open3D primary (unknown) documentation, https://www.open3d.org/docs/latest/tutorial/pipelines/index.html</li>
<li>Global registration - Open3D primary (unknown) documentation, https://www.open3d.org/docs/latest/tutorial/pipelines/global_registration.html</li>
<li>Global registration - Open3D 0.18.0 documentation, https://www.open3d.org/docs/0.18.0/tutorial/pipelines/global_registration.html</li>
<li>Global registration - Open3D primary (252c867) documentation, https://www.open3d.org/html/tutorial/pipelines/global_registration.html</li>
<li>Global registration - Open3D latest (664eff5) documentation, https://www.open3d.org/docs/latest/tutorial/Advanced/global_registration.html</li>
<li>The KITTI Vision Benchmark Suite - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/</li>
<li>KITTI Odometry Benchmark - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/eval_odometry.php</li>
<li>HKUST-Aerial-Robotics/LiDAR-Registration-Benchmark - GitHub, https://github.com/HKUST-Aerial-Robotics/LiDAR-Registration-Benchmark</li>
<li>Registration performance comparision on KITTI and nuScenes dataset. - ResearchGate, https://www.researchgate.net/figure/Registration-performance-comparision-on-KITTI-and-nuScenes-dataset_tbl1_357778506</li>
<li>KITTI (FCGF setting) Benchmark (Point Cloud Registration) - Papers With Code, https://paperswithcode.com/sota/point-cloud-registration-on-kitti-fcgf</li>
<li>PREDATOR: Registration of 3D Point Clouds with Low Overlap - Supplementary material - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/supplemental/Huang_Predator_Registration_of_CVPR_2021_supplemental.pdf</li>
<li>Point cloud registration: A mini-review of current state, challenging issues and future directions - Xi’an Jiaotong-Liverpool University, https://scholar.xjtlu.edu.cn/en/publications/point-cloud-registration-a-mini-review-of-current-state-challengi</li>
<li>Comparison of Point Cloud Registration Techniques on Scanned Physical Objects - Vrije Universiteit Brussel, https://researchportal.vub.be/files/109814316/sensors_24_02142.pdf</li>
<li>A Comprehensive Survey and Taxonomy on Point Cloud Registration Based on Deep Learning | IJCAI, https://www.ijcai.org/proceedings/2024/922</li>
<li>(PDF) A BRIEF OVERVIEW OF THE CURRENT STATE, CHALLENGING ISSUES AND FUTURE DIRECTIONS OF POINT CLOUD REGISTRATION - ResearchGate, https://www.researchgate.net/publication/364895313_A_BRIEF_OVERVIEW_OF_THE_CURRENT_STATE_CHALLENGING_ISSUES_AND_FUTURE_DIRECTIONS_OF_POINT_CLOUD_REGISTRATION</li>
<li>Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Mei_Unsupervised_Deep_Probabilistic_Approach_for_Partial_Point_Cloud_Registration_CVPR_2023_paper.pdf</li>
<li>Unsupervised Point Cloud Registration with Self-Distillation - BMVA Archive, https://bmva-archive.org.uk/bmvc/2024/papers/Paper_663/paper.pdf</li>
<li>[2005.03190] A Dynamical Perspective on Point Cloud Registration - arXiv, https://arxiv.org/abs/2005.03190</li>
<li>Dynamic 3D Scene Analysis by Point Cloud Accumulation - European Computer Vision Association, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136980658.pdf</li>
<li>Advanced Point Cloud Registration Techniques - Number Analytics, https://www.numberanalytics.com/blog/advanced-point-cloud-registration-techniques</li>
<li>Comparison of Point Cloud Registration Techniques on Scanned Physical Objects - MDPI, https://www.mdpi.com/1424-8220/24/7/2142</li>
<li>Research on a Point Cloud Registration Method Based on Dynamic Neighborhood Features, https://www.mdpi.com/2076-3417/15/7/4036</li>
<li>How to incrementally register pairs of clouds - Point Cloud Library 1.15.0-dev documentation, https://pointclouds.org/documentation/tutorials/pairwise_incremental_registration.html</li>
<li>Deep Learning for 3D Point Clouds: A Survey - University of Oxford Department of Computer Science, https://www.cs.ox.ac.uk/files/12013/TPAMI_final_manuscripts.pdf</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>