# 객체 인식 모델 성능 평가에 대한 종합 분석 보고서
[인공지능 평가지표 (AI evaluation metrics)](./index.md)



객체 인식(Object Detection)은 컴퓨터 비전 분야의 핵심적인 과제 중 하나로, 이미지나 비디오 내에 존재하는 특정 객체들의 종류를 분류(Classification)하고, 그 위치를 경계 상자(Bounding Box)로 정확하게 표시(Localization)하는 기술을 포괄한다.1 이는 단순히 이미지 안에 무엇이 있는지를 넘어, '어디에' 있는지를 함께 알려준다는 점에서 단순 이미지 분류와 근본적인 차이를 가진다. 이러한 능력 덕분에 객체 인식 기술은 현대 사회의 다양한 영역에서 혁신을 주도하는 핵심 동력으로 자리매김했다.

자율주행 자동차는 객체 인식 기술의 대표적인 응용 분야로, 도로 위의 다른 차량, 보행자, 교통 신호 등을 실시간으로 정확하게 인식하여 안전한 주행 경로를 계획하고 돌발 상황에 대처하는 데 필수적으로 사용된다.3 의료 영상 분석 분야에서는 자기공명영상(MRI)이나 컴퓨터 단층촬영(CT) 스캔 이미지에서 종양이나 병변의 위치와 크기를 정밀하게 탐지하여 의사의 진단을 보조하고 치료 계획 수립에 기여한다.3 이 외에도 공장 자동화 라인에서의 불량품 검사, 소매점에서의 재고 관리, 보안 시스템에서의 침입자 감지, 스포츠 영상 분석 등 그 활용 범위는 무궁무진하게 확장되고 있다.5


경영학의 대가 피터 드러커(Peter Drucker)는 "측정되는 것이 개선된다(What gets measured gets improved)"라는 통찰을 남겼다.1 이 격언은 객체 인식 모델 개발 과정에도 그대로 적용된다. 모델의 성능을 정량적으로 측정하고 평가하는 과정 없이는 모델의 개선 방향을 설정하거나 그 효과를 검증하는 것이 불가능하다. 신뢰성 있는 성능 평가는 모델 개발의 전 과정에서 나침반과 같은 역할을 수행하며, 다음과 같은 핵심적인 기능을 담당한다.

첫째, **모델 간의 공정한 비교(Benchmarking)**를 가능하게 한다. 표준화된 데이터셋과 평가 지표를 통해 연구자들은 새로 개발한 모델의 성능을 기존의 최첨단(State-of-the-Art, SOTA) 모델들과 객관적으로 비교할 수 있으며, 이를 통해 기술의 발전을 가속화할 수 있다.1

둘째, **응용 분야에 최적화된 모델 선택**의 기준을 제공한다. 특정 응용 분야에서는 모델의 정확도 외에도 추론 속도, 메모리 사용량 등 다양한 요소가 중요하게 고려된다. 종합적인 성능 평가 지표는 이러한 요구사항에 가장 부합하는 모델을 선택하는 데 필요한 정량적 근거를 제공한다.2

셋째, **모델의 약점을 분석하고 최적화(Optimization)** 방향을 제시한다. 정밀도(Precision), 재현율(Recall) 등의 세부 지표를 분석함으로써 모델이 어떤 종류의 오류를 주로 범하는지(예: 객체를 놓치는가, 혹은 배경을 객체로 잘못 탐지하는가) 파악할 수 있다. 이러한 오류 분석은 모델의 구조를 개선하거나 학습 데이터를 보강하는 등 구체적인 개선 전략을 수립하는 데 결정적인 단서를 제공한다.1


본 보고서는 객체 인식 모델의 성능을 평가하는 데 사용되는 다양한 지표와 방법론을 기초적인 개념부터 최신 연구 동향에 이르기까지 체계적으로 정리하고 심층적으로 분석하는 것을 목표로 한다. 각 평가 지표의 이론적 배경과 수학적 정의를 명확히 하고, 그 지표가 가지는 실질적인 의미와 장단점, 그리고 다른 지표와의 관계를 다각적으로 조명할 것이다.

보고서는 다음과 같은 구조로 전개된다.

- **제1장**에서는 성능 평가의 가장 기본적인 요소인 Ground Truth, 예측(Prediction)의 개념을 정의하고, 객체 인식 평가의 핵심인 TP, FP, FN의 의미를 재해석한다.
- **제2장**에서는 위치 정확도를 측정하는 핵심 지표인 IoU를 시작으로, 그 한계를 극복하기 위해 제안된 GIoU, DIoU, CIoU 등 진화된 지표들을 상세히 다룬다.
- **제3장**에서는 분류 성능을 평가하는 정밀도와 재현율의 개념과 둘 사이의 상충 관계를 분석하고, 이를 종합하는 F1 점수와 시각화하는 정밀도-재현율 곡선을 설명한다.
- **제4장**에서는 단일 클래스 및 다중 클래스 모델의 종합 성능을 나타내는 핵심 지표인 AP와 mAP의 계산 방식과 그 변천 과정을 추적한다.
- **제5장**에서는 모델의 실용성을 평가하는 효율성 및 복잡도 지표(FPS, Latency, FLOPs, 파라미터 수)를 다루고, 이론적 계산량과 실제 성능 간의 간극을 분석한다.
- **제6장**에서는 객체 인식 연구의 양대 산맥인 PASCAL VOC와 MS COCO의 평가 프로토콜을 비교 분석하여 표준화된 평가의 중요성을 강조한다.
- **제7장**에서는 mAP라는 단일 숫자를 넘어 모델의 실패 원인을 심층적으로 진단하는 LRP, TIDE와 같은 고급 오류 분석 방법론을 소개한다.
- 마지막으로 **결론**에서는 전체 내용을 요약하고, 상황에 맞는 최적의 평가 지표 선택을 위한 가이드라인과 향후 전망을 제시한다.


객체 인식 모델의 성능을 평가하기 위해서는 먼저 평가의 기준이 되는 기본 요소들을 명확히 이해해야 한다. 이는 평가의 전 과정에서 사용되는 공통된 언어이자, 모든 복잡한 지표들의 근간을 이루는 초석이다.


성능 평가는 본질적으로 모델이 만들어낸 '예측'과 우리가 정답으로 간주하는 'Ground Truth'를 비교하는 과정이다.2

- **Ground Truth (실측값):** 평가의 절대적인 기준이 되는 '정답' 데이터를 의미한다. 이는 일반적으로 숙련된 인간 주석가(annotator)들이 이미지 내에 존재하는 모든 목표 객체에 대해 정확한 위치를 경계 상자(Bounding Box)로 그리고, 해당 객체의 클래스(예: '사람', '자동차')를 레이블링하여 생성된다.2 Ground Truth는 모델이 도달해야 할 이상적인 목표점을 제시한다.
- **Prediction (예측값):** 객체 인식 모델이 특정 이미지를 입력받아 출력하는 결과물을 의미한다. 각 예측은 일반적으로 세 가지 요소로 구성된다.1
  1. **경계 상자 (Bounding Box):** 모델이 객체가 존재한다고 판단한 위치를 나타내는 사각형 좌표.
  2. **예측 클래스 (Predicted Class):** 해당 경계 상자 안의 객체가 어떤 클래스에 속하는지에 대한 모델의 판단.
  3. **신뢰도 점수 (Confidence Score):** 해당 예측(클래스 및 위치)에 대해 모델이 얼마나 확신하는지를 나타내는 0과 1 사이의 확률값.1


단순 분류 문제에서 사용되는 혼동 행렬의 개념은 객체 인식 평가의 핵심 구성 요소인 True Positive (TP), False Positive (FP), False Negative (FN)를 이해하는 데 기초를 제공한다. 하지만 객체 인식에서는 '위치'라는 차원이 추가되기 때문에 이들의 정의가 더 복잡해진다.1

- **True Positive (TP, 진양성):** 모델이 객체를 **올바르게** 탐지한 경우를 의미한다. 어떤 예측이 TP로 인정받기 위해서는 다음 두 가지 조건을 **모두** 만족해야 한다.2
  1. **분류 정확성:** 예측된 클래스가 Ground Truth의 클래스와 일치해야 한다.
  2. **위치 정확성:** 예측된 경계 상자와 Ground Truth 경계 상자 간의 **Intersection over Union (IoU)** 값이 사전에 정의된 임계값(threshold, 예: 0.5) 이상이어야 한다. IoU에 대한 자세한 내용은 제2장에서 다룬다.
- **False Positive (FP, 위양성):** 모델이 **부정확하게** 탐지한 경우로, 다음과 같은 시나리오들이 포함된다.2
  1. **위치 오류 (Localization Error):** 예측된 클래스는 맞았지만, IoU 값이 임계값 미만인 경우. 즉, 객체를 찾기는 했으나 위치가 충분히 정확하지 않은 경우다.
  2. **분류 오류 (Classification Error):** IoU 값은 임계값 이상이지만, 예측된 클래스가 Ground Truth와 다른 경우.
  3. **배경 오탐 (Background Error):** 실제로는 객체가 없는 배경(background) 영역을 객체로 잘못 탐지한 경우.
- **False Negative (FN, 위음성):** 모델이 탐지해야 할 객체를 **놓친** 경우다. 즉, 이미지 내에 존재하는 Ground Truth 객체에 대해, IoU 임계값을 만족하는 어떠한 예측도 매칭되지 않았을 때 해당 Ground Truth는 FN으로 간주된다.2

이처럼 객체 인식에서의 TP, FP, FN 정의는 단순한 이진 분류와 근본적으로 다르다. 이는 '분류(Classification)'와 '위치(Localization)'라는 두 가지 과업이 결합된 평가의 복합성을 내포하기 때문이다. 동일한 모델의 동일한 예측이라도, 평가 기준이 되는 IoU 임계값이 0.5일 때는 TP가 될 수 있지만, 0.75로 더 엄격해지면 FP가 될 수 있다. 이처럼 평가 기준의 동적인 변화 가능성은 객체 인식 평가의 본질적인 특징이며, 이후에 설명할 mAP 지표가 단일 값이 아닌 다양한 IoU 임계값에 대해 계산되는 이유를 설명하는 근본적인 원인이 된다.


주목할 점은 객체 인식 평가에서는 일반적으로 True Negative (TN, 진음성)를 사용하지 않는다는 것이다.2 TN은 '모델이 배경을 배경으로 올바르게 판단한 경우'에 해당한다. 하지만 한 이미지 내에는 객체가 없는 무수히 많은 잠재적 위치(배경)가 존재하므로, 이를 모두 TN으로 계산하는 것은 실질적인 의미가 없으며 평가 지표를 왜곡시킬 수 있다. 객체 인식의 목표는 '존재하는 객체를 얼마나 잘 찾아내는가'에 맞춰져 있기 때문에, 평가는 존재하는 객체(Ground Truth)를 중심으로 TP, FP, FN을 통해 이루어진다.2


- **신뢰도 점수 (Confidence Score):** 모델이 각 예측에 대해 얼마나 확신하는지를 나타내는 0과 1 사이의 값이다.1 이 점수는 일반적으로 모델 내부에서 계산된 두 가지 확률의 곱으로 구성된다: (1) 해당 경계 상자 내에 어떤 객체든 존재할 확률(objectness score)과 (2) 그 객체가 특정 클래스일 확률(class score).1
- **임계값의 역할:** 신뢰도 점수는 모델의 모든 예측을 순위 매기는 기준으로 사용된다. 실제 평가나 응용에서는 신뢰도 점수 임계값(confidence threshold)을 설정하여, 이 값보다 낮은 점수를 가진 예측들은 무시하는 방식으로 예측을 필터링한다. 이 임계값을 어떻게 설정하느냐에 따라 모델의 성능 지표가 크게 달라진다. 임계값을 높이면 더 확신하는 소수의 예측만 남게 되어 정밀도(Precision)는 높아지는 경향이 있지만, 일부 올바른 예측까지 버려져 재현율(Recall)은 낮아질 수 있다. 반대로 임계값을 낮추면 더 많은 예측이 포함되어 재현율은 높아지지만, 불확실한 예측이 FP로 포함될 가능성이 커져 정밀도는 낮아질 수 있다.5 이 관계는 제3장에서 상술할 정밀도-재현율 상충 관계(trade-off)의 핵심적인 메커니즘이다.


객체 인식은 '무엇'을 찾았는지(분류)와 '어디서' 찾았는지(위치)를 모두 평가해야 하는 이중 과업이다. 이 중 위치 예측의 정확성을 정량적으로 측정하는 가장 기본적이고 핵심적인 지표가 바로 Intersection over Union (IoU)이다. IoU는 객체 인식 평가의 근간을 이루며, TP와 FP를 판정하는 기준이 되고, 더 나아가 모델 학습 과정에서 손실 함수로 직접 활용되기도 한다.


- **개념:** IoU는 이름 그대로 예측된 경계 상자(Predicted Bounding Box)와 실제 경계 상자(Ground Truth Bounding Box) 간의 겹침 정도를 측정하는 지표다. 이는 두 영역 간의 유사도를 나타내는 척도로, 'Jaccard Index' 또는 'Jaccard Similarity'라는 이름으로도 불린다.9 IoU 값은 0과 1 사이의 범위를 가지며, 0은 두 상자가 전혀 겹치지 않음을, 1은 두 상자가 완벽하게 일치함을 의미한다.3

- **수학적 공식:** 두 경계 상자 A와 B에 대하여, IoU는 두 상자의 교집합(Intersection) 영역의 넓이를 합집합(Union) 영역의 넓이로 나눈 값으로 정의된다.14
  $$
  IoU(A, B) = \frac{\rvert A \cap B \rvert}{\rvert A \cup B \rvert} = \frac{Area(Intersection)}{Area(Union)}
  $$

- **계산 과정:** IoU의 계산은 다음과 같은 단계로 이루어진다.9

  1. **교집합 영역 계산:** 두 상자 A와 B의 좌표 $(x_1, y_1, x_2, y_2)$가 주어졌을 때, 교집합 영역의 좌상단 x좌표는 $\max(A.x_1, B.x_1)$이고, 좌상단 y좌표는 $\max(A.y_1, B.y_1)$이다. 마찬가지로 우하단 x좌표는 $\min(A.x_2, B.x_2)$이고, 우하단 y좌표는 $\min(A.y_2, B.y_2)$이다. 이 좌표들을 이용해 교집합 영역의 너비와 높이를 구하고, 그 곱으로 교집합의 넓이 `Area(Intersection)`를 계산한다. 만약 계산된 너비나 높이가 0 이하라면, 두 상자는 겹치지 않으므로 교집합 넓이는 0이다.

  2. 합집합 영역 계산: 두 상자의 합집합 넓이는 각 상자의 넓이의 합에서 교집합 넓이를 뺀 값과 같다. 이는 교집합 영역이 두 번 더해지는 것을 방지하기 위함이다.
     $$
     Area(Union) = Area(A) + Area(B) - Area(Intersection)
     $$

  3. **IoU 계산:** 위에서 계산된 교집합 넓이를 합집합 넓이로 나누어 최종 IoU 값을 얻는다.


IoU는 직관적이고 계산이 간편하여 널리 사용되지만, 몇 가지 명백한 한계를 가지고 있다. 이러한 한계는 IoU를 평가 지표뿐만 아니라 손실 함수로 사용하고자 할 때 더욱 두드러진다.

- **비겹침(Non-overlapping) 문제:** 두 경계 상자가 전혀 겹치지 않는 경우, IoU 값은 항상 0이다. 이는 두 상자가 1픽셀 떨어져 있든, 100픽셀 떨어져 있든 동일한 IoU 값을 반환함을 의미한다. 즉, 겹치지 않는 상자들이 얼마나 가까운지에 대한 정보를 전혀 제공하지 못한다. 이로 인해 IoU를 손실 함수로 사용할 경우, 겹치지 않는 예측 상자에 대해서는 그래디언트(gradient)가 0이 되어 모델이 예측 상자를 Ground Truth 방향으로 이동시키는 방법을 학습할 수 없는 'plateau' 현상이 발생한다.14
- **민감도 부족:** IoU는 오직 겹치는 영역의 크기에만 초점을 맞춘다. 따라서 동일한 IoU 값을 갖더라도, 두 상자의 정렬 상태, 중심점 간의 거리, 종횡비(aspect ratio) 등 다른 중요한 기하학적 특성은 매우 다를 수 있다. 예를 들어, 두 상자가 중심이 잘 맞지만 크기가 다른 경우와, 크기는 비슷하지만 중심이 어긋난 경우가 동일한 IoU를 가질 수 있다. 이는 모델이 더 나은 방향으로 경계 상자를 미세 조정하도록 이끄는 세밀한 피드백을 제공하지 못하는 원인이 된다.19
- **작은 객체에 대한 민감성:** 이미지 내에서 작은 객체를 다룰 때, 경계 상자의 위치가 단 몇 픽셀만 이동해도 IoU 값은 급격하게 변동할 수 있다. 반면, 큰 객체는 동일한 픽셀만큼 이동해도 IoU 변화가 훨씬 완만하다. 이러한 민감성 때문에 작은 객체에 대한 학습 과정에서 앵커 박스(anchor box)와 Ground Truth 간의 매칭이 불안정해질 수 있으며, 이는 결국 작은 객체 탐지 성능 저하로 이어진다.21


IoU의 한계를 극복하고, 특히 손실 함수로서의 성능을 개선하기 위해 여러 변형 지표들이 제안되었다. IoU의 진화 과정은 '좋은 경계 상자'에 대한 정의를 점점 더 정교하게 수학적으로 모델링하고, 이를 모델 학습 과정에 직접 주입하여 성능을 근본적으로 향상시키려는 노력의 산물이다. 이는 평가 지표가 수동적인 측정 도구에서 능동적인 학습 가이드로 변모했음을 의미한다.

- **GIoU (Generalized IoU):**

  - **개념:** GIoU는 IoU의 비겹침 문제를 해결하기 위해 처음으로 제안된 주요 변형이다. 핵심 아이디어는 두 경계 상자 A와 B를 모두 포함하는 가장 작은 볼록 집합(2차원 사각형의 경우, 두 상자를 모두 감싸는 가장 작은 사각형) C를 도입하는 것이다. GIoU는 기존 IoU 값에서 페널티 항을 빼는 방식으로 계산되는데, 이 페널티 항은 전체 영역 C에서 두 상자의 합집합 영역이 차지하는 비율을 나타낸다. 즉, 두 상자가 멀리 떨어져 있을수록 합집합 영역이 C에서 차지하는 비중이 작아지고, 페널티가 커지게 된다.14

  - **수학적 공식:**
    $$
    GIoU = IoU - \frac{\rvert C \setminus (A \cup B) \rvert}{\rvert C \rvert}
    $$
    **장점:** 두 상자가 겹치지 않아 IoU가 0이더라도, 페널티 항을 통해 예측 상자를 Ground Truth 방향으로 이동시키는 그래디언트를 제공할 수 있어 모델 학습이 가능하다.14

  - **단점:** GIoU는 여전히 한계를 가진다. 예측 상자가 Ground Truth 상자 내부에 포함되거나, 수평 또는 수직 방향으로 정렬된 경우에는 페널티 항이 0에 가까워져 GIoU가 IoU와 거의 동일하게 퇴화(degenerate)된다. 이 경우, 모델은 여전히 상자를 어떻게 움직여야 할지에 대한 명확한 신호를 받지 못해 수렴이 느리고 부정확한 회귀 문제를 겪을 수 있다.20

- **DIoU (Distance-IoU):**

  - **개념:** DIoU는 GIoU의 느린 수렴 문제를 해결하기 위해, 두 경계 상자의 중심점(central point) 사이의 거리를 직접적으로 고려한다. GIoU가 전체 영역을 통해 간접적으로 거리를 페널티로 삼았다면, DIoU는 두 상자의 중심점 간의 정규화된 유클리드 거리를 직접적인 페널티 항으로 추가하여, 예측 상자의 중심이 Ground Truth의 중심에 더 가깝게 이동하도록 명확하게 유도한다.20

  - **수학적 공식 (손실 함수):**
    $$
    \mathcal{L}_{DIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2}
    $$
    여기서 $b$와 $b^{gt}$는 각각 예측 상자와 Ground Truth 상자의 중심점을, $\rho(\cdot)$는 두 점 사이의 유클리드 거리를 나타낸다. $c$는 두 상자를 모두 포함하는 가장 작은 사각형의 대각선 길이로, 거리 항을 정규화하는 역할을 한다.

  - **장점:** 중심점을 직접적으로 정렬하도록 유도함으로써 GIoU보다 훨씬 빠른 수렴 속도를 보인다. 특히 GIoU가 퇴화하는 포함 관계나 수평/수직 정렬 상황에서도 효과적으로 작동한다.20

- **CIoU (Complete-IoU):**

  - **개념:** CIoU는 좋은 경계 상자 회귀를 위한 세 가지 핵심 기하학적 요소를 모두 고려하는 가장 완성된 형태의 IoU 기반 손실 함수다. 세 가지 요소는 (1) 겹침 영역(IoU), (2) 중심점 거리(DIoU의 페널티 항), 그리고 (3) 종횡비(aspect ratio) 일관성이다. 종횡비 일관성 페널티 항을 추가함으로써, 예측 상자가 Ground Truth 상자와 비슷한 모양을 갖도록 유도한다.18

  - **수학적 공식 (손실 함수):**
    $$
    \mathcal{L}_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v
    $$
    여기서 DIoU 손실 함수에 추가된 $\alpha v$가 종횡비 일관성을 측정하는 페널티 항이다. $v = \frac{4}{\pi^2}(\arctan\frac{w^{gt}}{h^{gt}} - \arctan\frac{w}{h})^2$는 두 상자의 종횡비 차이를 측정하며, $\alpha$는 겹침 정도에 따라 종횡비 페널티의 중요도를 조절하는 가중치 파라미터다.

  - **장점:** 겹침, 거리, 모양이라는 세 가지 핵심 요소를 모두 최적화하여 가장 빠르고 정확한 회귀 성능을 제공한다. 이는 현재 많은 최신 객체 탐지 모델에서 표준적인 경계 상자 회귀 손실 함수로 채택되고 있다.18

아래 표는 IoU와 그 변형 지표들의 특징을 요약하여 비교한 것이다.

| 지표 (Metric) | 고려 요인 (Factors Considered)                         | 주요 특징 (Key Characteristics)                              |
| ------------- | ------------------------------------------------------ | ------------------------------------------------------------ |
| **IoU**       | 겹침 영역 (Overlap Area)                               | 가장 기본적인 위치 정확도 척도. 비겹침 시 그래디언트 소실 문제 발생. |
| **GIoU**      | 겹침 영역 + 전체적인 근접성 (Proximity)                | 비겹침 문제를 해결하기 위해 두 상자를 감싸는 최소 박스를 도입. 포함 관계 시 IoU로 퇴화. |
| **DIoU**      | 겹침 영역 + 중심점 거리 (Center Point Distance)        | 중심점 간 거리를 직접 페널티로 사용하여 GIoU보다 빠른 수렴 속도 달성. |
| **CIoU**      | 겹침 영역 + 중심점 거리 + 종횡비 일관성 (Aspect Ratio) | 겹침, 거리, 모양 세 가지 요소를 모두 고려하여 가장 빠르고 정확한 회귀 성능 제공. |


객체 인식 모델은 객체의 위치뿐만 아니라 클래스도 정확하게 예측해야 한다. 따라서 위치 정확도와 더불어 분류 성능을 평가하는 것이 필수적이다. 이를 위해 전통적인 분류 문제에서 널리 사용되는 정밀도(Precision)와 재현율(Recall) 개념이 객체 인식 평가에도 핵심적으로 사용된다.


정밀도와 재현율은 모델의 예측 결과가 얼마나 신뢰할 수 있는지, 그리고 얼마나 포괄적인지를 서로 다른 관점에서 측정하는 지표다.

- **정밀도 (Precision):** 모델이 'Positive'라고 예측한 결과들 중에서 실제로 'Positive'인 결과의 비율을 의미한다. 이는 "모델의 예측이 얼마나 정확한가?"라는 질문에 답한다. 높은 정밀도는 모델이 내놓은 예측 결과를 믿을 수 있음을 의미하며, 낮은 거짓 양성(False Positive, FP) 비율과 동의어다.5
  $$
  Precision = \frac{TP}{TP + FP} = \frac{\text{올바르게 탐지한 객체 수}}{\text{모델이 탐지했다고 예측한 총 객체 수}}
  $$
  **재현율 (Recall):** 실제 'Positive'인 모든 결과들 중에서 모델이 'Positive'라고 올바르게 예측한 결과의 비율을 의미한다. 이는 "모델이 찾아야 할 것을 얼마나 빠짐없이 찾아냈는가?"라는 질문에 답한다. 높은 재현율은 모델이 대부분의 실제 객체를 놓치지 않았음을 의미하며, 낮은 거짓 음성(False Negative, FN) 비율과 동의어다.5
  $$
  Recall = \frac{TP}{TP + FN} = \frac{\text{올바르게 탐지한 객체 수}}{\text{이미지 내 실제 총 객체 수}}
  $$
  정밀도와 재현율은 단순한 성능 지표를 넘어, 특정 응용 프로그램의 '비용 함수(cost function)'를 반영하는 전략적 도구로 활용될 수 있다. 어떤 종류의 오류(FP 또는 FN)가 더 큰 손실이나 위험을 초래하는지에 따라 두 지표 중 어느 것을 우선적으로 최적화할지 결정해야 한다.

예를 들어, 고양이가 화장실을 사용할 때마다 자동으로 물을 내리는 시스템을 개발한다고 가정해보자.27 이 경우, FP(고양이가 없는데 물을 내림)는 물 낭비와 고양이에게 스트레스를 주는 비용을 발생시킨다. 반면, FN(고양이가 사용했는데 물을 안 내림)은 약간의 불편함만 초래할 뿐이다. 따라서 FP로 인한 비용이 FN보다 크므로, 이 시스템은 FP를 최소화하는, 즉 높은 

**정밀도**를 갖도록 최적화되어야 한다.

반대로, 송유관의 파열 징후를 감지하는 시스템의 경우 27, FP(파열이 없는데 경보 발령)는 엔지니어가 현장을 확인하는 약간의 수고를 유발한다. 하지만 FN(실제 파열을 놓침)은 막대한 환경 재앙과 경제적 손실이라는 치명적인 결과를 초래한다. 이 경우, FN으로 인한 비용이 FP보다 압도적으로 크므로, 시스템은 FN을 최소화하는, 즉 높은 

**재현율**을 갖도록 설계되어야 한다. 이처럼 정밀도와 재현율의 선택은 기술적 문제를 넘어, 해당 기술이 적용되는 도메인의 요구사항과 위험 관리의 문제와 직결된다.


정밀도와 재현율은 일반적으로 서로 상충 관계(trade-off)에 있다.5 즉, 하나의 지표를 높이려고 하면 다른 지표가 낮아지는 경향이 있다. 이 관계의 중심에는 모델의 예측을 필터링하는 데 사용되는 신뢰도 임계값(confidence threshold)이 있다.

- **임계값을 높이면:** 모델은 매우 확신하는 예측만을 결과로 내놓는다. 이로 인해 불확실한 예측들이 걸러지면서 FP가 줄어들어 **정밀도는 상승**한다. 하지만, 신뢰도가 약간 낮았던 올바른 예측(TP)까지 함께 걸러져 FN이 증가하므로 **재현율은 하락**한다.
- **임계값을 낮추면:** 모델은 더 많은 예측을 결과로 포함시킨다. 이로 인해 이전에 놓쳤던 객체들을 탐지하게 되어 FN이 줄어들어 **재현율은 상승**한다. 그러나, 신뢰도가 낮은 부정확한 예측들이 FP로 포함될 가능성이 커져 **정밀도는 하락**한다.

이러한 상충 관계 때문에 모델의 성능을 단 하나의 정밀도 또는 재현율 값으로 평가하는 것은 불완전하다. 모델의 전반적인 성능을 이해하기 위해서는 모든 가능한 임계값에 대한 정밀도와 재현율의 변화를 함께 살펴보아야 한다.


- **정의:** 정밀도-재현율 곡선(PR Curve)은 위에서 설명한 상충 관계를 시각적으로 표현한 그래프다. 모델의 모든 예측을 신뢰도 점수 순으로 정렬한 뒤, 신뢰도 임계값을 1에서 0으로 점차 낮추면서 각 임계값 지점에서의 정밀도와 재현율 쌍을 계산하여 그린다. 관례적으로 x축에는 재현율(Recall)을, y축에는 정밀도(Precision)를 표시한다.8
- **해석:** 이상적인 객체 인식 모델은 재현율이 증가하더라도 높은 정밀도를 계속 유지해야 한다. 따라서 PR 곡선이 그래프의 우상단에 가까울수록, 즉 곡선 아래의 면적이 넓을수록 모델의 성능이 우수함을 의미한다.12 PR 곡선은 모델이 낮은 재현율에서는 높은 정밀도를 보이다가 재현율이 높아짐에 따라 정밀도가 어떻게 감소하는지와 같은 모델의 성능 특성을 한눈에 파악할 수 있게 해준다.8 또한, 이 곡선을 통해 "재현율을 최소 90% 이상 확보해야 할 때, 우리가 감수해야 하는 최대 정밀도 손실은 얼마인가?"와 같은 구체적인 질문에 대한 답을 찾을 수 있으며, 이는 특정 응용 분야의 요구사항에 맞는 최적의 운영 임계값을 선택하는 데 도움을 준다.5


정밀도와 재현율의 상충 관계 속에서 두 지표의 균형을 고려한 단일 평가 지표가 필요할 때 F1 점수가 유용하게 사용된다.

- **정의:** F1 점수는 정밀도와 재현율의 조화 평균(harmonic mean)으로, 두 지표를 하나의 숫자로 요약하여 모델의 성능을 평가한다. FP와 FN 모두를 중요하게 고려해야 하는 상황에서 균형 잡힌 성능을 측정하는 데 적합하다.1

- **수학적 공식:**
  $$
  F1 Score = 2 \times \frac{Precision \times Recall}{Precision + Recall}
  $$
  **의미:** F1 점수는 산술 평균과 달리, 두 값 중 더 낮은 값에 더 큰 가중치를 부여하는 특성이 있다. 이로 인해 정밀도와 재현율이 한쪽으로 크게 치우쳐 불균형할 경우 낮은 점수를 받게 된다. 예를 들어, 정밀도가 1.0이지만 재현율이 0.1인 극단적인 경우, 산술 평균은 0.55로 비교적 높아 보이지만, F1 점수는 약 0.18로 매우 낮게 계산된다.30 이는 F1 점수가 두 지표의 균형을 얼마나 중요하게 평가하는지를 잘 보여준다. 따라서 F1 점수가 높다는 것은 정밀도와 재현율이 모두 준수한 수준을 유지하고 있음을 의미한다.


정밀도-재현율 곡선은 모델의 성능을 다각적으로 보여주지만, 여러 모델을 비교하거나 단일 모델의 성능을 간결하게 보고하기 위해서는 이 곡선을 대표하는 단일 숫자 값이 필요하다. 이를 위해 등장한 것이 평균 정밀도(Average Precision, AP)이며, 이를 여러 클래스에 대해 확장한 것이 평균 AP(mean Average Precision, mAP)이다. mAP는 오늘날 객체 인식 모델의 성능을 평가하는 가장 표준적이고 핵심적인 지표로 자리 잡고 있다.


- **개념:** AP는 단일 객체 클래스에 대한 모델의 종합적인 탐지 성능을 나타내는 단일 숫자 값이다. 이는 정밀도-재현율(PR) 곡선의 형태를 하나의 값으로 요약한 것으로, 일반적으로 PR 곡선 아래 면적(Area Under the Curve, AUC)으로 해석된다.1 AP 값이 1에 가까울수록 해당 클래스에 대한 탐지 성능이 완벽에 가깝다는 것을 의미한다.

- **AP 계산 방식의 변천:** AP를 계산하는 방식은 평가 프로토콜의 발전에 따라 변화해왔다.

  - **11-Point Interpolation (11점 보간법):** PASCAL VOC 챌린지 초기(2007-2009년)에 사용된 방식이다. PR 곡선을 0, 0.1, 0.2,..., 1.0까지 총 11개의 고정된 재현율 지점에서 샘플링하여 해당 지점들의 정밀도 값의 평균을 계산한다. 이때, 각 재현율 지점 $r$에서의 정밀도 값 $p(r)$은 실제 재현율이 $r$ 이상인 모든 지점에서 측정된 정밀도 값 중 가장 큰 값으로 보간(interpolate)된다. 이 보간 과정은 실제 PR 곡선이 신뢰도 임계값의 미세한 변화에 따라 지그재그 형태로 나타나는 것을 완화하여, 순위의 작은 변동에 덜 민감한 안정적인 평가를 하기 위해 도입되었다.9
    $$
    AP = \frac{1}{11} \sum_{r \in \{0, 0.1,..., 1.0\}} p_{interp}(r) \quad \text{where} \quad p_{interp}(r) = \max_{\tilde{r}:\tilde{r} \ge r} p(\tilde{r})
    $$
    **All-Point Interpolation (모든 지점 보간법 / AUC):** PASCAL VOC 2010 이후부터 채택된 방식으로, 11개의 고정된 지점만 샘플링하는 대신, 재현율이 변하는 모든 지점에서의 정밀도 값을 고려하여 PR 곡선 아래 면적을 더 정확하게 계산(적분)한다. 이는 보간된 PR 곡선 아래의 면적을 구하는 것과 같다. Scikit-learn의 `average_precision_score`와 같은 대부분의 최신 라이브러리들은 이 방식을 따른다. 이는 11점 보간법보다 더 정밀한 AP 값을 제공한다.26
    $$
    AP = \sum_{n} (R_n - R_{n-1}) P_n
    $$
    여기서 $P_n$과 $R_n$은 n번째 임계값(신뢰도 점수 순으로 정렬된 예측 중 n번째 예측을 임계값으로 삼을 때)에서의 정밀도와 재현율을 의미하며, $(R_n - R_{n-1})$은 재현율의 증가분을 나타낸다.


- **개념:** mAP는 데이터셋에 포함된 모든 객체 클래스에 대한 AP 값들의 산술 평균이다. 예를 들어, 데이터셋에 '사람', '자동차', '자전거' 세 개의 클래스가 있고 각각의 AP가 0.8, 0.7, 0.6이라면, mAP는 $(0.8 + 0.7 + 0.6) / 3 = 0.7$이 된다. 이 지표는 다중 클래스 객체 탐지 모델의 전반적인 성능을 나타내는 가장 대표적인 단일 지표로 사용된다.1
  $$
  mAP = \frac{1}{N_{classes}} \sum_{i=1}^{N_{classes}} AP_i
  $$
  **COCO에서의 용어 혼용:** MS COCO 챌린지에서는 AP와 mAP를 엄격히 구분하지 않고 혼용하는 경향이 있다. COCO의 공식 문서에서 'AP'라고 표기된 지표는 실제로는 모든 클래스와 여러 IoU 임계값에 대해 평균을 낸 값이므로, 전통적인 의미의 mAP에 해당한다. 이는 COCO 평가가 다중 클래스와 다중 IoU 임계값을 기본으로 하기 때문이다.26

- **IoU 임계값에 따른 mAP 변형:** mAP 값은 TP/FP를 판정하는 기준인 IoU 임계값에 따라 크게 달라진다. 따라서 어떤 IoU 임계값에서 계산된 mAP인지를 명시하는 것이 매우 중요하다.

  - **mAP@.5 (또는 AP50):** PASCAL VOC에서 사용된 전통적인 평가 방식으로, IoU 임계값을 0.5로 고정하여 mAP를 계산한다. 이는 예측된 경계 상자와 실제 상자가 50% 이상 겹치면 올바른 위치로 간주하는, 상대적으로 느슨한 위치 정확도 기준이다.9
  - **mAP@.75 (또는 AP75):** IoU 임계값을 0.75로 더 엄격하게 설정하여 계산한 mAP다. 이는 더 높은 수준의 위치 정확도를 요구한다.9
  - **mAP@[.5:.95] (COCO Primary Metric):** MS COCO 챌린지의 주 평가지표(primary metric)로, 가장 널리 사용되는 엄격한 기준이다. 이는 IoU 임계값을 0.5에서 시작하여 0.95까지 0.05 간격으로 변화시키면서(즉, 0.5, 0.55, 0.6,..., 0.95의 10개 레벨) 각 임계값에 대해 mAP를 계산한 후, 그 결과들을 다시 평균 낸 값이다. 이 지표는 모델이 다양한 수준의 위치 정확도 요구사항에 대해 얼마나 강건하게(robust) 성능을 유지하는지를 종합적으로 평가한다. 위치 정확도가 매우 뛰어난 모델만이 이 지표에서 높은 점수를 얻을 수 있다.9

mAP 지표의 발전 과정, 특히 PASCAL VOC의 mAP@.5에서 COCO의 mAP@[.5:.95]로의 전환은 객체 탐지 연구 커뮤니티가 '좋은 탐지'에 대한 기준을 어떻게 점진적으로 상향 조정해왔는지를 보여주는 역사적 기록이다. 초기에는 객체의 존재를 탐지하는 것 자체에 중점을 두었기 때문에 IoU 0.5라는 기준은 합리적이었다.26 그러나 딥러닝 기반 모델들의 성능이 비약적으로 발전하면서, 단순히 객체를 '찾는 것'을 넘어 '얼마나 더 정밀하게 위치를 특정하는가'가 새로운 경쟁의 초점이 되었다. mAP@.5만으로는 IoU가 0.6인 모델과 0.9인 모델의 성능 차이를 변별하기 어려웠고, 이는 더 정밀한 평가 기준의 필요성으로 이어졌다.31 COCO의 mAP@[.5:.95]는 이러한 요구에 부응하여, 마치 높이뛰기 경기에서 바(bar)의 높이를 계속 올려가며 선수의 한계를 시험하듯, 점진적으로 더 높은 IoU 임계값을 요구함으로써 모델의 위치 정확도 성능을 세밀하게 평가한다.11 이 지표의 도입은 객체 탐지 기술의 성숙을 상징하며, 이제 커뮤니티가 실제 응용(예: 자율주행)에 필수적인 높은 수준의 위치 정밀도(localization precision)를 모델에게 요구하고 있음을 명확히 보여준다.


현대의 객체 인식 모델을 평가할 때, mAP로 대표되는 정확도만으로는 충분하지 않다. 특히 자율주행, 모바일 기기, 엣지 컴퓨팅과 같이 실시간성과 제한된 자원이 중요한 응용 분야에서는 모델의 효율성과 복잡도가 정확도만큼이나 중요한 평가 기준이 된다. 따라서 모델의 추론 속도와 계산적 복잡도를 나타내는 지표들을 함께 고려하는 다각적인 평가가 필수적이다.


추론 속도 지표는 모델이 얼마나 빠르게 예측을 수행할 수 있는지를 측정한다.

- **FPS (Frames Per Second):** 모델이 1초 동안 처리할 수 있는 이미지(프레임)의 수를 의미한다. 이 값이 높을수록 단위 시간당 더 많은 데이터를 처리할 수 있음을 나타내며, 특히 비디오 스트림을 실시간으로 분석해야 하는 응용에서 핵심적인 성능 지표다.36
- **Latency (지연 시간):** 단일 이미지가 모델에 입력된 순간부터 예측 결과가 출력되기까지 걸리는 총 시간을 의미한다. 보통 밀리초(ms) 단위로 측정되며, 이 값이 낮을수록 사용자가 느끼는 응답성이 좋다.36

FPS와 Latency는 역수 관계($FPS \approx 1000 / Latency(ms)$)에 있지만, 평가하는 관점에 차이가 있다. FPS는 시스템의 처리량(throughput)을, Latency는 단일 요청에 대한 응답 시간(response time)을 나타낸다.

실시간 시스템, 특히 안전이 최우선인 자율주행과 같은 분야에서 이 지표들은 매우 중요하다. 자율주행 시스템은 주변 환경의 변화에 즉각적으로 반응해야 하므로, 객체 탐지의 지연 시간은 치명적인 사고로 이어질 수 있다. 일반적으로 자율주행 분야에서는 실시간 처리를 위해 최소 30 FPS 이상이 요구되는 것으로 알려져 있다.4 이상적으로는 차량에 장착된 카메라의 프레임 속도(예: 10~40 FPS)에 맞춰 지연 없이 처리하는 것이 목표이며, 예를 들어 40 FPS 카메라의 경우 프레임 간 시간 간격인 25ms 이내에 처리가 완료되어야 한다.41


모델 복잡도 지표는 모델의 규모와 계산 요구량을 나타내며, 이는 하드웨어 제약 조건과 밀접한 관련이 있다.

- **파라미터 수 (Number of Parameters):** 모델이 학습 과정에서 최적화하는 학습 가능한 가중치(weight)와 편향(bias)의 총 개수를 의미한다. 이는 모델의 크기(model size)와 직결되며, 모델을 저장하는 데 필요한 메모리 공간(storage)을 결정한다. 일반적으로 파라미터 수가 많을수록 모델의 표현력(capacity)이 높아져 더 복잡한 패턴을 학습할 수 있지만, 동시에 과적합(overfitting)의 위험이 커지고 추론에 필요한 메모리(RAM) 요구량도 증가한다.36
- **FLOPs (Floating Point Operations):** 모델이 단일 이미지를 추론하기 위해 수행하는 부동소수점 연산(덧셈, 곱셈 등)의 총량을 의미한다. 이는 모델의 계산 복잡도(computational complexity)를 나타내는 하드웨어 독립적인(hardware-agnostic) 이론적 척도다. 보통 수십억 단위를 의미하는 GFLOPs (Giga-FLOPs)로 표기된다. FLOPs가 낮을수록 더 적은 계산으로 추론이 가능하므로, 이론적으로 더 효율적인 모델이라고 할 수 있다.36


모델의 효율성을 평가할 때, FLOPs는 유용한 지표지만 맹신해서는 안 된다. 이론적인 계산량인 FLOPs와 실제 하드웨어에서 측정되는 Latency 사이에는 종종 상당한 괴리가 존재하기 때문이다. 이 간극을 이해하는 것은 실제 환경에 모델을 배포(deploy)할 때 매우 중요하다.23

연구 단계에서 개발자들은 mAP를 높이면서도 FLOPs를 낮추는 것을 목표로 새로운 아키텍처를 설계한다. 이는 하드웨어에 구애받지 않고 모델의 근본적인 계산 효율성을 보여주기 때문이다.42 그러나 실제 제품에 모델을 탑재하는 배포 단계에서는 이론적인 FLOPs보다 사용자가 체감하는 '응답 속도', 즉 실제 Latency가 훨씬 중요하다.23

FLOPs가 낮은 모델이 특정 하드웨어에서 오히려 FLOPs가 높은 모델보다 느린 역설적인 현상이 발생할 수 있다. 이 간극은 다음과 같은 다양한 '숨겨진 변수'들에 의해 발생한다.

- **하드웨어 아키텍처 및 병렬성:** 실제 Latency는 CPU, GPU, NPU 등 프로세서의 종류, 코어 수, 캐시 메모리의 크기, 그리고 연산을 얼마나 효율적으로 병렬 처리할 수 있는지에 따라 크게 좌우된다. 특히 모바일 GPU와 같은 고도로 병렬화된 하드웨어에서는 연산량과 Latency 간의 관계가 매우 비선형적일 수 있다.23
- **메모리 접근 비용 (Memory Access Cost):** FLOPs는 순수하게 산술 연산의 수만 계산하고, 모델의 파라미터나 중간 계산 결과(feature map)를 메모리에서 읽고 쓰는 데 걸리는 시간은 포함하지 않는다. 메모리 대역폭(bandwidth)이 제한적인 엣지 디바이스에서는 이 메모리 접근 시간이 전체 Latency에서 상당한 비중을 차지하는 병목(bottleneck)이 될 수 있다.42
- **소프트웨어 및 라이브러리 최적화:** 동일한 모델이라도 어떤 딥러닝 프레임워크(예: TensorFlow Lite, PyTorch Mobile)와 저수준 연산 라이브러리(예: cuDNN, Intel MKL)를 사용하느냐에 따라 성능이 크게 달라진다. 특정 하드웨어에 최적화된 연산자(operator)를 사용하는지 여부가 실제 Latency에 큰 영향을 미친다.23
- **연산의 종류와 밀도:** FLOPs는 모든 부동소수점 연산을 동일한 비용으로 간주하지만, 실제 하드웨어에서는 연산의 종류(예: 일반적인 합성곱 vs. 깊이별 분리 합성곱)나 데이터의 희소성(sparsity)에 따라 처리 속도가 다를 수 있다. 또한, 활성화 함수(ReLU), 풀링(Pooling), 데이터 재배열(Reshape) 등 FLOPs가 거의 없거나 없는 연산들도 실제로는 시간을 소모한다.42

결론적으로, 모델 효율성 평가는 '정확도-속도 상충관계(Accuracy-Speed Trade-off)'라는 다차원적인 최적화 문제다. 이 문제에서 FLOPs는 이론적 계산량이라는 중요한 한 축을 보여주는 유용한 지표지만, 실제 배포 환경에서는 Latency라는 최종 결과에 영향을 미치는 수많은 변수가 존재한다. 따라서 진정한 모델 효율성 평가는 연구 단계에서 FLOPs를 통한 아키텍처 수준의 효율성 분석과, 배포 단계에서 목표 하드웨어에서의 직접적인 프로파일링을 통한 실제 Latency 및 병목 구간 분석이 반드시 함께 이루어져야 한다. FLOPs가 모델의 '가능성'을 보여준다면, Latency는 그 가능성이 실현된 '현실'을 보여준다.


객체 인식 모델의 성능을 객관적으로 비교하고 기술 발전을 측정하기 위해서는 모두가 동의하는 표준화된 규칙, 즉 평가 프로토G콜(evaluation protocol)이 필수적이다. 이 분야에서는 역사적으로 PASCAL VOC와 MS COCO라는 두 개의 대규모 챌린지 및 데이터셋이 표준 프로토콜의 발전을 이끌어왔다. 두 프로토콜은 단순히 데이터셋의 크기 차이를 넘어, 객체 인식 문제를 바라보는 '철학'의 차이를 반영하며, 이는 평가 방식의 설계에 그대로 드러난다.


- **데이터셋 특징:** 2005년부터 2012년까지 매년 개최된 PASCAL VOC 챌린지는 객체 탐지 연구의 초기 표준을 확립하는 데 결정적인 역할을 했다. 특히 VOC 2007과 VOC 2012 데이터셋이 벤치마크로 널리 사용된다. 이 데이터셋은 '사람', '자동차', '고양이', '개' 등을 포함한 20개의 핵심 객체 클래스로 구성되어 있다.45 이미지 수는 총 수만 장 규모로, 최신 데이터셋에 비하면 상대적으로 작지만, 여전히 많은 연구에서 기본적인 성능 검증을 위한 벤치마크로 활용되고 있다.45 VOC 데이터셋의 이미지는 비교적 명확하고 중심적인 객체를 포함하는 경우가 많아, '분류 중심의 인식(Recognition-centric)' 문제에 초점을 맞추고 있다고 볼 수 있다.
- **평가 프로토콜:**
  - **주요 지표:** mAP (mean Average Precision)를 주 평가지표로 사용한다.11
  - **IoU 임계값:** TP와 FP를 판정하는 기준으로 **단일 IoU 임계값 0.5**를 사용한다. 이는 예측된 경계 상자와 실제 상자가 50% 이상 겹치면 위치가 정확하다고 간주하는 비교적 관대한 기준이다. 이는 객체의 존재를 '인식'하는 것 자체에 더 큰 비중을 둔 평가 방식이라 할 수 있다.11
  - **AP 계산:** 초기(VOC 2007)에는 11-point interpolation 방식을 사용했으나, 2010년부터는 모든 재현율 지점을 고려하는 Area Under Curve (AUC) 방식으로 변경하여 더 정확한 AP를 계산한다.31


- **데이터셋 특징:** 2014년에 처음 공개된 MS COCO는 이름('문맥 속의 일상 객체')에서 알 수 있듯이, 복잡한 실제 환경을 모사하는 데 중점을 둔 대규모 데이터셋이다. 현재 객체 탐지, 인스턴스 분할(instance segmentation), 이미지 캡셔닝 등 여러 컴퓨터 비전 분야에서 사실상의 표준(de facto standard) 벤치마크로 자리 잡았다.45 COCO는 80개의 객체 카테고리('things')와 91개의 배경 카테고리('stuff')를 포함하며, 33만 장 이상의 이미지와 150만 개가 넘는 방대한 객체 인스턴스를 제공한다.50 특히, PASCAL VOC에 비해 이미지당 객체 수가 많고, 작은 크기의 객체가 차지하는 비중이 높으며, 객체 간 가림(occlusion)이 빈번하게 나타나 훨씬 더 도전적인 환경을 제공한다. 이는 "이 복잡한 장면 속에서 저기 작게 보이는 저것이 정확히 무엇이며, 그 경계는 어디까지인가?"와 같은, '문맥과 위치 중심의 이해(Context and Localization-centric)' 문제를 다룬다고 볼 수 있다.52
- **평가 프로토콜:**
  - **주요 지표:** AP (Average Precision)를 주 지표로 사용한다. COCO에서는 mAP와 AP를 동일한 의미로 사용하며, 이는 모든 클래스와 모든 IoU 임계값에 대한 평균을 의미한다.35
  - **IoU 임계값:** COCO 프로토콜의 가장 큰 특징은 **다중 IoU 임계값**을 사용한다는 점이다. 주 평가지표는 IoU 임계값을 0.5부터 0.95까지 0.05 간격으로 변화시키면서 계산한 10개의 AP 값을 평균 낸 **AP@[.5:.95]**이다. 이는 모델의 위치 정확도에 대한 성능을 매우 엄격하고 종합적으로 평가한다. 이 외에도 PASCAL VOC와의 비교를 위한 AP@.5 (AP50), 더 엄격한 기준인 AP@.75 (AP75) 등도 함께 보고된다.11
  - **객체 크기별 평가:** COCO는 객체의 면적(픽셀 수)을 기준으로 small ($area < 32^2$), medium ($32^2 < area < 96^2$), large ($area > 96^2$) 세 가지 크기로 구분하여 각각에 대한 AP (AP_S, AP_M, AP_L)를 별도로 계산한다. 이를 통해 모델이 특정 크기의 객체에 대해 성능 편향을 보이는지, 예를 들어 작은 객체 탐지에 취약하지는 않은지를 세밀하게 분석할 수 있다.9


표준화된 평가 프로토콜은 서로 다른 모델의 성능을 동일한 잣대로 공정하게 비교할 수 있게 해주는 필수적인 장치다. 어떤 프로토콜을 사용하느냐에 따라 모델 성능에 대한 평가가 달라질 수 있으므로, 결과를 해석할 때는 어떤 프로토콜 하에서 측정된 값인지를 명확히 인지해야 한다.

PASCAL VOC의 mAP@.5는 모델의 전반적인 탐지 능력을 평가하는 데 중점을 두는 반면, COCO의 AP@[.5:.95]는 높은 수준의 위치 정확도를 달성하는 능력을 훨씬 더 중요하게 평가한다. 따라서 한 모델이 mAP@.5에서는 다른 모델보다 우수하지만, AP@[.5:.95]에서는 뒤처지는 경우가 발생할 수 있다. 이는 전자의 모델이 객체를 잘 찾아내지만 위치는 다소 부정확하게 예측하는 경향이 있고, 후자의 모델은 일부 객체를 놓치더라도 일단 찾은 객체의 위치는 매우 정밀하게 예측하는 특성이 있음을 시사한다.

연구 동향의 관점에서 볼 때, COCO 프로토콜의 등장은 객체 탐지 기술이 높은 수준의 위치 정확도를 요구하는 방향으로 발전하고 있음을 명확히 보여준다. 오늘날 발표되는 대부분의 새로운 모델들은 COCO 데이터셋에서의 성능을 기준으로 그 우수성을 입증하며, AP@[.5:.95]는 모델의 종합적인 성능을 가늠하는 가장 중요한 척도로 받아들여지고 있다.26

연구자나 개발자가 자신의 모델을 어떤 데이터셋으로 평가할지 선택하는 것은, 단순히 벤치마크 점수를 얻는 행위를 넘어, 자신의 모델이 어떤 종류의 '지능'을 갖추기를 원하는지에 대한 철학적 선택과 같다. VOC는 강건한 '인식기'를, COCO는 정교한 '장면 분석기'를 만드는 것을 목표로 한다고 비유할 수 있다. 아래 표는 두 표준 프로토콜의 핵심적인 차이점을 요약한 것이다.

| 평가 항목 (Evaluation Item) | PASCAL VOC                             | MS COCO                                   |
| --------------------------- | -------------------------------------- | ----------------------------------------- |
| **발표 연도**               | 2005-2012                              | 2014-현재                                 |
| **클래스 수**               | 20개                                   | 80개 (Things) + 91개 (Stuff)              |
| **이미지/인스턴스 수**      | ~1.1만 장 / ~2.7만 개                  | ~33만 장 / ~150만 개                      |
| **주요 평가지표**           | mAP                                    | AP (실질적으로 mAP)                       |
| **IoU 임계값**              | **단일 임계값**: 0.5                   | **다중 임계값**: 0.5 ~ 0.95 (0.05 간격)   |
| **객체 크기별 평가**        | 없음                                   | **있음**: Small, Medium, Large            |
| **주요 철학**               | 분류 중심의 인식 (Recognition-centric) | 문맥과 위치 중심의 이해 (Context-centric) |


mAP는 모델의 전반적인 성능을 하나의 편리한 숫자로 요약해주지만, 그 이면의 복잡한 정보를 압축하는 과정에서 중요한 세부 사항을 놓치게 된다. mAP 점수가 낮은 이유가 무엇인지, 모델이 주로 어떤 종류의 실수를 저지르는지에 대한 구체적인 정보를 제공하지 못한다.56 예를 들어, 두 모델이 동일한 mAP 점수를 기록했더라도, 한 모델은 객체의 위치를 잘 못 잡는 '위치 오류'가 주된 문제일 수 있고, 다른 모델은 배경을 객체로 잘못 탐지하는 '배경 오탐'이 문제일 수 있다.59 모델을 실질적으로 개선하기 위해서는 이러한 실패의 근본 원인을 진단하는 과정이 필수적이다.

이러한 필요성에 따라, mAP를 넘어서 모델의 오류를 체계적으로 분석하고 정량화하는 심층 오류 분석 방법론들이 등장했다. 이는 객체 탐지 연구의 패러다임을 단순히 '성능 경쟁'에서 '과학적 분석과 디버깅'으로 전환시키는 중요한 흐름이다. 이 접근법은 모델을 더 이상 블랙박스로 취급하지 않고, 그 내부의 실패 메커니즘을 이해하고 체계적으로 개선하려는 성숙한 공학적 접근을 반영한다.


mAP와 같은 단일 종합 지표는 다음과 같은 한계를 가진다.

- **정보의 손실:** PR 곡선 전체의 정보를 하나의 숫자로 압축하면서, 모델의 구체적인 행동 특성(예: 특정 재현율 구간에서의 정밀도 급락)을 파악하기 어렵다.
- **진단 능력의 부재:** "왜 성능이 낮은가?"라는 질문에 답을 주지 못한다. 단지 '성능이 낮다'는 사실만을 알려줄 뿐, 개선을 위한 구체적인 방향을 제시하지 못한다.
- **오류 유형의 모호성:** FP나 FN이라는 결과 뒤에 숨겨진 다양한 원인(위치 부정확, 클래스 오분류, 중복 탐지 등)을 구분하지 못한다.

따라서 모델의 약점을 정확히 진단하고, 한정된 자원을 가장 효과적인 개선 방향에 집중시키기 위해서는 체계적인 오류 분석이 반드시 필요하다.1


- **개념:** LRP Error는 AP가 위치 정확도를 간접적으로만 반영하고 서로 다른 PR 곡선을 구분하지 못하는 단점을 보완하기 위해 제안된 새로운 평가 지표다. 이름에서 알 수 있듯이, 객체 탐지의 세 가지 핵심 요소인 **위치(Localization)**, **재현율(Recall)**, **정밀도(Precision)**와 관련된 오류를 명시적으로 분리하여 측정하고 이를 하나의 통합된 오류 점수로 나타낸다. LRP Error는 점수가 낮을수록 성능이 우수함을 의미한다.61
- **LRP의 구성 요소:** LRP Error는 주어진 신뢰도 임계값에서 계산되며, 세 가지 주요 오류 구성 요소의 가중 합으로 정의된다.62
  - **$LRP_{IoU}$ (위치 오류):** TP로 판정된 예측들의 평균 $(1 - IoU)$ 값이다. 이는 모델이 객체를 올바르게 탐지했지만, 그 위치가 얼마나 부정확했는지를 직접적으로 측정한다.
  - **$LRP_{FP}$ (정밀도 오류):** FP의 비율로, $(1 - Precision)$과 같다. 이는 모델이 얼마나 많은 잘못된 예측을 했는지를 나타낸다.
  - **$LRP_{FN}$ (재현율 오류):** FN의 비율로, $(1 - Recall)$과 같다. 이는 모델이 얼마나 많은 실제 객체를 놓쳤는지를 나타낸다.
- **최적 LRP (oLRP):** LRP의 가장 큰 장점 중 하나는 '최적 LRP(Optimal LRP, oLRP)' 개념이다. 이는 신뢰도 임계값을 0에서 1까지 변화시키면서 계산된 모든 LRP Error 값 중 가장 낮은 최소값을 의미한다. oLRP는 해당 모델이 달성할 수 있는 최상의 성능(최소 오류)을 나타낼 뿐만 아니라, 그 성능을 달성하기 위한 **최적의 신뢰도 임계값**을 부수적으로 제공한다. 이는 AP가 제공하지 못하는 정보로, 실제 모델을 배포하여 운영할 때 어떤 임계값을 사용해야 최적의 성능을 낼 수 있는지에 대한 매우 실용적인 가이드를 제공한다.59


- **개념:** TIDE는 mAP를 대체할 수 있는 범용 오류 분석 툴박스로, 모델의 예측 결과 파일을 입력받아 mAP 저하에 기여하는 오류들을 6가지 상호 배타적인 유형으로 자동 분류하고, 각 오류 유형이 전체 성능에 미치는 영향을 정량적으로 분석해준다. 이는 모델의 종류나 데이터셋에 구애받지 않고 적용할 수 있는 강력하고 일반적인 진단 도구다.56
- **6가지 주요 오류 유형:** TIDE는 모든 FP와 FN을 다음과 같은 6가지 유형으로 분해한다.

| 오류 유형 (Error Type)        | 약어 (Abbr.) | 정의 (Definition)                                            |
| ----------------------------- | ------------ | ------------------------------------------------------------ |
| **Classification Error**      | Cls          | 위치는 정확하지만(IoU ≥ 0.5), 클래스를 잘못 예측한 경우.     |
| **Localization Error**        | Loc          | 클래스는 맞지만, 위치가 부정확한 경우(0.1 ≤ IoU < 0.5).      |
| **Both Cls & Loc Error**      | Both         | 클래스도 틀리고, 위치도 부정확한 경우.                       |
| **Duplicate Detection Error** | Dup          | 올바른 탐지지만, 이미 더 높은 점수의 예측이 해당 Ground Truth에 매칭된 중복 탐지. |
| **Background Error**          | Bkg          | 배경을 객체로 잘못 탐지한 경우(모든 GT와의 IoU < 0.1).       |
| **Missed GT Error**           | Miss         | 아예 탐지되지 않은 Ground Truth 객체.                        |

- **영향 정량화 방식:** TIDE의 핵심적인 기여는 각 오류 유형의 '비용'을 정량화하는 독창적인 방식에 있다. TIDE는 각 오류 유형을 마법처럼 '수정'해주는 가상의 '오라클(Oracle)'을 가정한다. 예를 들어, '분류 오라클'은 모든 분류 오류(Cls)를 올바른 클래스로 수정해준다. 이 오라클을 적용했을 때 mAP가 얼마나 상승하는지($\Delta AP$)를 측정하여, 해당 오류 유형이 원래 모델의 mAP를 얼마나 깎아 먹고 있었는지를 계산한다. 중요한 점은 각 오류의 영향을 **독립적으로** 측정한다는 것이다. 즉, 항상 원래 모델의 성능을 기준으로 각 오라클을 개별적으로 적용하여 $\Delta AP$를 계산한다. 이는 오류 간의 상호작용 효과를 배제하고 각 오류 유형의 순수한 기여도를 분리해낼 수 있게 해주는 강력한 방법론이다.56


이러한 심층 오류 분석 도구들은 모델 개선을 위한 구체적인 '처방'을 내리는 데 활용될 수 있다. 이는 '추측 기반의 튜닝'을 '데이터 기반의 체계적인 디버깅'으로 전환시키는 과학적인 개선 사이클을 가능하게 한다.

- **사례 분석:** COCO 데이터셋으로 학습된 Mask R-CNN 모델을 TIDE로 분석했다고 가정해보자.60 분석 결과, 전체 mAP 손실의 가장 큰 부분이 'Localization Error'에서 비롯된 것으로 나타났다고 하자.58
- **진단 및 처방:**
  - **진단 1: 높은 위치 오류 (Localization Error):** 이는 모델의 경계 상자 회귀(bounding box regression) 기능에 근본적인 약점이 있음을 명확히 시사한다.
  - **처방 1:** 이 문제를 해결하기 위해, 다음과 같은 전략을 시도해볼 수 있다. (a) 모델의 경계 상자 회귀 손실 함수를 기존에 사용하던 Smooth L1 Loss에서 제2장에서 다룬 CIoU Loss나 DIoU Loss와 같이 기하학적 특성을 더 잘 반영하는 정교한 함수로 교체한다.20 (b) 데이터셋의 객체 크기 분포를 분석하여, 모델이 사용하는 앵커 박스(anchor box)의 스케일과 종횡비를 데이터셋의 특성에 맞게 재조정(re-tuning)한다.60
  - **진단 2: 특정 클래스에서의 분류 오류 (Classification Error):** TIDE 분석을 더 깊이 들어가, '가위', '토스터', '헤어드라이어'와 같은 특정 클래스에서 유독 분류 오류와 미탐지 오류(Missed GT Error)가 집중적으로 발생함을 확인했다고 가정하자.60
  - **처방 2:** 이는 해당 클래스들의 훈련 데이터가 부족하거나, 다른 클래스들과 시각적으로 유사하여 모델이 혼동하기 때문일 가능성이 높다. 이에 대한 해결책으로는, (a) 문제가 되는 클래스의 이미지를 추가로 수집하거나 데이터 증강(data augmentation) 기법을 적극적으로 활용하여 데이터 불균형을 완화한다. (b) Hard Negative Mining과 같은 기법을 통해 모델이 혼동하기 쉬운 샘플들을 학습 과정에서 더 집중적으로 학습하도록 유도하여 변별력을 높인다.66

이처럼 심층 오류 분석은 모델의 성능을 단순히 하나의 숫자로 평가하는 것을 넘어, 성능 저하의 원인을 구체적으로 진단하고, 그에 맞는 효과적인 해결책을 제시함으로써 모델 개발 과정을 과학적이고 체계적으로 이끌어 나가는 핵심적인 역할을 수행한다.



객체 인식 모델의 성능 평가는 기술의 발전과 함께 끊임없이 진화해왔다. 초기의 평가는 IoU, 정밀도, 재현율과 같은 기본적인 지표에서 시작하여, 이들을 종합적으로 나타내는 AP와 mAP로 발전했다. 평가의 표준을 제시한 PASCAL VOC 챌린지는 mAP@.5라는 단일 IoU 임계값 기반의 평가 방식을 정립하며 초기 연구를 선도했다. 이후, 더 복잡하고 현실적인 시나리오를 담은 MS COCO 데이터셋의 등장은 평가의 패러다임을 한 단계 끌어올렸다. COCO는 mAP@[.5:.95]라는 다중 IoU 임계값과 객체 크기별 평가 방식을 도입함으로써, 모델의 위치 정확도와 다양한 스케일에 대한 강건성을 더욱 엄격하고 종합적으로 평가하는 새로운 표준을 제시했다.


현대에 이르러, mAP만으로는 모델의 가치를 온전히 평가할 수 없다는 인식이 확산되었다. 특히 실제 시스템에 모델을 배포하는 관점에서는 정확도와 함께 실용성을 평가하는 것이 필수적이다. 이에 따라, 모델의 추론 속도를 나타내는 FPS와 Latency, 그리고 계산 복잡도와 모델 크기를 나타내는 FLOPs와 파라미터 수를 함께 고려하는 다각적인 평가 방식이 표준으로 자리 잡고 있다. 이는 모델의 이론적 성능뿐만 아니라, 제한된 하드웨어 자원과 실시간 요구사항 속에서의 실제적인 효율성까지 종합적으로 평가하려는 시도다.


최근에는 mAP라는 단일 점수 뒤에 숨겨진 모델의 구체적인 실패 원인을 파악하려는 노력이 강조되고 있다. LRP, TIDE와 같은 심층 오류 분석 방법론의 등장은 이러한 흐름을 대표한다. 이들 도구는 모델의 오류를 위치, 분류, 배경 오탐 등 다양한 유형으로 분해하고 각 오류의 영향을 정량화함으로써, 개발자가 모델의 약점을 명확히 진단하고 데이터 기반의 체계적인 개선 전략을 수립할 수 있도록 돕는다. 이는 모델 개발 과정을 '성능 경쟁'에서 '과학적 디버깅'의 차원으로 격상시키는 중요한 패러다임 전환이다.


다양한 평가 지표 중에서 어떤 것을 선택하고 집중해야 하는지는 당면한 과제와 목표에 따라 달라진다. 다음은 상황별 최적 지표 선택을 위한 가이드라인이다.

- **빠른 프로토타이핑 및 일반 성능 확인:** **mAP@.5**는 계산이 빠르고 전반적인 탐지 능력을 직관적으로 보여주므로, 초기 모델 개발 단계나 빠른 검증에 적합하다.
- **높은 위치 정확도가 요구되는 학술 연구 및 벤치마킹:** **mAP@[.5:.95]** (COCO standard)는 현재 학계와 산업계에서 가장 널리 인정받는 표준 지표로, 모델의 종합적이고 강건한 성능을 입증하는 데 필수적이다.
- **실시간 응용 프로그램 (자율주행, CCTV 등):** **FPS/Latency**를 mAP와 함께 최우선으로 고려해야 한다. 아무리 정확도가 높아도 실시간 요구사항을 만족하지 못하면 무용지물이다.
- **엣지 디바이스 배포 및 모델 최적화:** **FLOPs/파라미터 수**를 통해 모델의 이론적 효율성을 가늠하고, 반드시 **실제 타겟 하드웨어에서의 Latency**를 측정하여 이론과 현실의 간극을 확인해야 한다.
- **모델 성능 한계 돌파 및 디버깅:** mAP 개선이 정체될 때, **TIDE, LRP**와 같은 오류 분석 도구를 사용하여 모델의 근본적인 약점을 진단하고, 이를 바탕으로 개선 방향을 설정하는 것이 효과적이다.


객체 인식 성능 평가 방법론은 앞으로도 기술과 응용의 요구에 맞춰 계속해서 발전할 것이다. 미래의 평가 방법론은 더욱 세분화되고 특정 도메인에 특화된 형태로 진화할 것으로 전망된다. 예를 들어, 안전이 중요한(safety-critical) 자율주행 분야에서는 단순히 FP/FN을 계산하는 것을 넘어, '보행자를 차량으로 오인하는 오류'와 '보행자를 완전히 놓치는 오류'에 서로 다른 위험도 가중치를 부여하여 평가하는 방식이 도입될 수 있다. 또한, 악천후, 야간, 센서 노이즈 등 특정 환경에 대한 모델의 강건성(robustness)을 정량적으로 측정하는 지표나, 특정 인구 집단에 대한 편향성을 평가하는 공정성(fairness) 지표의 중요성도 더욱 커질 것이다. 이처럼, 성능 평가는 앞으로도 객체 인식 기술의 신뢰성을 담보하고 올바른 발전 방향을 제시하는 선도적인 역할을 계속해서 수행할 것이다.


1. Key Object Detection Metrics for Computer Vision - Roboflow Blog, 8월 15, 2025에 액세스, https://blog.roboflow.com/object-detection-metrics/
2. Object Detection: Key Metrics for Computer Vision Performance - Label Your Data, 8월 15, 2025에 액세스, https://labelyourdata.com/articles/object-detection-metrics
3. Intersection over Union (IoU) Explained - Ultralytics, 8월 15, 2025에 액세스, https://www.ultralytics.com/glossary/intersection-over-union-iou
4. Adaptive Real-Time Object Detection for Autonomous Driving Systems - MDPI, 8월 15, 2025에 액세스, https://www.mdpi.com/2313-433X/8/4/106
5. Precision vs. Recall - Full Guide to Understanding Model Output, 8월 15, 2025에 액세스, https://viso.ai/computer-vision/precision-recall/
6. ROMA: Run-Time Object Detection To Maximize Real-Time Accuracy, 8월 15, 2025에 액세스, https://research-portal.st-andrews.ac.uk/files/283409783/Lee_2023_IEEE_CVF_ROMA_AAM.pdf
7. Mean Average Precision in Object Detection : A Comprehensive Guide - Encord, 8월 15, 2025에 액세스, https://encord.com/blog/mean-average-precision-object-detection/
8. Performance Metrics Deep Dive - Ultralytics YOLO Docs, 8월 15, 2025에 액세스, https://docs.ultralytics.com/guides/yolo-performance-metrics/
9. The Complete Guide to Object Detection Evaluation Metrics: From IoU to mAP and More | by Prathamesh Amrutkar | Medium, 8월 15, 2025에 액세스, https://medium.com/@prathameshamrutkar3/the-complete-guide-to-object-detection-evaluation-metrics-from-iou-to-map-and-more-1a23c0ea3c9d
10. Checklist to Define the Identification of TP, FP, and FN Object Detections in Automated Driving - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2308.07106
11. Evaluation Metrics for Object detection algorithms | by Vijay Dubey ..., 8월 15, 2025에 액세스, https://medium.com/@vijayshankerdubey550/evaluation-metrics-for-object-detection-algorithms-b0d6489879f3
12. Object-Detection-Metrics/README.md at master / rafaelpadilla ..., 8월 15, 2025에 액세스, https://github.com/rafaelpadilla/Object-Detection-Metrics/blob/master/README.md
13. mAP in Object Detection: Mean Average Precision Explained - Roboflow Blog, 8월 15, 2025에 액세스, https://blog.roboflow.com/mean-average-precision/
14. Generalized Intersection over Union, 8월 15, 2025에 액세스, https://giou.stanford.edu/
15. viso.ai, 8월 15, 2025에 액세스, [https://viso.ai/computer-vision/intersection-over-union-iou/#:~:text=We%20compute%20the%20IoU%20by,and%20ground%20truth%20bounding%20boxes.](https://viso.ai/computer-vision/intersection-over-union-iou/#:~:text=We compute the IoU by,and ground truth bounding boxes.)
16. Intersection over Union (IoU): Definition, Calculation, Code - V7 Labs, 8월 15, 2025에 액세스, https://www.v7labs.com/blog/intersection-over-union-guide
17. Generalized Intersection over Union: A Metric and A Loss for ..., 8월 15, 2025에 액세스, https://arxiv.org/abs/1902.09630
18. GIoU, CIoU and DIoU: Variants of IoU and how they are better compared to IoU | by Abhishek Jain | Medium, 8월 15, 2025에 액세스, https://medium.com/@abhishekjainindore24/giou-ciou-and-diou-variants-of-iou-and-how-they-are-better-compared-to-iou-4610a015643a
19. Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/331371000_Generalized_Intersection_over_Union_A_Metric_and_A_Loss_for_Bounding_Box_Regression
20. Distance-IoU Loss: Faster and Better Learning for Bounding ... - AAAI, 8월 15, 2025에 액세스, https://cdn.aaai.org/ojs/6999/6999-13-10228-1-10-20200525.pdf
21. Illustration of the limitations of IoU. A is the ground truth bounding... | Download Scientific Diagram - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/figure/llustration-of-the-limitations-of-IoU-A-is-the-ground-truth-bounding-box-B-and-C-are_fig1_359451865
22. NGIoU Loss: Generalized Intersection over Union Loss Based on a New Bounding Box Regression - MDPI, 8월 15, 2025에 액세스, https://www.mdpi.com/2076-3417/12/24/12785
23. Latency Estimation Tool and Investigation of Neural Networks ..., 8월 15, 2025에 액세스, https://www.mdpi.com/2073-431X/10/8/104
24. Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/337386850_Distance-IoU_Loss_Faster_and_Better_Learning_for_Bounding_Box_Regression
25. Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/342308234_Distance-IoU_Loss_Faster_and_Better_Learning_for_Bounding_Box_Regression
26. mAP (mean Average Precision) for Object Detection | by Jonathan ..., 8월 15, 2025에 액세스, https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173
27. Precision and Recall in Machine Learning - Roboflow Blog, 8월 15, 2025에 액세스, https://blog.roboflow.com/precision-and-recall/
28. How Compute Accuracy For Object Detection works-ArcGIS Pro | Documentation, 8월 15, 2025에 액세스, https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/how-compute-accuracy-for-object-detection-works.htm
29. Mean Average Precision (mAP): A Complete Guide - Kili Technology, 8월 15, 2025에 액세스, https://kili-technology.com/data-labeling/machine-learning/mean-average-precision-map-a-complete-guide
30. Evaluating Object Detection Models Using Mean Average Precision (mAP) - DigitalOcean, 8월 15, 2025에 액세스, https://www.digitalocean.com/community/tutorials/mean-average-precision
31. Evaluation metrics for object detection and segmentation: mAP, 8월 15, 2025에 액세스, https://kharshit.github.io/blog/2019/09/20/evaluation-metrics-for-object-detection-and-segmentation
32. Evaluation Metrics for Object Detection - DebuggerCafe, 8월 15, 2025에 액세스, https://debuggercafe.com/evaluation-metrics-for-object-detection/
33. average_precision_score - scikit-learn 1.7.1 documentation, 8월 15, 2025에 액세스, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html
34. Area under Precision-Recall Curve (AUC of PR-curve) and Average Precision (AP), 8월 15, 2025에 액세스, https://stats.stackexchange.com/questions/157012/area-under-precision-recall-curve-auc-of-pr-curve-and-average-precision-ap
35. What is Mean Average Precision (MAP) and how does it work | Xailient, 8월 15, 2025에 액세스, https://xailient.com/blog/what-is-mean-average-precision-and-how-does-it-work/
36. Understanding Evaluation parameters for Object Detection Models ..., 8월 15, 2025에 액세스, https://medium.com/@nikitamalviya/evaluation-of-object-detection-models-flops-fps-latency-params-size-memory-storage-map-8dc9c7763cfe
37. computer vision - What does the notation mAP@[.5:.95] mean ..., 8월 15, 2025에 액세스, https://datascience.stackexchange.com/questions/16797/what-does-the-notation-map-5-95-mean
38. Release the mAP5 - Kaggle, 8월 15, 2025에 액세스, https://www.kaggle.com/code/mpwolke/release-the-map5
39. The Confusing Metrics of AP and mAP for Object Detection / Instance Segmentation, 8월 15, 2025에 액세스, https://yanfengliux.medium.com/the-confusing-metrics-of-ap-and-map-for-object-detection-3113ba0386ef
40. Real-time Traffic Object Detection for Autonomous Driving - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2402.00128v2
41. Re-thinking CNN Frameworks for Time-Sensitive Autonomous ..., 8월 15, 2025에 액세스, https://www.cs.unc.edu/~anderson/papers/rtas19.pdf
42. FLOPs: Machine Learning Model Computational Complexity - Ultralytics, 8월 15, 2025에 액세스, https://www.ultralytics.com/glossary/flops
43. How can I measure time and memory complexity for a deep learning model?, 8월 15, 2025에 액세스, https://datascience.stackexchange.com/questions/104676/how-can-i-measure-time-and-memory-complexity-for-a-deep-learning-model
44. How are FLOPS impacting LLM development? - Deepchecks, 8월 15, 2025에 액세스, https://www.deepchecks.com/question/flops-impact-on-llm-development/
45. Mean Average Precision (mAP) in Object Detection - Learn OpenCV, 8월 15, 2025에 액세스, https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/
46. voc | TensorFlow Datasets, 8월 15, 2025에 액세스, https://www.tensorflow.org/datasets/catalog/voc
47. The Pascal Visual Object Classes (VOC) Challenge - Microsoft Research, 8월 15, 2025에 액세스, https://www.microsoft.com/en-us/research/publication/the-pascal-visual-object-classes-voc-challenge/
48. merve/pascal-voc / Datasets at Hugging Face, 8월 15, 2025에 액세스, https://huggingface.co/datasets/merve/pascal-voc
49. PASCAL VOC 2012 DATASET - Kaggle, 8월 15, 2025에 액세스, https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset
50. What is the COCO Dataset? What You Need to Know - viso.ai, 8월 15, 2025에 액세스, https://viso.ai/computer-vision/coco-dataset/
51. COCO - Common Objects in Context, 8월 15, 2025에 액세스, https://cocodataset.org/
52. Object-Detection-and-Tracking/COCO and Pascal VOC.md at master - GitHub, 8월 15, 2025에 액세스, [https://github.com/yehengchen/Object-Detection-and-Tracking/blob/master/COCO%20and%20Pascal%20VOC.md](https://github.com/yehengchen/Object-Detection-and-Tracking/blob/master/COCO and Pascal VOC.md)
53. COCO Dataset - Ultralytics YOLO Docs, 8월 15, 2025에 액세스, https://docs.ultralytics.com/datasets/detect/coco/
54. [1405.0312] Microsoft COCO: Common Objects in Context - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/1405.0312
55. models/research/object_detection/g3doc/evaluation_protocols.md at master - GitHub, 8월 15, 2025에 액세스, https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/evaluation_protocols.md
56. TIDE: A General Toolbox for Identifying Object ... - Daniel Bolya, 8월 15, 2025에 액세스, https://dbolya.github.io/tide/paper.pdf
57. TIDE: A General Toolbox for Identifying Object Detection Errors - European Computer Vision Association, 8월 15, 2025에 액세스, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480562.pdf
58. Diagnosing Error in Object Detectors - Derek Hoiem, 8월 15, 2025에 액세스, https://dhoiem.web.engr.illinois.edu/publications/eccv2012_detanalysis_derek.pdf
59. cancam/LRP: Localization Recall Precision Performance Metric toolkit for PASCAL-VOC, COCO datasets with Python and MATLAB implementations. - GitHub, 8월 15, 2025에 액세스, https://github.com/cancam/LRP
60. How to Analyze Failure Modes of Object Detection Models for Debugging - Encord, 8월 15, 2025에 액세스, https://encord.com/blog/error-analysis-object-detection-models/
61. One Metric to Measure Them All: Localisation Recall Precision (LRP) for Evaluating Visual Detection Tasks | Request PDF - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/356480571_One_Metric_to_Measure_them_All_Localisation_Recall_Precision_LRP_for_Evaluating_Visual_Detection_Tasks
62. Localization Recall Precision (LRP): A New Performance Metric for ..., 8월 15, 2025에 액세스, https://arxiv.org/pdf/1807.01696
63. Object Detection Metrics - AIgents, 8월 15, 2025에 액세스, https://aigents.co/data-science-blog/case-study/object-detection-metrics
64. TIDE: A General Toolbox for Identifying Object Detection Errors - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/347456308_TIDE_A_General_Toolbox_for_Identifying_Object_Detection_Errors
65. dbolya/tide: A General Toolbox for Identifying Object Detection Errors - GitHub, 8월 15, 2025에 액세스, https://github.com/dbolya/tide
66. Road defect detection based on improved YOLOv8s model - PMC - PubMed Central, 8월 15, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC11271263/
67. Improvement in Error Recognition of Real-Time Football Images by an Object-Augmented AI Model for Similar Objects - MDPI, 8월 15, 2025에 액세스, https://www.mdpi.com/2079-9292/11/23/3876
