# 메트릭 러닝에 대한 고찰
[인공지능 훈련 데이터의 클래스 불균형 문제](./index.md)


기계 학습의 전통적인 패러다임은 주어진 데이터를 사전 정의된 클래스로 분류하는 '분류 학습(learning to classify)'에 중점을 두어 왔다. 그러나 안면 인식, 이미지 검색, 추천 시스템과 같이 클래스의 수가 방대하거나 훈련 시점에는 존재하지 않았던 새로운 클래스가 등장하는 '오픈-셋(open-set)' 문제에서는 이러한 접근 방식이 한계를 드러낸다.1 이러한 배경 속에서 '비교 학습(learning to compare)'이라는 새로운 패러다임이 등장했으며, 그 중심에는 메트릭 러닝(Metric Learning)이 자리 잡고 있다.3 메트릭 러닝의 핵심 목표는 단순히 분류 경계면을 학습하는 것이 아니라, 데이터 간의 의미론적 유사성(semantic similarity)을 거리(distance)로 표현하는 함수, 즉 '메트릭(metric)' 자체를 학습하는 것이다.5

이러한 접근법은 데이터를 고차원의 원본 공간에서 저차원의 의미론적 '임베딩 공간(embedding space)'으로 매핑하는 함수를 학습함으로써 달성된다.2 이 임베딩 공간은 유사한 객체들은 서로 가깝게 모이고(clustering), 상이한 객체들은 서로 멀리 떨어지도록 구조화된 다양체(manifold)이다. 이렇게 잘 학습된 임베딩 공간은 그 자체로 강력한 표현력을 가지며, k-최근접 이웃(k-Nearest Neighbors, k-NN) 분류, 군집화(clustering), 정보 검색(information retrieval)과 같은 다양한 다운스트림 태스크(downstream task)의 성능을 비약적으로 향상시키는 기반이 된다.7

메트릭 러닝의 역사는 고전적인 선형 변환 학습에서부터 시작되었다. 초기 연구들은 데이터 전체에 적용되는 단일 선형 변환 행렬을 학습하는 데 초점을 맞추었으나 12, 이는 복잡한 비선형 데이터 구조를 표현하는 데 명백한 한계를 가졌다. 이러한 한계를 극복하기 위해 등장한 것이 바로 딥러닝을 접목한 '딥 메트릭 러닝(Deep Metric Learning, DML)'이다. DML은 심층 신경망(Deep Neural Network, DNN)의 강력한 함수 근사 능력을 활용하여, 데이터로부터 임베딩 공간으로의 복잡하고 비선형적인 매핑을 종단간(end-to-end) 방식으로 학습한다.8

본 보고서는 메트릭 러닝의 광범위한 분야를 체계적으로 조망하는 것을 목표로 한다. 총 4부로 구성된 본 보고서는 먼저 메트릭의 수학적 정의와 고전적 접근법을 다루는 기초 이론부터 시작하여, 현대 DML을 구성하는 세 가지 핵심 요소인 네트워크 아키텍처, 손실 함수, 데이터 샘플링 전략을 심도 있게 분석한다. 이어서 안면 인식, 이미지 검색, 이상 탐지 등 다양한 응용 분야에서의 실제 사례 연구를 통해 메트릭 러닝의 실용적 가치를 조명하고, 마지막으로 메트릭 러닝과 다른 학습 패러다임과의 관계를 고찰하며 미래 연구 방향을 제시함으로써 이 분야에 대한 포괄적이고 깊이 있는 이해를 제공하고자 한다.


이 장에서는 메트릭 러닝 분야 전체를 지탱하는 이론적, 수학적 기반을 확립한다. 메트릭의 추상적인 정의에서부터 구체적인 초기 알고리즘의 구현에 이르기까지, 이 분야의 근간을 이루는 핵심 개념들을 순차적으로 탐구한다.


메트릭 러닝의 근간을 이해하기 위해서는 먼저 '메트릭' 또는 '거리 함수(distance function)'의 엄격한 수학적 정의를 살펴보는 것이 필수적이다. 이는 단순히 두 객체 간의 유사도를 수치화하는 것을 넘어, 학습된 공간의 기하학적 구조와 안정성을 보장하는 핵심적인 역할을 하기 때문이다.


수학적으로, 집합 $X$ 위의 거리 함수 $d: X \times X \rightarrow \mathbb{R}$는 임의의 원소 $x,y,z∈X$에 대해 다음 네 가지 공리적 속성을 반드시 만족해야 한다.

1. 비음수성 (Non-negativity): $d(x,y)≥0$

   두 객체 간의 거리는 항상 0보다 크거나 같아야 한다. 이는 거리의 물리적 개념에 대한 직관적인 확장이다.7

2. 동일성 원리 (Identity of Indiscernibles): $d(x,y)=0⟺x=y$

   두 객체 간의 거리가 0일 필요충분조건은 두 객체가 동일하다는 것이다. 즉, 서로 다른 객체는 반드시 0보다 큰 거리를 가져야 한다.7

3. 대칭성 (Symmetry): $d(x,y)=d(y,x)$

   객체 $x$에서 $y$까지의 거리는 $y$에서 $x$까지의 거리와 동일해야 한다. 이는 거리 측정이 방향에 의존하지 않음을 의미한다.7

4. 삼각 부등식 (Triangle Inequality): $d(x,z)≤d(x,y)+d(y,z)$

   객체 $x$에서 $z$로 직접 가는 거리는 항상 중간에 다른 객체 $y$를 거쳐 가는 거리의 합보다 작거나 같아야 한다. 이 속성은 임베딩 공간에 일관된 기하학적 구조를 부여하는 데 결정적인 역할을 한다.7


데이터 분석에서 흔히 사용되는 거리 함수로는 유클리드 거리(Euclidean distance, L2 norm), 맨해튼 거리(Manhattan distance, L1 norm), 코사인 유사도(Cosine Similarity) 등이 있다.6 이들은 별도의 학습 없이 즉시 적용할 수 있다는 장점이 있지만, 대부분의 실제 문제에서 최적의 성능을 보장하지 못한다. 그 이유는 이러한 '기성품(off-the-shelf)' 메트릭들이 데이터의 내재적 특성을 고려하지 않기 때문이다. 구체적으로, 이들은 모든 특징(feature)이 동등하게 중요하다고 가정하며, 특징들 간의 상관관계나 데이터의 전체적인 분포를 무시한다. 예를 들어, 폐암 발병 여부를 예측하는 문제에서 결혼 유무나 IQ와 같은 무관한 변수들까지 동일한 가중치로 거리 계산에 포함시키면, "입력 $X$가 유사하면 출력 $y$도 유사하다"는 핵심 가정이 깨지게 된다.15 바로 이러한 한계가 주어진 데이터와 과제에 최적화된 메트릭을 '학습'해야 할 필요성을 제기하는 핵심 동기이다.7


네 가지 공리 중에서도 삼각 부등식은 메트릭 러닝과 일반적인 유사도 학습(similarity learning)을 구분 짓는 가장 중요한 특징이다. 삼각 부등식의 존재는 단순한 수학적 형식주의를 넘어, 학습된 임베딩 공간의 안정성과 예측 가능성을 보장하는 실질적인 이점을 제공한다.

이 점은 특히 추천 시스템의 발전 과정에서 명확하게 드러난다. 전통적인 행렬 분해(Matrix Factorization) 기반의 추천 모델들은 사용자-아이템 간의 선호도를 내적(dot product)으로 모델링했다. 그러나 내적은 삼각 부등식을 만족하지 않는 단순한 유사도 점수에 불과하다.17 이로 인해 "사용자 A가 B와 유사하고, B가 C와 유사하다면, A도 C와 유사할 것이다"와 같은 직관적인 추론이 보장되지 않는 불안정한 임베딩 공간이 만들어졌다. 만약 A와 B가 가깝고 B와 C가 가깝더라도, A와 C는 임의로 멀리 떨어져 있을 수 있는 것이다. 이러한 전역적 일관성(global consistency)의 부재는 아이템-아이템 또는 사용자-사용자 간의 신뢰성 있는 유사도 포착을 방해하고, 미세한 선호도 차이를 모델링하는 데 한계를 야기했다.17

이러한 문제를 해결하기 위해 현대 추천 시스템들은 삼각 부등식을 만족하는 '진정한 메트릭'을 학습하는 메트릭 러닝 접근법을 적극적으로 도입했다.17 이는 학습된 공간에서 거리 관계가 일관되고 예측 가능하게 유지되도록 보장하며, 결과적으로 더 안정적이고 신뢰할 수 있는 랭킹 및 검색 성능으로 이어진다. 결국, 삼각 부등식은 임베딩 공간이 무질서한 점들의 집합이 아니라, 일관된 기하학적 구조를 갖춘 '공간'으로서 기능하게 만드는 핵심적인 제약 조건인 셈이다.


메트릭의 수학적 정의가 '무엇을' 학습해야 하는지에 대한 목표를 설정했다면, '어떻게' 학습할 것인가에 대한 구체적인 방법론의 실마리는 통계학에서 유래한 마할라노비스 거리(Mahalanobis Distance)에서 찾을 수 있다. 이 개념은 고정된 통계적 측정 도구에서 출발하여, 학습 가능한 파라미터를 도입함으로써 기계 학습의 영역으로 진화하는 교두보 역할을 했다.


1930년대에 P. C. Mahalanobis에 의해 제안된 마할라노비스 거리는 데이터의 분포를 고려한 통계적 거리 척도이다.12 두 점 $x_i$와 $x_j$ 사이의 마할라노비스 거리는 다음과 같이 정의된다.
$$
d_M(x_i, x_j) = \sqrt{(x_i - x_j)^T S^{-1}(x_i - x_j)}
$$
여기서 $S$는 데이터의 공분산 행렬(covariance matrix)이다.12 이 거리의 핵심적인 장점은 유클리드 거리를 뛰어넘는 몇 가지 중요한 특성에 있다. 첫째, 각 변수의 스케일에 영향을 받지 않는 '척도 불변성(scale-invariant)'을 가진다. 둘째, 변수들 간의 '상관관계를 고려'한다. 셋째, 데이터의 전체적인 '분포(분산)'를 반영한다.19 본질적으로 마할라노비스 거리는 두 점 사이의 거리를 유클리드 공간상의 직선 거리가 아닌, 데이터 분포의 중심(평균)으로부터의 '표준 편차 단위'로 측정하는 것과 같다.19


마할라노비스 거리의 우수성은 데이터 분포가 비등방성(anisotropic)일 때 명확하게 드러난다. 아래 그림과 같이 두 개의 데이터 군집(파란색과 빨간색)과 하나의 점 $x$가 있다고 가정하자.

그림 2.1: 유클리드 거리와 마할라노비스 거리의 차이. 점 $x$는 유클리드 거리상으로는 $μ_2$에 더 가깝지만, 데이터 분포를 고려한 마할라노비스 거리상으로는 $μ_1$에 더 가깝다.

유클리드 거리로 측정하면 점 $x$는 군집 2의 평균 $μ_2$에 더 가깝다. 하지만 군집 1은 가로 방향으로 분산이 매우 크고 세로 방향으로는 분산이 작다. 점 $x$는 분산이 큰 방향으로 $μ_1$에서 약간 벗어나 있지만, 이는 통계적으로 흔한 일이다. 반면, 군집 2는 모든 방향으로 분산이 작기 때문에, 점 $x$는 통계적으로 매우 이례적인 위치에 있다. 마할라노비스 거리는 이러한 분포의 형태를 고려하여 점 $x$가 실제로는 군집 1에 속할 가능성이 더 높다고 판단한다.18 이처럼 마할라노비스 거리는 단순한 기하학적 거리를 넘어 데이터의 통계적 구조를 반영하는 더 정교한 거리 척도이다.


메트릭 러닝의 결정적인 도약은 마할라노비스 거리의 공분산 행렬의 역행렬 $S^{-1}$을 학습 가능한 임의의 가중치 행렬 W로 대체하면서 이루어졌다. 이를 '일반화된 마할라노비스 거리(Generalized Mahalanobis Distance)'라고 하며, 그 제곱은 다음과 같이 표현된다.
$$
d_W(x_i,x_j)^2=(x_i−x_j)^TW(x_i−x_j)
$$
여기서 핵심적인 제약 조건은 행렬 $W$가 양의 준정부호(positive semi-definite, PSD) 행렬, 즉 $W⪰0$이어야 한다는 점이다. 이 PSD 조건은 W의 모든 고유값이 $0$ 이상임을 의미하며, 학습된 거리 함수 $d_W$가 비음수성, 볼록성(convexity), 그리고 가장 중요한 삼각 부등식을 만족하는 유효한 메트릭임을 보장한다.7

이로써 고정된 통계량이었던 $S^{-1}$이 학습 가능한 파라미터 W로 대체되면서, 문제는 더 이상 데이터의 기존 구조를 '설명'하는 통계적 분석이 아니게 되었다. 대신, 주어진 과제(예: 유사한 쌍은 가깝게, 상이한 쌍은 멀게)에 최적화되도록 공간 자체를 능동적으로 '변형'시키는 기계 학습 문제로 전환되었다.


결론적으로, 고전적인 메트릭 러닝의 문제는 '최적의 PSD 행렬 W를 학습하는 것'으로 귀결된다.12 이 행렬 

W는 데이터에 대한 선형 변환(linear transformation) 또는 투영(projection) L을 학습하는 것과 동일하게 해석될 수 있다. W는 항상 W=LTL 형태로 분해될 수 있기 때문에, 일반화된 마할라노비스 거리는 데이터를 $L$로 선형 변환한 새로운 공간에서 유클리드 거리를 측정하는 것과 수학적으로 동일하다.7
$$
d_W(x_i, x_j)^2 = (x_i - x_j)^T L^T L (x_i - x_j) = (L(x_i - x_j))^T (L(x_i - x_j)) = |L x_i - L x_j|_2^2
$$
이 관점에서 메트릭 러닝은 원본 공간을 늘리거나(stretching), 회전시키거나(rotating), 차원을 축소(projecting)하여 유클리드 거리가 의미론적 유사도를 잘 반영하는 새로운 공간을 찾는 과정이라고 할 수 있다. 이는 통계학의 기술적 도구가 기계 학습의 예측적 도구로 진화하는 결정적인 연결고리가 되었다.


일반화된 마할라노비스 거리의 개념을 바탕으로, 2000년대 초중반에 다양한 고전적 메트릭 러닝 알고리즘들이 개발되었다. 이들은 주로 k-NN 분류기의 성능 향상을 목표로 했으며, 최적의 변환 행렬 W를 찾기 위해 각기 다른 최적화 목표와 전략을 사용했다. 이 중 가장 대표적인 두 가지 알고리즘은 LMNN(Large Margin Nearest Neighbor)과 NCA(Neighborhood Component Analysis)이다.


LMNN은 이름에서 알 수 있듯이, 서포트 벡터 머신(SVM)의 '라지 마진(large margin)' 개념을 k-NN 분류에 도입한 알고리즘이다.22 LMNN의 핵심 아이디어는 k-NN 분류의 정확도를 높이기 위해, 모든 데이터 포인트가 같은 클래스 레이블을 가진 k개의 '타겟 이웃(target neighbors)'으로 둘러싸이도록 하면서, 동시에 다른 클래스의 '침입자(impostors)'와는 큰 마진을 두고 분리되도록 마할라노비스 메트릭을 학습하는 것이다.22

LMNN의 목적 함수는 두 가지 상반된 목표를 동시에 최적화한다.

1. **끌어당기기 (Pull):** 각 데이터 포인트 xi와 그 타겟 이웃들(같은 클래스에 속하는 k개의 가장 가까운 이웃) 간의 평균 거리를 최소화한다. 이는 같은 클래스 내의 분산(intra-class variance)을 줄이는 역할을 한다.
2. **밀어내기 (Push):** 각 데이터 포인트 xi에 대해, 다른 클래스에 속하면서 타겟 이웃보다 더 가까이 침범한 '침입자' xj가 있다면, 이 침입자와의 거리가 타겟 이웃과의 거리보다 최소한 1(마진)만큼 더 멀어지도록 강제한다. 만약 침입자가 이미 마진 밖에 있다면 페널티를 부과하지 않는다. 이는 힌지 손실(hinge loss) 함수를 통해 구현되며, 클래스 간 분산(inter-class variance)을 최대화하는 효과를 낳는다.

이 두 목표를 결합한 목적 함수는 볼록 최적화(convex optimization) 문제, 구체적으로는 준정부호 계획법(Semidefinite Programming, SDP)으로 공식화되어 전역 최적해(global optimum)를 찾을 수 있다는 장점이 있다.22


NCA는 LMNN과 유사하게 k-NN 분류 성능 향상을 위한 마할라노비스 메트릭 학습을 목표로 하지만, 확률론적인 관점에서 접근한다.12 NCA의 목표는 학습된 메트릭 공간에서 각 데이터 포인트가 자신의 클래스 레이블을 올바르게 예측할 확률을 최대화하는 것이다.

NCA의 작동 방식은 다음과 같다.

1. 이웃 선택 확률 정의: 데이터 포인트 xi가 다른 포인트 xj를 이웃으로 선택할 확률 $p_{ij}$를 정의한다. 이 확률은 학습된 마할라노비스 거리 $d_W(x_i, x_j)$가 가까울수록 높아지도록 소프트맥스(softmax) 함수를 사용하여 모델링된다.
   $$
   p_{ij}=\frac{\exp(-d_W (x_i, x_j)^2)}{\sum_{k \ne i} \exp(-d_w(x_i, x_k)^2)}
   $$
   **정확한 분류 확률 최대화:** 각 포인트 $x_i$가 올바르게 분류될 확률 $p_i$는, $x_i$가 같은 클래스에 속하는 이웃들을 선택할 확률의 합으로 정의된다 ($p_i=∑_{j∈C_i}p_{ij}$, 여기서 $C_i$는 $x_i$와 같은 클래스의 포인트 집합).

2. **최적화:** NCA는 모든 훈련 데이터에 대한 올바른 분류 확률의 총합(∑ipi)을 최대화하는 변환 행렬 W를 경사 상승법(gradient ascent)과 같은 방법으로 찾는다.

NCA는 확률적이고 미분 가능한 목적 함수를 사용하므로, 경사 기반 최적화 기법을 쉽게 적용할 수 있다는 장점이 있다.


LMNN과 NCA를 비롯한 고전적 메트릭 러닝 알고리즘들은 선형 대수와 볼록 최적화에 기반하여 명확한 이론적 토대를 제공했지만, 근본적인 한계를 가지고 있었다. 이들의 가장 큰 한계는 **단일한 전역 선형 변환(single global linear transformation)**만을 학습한다는 점이다.8 즉, 이 알고리즘들은 모든 데이터 포인트에 동일하게 적용되는 하나의 행렬 $W$를 학습한다. 이는 데이터 공간을 전체적으로 늘리거나 회전시키는 것과 같아서, 이미지나 텍스트와 같이 본질적으로 복잡하고 비선형적인 구조를 가진 데이터를 효과적으로 표현하기에는 역부족이었다.

예를 들어, 얼굴 이미지 데이터에서 '안경 유무'라는 특징과 '개인의 정체성'이라는 특징이 비선형적으로 얽혀 있을 때, 단일 선형 변환으로는 이 두 가지를 효과적으로 분리하여 정체성 기반의 유사도를 측정하기 어렵다. 이러한 한계는 자연스럽게 더 강력한 표현력을 가진 모델의 필요성으로 이어졌다.

이 지점에서 딥러닝이 대안으로 부상했다. 심층 신경망은 '보편적 함수 근사기(universal function approximator)'로서, 복잡한 비선형 함수를 학습할 수 있는 능력을 갖추고 있다.26 딥 메트릭 러닝은 고전적 방법처럼 단일 행렬 $W$를 학습하는 대신, 입력 데이터 $x$를 의미론적 임베딩 공간으로 매핑하는 고도로 비선형적인 함수 $f(x;θ)$ 자체를 학습한다.13 이 경우, '메트릭'은 더 이상 명시적인 행렬 하나로 정의되지 않고, 심층 신경망의 출력에 의해 암묵적으로(implicitly) 정의된다. 이는 데이터 공간을 단순히 선형적으로 변환하는 것을 넘어, 마치 고무판을 구부리고 접는 것처럼 임의로 왜곡(warping)하여 의미론적으로 이상적인 구조를 만들어내는 것과 같다. 이처럼 학습 패러다임이 '전역 변환 학습'에서 '비선형 임베딩 함수 학습'으로 전환된 것이 바로 고전적 메트릭 러닝과 현대 딥 메트릭 러닝을 가르는 가장 근본적인 차이점이자, DML이 가진 강력한 성능의 원천이다.


이 장은 현대 딥 메트릭 러닝(DML)의 핵심을 이루는 세 가지 기둥, 즉 임베딩을 생성하는 네트워크 아키텍처, 학습 과정을引导하는 손실 함수, 그리고 이 과정에 데이터를 공급하는 샘플링 전략을 심도 있게 탐구한다. 이 세 요소의 상호작용과 발전 과정은 DML 분야의 진보를 이끌어온 원동력이다.


현대 DML의 가장 기본적인 아키텍처 패턴은 동일한 구조와 가중치를 공유하는 두 개 이상의 서브 네트워크를 사용하는 것이다.12 이 '가중치 공유(shared-weight)' 패러다임은 입력되는 모든 샘플(쌍 또는 삼중항)이 정확히 동일한 함수에 의해 동일한 임베딩 공간으로 투영되도록 보장한다. 이는 임베딩 간의 거리를 의미 있게 비교하기 위한 필수적인 전제 조건이다.


샴 네트워크는 DML에서 가장 먼저 등장한 대표적인 가중치 공유 아키텍처로, 이름처럼 '샴쌍둥이'와 같이 동일한 두 개의 네트워크 '타워(tower)'로 구성된다. 이 구조는 한 쌍(pair)의 입력을 받아 처리한다.27

샴 네트워크의 일반적인 처리 과정은 다음과 같다.

1. **입력:** 두 개의 입력 샘플(예: 이미지 2장)이 각각의 타워에 독립적으로 입력된다.
2. **임베딩 생성:** 동일한 가중치를 공유하는 두 타워는 각 입력을 처리하여 두 개의 임베딩 벡터를 생성한다.
3. **거리 계산 및 손실:** 생성된 두 임베딩 벡터 간의 거리(예: 유클리드 거리)가 계산되고, 이 거리는 대조 손실(Contrastive Loss)과 같은 쌍 기반 손실 함수에 전달되어 모델의 가중치를 업데이트하는 데 사용된다.

이 구조는 두 객체가 유사한지 아닌지를 판단하는 비교 작업에 매우 직관적이고 효과적이다.


삼중항 네트워크는 샴 네트워크의 아이디어를 확장하여 세 개의 입력을 동시에 처리하는 구조이다.32 이 세 개의 입력은 기준이 되는 '앵커(anchor)', 앵커와 같은 클래스에 속하는 '포지티브(positive)', 그리고 앵커와 다른 클래스에 속하는 '네거티브(negative)'로 구성된다.5

삼중항 네트워크는 세 개의 동일한 타워를 사용하여 앵커, 포지티브, 네거티브 각각에 대한 임베딩 벡터를 생성한다. 이 세 개의 임베딩은 삼중항 손실(Triplet Loss) 함수에 입력되어, 앵커-포지티브 간의 거리는 가까워지고 앵커-네거티브 간의 거리는 멀어지도록 학습이 진행된다. 이 방식은 단순한 '유사/비유사' 이진 판단을 넘어, '무엇보다 더 유사한가'라는 상대적인 관계를 학습할 수 있게 해주어 더 정교한 임베딩 공간을 구축할 수 있다.


샴 또는 삼중항 네트워크의 각 타워를 구성하는 임베딩 네트워크는 일반적으로 다음과 같은 세 부분으로 나눌 수 있다.

1. **백본 (Backbone):** 입력 데이터로부터 유의미한 특징을 추출하는 역할을 하는 핵심 부분이다. 이미지 데이터의 경우 ResNet, VGG, Inception과 같은 컨볼루션 신경망(CNN)이 주로 사용되며, 텍스트나 시퀀스 데이터의 경우 순환 신경망(RNN)이나 트랜스포머(Transformer)가 백본으로 활용된다.26
2. **넥/헤드 (Neck/Head):** 백본에서 추출된 고차원의 특징 맵을 최종적인 저차원 임베딩 벡터로 압축하고 투영하는 역할을 한다. 일반적으로 평탄화 계층(flattening layer)과 하나 이상의 완전 연결 계층(fully-connected layer)으로 구성되며, 비선형 활성화 함수(예: ReLU)나 배치 정규화(batch normalization)가 포함될 수 있다.27
3. **정규화 (Normalization):** 임베딩 벡터를 생성하는 마지막 단계에서 종종 L2 정규화(L2 normalization)가 적용된다. 이는 모든 임베딩 벡터의 길이를 1로 만들어 단위 초구(hypersphere) 표면에 투영하는 효과를 가진다. 이렇게 하면 임베딩의 크기(magnitude)에 대한 민감도가 사라지고, 벡터 간의 유클리드 거리가 코사인 유사도와 비례 관계를 갖게 되어 각도 기반의 유사도 비교가 용이해진다.9

이러한 샴/삼중항 아키텍처의 설계는 DML의 핵심 철학을 우아하게 구현한다. 즉, 이 구조는 '특징 추출(feature extraction)' 작업과 '메트릭 학습(metric learning)' 작업을 효과적으로 분리한다. ResNet과 같은 백본 네트워크는 그 자체로 강력한 범용 특징 추출기이지만, 샴/삼중항 구조에서는 분류 헤드 대신 거리 기반 손실 함수와 연결된다.27 학습 과정에서 삼중항 손실과 같은 메트릭 손실로부터 계산된 그래디언트가 공유된 백본 네트워크를 통해 역전파된다. 이는 백본의 가중치가 특정 클래스를 분류하도록 학습되는 것이 아니라, 손실 함수가 요구하는 거리 제약 조건을 만족시키는 특징을 생성하도록 미세 조정(fine-tuning)됨을 의미한다. 결국, 아키텍처 자체가 '비교를 통한 학습'이라는 목표를 강제하는 것이다.


딥 메트릭 러닝의 발전은 곧 손실 함수의 발전 역사와 궤를 같이한다. 손실 함수는 임베딩 공간이 어떤 구조를 가져야 하는지를 정의하는 '설계도'이자, 모델이 그 목표를 향해 나아가도록 이끄는 '엔진'이다. 이 장에서는 DML 분야의 진보를 이끌어온 주요 손실 함수들의 계보를 연대기적으로, 그리고 비판적으로 분석한다.


대조 손실은 DML 초기에 샴 네트워크와 함께 제안된 가장 직관적인 손실 함수 중 하나이다.15 이 손실 함수는 입력으로 한 쌍의 데이터와, 이 쌍이 유사한지(positive pair, $y=1$) 혹은 상이한지(negative pair, $y=0$)를 나타내는 레이블을 받는다. 34

공식:
$$
L_\text{contrastive} = y ⋅ \frac{2}{1}d^2+(1−y) ⋅ \frac{2}{1}( \max(0,m−d))^2
$$
여기서 d는 두 임베딩 간의 유클리드 거리이고, $m$은 미리 정의된 마진(margin) 하이퍼파라미터이다.15

**목표:**

- **유사한 쌍 ($y=1$):** 손실 함수는 $\frac{1}{2}d^2$이 되어, 두 임베딩 간의 거리 $d$를 직접적으로 최소화하도록 작동한다. 즉, 유사한 샘플들을 임베딩 공간에서 서로 끌어당긴다.
- **상이한 쌍 ($y=0$):** 손실 함수는 $\frac{1}{2}(\max(0,m−d))^2$이 된다. 만약 두 임베딩 간의 거리가 마진 $m$보다 크면($d>m$), $\max$ 안의 값이 음수가 되어 손실은 $0$이 된다. 이는 이미 충분히 멀리 떨어진 쌍에 대해서는 더 이상 신경 쓰지 않겠다는 의미이다. 반면, 거리가 마진보다 가까우면($d<m$), 손실이 발생하여 두 임베딩을 마진 $m$만큼 떨어뜨리도록 밀어낸다.13

한계:

대조 손실은 직관적이고 구현이 간단하지만, 각 쌍을 독립적으로 고려한다는 점에서 '탐욕적(greedy)'인 접근 방식이라는 비판을 받는다. 이는 임베딩 공간의 전역적인 구조나 상대적인 거리 관계를 고려하지 못하는 한계를 가진다.35


삼중항 손실은 대조 손실의 한계를 극복하고 DML 분야의 표준으로 자리 잡은 매우 중요한 손실 함수이다. 이 손실은 구글의 FaceNet 논문에서 그 효과가 입증되면서 널리 알려졌다.36

공식:
$$
L_\text{triplet}=\max(0,d(a,p)^2−d(a,n)^2+m)
$$
여기서 $a$(anchor), $p$(positive), $n$(negative)은 각각 기준, 긍정, 부정 샘플의 임베딩을 의미하며, d(⋅,⋅)^2은 제곱 유클리드 거리, $m$은 마진이다.5

목표:

삼중항 손실의 목표는 단순한 거리 최소/최대화를 넘어, **상대적인 거리 제약(relative distance constraint)**을 강제하는 것이다. 즉, 앵커와 포지티브 간의 거리($d(a,p)$)가 앵커와 네거티브 간의 거리($d(a,n)$)보다 최소한 마진 $m$만큼 더 작아지도록 하는 것이다.
$$
d(a,p)^2+m<d(a,n)^2
$$
이 부등식이 만족되면($d(a,p)^2−d(a,n)^2+m<0$), $\max$ 함수에 의해 손실은 $0$이 된다. 이는 이미 충분히 좋은 상대적 위치 관계를 가진 삼중항에 대해서는 학습을 진행하지 않음을 의미한다. 이 상대적 제약 조건은 대조 손실보다 훨씬 강력하며, 클래스 간의 분별력을 높이는 데 더 효과적이다.5


DML 연구가 심화되면서, 메트릭 러닝과 분류(classification)가 사실은 동전의 양면과 같다는 인식이 확산되었다.3 특히, 소프트맥스 교차 엔트로피(softmax cross-entropy) 손실을 사용하는 분류 모델의 마지막 완전 연결 계층의 가중치 벡터는 각 클래스를 대표하는 '프록시(proxy)' 또는 '프로토타입(prototype)'으로 해석될 수 있다. 이 관점에서 분류 학습은 각 샘플의 임베딩을 해당 클래스의 프록시와 가깝게, 다른 클래스의 프록시와는 멀게 만드는 프록시 기반 메트릭 러닝의 한 형태로 볼 수 있다.

이러한 통찰은 분류 손실 함수를 직접 수정하여 메트릭 러닝의 목표를 달성하려는 일련의 연구로 이어졌다. 특히 안면 인식 분야에서는 L2 정규화를 통해 임베딩을 초구 표면에 배치하고, 유클리드 거리 대신 코사인 유사도(즉, 각도)를 기준으로 유사성을 측정하는 것이 표준이 되었다. 이를 바탕으로 각도 공간에서 직접 마진을 부여하는 손실 함수들이 개발되었다.

- **L-Softmax, A-Softmax (SphereFace), CosFace, ArcFace:** 이 손실 함수들은 표준 소프트맥스 손실을 점진적으로 개선한 형태이다.14 이 중 가장 널리 사용되는 

  ArcFace의 손실 함수는 다음과 같다.
  $$
  L_{ArcFace} = -\log \frac{e^{s(\cos(\theta_{y_i} + m))}}{e^{s(\cos(\theta_{y_i} + m))} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}}
  $$
  여기서 $\theta_{y_i}$는 샘플의 임베딩과 정답 클래스($y_i$)의 프록시(가중치 벡터) 사이의 각도이고, m은 '각도 마진(angular margin)'이며, $s$는 스케일링 팩터이다. ArcFace는 마진 $m$을 코사인 값에 더하는 CosFace와 달리, 각도 θ에 직접 더함으로써 초구 위에서 기하학적으로 더 일정하고 해석 가능한 결정 경계를 만든다. 이는 클래스 내(intra-class) 밀집도와 클래스 간(inter-class) 분리도를 극대화하여 안면 인식과 같은 세밀한 분류 작업에서 최고의 성능을 보여주었다.


최근 연구들은 기존 손실들의 한계를 극복하기 위해 더 정교한 메커니즘을 도입하고 있다.

- **CircleLoss:** 이 손실 함수는 쌍 기반 손실(대조/삼중항)과 분류 기반 손실의 장점을 통합하려는 시도이다.39 CircleLoss의 핵심 혁신은 고정된 마진을 사용하는 대신, 현재 쌍의 유사도 값에 따라 긍정 쌍과 부정 쌍에 대한 학습 강도를 

  **동적으로 조절하는 가중치**를 부여하는 것이다. 즉, 최적의 위치에서 멀리 떨어진 '어려운' 쌍에 대해서는 더 강한 그래디언트를, 이미 최적에 가까운 '쉬운' 쌍에 대해서는 약한 그래디언트를 적용한다. 이를 통해 고정 마진 방식보다 더 유연하고 효과적인 최적화를 달성한다.39

- **프록시 기반 손실 (ProxyNCA, Proxy-Anchor Loss):** 이 손실 함수들은 쌍 또는 삼중항 기반 손실의 근본적인 문제인 **샘플링 복잡도**를 해결하기 위해 설계되었다.3 훈련 데이터셋의 크기가 

  $N$일 때, 가능한 쌍의 수는 $O(N^2)$, 삼중항의 수는 $O(N^3)$에 달해 대규모 데이터셋에서는 학습이 비효율적이거나 불가능해진다. 프록시 기반 손실은 각 클래스마다 하나 또는 여러 개의 학습 가능한 '프록시' 벡터를 두고, 각 데이터 샘플을 다른 모든 샘플과 비교하는 대신 이 프록시들과만 비교한다. 이로써 비교 횟수가 $O(N×C)$ (여기서 C는 클래스 수)로 크게 줄어들어, 학습 속도와 효율성을 비약적으로 향상시킨다.41

- **기타 주목할 만한 손실:** PyTorch Metric Learning과 같은 라이브러리에는 이 외에도 수많은 손실 함수들이 구현되어 있다.39 예를 들어, 

  **Multi-Similarity Loss**나 **Lifted Structure Loss**는 미니배치 내의 더 많은 쌍 관계를 활용하여 학습 신호를 풍부하게 만들려는 시도들이다.42

이러한 손실 함수들의 발전 과정은 단독으로 이루어진 것이 아니다. 이는 샘플링 전략 및 아키텍처 패러다임의 변화와 긴밀하게 상호작용하며 공진화(co-evolution)해왔다. 초기 쌍/삼중항 기반 손실이 가진 $O(N^2)$ 또는 $O(N^3)$의 엄청난 샘플링 복잡도와 계산 비용은 DML의 실용화를 가로막는 가장 큰 장벽이었다.41 이에 대한 첫 번째 대응은 더 효율적인 샘플링 전략(예: 하드 네거티브 마이닝)을 개발하는 것이었지만, 이는 근본적인 해결책이 아니었다.

더 근본적인 해결책은 손실 함수 자체를 바꾸는 것이었고, 이는 두 가지 주요 흐름으로 이어졌다. 첫째는 앞서 언급한 **프록시 기반 손실**로, 샘플링 복잡도를 $O(N)$으로 낮추기 위해 명시적으로 설계되었다.10 둘째는 **분류 기반 손실(ArcFace 등)**로, 이 역시 $O(N)$의 복잡도를 가진다. 이들은 표준적인 분류 학습의 효율성을 그대로 활용하면서도, 손실 함수에 메트릭 러닝 스타일의 마진을 도입하여 임베딩 공간의 구조를 정교하게 제어했다. 이러한 흐름은 DML 분야가 대규모 문제에 대응하기 위해 계산 비용이 높은 쌍별 비교 방식에서 벗어나, 확장성이 뛰어난 프록시 기반 또는 분류 기반 공식으로 점차 수렴해가는 뚜렷한 경향을 보여준다.

| 표 5.1: 주요 딥 메트릭 러닝 손실 함수 비교 분석 |
| ----------------------------------------------- |
| **손실 함수**                                   |
| **대조 손실 (Contrastive Loss)**                |
| **삼중항 마진 손실 (Triplet Margin Loss)**      |
| **ArcFace / CosFace**                           |
| **CircleLoss**                                  |
| **프록시-앵커 손실 (Proxy-Anchor Loss)**        |


딥 메트릭 러닝 모델이 강력한 엔진(손실 함수)을 갖추었다 하더라도, 양질의 연료(데이터)가 공급되지 않으면 제대로 작동할 수 없다. DML에서 '연료'는 바로 **샘플링 전략**에 해당한다. 특히 쌍이나 삼중항을 기반으로 하는 손실 함수에서는 어떤 샘플들을 선택하여 손실을 계산할지가 모델의 성능과 수렴 속도에 결정적인 영향을 미친다.


DML에서 순진한(naive) 샘플링이 비효율적인 이유는 '조합 폭발' 문제 때문이다. 예를 들어, 크기가 B인 미니배치 안에 C개의 클래스가 있고 각 클래스마다 K개의 샘플이 있다고 가정하자. 이 배치 안에서 만들 수 있는 삼중항의 수는 엄청나게 많다. 더 큰 문제는, 이들 중 대부분은 d(a,p)2+m≪d(a,n)2를 이미 만족하는 '쉬운 삼중항(easy triplets)'이라는 점이다. 이러한 삼중항들은 손실 값이 0이므로, 모델 학습에 아무런 기여를 하지 못하고 계산 자원만 낭비하게 된다.41 따라서 학습을 효율적으로 만들기 위해서는 정보 가치가 높은, 즉 손실이 0이 아닌 '어려운 샘플(hard samples)'을 선별하는 과정이 필수적이다.


하드 샘플 마이닝의 핵심 철학은 모델이 결정 경계(decision boundary) 근처에 위치한 까다로운 예제들을 올바르게 처리하도록 집중적으로 학습시켜야 성능이 향상된다는 것이다.5 DML에서는 앵커와 매우 유사하게 생긴 네거티브 샘플이나, 앵커와 매우 다르게 생긴 포지티브 샘플이 바로 이러한 하드 샘플에 해당한다. 이러한 샘플들을 효과적으로 '채굴(mining)'하여 학습에 사용하는 것이 DML 성능의 핵심 비결 중 하나로 꼽힌다. 예를 들어, 정보 검색 모델인 DPR의 저자들은 BM25 점수가 높아 쿼리와 용어는 많이 겹치지만 실제로는 관련 없는 문서를 '하드 네거티브'로 사용하여 검색 성능을 크게 향상시켰다고 보고했다.5


'어려운 네거티브'를 선택하는 데에는 여러 전략이 있으며, 각 전략은 장단점을 가진다.

- **가장 어려운 네거티브 마이닝 (Hardest Negative Mining):** 주어진 앵커에 대해, 임베딩 공간에서 거리가 **가장 가까운** 네거티브 샘플을 선택하는 전략이다.43 직관적으로는 가장 많은 정보를 담고 있을 것 같지만, 이 방식은 심각한 문제를 야기할 수 있다. 학습 초기에 모델이 아직 불안정할 때 가장 어려운 네거티브에만 집중하면, 잘못된 특징에 과적합되어 나쁜 지역 최적해(local minima)에 빠지거나, 모든 임베딩을 한 점으로 수렴시키는 모델 붕괴(model collapse) 현상을 초래할 수 있다.43

- **준-어려운 네거티브 마이닝 (Semi-Hard Negative Mining):** FaceNet에서 제안되어 널리 사용되는 안정적인 전략이다.35 이 방식은 다음 조건을 만족하는 네거티브 샘플 $n$을 선택한다.
  $$
  d(a,p)<d(a,n)<d(a,p)+m
  $$
  즉, 앵커로부터 포지티브보다는 멀리 있지만($d(a,p)<d(a,n)$), 여전히 삼중항 제약 조건을 위반하는($d(a,n)<d(a,p)+m$) 네거티브를 선택하는 것이다. 이는 너무 쉬워서 배울 것이 없는 샘플과, 너무 어려워서 학습을 불안정하게 만드는 샘플을 모두 배제하고, 학습에 가장 유익한 '적당히 어려운' 샘플을 선택하는 효과적인 절충안이다.35

- **동적/어닐링 전략 (Dynamic/Annealing Strategies):** 최근 연구들은 고정된 전략 대신, 학습 진행 상황에 따라 샘플의 난이도를 동적으로 조절하는 방식을 제안한다. 예를 들어, 학습 초기에는 비교적 쉬운 네거티브를 사용해 모델이 전반적인 임베딩 공간의 구조를 안정적으로 학습하게 하고, 점차 어려운 네거티브의 비중을 높여(어닐링) 세밀한 결정 경계를 다듬도록 하는 방식이다.42 이는 각기 다른 마이닝 전략의 장점을 결합하려는 시도이다.


- **배치 내 샘플링 (In-Batch Sampling):** 현대 DML에서 가장 보편적으로 사용되는 방식이다. 쌍이나 삼중항을 구성할 때, 현재 처리 중인 미니배치 내의 샘플들만 사용하는 것이다.5 이 방식의 효과는 **배치 크기(batch size)**에 크게 의존한다. 배치 크기가 클수록 더 다양하고 잠재적으로 더 어려운 네거티브 샘플을 발견할 확률이 높아진다. 특히 대조 손실 계열의 손실 함수들은 큰 배치 크기에서 성능이 크게 향상되는 경향이 있다.14
- **교차 배치 메모리 (Cross-Batch Memory):** 배치 내 샘플링의 한계를 극복하기 위한 고급 기법이다. 이는 이전 배치들에서 계산된 임베딩 벡터들을 별도의 '메모리 뱅크(memory bank)'에 저장해두고, 현재 배치의 샘플에 대한 네거티브를 이 메모리 뱅크에서 찾는 방식이다.39 이를 통해 훨씬 더 크고 다양한 후보군으로부터 네거티브를 샘플링할 수 있어, 작은 배치 크기로도 큰 배치 크기의 효과를 일부 모방할 수 있다.

| 표 6.1: 삼중항 마이닝 전략 분석                           |
| --------------------------------------------------------- |
| **전략**                                                  |
| **무작위 마이닝 (Random Mining)**                         |
| **가장 어려운 네거티브 마이닝 (Hardest Negative Mining)** |
| **준-어려운 네거티브 마이닝 (Semi-Hard Negative Mining)** |
| **동적 어닐링 (Dynamic Annealing)**                       |


앞선 장들에서 다룬 추상적인 개념들이 실제 세계의 문제들을 어떻게 해결하는지 구체적인 사례를 통해 살펴봄으로써, 메트릭 러닝의 실질적인 영향력과 다재다능함을 입증한다.



안면 인식은 메트릭 러닝의 필요성과 효용성을 가장 극명하게 보여주는 대표적인 응용 분야이다. 세상에는 수십억 명의 사람이 존재하며, 시스템에는 매일 새로운 사용자가 등록된다. 이처럼 클래스(개인의 신원)의 수가 사실상 무한하고 계속해서 증가하는 '오픈-셋' 환경에서, 새로운 사람이 추가될 때마다 전체 분류 모델을 재학습시키는 것은 현실적으로 불가능하다.1 따라서 안면 인식 시스템은 훈련 데이터에 없었던 새로운 인물에 대해서도 효과적으로 작동할 수 있는 일반화 능력을 갖추어야 한다.


이러한 문제를 해결하기 위해 등장한 표준적인 접근법이 바로 구글의 FaceNet이다.36 FaceNet의 목표는 분류기를 학습하는 대신, 임의의 얼굴 이미지를 입력받아 그 사람의 신원을 고유하게 나타내는 고정된 길이(예: 128차원)의 임베딩 벡터 $f(x)$를 생성하는 함수를 학습하는 것이다.36 이 임베딩 공간은 다음과 같은 특성을 갖도록 설계된다.

- **클래스 내 밀집성 (Intra-class compactness):** 같은 사람의 다른 사진들(다른 각도, 조명, 표정)은 임베딩 공간에서 서로 매우 가까운 거리에 위치해야 한다.
- **클래스 간 분리성 (Inter-class separability):** 다른 사람들의 사진들은 임베딩 공간에서 서로 멀리 떨어져 있어야 한다.


FaceNet은 이러한 임베딩 공간을 학습하기 위해 앞서 설명한 **삼중항 손실(Triplet Loss)**과 **온라인 준-어려운 네거티브 마이닝(online semi-hard negative mining)** 전략을 사용한다.35 학습 과정은 다음과 같다.

1. 미니배치에서 앵커($A$), 포지티브($P$), 네거티브($N$) 이미지로 구성된 삼중항들을 구성한다.
2. 이 이미지들을 CNN 기반의 임베딩 네트워크에 통과시켜 각각의 128차원 임베딩 $f(A), f(P), f(N)$을 얻는다.
3. 삼중항 손실 함수를 통해 $d(f(A),f(P))^2+m<d(f(A),f(N))^2$ 제약 조건이 만족되도록 네트워크의 가중치를 업데이트한다.

이 과정을 반복함으로써, 네트워크는 점차적으로 동일 인물의 얼굴 임베딩 간의 제곱 L2 거리는 특정 임계값(예: 1.1)보다 작아지고, 다른 인물의 얼굴 임베딩 간의 거리는 이보다 커지도록 학습된다.33


일단 임베딩 함수 $f(x)$가 성공적으로 학습되면, 시스템 배포는 매우 간단하고 효율적이다.

- **등록:** 새로운 사용자의 얼굴 이미지를 $f(x)$에 통과시켜 128바이트 크기의 임베딩 벡터를 생성하고 데이터베이스에 저장한다.
- **인증/식별:** 인증이 필요한 얼굴 이미지가 들어오면, 동일하게 임베딩 벡터를 생성한 후 데이터베이스에 저장된 벡터들과의 거리를 계산한다. 가장 가까운 벡터와의 거리가 미리 설정된 임계값보다 작으면 동일 인물로, 크면 다른 인물로 판단한다.

이 방식은 새로운 인물이 추가되어도 모델 재학습이 필요 없으며, 수백만 명의 데이터베이스에서도 빠른 검색이 가능하여 확장성이 매우 뛰어나다.33


FaceNet 이후에도 안면 인식 분야는 지속적으로 발전해왔다. 최신 연구들은 Inception ResNet-V1과 같은 더 강력한 백본 아키텍처를 사용하고 47, 삼중항 손실 대신 

**ArcFace**와 같은 각도 마진 기반 손실 함수를 사용하여 초구 상에서 더욱 명확한 클래스 분리를 달성함으로써 인식 정확도를 한층 더 끌어올렸다.40



콘텐츠 기반 이미지 검색(Content-Based Image Retrieval, CBIR)은 주어진 쿼리 이미지와 시각적으로 또는 의미적으로 유사한 이미지들을 대규모 데이터베이스에서 찾아내는 것을 목표로 한다.2 이는 전자상거래 플랫폼의 '유사 상품 찾기', 소셜 미디어의 '비슷한 이미지 추천' 등 다양한 서비스의 핵심 기술이다.


이미지 검색 문제에 대한 DML의 접근 방식은 안면 인식과 매우 유사하다. 핵심은 시각적으로 유사한 이미지들이 임베딩 공간에서 가까운 이웃이 되도록 매핑하는 임베딩 함수를 학습하는 것이다.2 검색 과정은 다음과 같이 이루어진다.

1. **오프라인 인덱싱:** 데이터베이스의 모든 이미지를 미리 학습된 DML 모델에 통과시켜 임베딩 벡터로 변환하고, 효율적인 검색을 위해 이를 인덱싱한다(예: Faiss, Annoy와 같은 근사 최근접 이웃 검색 라이브러리 사용).
2. **온라인 검색:** 사용자가 쿼리 이미지를 입력하면, 동일한 DML 모델을 사용해 쿼리의 임베딩 벡터를 생성한다.
3. **최근접 이웃 탐색:** 생성된 쿼리 벡터와 가장 가까운 이웃들을 인덱싱된 데이터베이스에서 찾아 사용자에게 결과로 제시한다.48


이미지 검색을 위한 DML 모델의 학습 파이프라인은 일반적으로 다음과 같은 요소들로 구성된다.46

- **백본 모델:** ImageNet 등으로 사전 학습된 CNN 모델을 백본으로 사용하고, 검색 대상 도메인 데이터로 미세 조정한다.
- **손실 함수:** 대조 손실, 삼중항 손실 또는 분류 기반 손실(예: ArcFace) 등이 사용된다. 대규모 실험 결과에 따르면, 대조 손실 계열은 큰 배치 크기를 사용할 때 성능이 크게 향상되는 경향이 있으며, 분류 기반 손실은 작은 배치 크기에서도 안정적인 성능을 보인다.46
- **데이터 샘플러:** 클래스별로 균등한 수의 이미지를 샘플링하여 배치 내에 다양한 클래스가 포함되도록 하는 것이 중요하다.
- **미세 조정의 중요성:** 대부분의 데이터셋에서 사전 학습된 모델을 그대로 사용하는 것보다, 대상 데이터셋으로 미세 조정하는 것이 좋은 성능을 위해 필수적이다.46


레이블이 없는 대규모 이미지 데이터셋을 활용하기 위해, 많은 현대 검색 시스템은 **자기 지도 학습(Self-Supervised Learning)** 방식을 채택한다. 이 경우, 별도의 클래스 레이블 없이 동일한 이미지에 대해 서로 다른 데이터 증강(예: 랜덤 크롭, 색상 왜곡, 회전)을 적용하여 두 개의 '뷰(view)'를 만들고, 이들을 긍정 쌍(positive pair)으로 간주한다. 다른 모든 이미지는 부정 쌍(negative pair)이 된다. 이러한 방식으로 생성된 쌍들을 대조 손실 함수로 학습시키면, 모델은 이미지의 내용 자체에 대한 불변적인 표현을 학습하게 된다.48



메트릭 러닝은 다수의 클래스를 서로 '분리'하는 목적뿐만 아니라, 특정 데이터의 분포를 '모델링'하는 데에도 효과적으로 사용될 수 있다. 이러한 관점의 전환은 이상 탐지(Anomaly Detection) 및 분포 외(Out-of-Distribution, OOD) 탐지 분야에서 DML의 새로운 활용 가능성을 열었다. 여기서 목표는 여러 클래스를 분리하는 것이 아니라, **정상(in-distribution) 데이터**에 속하는 모든 샘플들을 임베딩 공간 내에 하나 또는 여러 개의 매우 조밀하고 잘 정의된 군집으로 모으는 것이다.31


이상 탐지를 위한 DML 모델의 학습 및 추론 과정은 다음과 같다.

1. **학습:** 정상 데이터만을 사용하여 DML 모델을 학습시킨다. 손실 함수는 모든 정상 데이터 샘플들이 임베딩 공간의 특정 영역에 매우 가깝게 모이도록 강제한다.
2. **정상 분포 모델링:** 학습이 완료되면, 정상 데이터가 형성하는 군집들의 중심(평균)과 분산(공분산)을 계산한다.
3. **추론 및 탐지:** 새로운 데이터 샘플이 입력되면, 학습된 모델을 통해 임베딩 벡터를 추출한다. 그런 다음, 이 임베딩 벡터와 가장 가까운 정상 데이터 군집의 중심까지의 거리를 계산한다. 이때, 군집의 형태를 고려하기 위해 유클리드 거리보다 **마할라노비스 거리**를 특징 공간(feature space)에서 사용하는 것이 더 효과적이다.31 만약 계산된 거리가 사전에 정의된 임계값을 초과하면, 해당 샘플은 정상 데이터의 분포에서 벗어난 것으로 간주되어 '이상' 또는 'OOD'로 판정된다.31


이러한 접근법은 표준적인 분류 모델을 사용한 이상 탐지에 비해 중요한 장점을 가진다. 분류 모델의 마지막 소프트맥스(softmax) 계층은 모든 가능한 클래스에 대한 확률의 합이 1이 되도록 강제한다. 이로 인해 모델이 한 번도 본 적 없는 OOD 샘플에 대해서도 특정 클래스에 대한 예측 확률을 매우 높게(즉, 높은 신뢰도로) 출력하는 문제가 발생할 수 있다. 반면, 메트릭 러닝은 소프트맥스 계층이 없으며, 임베딩 공간에서의 거리를 직접 측정한다. 따라서 학습된 정상 데이터의 분포에서 멀리 떨어진 샘플을 자연스럽게 식별할 수 있는, 보다 근본적인 메커니즘을 제공한다.49


일부 연구에서는 OOD 탐지를 두 가지 하위 문제로 세분화하기도 한다.49

- **신규성 탐지 (Novelty Detection):** 학습 데이터의 클래스들과 '관련된' 새로운 클래스를 탐지하는 문제이다. 예를 들어, MNIST 숫자(0~9)로 학습한 모델이 새로운 기호(예: 알파벳)를 탐지하는 경우이다.
- **이상 탐지 (Anomaly Detection):** 학습 데이터와 '전혀 관련 없는' 데이터를 탐지하는 문제이다. 예를 들어, MNIST로 학습한 모델이 고양이 이미지를 탐지하는 경우이다.

메트릭 러닝 기반 접근법은 이 두 가지 시나리오 모두에서 효과적인 해결책을 제시할 수 있다.



전통적인 추천 시스템의 주류였던 협업 필터링(Collaborative Filtering) 기반의 행렬 분해(Matrix Factorization, MF) 기법은 사용자-아이템 상호작용 행렬을 저차원의 사용자 잠재 벡터와 아이템 잠재 벡터로 분해한다. 그리고 이 두 벡터의 **내적(dot product)**을 통해 사용자의 아이템 선호도를 예측한다. 그러나 앞서 1장에서 논의했듯이, 내적은 삼각 부등식을 만족하지 않는 단순한 유사도 점수로, 신뢰할 수 있는 거리 메트릭이 아니다.17 이로 인해 MF는 사용자나 아이템 간의 복잡하고 미세한 관계를 포착하는 데 한계를 보였다.


이러한 한계를 극복하기 위해, 현대 추천 시스템은 DML 프레임워크를 적극적으로 도입하고 있다. DML 기반 추천 모델의 핵심 아이디어는 사용자와 아이템을 **동일한 저차원 메트릭 공간**에 함께 임베딩하는 것이다.17 이 공간에서 사용자의 아이템에 대한 선호도는 사용자 임베딩과 아이템 임베딩 사이의 

**거리**로 모델링된다. 거리가 가까울수록 선호도가 높음을 의미한다.17


이 프레임워크는 훨씬 더 풍부하고 복잡한 관계를 모델링할 수 있는 유연성을 제공한다. 학습 목표를 다음과 같이 복합적으로 설정할 수 있다.

- 사용자 임베딩을 그가 긍정적으로 평가한 아이템 임베딩 쪽으로 **끌어당긴다**.
- 사용자 임베딩을 그가 부정적으로 평가한 아이템 임베딩으로부터 **밀어낸다**.
- 사용자 임베딩을 그와 취향이 비슷한 다른 사용자(예: 신뢰하는 친구)의 임베딩 쪽으로 **끌어당긴다**.
- 사용자 임베딩을 그와 취향이 다른 사용자의 임베딩으로부터 **밀어낸다**.

이처럼 사용자-아이템, 사용자-사용자, 아이템-아이템 간의 다양한 유사성/비유사성 관계를 기하학적으로 일관된 하나의 공간 안에서 동시에 최적화할 수 있다.17


CML(Collaborative Metric Learning), SRMC(Social Recommendation based on Metric Learning and Users' Co-Occurrence Pattern)와 같은 모델들이 이러한 원리를 실제로 구현한 예시이다.17 이들 모델은 종종 임베딩 공간에서의 거리 척도로 유클리드 거리나 데이터 분포를 고려하는 마할라노비스 거리를 사용한다. 예를 들어, SRMC 모델에서 사용자 $u$의 아이템 i에 대한 예측 평점 $\hat{r}_{ui}$는 다음과 같이 모델링될 수 있다.
$$
\hat{r}_{ui}=μ + b_u + b_i − ∥ x_u − y _i ∥_W^2
$$
여기서 μ는 전역 평균 평점, bu와 bi는 각각 사용자 및 아이템 편향, $x_u$와 $y_i$는 메트릭 공간에서의 사용자 및 아이템 임베딩, 그리고 $\| \cdot \|_W^2$는 학습된 마할라노비스 거리의 제곱이다.17 이 접근법은 데이터 희소성(data sparsity) 문제를 완화하고, 사용자 선호도를 더 정확하게 반영하며, 추천 결과를 더 해석 가능하게 만드는 장점을 가진다.



퓨샷 러닝(Few-Shot Learning, FSL)은 기계 학습 모델이 각 클래스당 단 몇 개(K-shot)의 레이블된 예제만 보고도 새로운 클래스를 인식하고 분류하도록 학습하는 것을 목표로 한다.52 이는 방대한 양의 데이터를 요구하는 표준적인 딥러닝 모델에게는 매우 어려운 과제이며, 인간이 소수의 예제만으로 새로운 개념을 빠르게 학습하는 능력에 더 가깝다.


FSL을 해결하기 위한 지배적인 접근법 중 하나는 **메트릭 기반 메타-러닝(metric-based meta-learning)**이다. 여기서 '메타-러닝' 또는 '학습을 위한 학습(learning to learn)'의 핵심 아이디어는 특정 분류 과제를 직접 학습하는 대신, 새로운 과제에 빠르게 적응할 수 있는 **일반화 가능한 임베딩 공간**을 학습하는 것이다.2


메트릭 기반 FSL의 전형적인 파이프라인은 '메타-훈련'과 '메타-테스트'의 두 단계로 구성된다.

1. **메타-훈련 (Meta-Training):** 먼저, 수많은 클래스를 포함하는 대규모 기반 데이터셋(base dataset)을 사용하여 DML 모델을 훈련시킨다. 이 과정의 목표는 특정 클래스를 외우는 것이 아니라, '유사성'이라는 일반적인 개념을 포착하는 풍부한 임베딩 공간을 구축하는 것이다.
2. **메타-테스트 (Meta-Testing):** 새로운 FSL 과제(예: 5-way 1-shot, 즉 5개의 새로운 클래스에 대해 각각 1개의 예제만 주어진 상황)가 주어지면, 다음과 같이 추론한다.
   - **프로토타입 생성:** 주어진 N개의 새로운 클래스 각각에 대해 K개의 '서포트(support)' 샘플들을 미리 학습된 임베딩 함수에 통과시킨다. 그리고 각 클래스별로 K개 임베딩의 평균을 계산하여 해당 클래스를 대표하는 '프로토타입(prototype)' 벡터를 생성한다.
   - **분류:** 새로운 '쿼리(query)' 이미지가 들어오면, 이 역시 임베딩 공간으로 투영한 뒤, $N$개의 프로토타입 중 어떤 것과 가장 가까운지(예: 유클리드 거리 기준)를 계산하여 클래스를 예측한다.52


이 접근법이 효과적인 이유는 모델이 '고양이가 무엇인지'와 같은 특정 클래스에 대한 지식을 학습한 것이 아니라, '동물 이미지들 간의 유사도를 어떻게 측정할 것인가'라는 보다 근본적이고 일반화 가능한 기술을 학습했기 때문이다.2 이렇게 학습된 '메트릭'은 훈련 때 보지 못했던 완전히 새로운 클래스에도 백본 네트워크의 재훈련 없이 즉시 적용될 수 있다. 이는 메트릭 러닝이 가진 강력한 일반화 능력을 명확히 보여주는 사례이다.

안면 인식(오픈-셋), OOD 탐지(분포 외), 퓨샷 러닝(소수 예제)과 같은 다양한 응용 분야에서 메트릭 러닝이 성공적으로 활용되는 현상은 한 가지 통일된 원리를 시사한다. 그것은 바로 **견고하고 의미 있는 메트릭 공간을 학습하는 것이, 특정 닫힌-셋 분류 문제를 해결하는 것보다 더 근본적이고 일반화 가능한 학습 과제라는 점이다.** 안면 인식에서는 학습된 메트릭이 새로운 신원을 비교할 수 있게 하고, OOD 탐지에서는 학습된 메트릭이 '정상'의 범위를 정의하여 벗어나는 모든 것을 탐지하게 한다. 퓨샷 러닝에서는 학습된 메트릭이 새로운 클래스에 즉시 적용할 수 있는 '유사성의 청사진' 역할을 한다. 이 모든 사례의 공통분모는 모델이 특정 지식을 배우는 것이 아니라, 지식을 비교하고 측정하는 '기술' 자체를 배운다는 점이다. 이 추상화 수준의 차이가 바로 메트릭 러닝이 다양한 도메인에서 강력한 힘을 발휘하는 이유이다.


본 보고서의 마지막 장에서는 지금까지의 논의를 종합하고, 메트릭 러닝을 다른 학습 패러다임과의 관계 속에서 조망하며, 이 분야가 나아갈 미래의 연구 방향을 탐색한다.


기계 학습 분야에서 메트릭 러닝, 표현 학습(Representation Learning), 그리고 분류(Classification)는 종종 혼용되거나 서로 다른 개념으로 간주되지만, 실제로는 깊이 얽혀 있는 관계를 가진다.3 이들의 관계를 명확히 이해하는 것은 각 접근법의 장단점을 파악하고 주어진 문제에 가장 적합한 방법을 선택하는 데 중요하다.


- **표현 학습 (Representation Learning):** 가장 상위의 포괄적인 목표이다. 표현 학습의 목적은 원본 데이터(raw data)를 기계 학습 모델이 더 쉽게 처리하고 유용한 패턴을 발견할 수 있는 '표현(representation)' 또는 '특징(feature)'으로 변환하는 것이다.4 이 관점에서, 심층 신경망을 사용하는 거의 모든 현대적 학습 방법은 표현 학습 모델이라고 할 수 있다.
- **메트릭 러닝과 분류:** 이 둘은 유용한 표현을 학습하기 위한 두 가지 주요 **접근법** 또는 **방법론**이다.3 즉, 표현 학습이라는 큰 목표를 달성하기 위한 구체적인 수단이라고 볼 수 있다.


두 접근법은 목표하는 표현의 특성과 학습 방식에서 차이를 보인다.

- **메트릭 러닝:**
  - **목표:** 샘플들 간의 **거리**를 직접적으로 최적화하여 의미론적 유사성을 반영하는 임베딩 공간을 구축한다.
  - **손실 함수:** 대조 손실, 삼중항 손실 등 쌍(pair) 또는 삼중항(triplet) 기반의 손실 함수를 주로 사용한다.
  - **장점:** 학습된 거리가 직접적인 유사도 척도가 되므로, 검색(retrieval), 검증(verification), 순위(ranking)와 같은 과제나 오픈-셋 문제에 매우 강력하다.3
  - **단점:** 샘플링 과정 때문에 훈련이 상대적으로 느리고 계산 복잡도가 높을 수 있다.3
- **분류:**
  - **목표:** 주어진 샘플을 사전 정의된 클래스 중 하나로 정확하게 **분류**하는 결정 경계(decision boundary)를 학습한다.
  - **손실 함수:** 주로 교차 엔트로피(cross-entropy) 손실을 사용한다.
  - **장점:** 일반적으로 훈련이 빠르고, 닫힌-셋(closed-set) 분류 문제에서 매우 높은 정확도를 달성한다.3
  - **단점:** 학습된 특징(임베딩)을 거리 기반 작업에 사용할 수는 있지만, 임베딩 공간이 메트릭 러닝만큼 유사도에 최적화된 구조를 갖지 않을 수 있다. 즉, 클래스 간 분리는 잘 되지만, 클래스 내 샘플들이 흩어져 있거나 클래스 간 거리가 의미를 갖지 않을 수 있다.58


초기에는 두 패러다임이 명확히 구분되었지만, 최근 연구들은 이 둘 사이에 깊은 이론적 연결고리가 있음을 밝혀냈다.3 앞서 5장에서 논의했듯이, 

**프록시 기반 메트릭 러닝**은 분류 학습과 개념적으로 매우 유사하며, 특정 조건 하에서는 수학적으로 등가(equivalent)가 될 수 있다. 분류 모델의 마지막 선형 계층의 가중치 벡터는 각 클래스를 대표하는 '프록시'로 볼 수 있으며, 교차 엔트로피 손실은 샘플 임베딩을 정답 클래스의 프록시에 가깝게 만드는 역할을 한다. 이러한 통찰은 두 패러다임의 경계를 허물고, ArcFace와 같이 분류의 효율성과 메트릭 러닝의 마진 개념을 결합한 하이브리드 접근법의 등장을 촉진했다.

| 표 12.1: 학습 패러다임 비교 |
| --------------------------- |
| **패러다임**                |
| **분류**                    |
| **메트릭 러닝 (쌍/삼중항)** |
| **메트릭 러닝 (프록시)**    |


딥 메트릭 러닝의 성공은 대규모의 레이블된 데이터셋에 크게 의존해왔다. 그러나 현실 세계에서는 레이블링 비용이 매우 비싸거나 불가능한 경우가 많다. 이러한 한계를 극복하기 위해, 레이블 없이 데이터 자체의 내재적 구조를 활용하여 학습하는 **자기 지도 학습(Self-Supervised Learning, SSL)** 및 비지도 학습(Unsupervised Learning)이 DML 분야에서도 중요한 연구 방향으로 부상하고 있다.60


SSL은 데이터 자체로부터 감독 신호(supervisory signal)를 인공적으로 생성하여 모델을 학습시킨다. DML 분야에서 가장 널리 사용되는 SSL 패러다임은 **대조 학습(Contrastive Learning)**이다. 그 과정은 다음과 같다.

1. 데이터셋에서 임의의 샘플을 '앵커(anchor)'로 선택한다.
2. 앵커에 데이터 증강(예: 이미지의 다른 부분 자르기, 색상 변환, 회전 등)을 적용하여 '포지티브(positive)' 샘플을 생성한다. 앵커와 포지티브는 근본적으로 같은 이미지에서 파생되었으므로 의미적으로 유사하다고 가정한다.15
3. 미니배치 내의 다른 모든 샘플들을 '네거티브(negative)'로 간주한다.
4. 대조 손실 함수(예: NT-Xent, InfoNCE)를 사용하여, 앵커와 포지티브 임베딩은 서로 끌어당기고, 앵커와 모든 네거티브 임베딩은 서로 밀어내도록 학습한다.

이 과정을 통해 모델은 데이터 증강에 불변하는, 즉 이미지의 본질적인 콘텐츠를 포착하는 표현을 학습하게 된다. 이는 명시적인 레이블 없이도 강력한 의미론적 임베딩 공간을 구축할 수 있게 해준다.


SSL 분야는 현재 폭발적인 성장을 경험하고 있으며, 다음과 같은 핵심적인 질문들을 탐구하는 방향으로 나아가고 있다.60

- **이론적 기반:** 왜 특정 보조 과제(auxiliary task)가 다른 과제보다 더 나은 표현을 학습하게 하는가에 대한 이론적 이해.
- **데이터 요구량:** 효과적인 표현을 학습하는 데 필요한 레이블 없는 데이터의 양은 얼마인가.
- **아키텍처의 영향:** 신경망 아키텍처가 SSL 성능에 미치는 영향 분석.
- **응용 분야 확장:** 이미지, 텍스트를 넘어 그래프, 시계열, 음성 등 다양한 데이터 모달리티로의 확장.


메트릭 러닝은 많은 발전을 이루었지만, 여전히 해결해야 할 여러 도전 과제와 한계를 안고 있다. 본 보고서 전반에 걸쳐 논의된 문제들을 종합하면 다음과 같다.

- **계산 및 샘플링 복잡도:** 쌍/삼중항 기반 손실의 $O(N^2)/O(N^3)$ 복잡도는 대규모 데이터셋에 대한 적용을 어렵게 만드는 가장 근본적인 장벽이다. 프록시 기반 방법이 이를 완화했지만, 여전히 계산 비용은 중요한 고려사항이다.41
- **하이퍼파라미터 민감도:** 모델의 성능은 손실 함수의 마진 값, 학습률, 샘플링 전략, 배치 크기 등 다양한 하이퍼파라미터의 선택에 매우 민감하다. 최적의 조합을 찾는 것은 많은 실험과 전문 지식을 요구하는 어려운 작업이다.44
- **데이터 편향과 허위 상관관계:** 모델은 데이터에 내재된 편향(bias)이나 허위 상관관계(spurious correlation)를 학습할 위험이 있다. 예를 들어, '숲' 배경에서만 호랑이 이미지를 학습한 모델은, '눈' 배경에 있는 호랑이 이미지를 보고 호랑이로 인식하지 못하고 '숲' 배경에 있는 사자 이미지와 더 가깝다고 판단할 수 있다. 이러한 문제는 모델의 일반화 성능을 심각하게 저해한다.41
- **클래스 내 분산 문제:** 대부분의 손실 함수는 같은 클래스에 속하는 모든 포지티브 샘플들을 동등하게 취급하여 하나의 점으로 모으려는 경향이 있다. 이는 클래스 내부의 미묘한 구조나 다양성(intra-class variance), 즉 지역적 다양체 구조를 파괴하여, 세밀한 순위가 중요한 검색 작업에서 성능을 제한할 수 있다.10
- **임베딩 공간 검색의 확장성:** 모델 학습이 성공적으로 끝난 후에도, 수백만 또는 수십억 개의 고차원 임베딩 벡터로 구성된 데이터베이스에서 효율적으로 최근접 이웃을 검색하는 것은 그 자체로 매우 어려운 시스템 엔지니어링 문제이다. KD-Tree와 같은 전통적인 인덱싱 구조는 고차원에서 성능이 급격히 저하되며, 근사 최근접 이웃(ANN) 검색 기술이 필수적이다.44


본 보고서는 메트릭 러닝이라는 학습 패러다임을 기초적인 수학적 원리부터 최신 딥러닝 프레임워크에 이르기까지 포괄적으로 조망했다. 고전적인 마할라노비스 거리 학습에서 시작하여, 샴 및 삼중항 네트워크, 다양한 손실 함수와 샘플링 전략으로 대표되는 현대 딥 메트릭 러닝의 발전을 체계적으로 추적했다. 또한 안면 인식, 이미지 검색, 이상 탐지, 추천 시스템, 퓨샷 러닝 등 광범위한 응용 사례를 통해 '분류'가 아닌 '비교'를 학습하는 패러다임이 가진 강력한 힘과 유연성을 확인했다.

메트릭 러닝은 단순히 새로운 알고리즘의 집합이 아니라, 기계가 세상을 이해하는 방식에 대한 근본적인 관점의 전환을 제시한다. 이는 특히 클래스가 고정되지 않은 동적인 실제 환경에서 그 가치를 발휘하며, 일반화 가능한 표현을 학습하는 데 있어 핵심적인 역할을 수행한다.

앞으로 메트릭 러닝 분야는 다음과 같은 흥미로운 최전선에서 더욱 발전할 것으로 기대된다.

- **비유클리드 기하학의 도입:** 시각 데이터가 가진 자연스러운 계층 구조를 더 잘 표현할 수 있는 쌍곡 공간(Hyperbolic space)과 같은 비유클리드 기하학을 임베딩 공간으로 활용하여, OOD 탐지나 계층적 분류 문제에서 성능을 향상시키려는 연구가 활발히 진행되고 있다.61
- **설명가능 AI (XAI)와의 결합:** DML 모델이 '왜' 두 객체를 유사하다고 판단했는지 그 근거를 시각화하거나 설명하는 기술의 개발은, 모델의 신뢰성을 높이고 디버깅을 용이하게 하여 금융, 의료 등 고위험 분야에서의 채택을 가속화할 것이다.62
- **인과적 메트릭 러닝 (Causal Metric Learning):** 데이터의 상관관계를 넘어 인과관계를 학습하여, 배경과 같은 허위 변수의 영향에 강건하고 환경 변화에 더 잘 일반화되는 '인과적 메트릭'을 학습하려는 시도가 이루어지고 있다.41
- **파운데이션 모델과의 융합:** 대규모 언어 모델(LLM)이나 비전 트랜스포머(ViT)와 같은 거대 파운데이션 모델이 학습한 풍부하고 일반적인 표현을 메트릭 러닝 프레임워크 내에서 효과적으로 활용하고 미세 조정하는 방법론은, 향후 DML 성능을 한 단계 더 끌어올릴 핵심적인 연구 주제가 될 것이다.

결론적으로, 메트릭 러닝은 데이터로부터 의미 있는 '거리'의 개념을 창조해내는 강력한 도구로서, 인공지능이 더욱 정교하고 인간과 유사한 방식으로 세상을 이해하고 상호작용하게 만드는 데 지속적으로 기여할 것이다.


1. An Introduction to Metric Learning | dida blog, accessed July 19, 2025, https://dida.do/blog/metric-learning
2. Few-Shot Metric Learning: Online Adaptation of Embedding for Retrieval - CVF Open Access, accessed July 19, 2025, https://openaccess.thecvf.com/content/ACCV2022/papers/Jung_Few-Shot_Metric_Learning_Online_Adaptation_of_Embedding_for_Retrieval_ACCV_2022_paper.pdf
3. METRIC LEARNING VS CLASSIFICATION FOR ... - Justin Salamon, accessed July 19, 2025, https://www.justinsalamon.com/uploads/4/3/9/4/4394963/lee_metric_vs_classification_ismir2020.pdf
4. Metric and Representation Learning - Laboratoire Hubert Curien, accessed July 19, 2025, https://laboratoirehubertcurien.univ-st-etienne.fr/en/teams/data-intelligence/recherch-areas/metric-and-representation-learning.html
5. Metric learning - ratsgo's insight notes, accessed July 19, 2025, https://ratsgo.github.io/insight-notes/docs/qa/metric
6. Deep Metric Learning - Douglas' Space - 티스토리, accessed July 19, 2025, https://doug.tistory.com/63
7. Tutorial on Metric Learning - Inria, accessed July 19, 2025, https://researchers.lille.inria.fr/abellet/talks/metric_learning_tutorial_CIL.pdf
8. Deep Metric Learning: A Survey - MDPI, accessed July 19, 2025, https://www.mdpi.com/2073-8994/11/9/1066
9. Metric Learning for Image Search With Weights & Biases - Wandb, accessed July 19, 2025, https://wandb.ai/ayush-thakur/metric-learning/reports/Metric-Learning-for-Image-Search-With-Weights-Biases--VmlldzoyNTM0NDc
10. Deep Metric Learning with Self-Supervised Ranking - Association for the Advancement of Artificial Intelligence (AAAI), accessed July 19, 2025, https://cdn.aaai.org/ojs/16226/16226-13-19720-1-2-20210518.pdf
11. 1. What is Metric Learning? - scikit-learn-contrib, accessed July 19, 2025, https://contrib.scikit-learn.org/metric-learn/introduction.html
12. Spectral, Probabilistic, and Deep Metric Learning: Tutorial ... - arXiv, accessed July 19, 2025, https://arxiv.org/pdf/2201.09267
13. [머신 러닝/딥 러닝] Metric Learning의 개념과 Deep Metric Learning - untitled blog - 티스토리, accessed July 19, 2025, https://untitledtblog.tistory.com/164
14. Metric learning - Kyungjin Cho - 티스토리, accessed July 19, 2025, https://kjcho-dl-ml.tistory.com/3
15. [강의 정리] Deep Metric Learning - velog, accessed July 19, 2025, [https://velog.io/@sjina0722/%EA%B0%95%EC%9D%98-%EC%A0%95%EB%A6%AC-Deep-Metric-Learning](https://velog.io/@sjina0722/강의-정리-Deep-Metric-Learning)
16. Metric Learning - Schneppat AI, accessed July 19, 2025, https://schneppat.com/metric-learning.html
17. A Social Recommendation Based on Metric Learning and Users' Co ..., accessed July 19, 2025, https://www.mdpi.com/2073-8994/13/11/2158
18. 마할라노비스 거리(Mahalanobis distance) - gaussian37 - JINSOL KIM, accessed July 19, 2025, https://gaussian37.github.io/ml-concept-mahalanobis_distance/
19. 마할라노비스 거리 - 위키백과, 우리 모두의 백과사전, accessed July 19, 2025, [https://ko.wikipedia.org/wiki/%EB%A7%88%ED%95%A0%EB%9D%BC%EB%85%B8%EB%B9%84%EC%8A%A4_%EA%B1%B0%EB%A6%AC](https://ko.wikipedia.org/wiki/마할라노비스_거리)
20. 마할라노비스 거리 - 상훈's CANVAS - 티스토리, accessed July 19, 2025, https://canvas4sh.tistory.com/400
21. Mahalanobis distance - Wikipedia, accessed July 19, 2025, https://en.wikipedia.org/wiki/Mahalanobis_distance
22. Large margin nearest neighbor - Wikipedia, accessed July 19, 2025, https://en.wikipedia.org/wiki/Large_margin_nearest_neighbor
23. LMNN(Large Margin Nearest Neighbors) - Hwani - 티스토리, accessed July 19, 2025, https://hwa-a-nui.tistory.com/34
24. shiivashaakeri/KNN-From-Scratch: This project implements two algorithms, K-Nearest Neighbors (KNN) and Large Margin Nearest Neighbor (LMNN) using the Neighbourhood Component Analysis (NCA) approach. - GitHub, accessed July 19, 2025, https://github.com/shiivashaakeri/KNN-From-Scratch
25. Distance Metric Learning for Conditional Anomaly Detection - PMC, accessed July 19, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC2871323/
26. 딥러닝, accessed July 19, 2025, https://kr.mathworks.com/discovery/deep-learning.html
27. Siamese Network - Deep Learning - FR, accessed July 19, 2025, https://perso.esiee.fr/~chierchg/deep-learning/tutorials/metric/metric-1.html
28. Image similarity estimation using a Siamese Network with a triplet loss, accessed July 19, 2025, https://keras.io/examples/vision/siamese_network/
29. Deep metric learning using Triplet network - Semantic Scholar, accessed July 19, 2025, https://pdfs.semanticscholar.org/b1f7/9d796776839aa6ca15b8c552bae1de1029af.pdf
30. DataHour: A Simple Guide to Deep Metric Learning - Analytics Vidhya, accessed July 19, 2025, https://www.analyticsvidhya.com/events/datahour/datahour-a-simple-guide-to-deep-metric-learning/
31. Out-of-Distribution Detection Based on Distance Metric Learning - AWS, accessed July 19, 2025, https://manuscriptlink-society-file.s3-ap-northeast-1.amazonaws.com/kism/conference/sma2020/presentation/SMA-2020_paper_60.pdf
32. deep metric learning using triplet network - deepsense.ai, accessed July 19, 2025, https://deepsense.ai/wp-content/uploads/2017/08/1412.6622-3.pdf
33. Face identification (Metric Learning) | Curso de Ciencia de Datos, accessed July 19, 2025, https://centicmurcia.github.io/curso-ciencia-datos/3.5-image/4-face-id/
34. What is the difference between triplet and contrastive loss?, accessed July 19, 2025, https://www.educative.io/answers/what-is-the-difference-between-triplet-and-contrastive-loss
35. Triplet loss - Wikipedia, accessed July 19, 2025, https://en.wikipedia.org/wiki/Triplet_loss
36. [1503.03832] FaceNet: A Unified Embedding for Face Recognition and Clustering - arXiv, accessed July 19, 2025, https://arxiv.org/abs/1503.03832
37. arXiv:1503.03832v3 [cs.CV] 17 Jun 2015, accessed July 19, 2025, http://arxiv.org/pdf/1503.03832
38. [D] What are current SOTA algorithms and loss functions for learning high-level image similarity? : r/MachineLearning - Reddit, accessed July 19, 2025, https://www.reddit.com/r/MachineLearning/comments/12mwzhq/d_what_are_current_sota_algorithms_and_loss/
39. Losses - PyTorch Metric Learning, accessed July 19, 2025, https://kevinmusgrave.github.io/pytorch-metric-learning/losses/
40. PyTorch Metric Learning: A Practical Guide for Advanced Applications - Medium, accessed July 19, 2025, https://medium.com/@heyamit10/pytorch-metric-learning-a-practical-guide-for-advanced-applications-075634bff016
41. Deep Causal Metric Learning, accessed July 19, 2025, https://proceedings.mlr.press/v162/deng22c/deng22c.pdf
42. Dynamic sampling for deep metric learning - Computing Multimedia Lab, accessed July 19, 2025, https://cmmlab.xmu.edu.cn/pubs/prl2021.pdf
43. Deep Metric Learning Using Negative Sampling Probability Annealing, accessed July 19, 2025, https://www.mdpi.com/1424-8220/22/19/7579
44. Metric Learning with Deep Neural Networks [0.4in]Avinash Kak ..., accessed July 19, 2025, https://engineering.purdue.edu/kak/pdf-kak/MetricLearning.pdf
45. Deep Metric Learning Using Negative Sampling Probability Annealing - PubMed, accessed July 19, 2025, https://pubmed.ncbi.nlm.nih.gov/36236678/
46. All You Need to Know About Training Image Retrieval Models - arXiv, accessed July 19, 2025, https://arxiv.org/html/2503.13045v1
47. Facial Recognition Leveraging Generative Adversarial Networks - arXiv, accessed July 19, 2025, https://arxiv.org/html/2505.11884v1
48. Image Similarity Search by using Metric Learning - Zep Analytics, accessed July 19, 2025, https://www.zepanalytics.com/blogs/image-similarity-search-by-using-metric-learning
49. Metric Learning for Novelty and Anomaly Detection - BMVA Archive, accessed July 19, 2025, https://bmva-archive.org.uk/bmvc/2018/contents/papers/0178.pdf
50. Metric Learning for Novelty and Anomaly Detection - Papers With Code, accessed July 19, 2025, https://paperswithcode.com/paper/metric-learning-for-novelty-and-anomaly/review/
51. [1808.05492] Metric Learning for Novelty and Anomaly Detection - arXiv, accessed July 19, 2025, https://arxiv.org/abs/1808.05492
52. What Is Few-Shot Learning? | IBM, accessed July 19, 2025, https://www.ibm.com/think/topics/few-shot-learning
53. Few-Shot Learning Based on Metric Learning Using Class Augmentation, accessed July 19, 2025, https://www.computer.org/csdl/proceedings-article/icpr/2021/09411993/1tmhBL5VW2Q
54. Few-shot classification based on manifold metric learning - SPIE Digital Library, accessed July 19, 2025, https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-33/issue-1/013026/Few-shot-classification-based-on-manifold-metric-learning/10.1117/1.JEI.33.1.013026.full
55. Knowledge Guided Metric Learning for Few-Shot Text Classification - ACL Anthology, accessed July 19, 2025, https://aclanthology.org/2021.naacl-main.261.pdf
56. Representation Learning - Terence L. van Zyl, accessed July 19, 2025, https://tvanzyl.github.io/blog/representation_learning/
57. Deep Metric and Representation Learning - Computer Vision & Learning Group, accessed July 19, 2025, https://ommer-lab.com/research/deep-metric-and-representation-learning/
58. Distinction between classification and metric-learning method in... | Download Scientific Diagram - ResearchGate, accessed July 19, 2025, https://www.researchgate.net/figure/Distinction-between-classification-and-metric-learning-method-in-training-embedding_fig1_366333427
59. metric learning and contrastive learning difference - Stack Overflow, accessed July 19, 2025, https://stackoverflow.com/questions/71809044/metric-learning-and-contrastive-learning-difference
60. NeurIPS 2024 Workshop: Self-Supervised Learning - Theory and ..., accessed July 19, 2025, https://sslneurips2024.github.io/
61. [2403.15260] Hyperbolic Metric Learning for Visual Outlier Detection - arXiv, accessed July 19, 2025, https://arxiv.org/abs/2403.15260
62. Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective - arXiv, accessed July 19, 2025, https://arxiv.org/html/2401.04374v2

