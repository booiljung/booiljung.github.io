[자기지도학습(Self-Supervised Learning)](./index.md)
# 자기지도학습 (Self-Supervised Learning)



지난 수십 년간 인공지능(AI) 분야, 특히 기계학습의 발전은 지도 학습(Supervised Learning) 패러다임에 의해 주도되어 왔다.1 지도 학습은 입력 데이터와 그에 상응하는 정답(레이블) 쌍으로 구성된 대규모 데이터셋을 기반으로 모델을 훈련시켜, 입력과 출력 간의 복잡한 관계를 학습하는 방식이다.2 이미지 분류, 객체 탐지, 기계 번역 등 수많은 분야에서 인간의 능력을 뛰어넘는 성능을 달성한 모델들은 대부분 이러한 지도 학습의 산물이다. 그러나 이 눈부신 성공의 이면에는 '레이블'이라는 근본적인 제약이 존재한다.

지도 학습의 성능은 양질의 레이블 데이터 양에 비례하지만, 이러한 데이터를 구축하는 과정은 막대한 시간과 비용, 그리고 노력을 수반한다.5 예를 들어, ImageNet과 같은 대규모 이미지 데이터셋을 구축하기 위해서는 수백만 장의 이미지에 대해 인간이 직접 정확한 레이블을 부여해야 한다. 의료 영상 분석이나 법률 문서 판독과 같이 고도의 전문 지식이 요구되는 도메인에서는 레이블링 작업의 난이도와 비용이 기하급수적으로 증가하여 데이터 확보 자체가 연구의 가장 큰 병목이 되기도 한다.7

이러한 상황에서 자기지도학습(Self-Supervised Learning, SSL)은 기존 패러다임의 근본적인 한계를 극복하기 위한 대안으로 부상했다. SSL의 핵심 철학은 인터넷, 서적, 영상 등 세상에 존재하는 방대한 양의 비정형(unstructured) 데이터, 즉 레이블이 없는 데이터를 학습 자원으로 활용하는 것이다.8 이는 레이블링이라는 값비싼 가공 과정을 거치지 않고 '데이터'라는 원자재를 직접 활용하여 모델을 훈련시키는 방식으로, AI 개발의 공급망을 혁신하는 것과 같다. 실제로 AI 모델의 크기와 성능 요구사항이 기하급수적으로 증가함에 따라, 지도 학습에 필요한 데이터 수급 비용은 감당하기 어려운 수준에 도달했다. SSL은 이러한 '규모의 경제' 문제를 해결하는 경제적 필연성을 제시하며, GPT나 CLIP과 같은 초거대 파운데이션 모델(Foundation Models)의 등장을 가능하게 한 핵심 동력이 되었다.

결론적으로, 자기지도학습은 단순히 레이블이 없는 데이터를 활용하는 기술을 넘어, 데이터 수집의 병목 현상을 해소하고 AI 기술의 확장성과 접근성을 극대화하는 패러다임의 전환을 이끌고 있다.10 이는 레이블의 제약에서 벗어나, 데이터 자체가 가진 무한한 잠재력을 온전히 활용하여 기계가 스스로 세상을 학습하도록 만드는 새로운 지평을 열고 있다.


자기지도학습(SSL)은 비지도 학습(Unsupervised Learning)의 한 형태로 분류되지만, 그 작동 방식은 지도 학습과 매우 유사하다. SSL의 핵심 정의는 **"데이터 자체의 내재적 구조(inherent structure)를 활용하여 감독 신호(supervisory signal)를 생성하고, 이를 학습에 이용하는 기계학습 패러다임"**이다.8 다시 말해, 인간의 개입 없이 데이터로부터 유사-레이블(pseudo-label)을 자동으로 생성하고, 이 유사-레이블을 정답 삼아 지도 학습과 동일한 방식으로 모델의 가중치를 최적화한다.5

이 과정은 일반적으로 두 개의 주요 단계로 구성된다. 첫 번째 단계는 **전처리 과제(Pretext Task)를 통한 사전 학습(pre-training)**이다. Pretext task는 SSL의 핵심으로, 데이터의 일부를 의도적으로 숨기거나 변형한 뒤 모델이 원래 상태를 예측하거나 복원하도록 설계된 인위적인 과제를 의미한다.8 예를 들어, 문장에서 임의의 단어를 가리고 주변 단어를 통해 맞추게 하거나, 이미지의 일부를 잘라내고 나머지 부분으로 복원하게 하는 식이다. 모델은 이 '스스로 만든 문제'를 푸는 과정에서 단어의 문맥적 의미나 객체의 형태적 특징과 같은 데이터의 고차원적인 의미론적, 구조적 특징을 담은 풍부한 표현(rich representation)을 학습하게 된다.18 이 과제는 그 자체로 유용하기보다는, 유용한 표현을 학습시키기 위한 '구실' 또는 '명분'이라는 의미에서 'pretext'라는 이름이 붙었다.8

두 번째 단계는 **후속 과제(Downstream Task)에 대한 전이 학습(Transfer Learning)**이다. Pretext task를 통해 사전 학습된 모델의 가중치나 학습된 특징 추출기(feature extractor)를 실제 우리가 풀고자 하는 목표 과제(예: 이미지 분류, 감성 분석, 객체 탐지)에 적용한다.8 이때, 소량의 레이블 데이터만을 사용하여 모델 전체 또는 일부를 미세 조정(fine-tuning)하는 것만으로도, 처음부터 지도 학습을 수행한 모델에 필적하거나 이를 능가하는 높은 성능을 달성할 수 있다.

이러한 SSL의 철학을 저명한 AI 학자 얀 르쿤(Yann LeCun)은 **"입력의 일부를 모른다고 가정하고 그것을 예측하는 것(pretend there is a part of the input you don't know and predict it)"**이라는 간결한 문장으로 요약했다.8 이는 마치 우리가 퍼즐의 몇 조각을 보고 전체 그림을 유추하거나, 대화의 일부만 듣고도 전체 맥락을 파악하는 인간의 학습 방식과 유사하다.1

Pretext task의 설계는 단순히 임의의 과제를 설정하는 것을 넘어, 모델이 학습해야 할 세상에 대한 특정 가정, 즉 귀납적 편향(inductive bias)을 주입하는 정교한 과정이다. 예를 들어, 이미지의 회전 각도를 맞추는 pretext task는 모델에게 '객체의 정체성은 방향과 무관하다'는 회전 불변성(rotational invariance)을 학습하도록 강제한다.20 이는 대부분의 객체 인식 과제에서 매우 유용한 속성이다. 마찬가지로, 문맥 속에서 가려진 단어를 예측하는 마스킹(masking) 과제는 '단어의 의미는 주변 단어에 의해 결정된다'는 언어학의 분포 가설(distributional hypothesis)을 모델에 주입하는 역할을 한다.22 따라서 어떤 pretext task를 선택하고 설계하느냐는 후속 과제에 얼마나 유용한 표현을 학습할 수 있는지를 결정하는 핵심 요소가 된다. 이는 SSL의 성공이 단순히 어려운 문제를 푸는 능력이 아니라, 풀고자 하는 문제에 유용한 편향을 효과적으로 주입하는 과제를 설계하는 능력에 달려 있음을 시사한다.


자기지도학습(SSL)은 기존의 기계학습 패러다임 분류 체계에서 독특한 위치를 차지하며, 지도 학습, 비지도 학습, 준지도 학습의 경계를 넘나드는 특징을 보인다. 각 패러다임과의 관계를 명확히 이해하는 것은 SSL의 본질을 파악하는 데 매우 중요하다.

**지도 학습(Supervised Learning)과의 관계:** SSL은 학습 메커니즘 측면에서 지도 학습과 가장 유사하다. 두 패러다임 모두 명확한 정답(ground truth)을 설정하고, 모델의 예측과 정답 간의 오차를 측정하는 손실 함수(loss function)를 정의하며, 역전파(backpropagation) 알고리즘을 통해 이 오차를 최소화하는 방향으로 모델의 가중치를 업데이트한다.8 가장 큰 차이점은 감독 신호의 출처다. 지도 학습의 감독 신호는 인간이 직접 부여한 외부 레이블인 반면, SSL의 감독 신호는 데이터 자체에서 파생된 내부적인 유사-레이블이다.3

**비지도 학습(Unsupervised Learning)과의 관계:** SSL은 인간이 만든 레이블을 사용하지 않는다는 점에서 비지도 학습의 큰 틀에 속한다.1 그러나 목표와 접근 방식에서 전통적인 비지도 학습과 뚜렷한 차이를 보인다. K-평균 군집화(K-means clustering)나 주성분 분석(PCA)과 같은 전통적인 비지도 학습은 데이터의 내재적 구조나 패턴을 '발견(discovery)'하는 데 초점을 맞춘다.2 여기에는 명시적인 예측 과제나 정답이 존재하지 않는다. 반면, SSL은 데이터의 일부를 예측하거나 복원하는 구체적인 pretext task를 '해결(solving)'함으로써 유용한 표현을 학습한다.24 즉, SSL은 비지도적 데이터 환경에서 지도 학습의 방법론을 차용한, 보다 목표 지향적이고 구조화된 비지도 학습이라 할 수 있다.

**준지도 학습(Semi-supervised Learning)과의 관계:** 준지도 학습은 소량의 레이블 데이터와 대량의 비레이블 데이터를 '동시에' 활용하여 모델의 성능을 높이는 것을 목표로 한다.26 반면, SSL은 사전 학습 단계에서는 오직 비레이블 데이터'만'을 사용한다는 점에서 근본적인 차이가 있다.26 하지만 SSL의 가장 일반적인 활용 시나리오인 '사전학습-미세조정' 파이프라인 전체를 보면, 이는 준지도 학습의 가장 성공적인 전략 중 하나로 해석될 수 있다.26 대규모 비레이블 데이터로 표현을 학습(SSL 사전 학습)한 뒤, 소규모 레이블 데이터로 특정 과제에 맞게 모델을 조정(지도 미세 조정)하는 과정은 결과적으로 두 종류의 데이터를 모두 활용하여 최종 모델을 완성하기 때문이다.

이러한 관계를 종합해 볼 때, SSL은 단순히 네 가지 학습 패러다임 중 하나로 존재하는 것을 넘어, 다른 패러다임들을 연결하고 통합하는 '메타 패러다임(metaparadigm)'으로서 기능한다. SSL은 비지도 데이터로부터 감독 신호를 생성하여 **지도 학습의 방법론을 비지도 문제에 적용**하고, 그 자체로 **구조화된 비지도 학습**의 역할을 수행하며, 전체 파이프라인은 **가장 효과적인 준지도 학습 전략**으로 자리 잡았다. 나아가 SSL로 학습된 표현은 강화학습(Reinforcement Learning) 에이전트의 초기 정책을 설정하여 학습 효율을 높이는 데 사용되기도 한다.8 이처럼 SSL은 각 패러다임의 경계를 허물고 그 장점들을 융합하여, 현대 AI의 표준 절차로 자리 잡은 '대규모 사전 학습'을 가능하게 한 핵심 기술이라 할 수 있다.


| 구분                        | 지도 학습 (Supervised)                      | 비지도 학습 (Unsupervised)            | **자기지도학습 (Self-Supervised)**                | 준지도 학습 (Semi-supervised)                                |
| --------------------------- | ------------------------------------------- | ------------------------------------- | ------------------------------------------------- | ------------------------------------------------------------ |
| **데이터 요구사항**         | 대규모 레이블 데이터 ($X$, $Y$)             | 비레이블 데이터 ($X$)                 | 대규모 비레이블 데이터 ($X$)                      | 소량의 레이블 데이터 + 다량의 비레이블 데이터                |
| **주요 목표**               | 입력과 출력 간의 매핑 함수 학습 (예측)      | 데이터의 내재적 구조, 패턴, 분포 발견 | **데이터로부터 유용한 표현(representation) 학습** | 레이블 데이터와 비레이블 데이터를 함께 사용하여 예측 성능 향상 |
| **감독 신호 (Supervision)** | 인간이 명시적으로 제공한 정답 레이블        | 감독 신호 없음                        | **데이터 자체의 구조로부터 생성된 유사-레이블**   | 소량의 명시적 레이블 + 비레이블 데이터의 구조                |
| **대표 알고리즘**           | 선형/로지스틱 회귀, SVM, 신경망 (분류/회귀) | K-평균 군집화, PCA, 오토인코더 (초기) | BERT, SimCLR, MoCo, MAE, BYOL                     | 자기-훈련(Self-training), 공동-훈련(Co-training), SSL 사전학습+미세조정 |


자기지도학습은 하루아침에 등장한 개념이 아니다. 레이블 없이 데이터로부터 유용한 표현을 학습하려는 시도는 기계학습의 오랜 역사와 궤를 같이하며, 여러 아이디어와 기술적 돌파구가 축적된 결과 오늘날의 형태로 발전했다. 그 발전 과정을 추적하는 것은 SSL의 핵심 원리를 이해하는 데 중요한 단서를 제공한다.


'자기지도학습'이라는 용어가 널리 사용되기 훨씬 이전부터 그 철학을 공유하는 기법들이 존재했다. 그 가장 대표적인 예가 바로 오토인코더(Autoencoder)이다.8 1980년대에 처음 개념이 등장한 오토인코더는 입력 데이터를 받아 이를 저차원의 잠재 공간(latent space)으로 압축하는 인코더(encoder)와, 이 압축된 잠재 표현으로부터 원본 입력을 최대한 유사하게 복원하는 디코더(decoder)로 구성된 신경망 구조다.12

오토인코더의 학습 목표는 입력과 출력(복원된 입력) 간의 재구성 오류(reconstruction error)를 최소화하는 것이다. 즉, 입력 데이터 $x$가 주어졌을 때, 모델은 $x$ 자체를 정답 레이블로 사용하여 학습한다.31 이는 외부의 레이블 없이 데이터 스스로가 감독 신호를 제공하는 SSL의 핵심 원리를 명확하게 보여주는 초기 사례다.33 인코더가 입력을 더 낮은 차원의 벡터로 압축하도록 강제함으로써(undercomplete autoencoder), 모델은 단순히 입력을 복사하는 것이 아니라 데이터의 가장 본질적이고 중요한 특징(latent variables)을 잠재 공간에 인코딩하는 법을 배우게 된다.8 이 과정은 데이터의 주요 변동성을 포착한다는 점에서 주성분 분석(PCA)과 유사하지만, 신경망의 비선형성을 활용하여 훨씬 더 복잡하고 추상적인 표현을 학습할 수 있다.30

이처럼 오토인코더는 레이블 없이도 의미 있는 표현 학습(representation learning)이 가능함을 보여주었으며, 이후 등장할 다양한 SSL 기법들의 이론적, 구조적 기반을 마련했다. 데이터의 일부에 노이즈를 추가하여 복원 능력을 강화하는 디노이징 오토인코더(Denoising Autoencoder)와 같은 변형 모델들은 후대의 마스크 기반 SSL 모델의 아이디어와도 직접적으로 연결된다.


'자기지도학습'이라는 용어가 공식적으로 등장하고 그 개념이 구체화된 것은 2000년대 후반과 2010년대에 들어서면서부터다. 일부 문헌에 따르면 이 용어는 로보틱스 분야에서 처음 사용되었는데, 로봇에 장착된 여러 센서(예: 카메라, 모터 엔코더) 간의 물리적 관계를 활용하여 한 센서의 신호로 다른 센서의 신호를 예측함으로써 자동으로 레이블을 생성하고 학습하는 연구에서 비롯되었다.1 학계에서 공식적인 개념의 기원 중 하나로 자주 언급되는 것은 2007년 Raina 연구팀이 발표한 "Self-taught learning: Transfer learning from unlabeled data" 논문이다.8

SSL 연구가 본격적으로 활성화된 것은 2010년대 중반, 딥러닝의 부흥과 함께 컴퓨터 비전 분야에서 다양한 Pretext Task가 제안되면서부터다. 연구자들은 이미지 데이터가 가진 풍부한 내재적 구조를 활용하여 창의적인 '가짜 문제'들을 고안해냈다. 대표적인 초기 Pretext Task들은 다음과 같다.

- **상대적 패치 위치 예측 (Relative Patch Location):** 이미지를 9개의 격자로 나눈 뒤, 중앙 패치를 기준으로 주변 8개 패치 중 하나를 무작위로 선택하여 두 패치의 상대적 위치(예: '오른쪽 위')를 모델이 맞추도록 하는 과제.18 이를 통해 모델은 객체의 부분 간 공간적 관계를 학습하게 된다.
- **직소 퍼즐 맞추기 (Jigsaw Puzzle):** 이미지를 여러 조각(patch)으로 나눈 뒤 순서를 섞고, 모델이 원래의 배열을 맞추도록 하는 과제.17 이 과제를 해결하기 위해 모델은 각 조각의 내용뿐만 아니라 객체의 전체적인 형태와 구조를 이해해야 한다.
- **이미지 색채화 (Image Colorization):** 컬러 이미지를 흑백으로 변환한 뒤, 모델이 원래의 색상을 복원하도록 학습시키는 과제.14 성공적인 색채화는 객체의 정체성(예: '바나나는 노란색')에 대한 의미론적 이해를 필요로 한다.
- **이미지 회전 예측 (Image Rotation Prediction):** 이미지를 0, 90, 180, 270도 중 하나로 무작위 회전시킨 뒤, 모델이 적용된 회전 각도를 예측하도록 하는 과제.18 모델은 객체의 표준적인 방향(canonical orientation)을 학습해야만 이 문제를 풀 수 있다.

이러한 초기 Pretext Task 기반 연구들은 지도 학습에 비해 아직 성능이 뒤처졌지만, 레이블 없이도 유용한 시각적 표현을 학습할 수 있다는 가능성을 명확히 입증했다.18 또한, 어떤 Pretext Task를 설계하느냐에 따라 모델이 학습하는 표현의 종류와 질이 달라진다는 점을 보여주며, SSL 연구의 방향성을 제시하는 중요한 이정표가 되었다.


2018년을 기점으로 자기지도학습 분야에는 거대한 패러다임 전환이 일어났다. 바로 대조 학습(Contrastive Learning)의 등장이었다. 대조 학습은 SSL의 성능을 비약적으로 향상시켜, 여러 핵심 벤치마크에서 지도 학습으로 사전 학습된 모델의 성능에 도달하거나 이를 뛰어넘는 최초의 사례들을 만들어냈다.37

대조 학습의 핵심 아이디어는 **"유사한 것은 가깝게, 다른 것은 멀게"**라는 직관적인 원리에 기반한다.15 구체적으로, 하나의 원본 이미지에 서로 다른 데이터 증강(data augmentation, 예: 자르기, 색상 변경, 흐림 처리)을 적용하여 두 개의 '긍정 쌍(positive pair)'을 만든다. 그리고 이 이미지와는 아무 관련이 없는 다른 이미지들을 '부정 쌍(negative pair)'으로 간주한다. 학습 목표는 임베딩 공간(embedding space)에서 긍정 쌍의 표현(representation)은 서로 거리를 좁히고, 부정 쌍의 표현과는 거리를 최대한 멀게 만드는 것이다.37 이 과정을 통해 모델은 이미지의 색상, 위치, 크기와 같은 피상적인 변화에는 둔감하면서도(invariant), 객체의 본질적인 정체성과 같은 핵심적인 의미 정보(semantic information)를 포착하는 표현을 학습하게 된다.

SimCLR 40, MoCo 41와 같은 대표적인 대조 학습 모델들은 이 접근법을 통해 ImageNet 분류와 같은 고난도 벤치마크에서 기존의 모든 SSL 기법들을 압도하는 성능을 달성했다. 이러한 성공은 학계에 큰 충격을 주었고, SSL이 더 이상 지도 학습의 보조적인 수단이 아니라, 그 자체로 강력한 표현 학습 패러다임이 될 수 있음을 증명했다.

이러한 대조 학습의 성공과 맞물려, 얀 르쿤과 같은 영향력 있는 연구자들은 2019년경부터 '비지도 학습'이라는 포괄적이고 때로는 모호한 용어 대신 '자기지도학습'이라는 용어를 적극적으로 사용하기 시작했다.8 이는 대조 학습이 제공하는 명확한 학습 목표와 손실 함수가 전통적인 비지도 학습보다는 지도 학습의 프레임워크에 더 가깝다는 인식을 반영한 것이었다. 결과적으로 대조 학습의 혁명은 SSL이라는 연구 분야의 정체성을 확립하고, 이후 수많은 후속 연구를 촉발하는 기폭제가 되었다.

대조 학습의 등장은 Pretext Task 설계 방식에도 근본적인 변화를 가져왔다. 초기 SSL 연구가 각 데이터의 특성에 맞는 독창적이고 복잡한 Pretext Task(직소 퍼즐, 색채화 등)를 고안하는 데 집중했다면, 대조 학습은 '데이터 증강'이라는 하나의 일반화된 프레임워크로 이를 통합했다. 이제 연구자들은 더 이상 복잡한 퍼즐을 설계할 필요 없이, '어떤 데이터 증강 기법의 조합이 가장 효과적인 표현을 학습시키는가'라는 보다 표준화되고 체계적인 문제에 집중할 수 있게 되었다. 이는 SSL 연구의 진입 장벽을 낮추고, 다양한 방법론 간의 공정한 성능 비교를 가능하게 함으로써, 이후 SSL 연구가 폭발적으로 발전하는 중요한 계기를 마련했다. 즉, 대조 학습은 Pretext Task 설계의 초점을 '무엇을 풀 것인가(what to solve)'에서 '어떻게 변형할 것인가(how to transform)'로 이동시킨 것이다.


자기지도학습의 큰 축 중 하나는 데이터의 일부를 변형하거나 제거한 뒤, 모델이 이를 원래 상태로 복원하도록 학습시키는 생성 및 재구성 기반의 접근법이다. 이 방법론은 데이터의 내재적 구조와 통계적 특성을 모델이 직접 학습하도록 유도하며, 오토인코더에서 시작하여 현대의 대규모 언어 및 비전 모델에 이르기까지 그 명맥을 이어오고 있다.

3.1. 오토인코더(Autoencoders)와 그 변형: 정보 압축과 복원을 통한 학습

오토인코더는 자기지도학습의 가장 고전적이면서도 근본적인 형태를 보여주는 모델이다. 기본적인 구조는 입력을 저차원의 잠재 벡터 $z$로 압축하는 인코더 $f$와, 이 잠재 벡터로부터 원본 입력을 복원하는 디코더 $g$로 구성된다.30 학습 과정은 입력 $x$와 복원된 출력 $\hat{x} = g(f(x))$ 간의 재구성 오류를 최소화하는 것을 목표로 하며, 손실 함수로는 주로 평균 제곱 오차(Mean Squared Error, MSE)가 사용된다.12 수식으로는 다음과 같이 표현할 수 있다.
$$
\mathcal{L}(x, \hat{x}) = \|x - g(f(x))\|^2
$$
이 단순한 재구성 과정을 통해 모델은 레이블 없이도 데이터의 핵심적인 특징을 잠재 벡터 $z$에 압축하는 방법을 학습한다. 그러나 인코더와 디코더의 용량(capacity)이 충분히 크면, 모델은 단순히 입력을 그대로 복사하는 항등 함수(identity function)를 학습하여 의미 있는 특징을 추출하지 못하는 문제가 발생할 수 있다. 이러한 문제를 해결하고 더 유용하고 강건한 표현을 학습하기 위해 다양한 변형 오토인코더가 제안되었다.

- **디노이징 오토인코더 (Denoising Autoencoders, DAE):** 이 모델은 원본 입력 $x$에 의도적으로 노이즈를 추가하여 손상된 입력 $\tilde{x}$를 만든 뒤, 모델이 손상되지 않은 원본 $x$를 복원하도록 학습한다.30 이 과정을 통해 모델은 데이터의 사소한 변화나 노이즈를 제거하고, 데이터 분포의 핵심적인 구조(manifold)를 학습하게 되어 보다 강건한(robust) 특징 표현을 얻을 수 있다.

- **희소 오토인코더 (Sparse Autoencoders):** 잠재 공간의 차원이 입력보다 크더라도(overcomplete), 잠재 벡터 $z$의 대부분의 요소가 0에 가깝도록 희소성 제약(sparsity constraint)을 가하는 방식이다.30 이는 특정 입력에 대해 소수의 뉴런만이 활성화되도록 유도하여, 데이터의 각기 다른 특징을 전문적으로 포착하는 표현을 학습하게 한다. L1 정규화(regularization)나 KL 발산(KL-divergence)과 같은 기법이 희소성 제약을 위해 사용된다.

- **수축 오토인코더 (Contractive Autoencoders):** 이 모델은 손실 함수에 추가적인 항을 두어, 입력의 작은 변화에 대해 잠재 표현 $z$가 거의 변하지 않도록(수축하도록) 규제를 가한다.30 즉, 인코더 함수 

  $f$의 자코비안 행렬(Jacobian matrix)의 프로베니우스 놈(Frobenius norm)을 최소화한다. 이를 통해 모델은 입력 데이터의 지역적 변화에 둔감한, 보다 안정적이고 불변하는(invariant) 특징을 학습하게 된다.

이러한 오토인코더와 그 변형들은 입력 자체를 감독 신호로 삼아 데이터의 유용한 표현을 학습한다는 점에서 현대 자기지도학습의 중요한 이론적, 실용적 토대를 제공했다.


자연어 처리(NLP) 분야에서 자기지도학습의 혁명을 이끈 모델은 단연 BERT(Bidirectional Encoder Representations from Transformers)이다.42 BERT 이전의 언어 모델들(예: GPT-1, ELMo)은 주로 문장을 왼쪽에서 오른쪽으로, 또는 오른쪽에서 왼쪽으로 순차적으로 처리하며 다음 단어를 예측하는 단방향(unidirectional) 또는 얕은 양방향(shallow bidirectional) 구조를 가졌다. 이로 인해 문장 전체의 완전한 문맥 정보를 파악하는 데 한계가 있었다.

BERT는 '마스크 언어 모델링(Masked Language Modeling, MLM)'이라는 독창적인 pretext task를 도입하여 이 문제를 해결했다.43 MLM의 핵심 아이디어는 입력 문장에서 일부 토큰을 무작위로 선택하여 특수한 `` 토큰으로 가린 뒤, 모델이 문장의 왼쪽과 오른쪽 문맥을 모두 이용하여 가려진 원래 토큰이 무엇이었는지를 예측하도록 하는 것이다. Transformer의 인코더 구조는 셀프 어텐션(self-attention) 메커니즘을 통해 문장 내 모든 토큰 쌍의 관계를 동시에 계산할 수 있기 때문에, 이러한 진정한 의미의 양방향(bidirectional) 문맥 학습이 가능하다.

BERT는 사전 학습의 효율성과 성능을 극대화하기 위해 정교한 마스킹 전략을 사용한다. 전체 입력 토큰의 15%를 예측 대상으로 삼되, 다음과 같은 세 가지 규칙을 확률적으로 적용한다 43:

1. **80%의 경우:** 선택된 토큰을 `` 토큰으로 교체한다.
   - 예: "The dog chased the ball" → "The dog chased the ``"
2. **10%의 경우:** 선택된 토큰을 어휘 사전(vocabulary) 내의 다른 무작위 토큰으로 교체한다.
   - 예: "The dog chased the ball" → "The dog chased the `apple`"
3. **10%의 경우:** 선택된 토큰을 변경하지 않고 그대로 둔다.
   - 예: "The dog chased the ball" → "The dog chased the `ball`"

이러한 복합적인 전략은 사전 학습 단계와 실제 후속 과제에 모델을 적용하는 미세 조정 단계 간의 불일치 문제(pretrain-finetune discrepancy)를 완화하기 위해 고안되었다.43 만약 항상 

`토큰으로만 교체한다면, 모델은 입력에` 토큰이 존재할 때만 예측을 잘하도록 학습될 수 있다. 그러나 실제 미세 조정 데이터에는 `` 토큰이 존재하지 않으므로 성능 저하가 발생할 수 있다. 무작위 단어로 교체하거나 원래 단어를 그대로 둠으로써, 모델은 모든 입력 토큰에 대해 의미 있는 표현을 학습하도록 강제된다. 이 MLM 과제를 통해 BERT는 전례 없는 수준의 깊은 문맥적 이해 능력을 갖추게 되었고, 다양한 NLP 후속 과제에서 압도적인 성능을 기록하며 새로운 표준을 제시했다.


BERT가 NLP 분야에서 거둔 압도적인 성공은 비전 분야 연구자들에게 큰 영감을 주었다. 이미지에도 마스킹 기반의 자기지도학습을 적용하려는 여러 시도가 있었지만, 이미지와 언어라는 두 모달리티의 근본적인 차이로 인해 BERT만큼의 성공을 거두기는 어려웠다. 이러한 상황에서 등장한 Masked Autoencoders(MAE)는 비전 분야에 맞는 독창적인 설계를 통해 마스킹 기법을 성공적으로 적용한 대표적인 모델이다.47

MAE의 핵심 아이디어는 이미지를 일정한 크기의 여러 패치(patch)로 나눈 뒤, 이 중 매우 높은 비율(예: 75%)의 패치를 무작위로 마스킹(제거)하고, 모델이 나머지 보이는 패치들만을 이용해 제거된 패치들을 픽셀 단위로 복원하도록 학습하는 것이다.47 MAE의 성공을 이끈 두 가지 핵심적인 설계는 다음과 같다.

1. **비대칭적 인코더-디코더 구조 (Asymmetric Encoder-Decoder Architecture):** MAE는 인코더와 디코더의 역할을 명확히 분리하고 구조적으로 비대칭성을 부여했다.47
   - **경량 인코더:** 인코더는 마스킹되지 않은 소수의 보이는 패치(예: 전체의 25%)만을 입력으로 받는다. 이는 Vision Transformer(ViT)와 같은 대규모 인코더의 계산량과 메모리 사용량을 획기적으로 줄여, 사전 학습 속도를 3배 이상 가속화하는 효과를 가져온다.47
   - **경량 디코더:** 인코더가 출력한 잠재 표현과 함께, 마스킹된 패치들의 위치 정보를 담은 학습 가능한 마스크 토큰(mask token)을 입력받는다. 디코더는 이 정보를 종합하여 원본 이미지의 모든 패치를 픽셀 단위로 재구성하는 역할을 한다. 디코더는 인코더에 비해 훨씬 작은 규모로 설계되어 전체적인 계산 효율성을 높인다.47
2. **높은 마스킹 비율 (High Masking Ratio):** MAE는 75%라는 매우 높은 마스킹 비율을 사용한다. 이는 언어와 이미지의 정보 밀도 차이에 대한 깊은 통찰에서 비롯된다.47 언어는 정보 밀도가 매우 높은 이산적인(discrete) 신호로, 몇 개의 단어만 빠져도 문맥 추론이 매우 어려워진다. 따라서 BERT의 15% 마스킹은 충분히 도전적인 과제가 된다. 반면, 이미지는 인접 픽셀 간 상관관계가 높은, 즉 공간적 중복성(spatial redundancy)이 매우 큰 연속적인(continuous) 신호다. 만약 마스킹 비율이 낮다면, 모델은 주변 픽셀 정보를 단순히 보간(interpolation)하는 방식으로 문제를 쉽게 해결할 수 있어, 고차원적인 의미 정보를 학습할 필요가 없게 된다. 75%라는 극단적인 마스킹 비율은 이러한 중복성을 효과적으로 제거하고, 모델이 단순히 주변부를 모방하는 것을 넘어 객체의 형태, 장면의 구성과 같은 전체적인 맥락(gestalt)을 이해해야만 누락된 부분을 '추론'할 수 있도록 강제한다.47

이러한 설계 덕분에 MAE는 대규모 비전 모델을 매우 효율적으로 사전 학습시킬 수 있음을 증명했으며, ImageNet 분류, COCO 객체 탐지 및 분할과 같은 다양한 후속 과제에서 기존의 대조 학습 기반 모델들을 능가하는 우수한 성능을 달성했다.47 이는 재구성 기반의 자기지도학습이 여전히 강력한 패러다임임을 재확인시킨 중요한 성과다.


재구성 기반 모델이 데이터의 내재적 분포를 학습하는 데 초점을 맞춘다면, 대조 학습(Contrastive Learning)은 데이터 인스턴스 간의 상대적인 관계를 학습하는 데 중점을 둔다. 이 접근법은 2018년 이후 자기지도학습의 주류로 부상하며, 지도 학습과의 성능 격차를 극적으로 줄이는 데 결정적인 역할을 했다.


대조 학습의 근본적인 목표는 잠재 공간(latent space)에서 의미론적으로 유사한 샘플들의 표현은 서로 가깝게 끌어당기고(attract), 유사하지 않은 샘플들의 표현은 서로 밀어내는(repel) 것이다.15 자기지도학습의 맥락에서 '유사한 샘플'은 보통 하나의 원본 데이터에 서로 다른 데이터 증강(augmentation)을 적용하여 생성된 긍정 쌍(positive pair)을 의미한다. 반면, '유사하지 않은 샘플'은 서로 다른 원본 데이터에서 파생된 부정 쌍(negative pair)을 뜻한다. 이 과정을 통해 모델은 데이터 증강으로 인한 피상적인 변화(예: 색상, 크기, 방향)에는 불변하는(invariant) 반면, 각 인스턴스의 고유한 정체성(identity)을 구분할 수 있는 표현을 학습하게 된다. 이는 각 이미지를 고유한 클래스로 취급하는 인스턴스 식별(instance discrimination) 과제로 해석할 수 있다.53

이러한 목표를 수학적으로 구현하기 위해 널리 사용되는 손실 함수가 바로 **InfoNCE(Information Noise Contrastive Estimation)**이다.54 InfoNCE는 앵커(anchor) 샘플 

$q$의 표현이 주어졌을 때, 수많은 부정 키(negative key)들 사이에서 유일한 긍정 키(positive key) $k_{+}$를 올바르게 식별하는 확률을 최대화하는 방식으로 작동한다. 손실 함수는 다음과 같이 정의된다 41:
$$
\mathcal{L}_{q} = -\log \frac{\exp(\text{sim}(q, k_{+}) / \tau)}{\exp(\text{sim}(q, k_{+}) / \tau) + \sum_{i=1}^{K} \exp(\text{sim}(q, k_{i}) / \tau)}
$$
이 수식의 각 구성 요소는 다음과 같은 의미를 가진다.

- $\text{sim}(u, v)$는 두 벡터 $u, v$ 간의 유사도를 측정하는 함수로, 보통 코사인 유사도($u \cdot v / \|u\|\|v\|$)가 사용된다.57

- $k_{+}$는 $q$와 긍정 쌍을 이루는 키의 표현이며, $\{k_i\}_{i=1}^K$는 $K$개의 부정 키들의 표현이다.

- $\tau$는 온도(temperature) 하이퍼파라미터로, 유사도 점수 분포의 집중도를 조절한다.54

  $\tau$ 값이 작을수록 손실 함수는 유사도 차이에 더 민감해져, 모델이 어려운 부정 샘플(hard negatives, 앵커와 유사하지만 다른 샘플)을 더 잘 구별하도록 학습을 유도한다. 반면, 값이 너무 작으면 학습이 불안정해질 수 있다.58

InfoNCE 손실 함수의 형태는 $K+1$개의 항목($k_{+}$와 $K$개의 $k_i$) 중에서 정답인 $k_{+}$를 맞추는 다중 클래스 분류 문제의 교차 엔트로피(cross-entropy) 손실과 동일하다. 즉, 모델은 긍정 쌍의 유사도를 높이고 부정 쌍의 유사도를 낮춤으로써 이 분류 문제를 해결하고, 그 과정에서 유용한 표현을 학습하게 된다.56


SimCLR(A Simple Framework for Contrastive Learning of Visual Representations)는 대조 학습 프레임워크를 극도로 단순화하면서도 SOTA(State-of-the-Art) 성능을 달성하여 학계에 큰 영향을 미친 모델이다.59 SimCLR는 이전 연구들에서 사용되던 메모리 뱅크나 복잡한 아키텍처 없이, 대조 학습의 핵심 요소들을 체계적으로 분석하고 최적화하는 데 집중했다. SimCLR의 성공을 이끈 핵심 요소는 다음과 같다.

1. **데이터 증강의 체계적 조합:** SimCLR 연구는 좋은 표현을 학습하기 위해서는 pretext task, 즉 데이터 증강의 설계가 매우 중요하다는 점을 실험적으로 입증했다. 단일 증강 기법을 사용하는 것보다 여러 증강을 조합할 때 성능이 크게 향상되었으며, 특히 **무작위 자르기(random crop and resize)와 색상 왜곡(color distortion)을 함께 사용하는 것**이 성능 향상에 결정적인 역할을 했다.40 이는 모델이 객체의 일부만 보거나 색상이 변하더라도 동일한 객체로 인식하는 능력을 기르도록 강제한다.

2. **비선형 프로젝션 헤드 (Non-linear Projection Head):** SimCLR는 인코더(예: ResNet)가 출력한 표현 벡터 $h$를 직접 대조 손실 계산에 사용하지 않는다. 대신, $h$를 입력으로 받아 더 낮은 차원의 벡터 $z$를 출력하는 작은 신경망, 즉 프로젝션 헤드 $g(\cdot)$를 추가했다 ($z = g(h)$). 이 프로젝션 헤드는 보통 하나 또는 두 개의 은닉층을 가진 다층 퍼셉트론(MLP)으로 구성된다. 대조 손실은 $z$ 공간에서 계산된다. 이 구조의 중요한 점은, 사전 학습이 끝난 후 **후속 과제를 수행할 때는 프로젝션 헤드 $g$를 버리고, 그 이전의 표현 벡터 $h$를 사용한다**는 것이다.38 이 설계를 통해, 대조 학습 pretext task에 특화된 정보(예: 어떤 증강이 적용되었는지에 대한 단서)는 

   $z$에 집중되도록 하고, 후속 과제에 더 일반적으로 유용한 풍부한 정보(예: 객체의 형태, 질감 등)는 $h$에 보존되도록 유도한다. 이는 $h$가 pretext task에 과적합되는 것을 방지하고 표현의 일반화 성능을 크게 향상시킨다.60

3. **대규모 배치 크기 (Large Batch Size):** SimCLR는 부정 샘플을 별도의 메모리 뱅크 없이 현재 처리 중인 미니배치(mini-batch) 내의 다른 모든 이미지들로부터 얻는다. 예를 들어, 배치 크기가 $N$이라면, 하나의 긍정 쌍에 대해 $2(N-1)$개의 부정 샘플이 존재하게 된다. 효과적인 대조 학습을 위해서는 다양한 부정 샘플을 많이 확보하는 것이 중요한데, SimCLR는 이를 오직 배치 크기를 키우는 방식으로 해결한다. 따라서 SimCLR는 4096이나 8192와 같은 매우 큰 배치 크기를 사용할 때 최상의 성능을 보이며, 이는 상당한 양의 GPU/TPU 메모리와 계산 자원을 요구하는 주요 단점으로 작용한다.38


MoCo(Momentum Contrast)는 SimCLR가 가진 대규모 배치 크기 의존성 문제를 해결하기 위해 제안된 혁신적인 대조 학습 프레임워크다.39 MoCo는 배치 크기와 부정 샘플의 수를 분리하여, 작은 배치 크기로도 수많은 부정 샘플을 활용할 수 있는 방법을 고안했다.

1. **동적 딕셔너리로서의 큐 (Dynamic Dictionary as a Queue):** MoCo는 부정 샘플 키(key)들을 저장하기 위한 '딕셔너리'를 도입한다. 이 딕셔너리는 고정된 크기의 큐(queue) 자료구조로 구현된다.41 매 학습 스텝마다 현재 미니배치에서 인코딩된 키들은 큐에 추가(enqueue)되고, 큐에서 가장 오래된 배치의 키들은 제거(dequeue)된다. 이 방식을 통해, 딕셔너리는 항상 최신의 그리고 다양한 부정 샘플들을 대량으로 유지할 수 있다. 딕셔너리의 크기(예: 65536)는 배치 크기와는 무관하게 설정될 수 있어, SimCLR와 같이 배치 크기를 키우기 위해 막대한 메모리를 사용할 필요가 없다.39

2. **모멘텀 인코더 (Momentum Encoder):** 딕셔너리를 사용할 때 발생하는 중요한 문제는 키들의 일관성(consistency)이다. 딕셔너리 내의 키들은 서로 다른 학습 시점에, 즉 서로 다른 버전의 인코더에 의해 생성되었다. 만약 인코더가 역전파를 통해 빠르게 변한다면, 딕셔너리 내 키들의 표현이 일관되지 않아 학습이 불안정해질 수 있다. MoCo는 이 문제를 해결하기 위해 두 개의 인코더를 사용한다: 쿼리(query)를 인코딩하는 **쿼리 인코더($f_q$)**와 키(key)를 인코딩하는 **키 인코더($f_k$)**. 쿼리 인코더는 일반적인 방식처럼 역전파를 통해 가중치($\theta_q$)가 업데이트된다. 반면, 키 인코더의 가중치($\theta_k$)는 역전파되지 않고, 대신 쿼리 인코더의 가중치를 지수이동평균(exponential moving average) 방식으로 천천히 따라가도록 업데이트된다. 이를 **모멘텀 업데이트**라고 하며, 수식은 다음과 같다 41:
   $$
   \theta_k \leftarrow m\theta_k + (1-m)\theta_q
   $$
   여기서 모멘텀 계수 $m$은 0.999와 같이 1에 매우 가까운 큰 값을 사용하여, 키 인코더가 매우 부드럽고 점진적으로 변하도록 한다. 이 덕분에 딕셔너리 내의 키들은 서로 다른 시점에 생성되었음에도 불구하고 비교적 일관된 표현을 유지할 수 있으며, 이는 안정적인 대조 학습을 가능하게 한다.41

MoCo는 이러한 독창적인 설계를 통해 계산 효율성과 성능이라는 두 마리 토끼를 모두 잡았으며, 이후 MoCo v2, v3로 발전하며 SimCLR와 같은 다른 방법론들의 장점(예: 프로젝션 헤드, 더 강한 데이터 증강)을 흡수하여 지속적으로 성능을 개선했다.56


대조 학습은 큰 성공을 거두었지만, 여전히 몇 가지 한계를 가지고 있다. 대량의 부정 샘플을 필요로 하는 계산적 부담 외에도, '거짓 부정(false negative)' 문제가 존재할 수 있다. 이는 실제로는 같은 클래스에 속하지만(예: 다른 각도에서 찍은 두 마리의 다른 고양이), 학습 과정에서는 부정 쌍으로 취급되어 서로 밀어내도록 학습되는 경우를 말한다. 이러한 문제들을 해결하기 위해, 부정 샘플을 전혀 사용하지 않고 긍정 쌍만으로 학습하는 **비대조 학습(Non-Contrastive Learning)** 방법론이 등장했다.

비대조 학습의 가장 큰 난제는 **표현 붕괴(representation collapse)**를 어떻게 방지하는가이다. 부정 샘플을 밀어내는 힘이 없다면, 모델은 가장 쉬운 해결책, 즉 모든 입력에 대해 동일한 상수 벡터를 출력하는 자명한 해(trivial solution)로 수렴해버릴 수 있다. BYOL과 Barlow Twins는 이 문제를 각기 다른 독창적인 방식으로 해결한다.

- **BYOL (Bootstrap Your Own Latent):** BYOL은 두 개의 신경망, 즉 **온라인(online) 네트워크**와 **타겟(target) 네트워크**를 사용한다.64 학습 과정은 다음과 같다.

  1. 하나의 이미지에 두 가지 다른 증강을 적용하여 뷰 $v$와 $v'$를 만든다.

  2. 온라인 네트워크는 뷰 $v$를 입력받아 예측값 $q_\theta(z_\theta)$를 출력한다.

  3. 타겟 네트워크는 뷰 $v'$를 입력받아 타겟 표현 $z'_\xi$를 출력한다.

  4. 학습 목표는 예측값과 타겟 표현 간의 평균 제곱 오차(MSE)를 최소화하는 것이다. 즉, 온라인 네트워크가 타겟 네트워크의 출력을 예측하도록 학습된다.

     BYOL이 붕괴를 방지하는 핵심 메커니즘은 두 가지다. 첫째, 구조적 비대칭성이다. 온라인 네트워크는 인코더와 프로젝터 외에 추가적으로 **예측기(predictor)**라는 MLP를 하나 더 가지고 있다. 이 예측기는 오직 온라인 네트워크에만 존재하여 두 네트워크의 구조를 비대칭적으로 만든다.64 둘째, 

     **가중치 업데이트의 비대칭성**이다. 타겟 네트워크의 가중치 $\xi$는 역전파를 통해 업데이트되지 않고(stop-gradient), 대신 온라인 네트워크의 가중치 $\theta$의 지수이동평균으로 천천히 업데이트된다 (MoCo의 모멘텀 업데이트와 동일).64 이 두 가지 비대칭적 장치가 결합하여, 온라인 네트워크가 타겟 네트워크의 출력을 단순히 복사하는 자명한 해로 수렴하는 것을 막고 의미 있는 표현을 학습하도록 유도한다.64

- **Barlow Twins:** Barlow Twins는 신경과학자 호레이스 발로(Horace Barlow)가 제안한 **'정보 중복성 감소(redundancy reduction)'** 원리에서 영감을 받았다.66 이 원리는 효율적인 신경 코딩을 위해 감각 신호 내의 통계적 중복성을 최소화해야 한다는 가설이다. Barlow Twins는 이 아이디어를 자기지도학습에 적용하여, 두 증강 뷰에서 얻은 임베딩 벡터들의 

  **교차 상관 행렬(cross-correlation matrix)** $\mathcal{C}$가 단위 행렬(identity matrix)에 가까워지도록 직접적으로 최적화한다.66 손실 함수는 다음과 같이 두 항으로 구성된다 66:
  $$
  \mathcal{L}_{\mathcal{BT}} \triangleq \underbrace{\sum_{i}(1 - \mathcal{C}_{ii})^2}_{\text{invariance term}} + \lambda \underbrace{\sum_{i} \sum_{j \neq i} \mathcal{C}_{ij}^2}_{\text{redundancy reduction term}}
  $$

  - **불변성 항 (Invariance Term):** 교차 상관 행렬의 대각 성분 $\mathcal{C}_{ii}$를 1에 가깝게 만든다. 이는 두 뷰의 임베딩 벡터에서 같은 차원($i$번째)의 특징 값이 서로 유사해지도록(상관관계가 높아지도록) 유도한다. 즉, 증강에 대해 표현이 불변하도록 만든다.

  - 중복성 감소 항 (Redundancy Reduction Term): 비대각 성분 $\mathcal{C}_{ij}$ ($i \neq j$)를 0에 가깝게 만든다. 이는 한 임베딩 벡터 내의 서로 다른 차원($i$번째와 $j$번째)의 특징 값들이 서로 무관하도록(decorrelated) 만든다. 즉, 각 차원이 서로 다른 정보를 담도록 하여 표현의 중복성을 최소화한다.

    이 중복성 감소 항이 자연스럽게 표현 붕괴를 방지하는 역할을 한다. 만약 모델이 모든 입력에 대해 상수 벡터를 출력한다면, 모든 특징 차원들이 완벽하게 상관관계를 가지게 되어 비대각 성분들이 커지므로 높은 페널티를 받게 된다.68

이러한 비대조 학습 방법론들의 발전은 자기지도학습에서 '표현 붕괴' 문제를 해결하는 방식이 어떻게 진화해왔는지를 보여준다. 초기 대조 학습(SimCLR, MoCo)이 부정 샘플이라는 '외부적 반발력'을 사용하여 표현들을 밀어냈다면, BYOL은 예측기와 stop-gradient라는 '구조적 비대칭성'을 통해, Barlow Twins는 표현 벡터 내부의 차원들이 서로 직교하도록 강제하는 '내부적 제약'을 통해 문제를 해결했다. 이는 문제 해결의 초점이 '샘플 간의 관계'에서 '표현의 내부 구조'로 이동했음을 의미하며, 더 우아하고 효율적인 자기지도학습으로의 발전을 상징한다.


| 알고리즘         | 접근 방식    | 핵심 아이디어                                                | 주요 혁신점                                         | 장점/단점                                                    |
| ---------------- | ------------ | ------------------------------------------------------------ | --------------------------------------------------- | ------------------------------------------------------------ |
| **BERT**         | 재구성 (MLM) | 문장의 일부 토큰을 마스킹하고 양방향 문맥으로 예측           | 양방향 Transformer 인코더, MLM Pretext Task         | 강력한 문맥 이해 / 사전-미세조정 불일치                      |
| **MAE**          | 재구성 (MIM) | 이미지 패치의 높은 비율을 마스킹하고 픽셀 단위로 복원        | 비대칭 인코더-디코더, 높은 마스킹 비율              | 높은 사전학습 효율성 / 저수준 픽셀 복원에 치중               |
| **SimCLR**       | 대조 학습    | 동일 이미지의 두 증강 뷰를 긍정 쌍으로, 나머지를 부정 쌍으로 대조 | 데이터 증강 조합, 프로젝션 헤드                     | 개념적 단순함, 높은 성능 / 매우 큰 배치 크기 요구            |
| **MoCo**         | 대조 학습    | 부정 샘플을 위한 동적 딕셔너리(큐)와 모멘텀 인코더 사용      | 큐 기반 딕셔너리, 모멘텀 업데이트                   | 배치 크기 의존성 완화 / 하이퍼파라미터(모멘텀, 큐 크기) 추가 |
| **BYOL**         | 비대조 학습  | 온라인 네트워크가 타겟 네트워크의 표현을 예측                | 예측기(Predictor)를 통한 비대칭 구조, Stop-gradient | 부정 샘플 불필요, 안정적 학습 / 붕괴 방지 메커니즘의 이론적 설명 복잡 |
| **Barlow Twins** | 비대조 학습  | 두 뷰의 임베딩 간 교차 상관 행렬을 단위 행렬로 만듦          | 중복성 감소 원리 기반 손실 함수                     | 부정 샘플, 큰 배치, 비대칭 구조 불필요 / 고차원 임베딩에서 효과적 |


자기지도학습의 진정한 가치는 다양한 실제 문제, 즉 후속 과제(downstream task)에서 얼마나 뛰어난 성능을 보이는가에 의해 증명된다. 지난 몇 년간 SSL 모델들은 컴퓨터 비전, 자연어 처리, 음성 인식 등 여러 분야의 핵심 벤치마크에서 기존의 지도 학습 기반 사전 학습 모델들과의 성능 격차를 빠르게 좁히거나 심지어 뛰어넘으며 그 실용적 가치를 입증해왔다.


컴퓨터 비전 분야는 SSL의 성능이 가장 극적으로 입증된 영역이다. 특히 대조 학습의 등장은 비전 SSL의 성능을 새로운 차원으로 끌어올렸다.

- **ImageNet 이미지 분류:** ImageNet 데이터셋을 사용한 선형 평가(linear evaluation)는 SSL로 학습된 표현의 질을 평가하는 표준적인 벤치마크다. 이 평가는 사전 학습된 인코더의 가중치를 고정(freeze)한 채, 그 위에 간단한 선형 분류기(linear classifier)만을 추가하여 ImageNet 분류 성능을 측정한다. 2020년 발표된 **SimCLR**는 ResNet-50 아키텍처를 사용하여 이 평가에서 **76.5%의 Top-1 정확도**를 달성했는데, 이는 동일한 아키텍처로 ImageNet 레이블을 전부 사용하여 지도 학습 방식으로 사전 학습한 모델의 성능과 정확히 일치하는 수치였다.40 이는 레이블 없이도 지도 학습과 동등한 수준의 표현을 학습할 수 있음을 최초로 보여준 충격적인 결과였다.
- **COCO 객체 탐지 및 분할:** 이미지 분류보다 더 복잡한 후속 과제인 객체 탐지(object detection)와 인스턴스 분할(instance segmentation)에서도 SSL의 우수성이 입증되었다. **MoCo**는 ImageNet으로 사전 학습한 뒤 PASCAL VOC, COCO와 같은 데이터셋에 적용했을 때, 7개의 주요 탐지/분할 후속 과제에서 **지도 학습으로 사전 학습한 모델을 능가하는 성능**을 보였다.41 이는 SSL로 학습된 표현이 분류뿐만 아니라 객체의 위치와 경계에 대한 정보를 더 잘 포착하는, 즉 더 일반적이고 전이 가능한(transferable) 특징을 학습할 수 있음을 시사했다.70
- **대규모 모델의 확장성:** **MAE**는 Vision Transformer(ViT)와 같은 대규모 모델을 효과적으로 사전 학습시킬 수 있는 SSL의 잠재력을 보여주었다. 데이터에 굶주려(data-hungry) 대규모 레이블 데이터가 필수적이라고 여겨졌던 ViT-Huge 모델을 ImageNet-1K라는 비교적 작은 데이터셋만으로 사전 학습했음에도 불구하고, 미세 조정 후 **87.8%라는 SOTA급 Top-1 정확도**를 달성했다.47 이는 SSL이 지도 학습보다 데이터 효율성이 높을 수 있으며, 대규모 모델의 잠재력을 최대한 이끌어내는 데 더 효과적인 패러다임이 될 수 있음을 증명했다.

이러한 결과들은 SSL이 더 이상 지도 학습의 열등한 대안이 아니라, 특정 과제에서는 오히려 더 우수한 일반화 성능을 제공하는 표준 사전 학습 방법론으로 자리매김할 수 있음을 명확히 보여준다.35


자연어 처리(NLP) 분야는 SSL의 원리가 가장 먼저 성공적으로 적용되고, '사전학습-미세조정' 패러다임을 확립한 분야다.

- **초기 성공과 단어 임베딩:** Word2Vec은 SSL의 철학을 담고 있는 초기 성공 사례다.73 중심 단어를 통해 주변 단어를 예측하거나(Skip-gram), 주변 단어를 통해 중심 단어를 예측하는(CBOW) pretext task를 통해, 레이블 없는 대규모 텍스트 코퍼스로부터 단어의 의미를 벡터 공간에 표현하는 단어 임베딩(word embedding)을 학습했다.74 이는 단어의 의미적, 문법적 관계를 벡터 연산으로 포착할 수 있게 하여 NLP 분야에 큰 발전을 가져왔다.
- **BERT의 등장과 패러다임 혁신:** 2018년 등장한 **BERT**는 NLP 분야에 혁명을 일으켰다. 마스크 언어 모델링(MLM)과 다음 문장 예측(NSP)이라는 두 가지 독창적인 SSL 과제를 통해 방대한 텍스트 데이터로 사전 학습된 BERT는, 이후 다양한 후속 과제에 미세 조정되는 방식으로 전례 없는 성능을 보여주었다. 특히, 자연어 이해 능력을 종합적으로 평가하는 **GLUE(General Language Understanding Evaluation) 벤치마크**에서 BERT-Large 모델은 **평균 80.5점**을 기록하며 기존의 모든 모델을 압도하고 새로운 SOTA를 달성했다.76 이는 당시 SOTA 모델들의 점수가 70점대 초반에 머물러 있던 것을 고려하면 엄청난 도약이었다. 또한, 질의응답 벤치마크인 **SQuAD(Stanford Question Answering Dataset)**에서도 인간의 성능에 근접하는 결과를 보여주었다.77

BERT의 성공은 NLP 연구의 패러다임을 완전히 바꾸어 놓았다. 개별 과제를 위해 처음부터 모델을 설계하고 학습시키는 대신, 대규모 비레이블 데이터로 SSL 사전 학습을 수행한 거대 언어 모델을 기반으로, 소량의 과제별 데이터로 미세 조정하는 방식이 표준으로 자리 잡았다. 이후 RoBERTa, ALBERT, DeBERTa 등 수많은 후속 모델들이 더 정교한 SSL 기법과 더 많은 데이터, 더 큰 모델 크기를 통해 GLUE와 SuperGLUE 벤치마크에서 지속적으로 성능을 경신하며, 현재는 여러 과제에서 인간의 평균 성능을 뛰어넘는 수준에 도달했다.79


음성 인식(Speech Recognition) 및 처리 분야 역시 SSL을 통해 큰 발전을 이루었다. 음성 데이터는 비디오만큼이나 레이블링 비용이 비싸고, 방대한 양의 비레이블 데이터가 존재하기 때문에 SSL이 적용되기에 이상적인 환경이다.

- **Wav2Vec 2.0:** 이 모델은 원시 오디오 파형(raw audio waveform)을 직접 입력으로 받아, 잠재 공간 표현의 일부를 마스킹한 뒤, 이를 대조 학습(contrastive learning)을 통해 예측하는 pretext task를 사용한다.82 즉, 마스킹된 위치의 올바른 양자화된(quantized) 표현을 여러 개의 오답(distractors) 중에서 구별하도록 학습한다.
- **HuBERT (Hidden Unit BERT):** HuBERT는 Wav2Vec 2.0에서 한 단계 더 나아가, BERT의 MLM과 유사한 방식을 음성 데이터에 적용했다.82 먼저, K-평균 군집화와 같은 오프라인 클러스터링 알고리즘을 사용하여 음성 신호를 이산적인 음향 단위(acoustic unit) 또는 '숨겨진 유닛(hidden unit)'으로 변환한다. 그런 다음, 입력 파형의 일부를 마스킹하고, 모델이 마스킹된 부분에 해당하는 숨겨진 유닛의 레이블을 예측하도록 학습한다.

이 두 모델은 Libri-Light와 같은 대규모 비레이블 음성 데이터셋(수만 시간 분량)으로 사전 학습한 뒤, LibriSpeech와 같은 표준 음성 인식 벤치마크의 소량 레이블 데이터(예: 10시간, 100시간)로 미세 조정했을 때, 기존의 지도 학습 기반 SOTA 모델들을 압도적인 성능 차이로 능가했다. 예를 들어, LibriSpeech의 가장 어려운 테스트셋인 'test-other'에서 **HuBERT는 단어 오류율(Word Error Rate, WER) 2.9%**를 기록했으며, 이는 이전 SOTA 모델들의 4~5%대 WER을 크게 개선한 것이다.84 이러한 강력한 표현력은 음성 감정 인식(Speech Emotion Recognition), 화자 인증(Speaker Verification) 등 다른 음성 관련 후속 과제에서도 SOTA 성능으로 이어지며 SSL의 범용성을 입증했다.83


현대 AI의 가장 중요한 흐름인 파운데이션 모델(Foundation Models)은 그 근간을 자기지도학습에 두고 있다. 이 모델들은 특정 과제가 아닌, 방대한 데이터로부터 세상에 대한 일반적인 표현을 학습하는 것을 목표로 하며, SSL은 이를 가능하게 하는 핵심 기술이다.

- **GPT 계열 (Generative Pre-trained Transformer):** GPT-3, GPT-4와 같은 대규모 언어 모델(LLM)은 가장 단순하면서도 강력한 SSL 과제 중 하나인 **'다음 단어 예측(next token prediction)'**을 통해 학습된다.8 모델은 인터넷, 서적 등에서 수집한 방대한 텍스트 코퍼스를 순차적으로 읽으며, 주어진 문맥 다음에 나올 가장 확률이 높은 단어를 예측하는 과정을 반복한다.87 이 자기회귀적(autoregressive) 학습 과정을 통해, 모델은 단순히 단어의 순서뿐만 아니라 문법, 사실 지식, 상식, 추론 능력 등 복잡하고 추상적인 언어 능력을 내재적으로 학습하게 된다.88

- **CLIP (Contrastive Language-Image Pre-training):** CLIP은 시각과 언어라는 두 가지 다른 모달리티(modality)를 연결하는 다중모드(multimodal) SSL의 대표적인 성공 사례다. CLIP은 웹에서 수집한 수억 개의 (이미지, 텍스트 캡션) 쌍 데이터를 활용한다.91 학습 방식은 대조 학습에 기반한다. 하나의 배치 내에서, 주어진 이미지와 그에 해당하는 텍스트 캡션은 긍정 쌍이 되고, 나머지 모든 텍스트 캡션들은 부정 쌍이 된다. 모델은 이미지 인코더와 텍스트 인코더를 각각 학습시켜, 긍정 쌍의 임베딩은 코사인 유사도가 높아지도록, 부정 쌍의 임베딩은 낮아지도록 최적화한다.12 이 과정을 통해 CLIP은 이미지와 텍스트를 동일한 의미론적 잠재 공간에 매핑하는 강력한 표현을 학습한다. 그 결과, 전혀 학습하지 않은 이미지 분류 과제에 대해서도 "a photo of a [class name]"과 같은 자연어 프롬프트를 이용해 높은 정확도를 보이는 놀라운 

  **제로샷(zero-shot) 일반화 능력**을 보여주었다.91

- **DALL-E:** DALL-E는 텍스트 설명으로부터 이미지를 생성하는 모델로, SSL의 생성적 측면을 극대화한 예시다. 초기 DALL-E는 GPT-3와 유사한 Transformer 아키텍처를 기반으로, 텍스트 토큰과 이미지 토큰(이미지를 이산적인 코드로 변환)을 하나의 시퀀스로 이어 붙여, 자기회귀적으로 다음 토큰을 예측하도록 학습되었다.91 이는 텍스트와 이미지 간의 복잡한 조건부 분포를 모델링하는 SSL의 한 형태로, 모델이 언어적 개념을 시각적 픽셀로 변환하는 능력을 학습하게 한다.

이처럼 현대 AI를 대표하는 파운데이션 모델들은 모두 그 핵심에 자기지도학습이라는 강력한 학습 엔진을 탑재하고 있으며, 이를 통해 전례 없는 수준의 일반화 능력과 생성 능력을 보여주고 있다.


| 벤치마크                     | 과제                     | 모델                 | 사전 학습 방식             | 성능               | 지도 학습 기준 모델 | 성능               | 출처 |
| ---------------------------- | ------------------------ | -------------------- | -------------------------- | ------------------ | ------------------- | ------------------ | ---- |
| **ImageNet**                 | 이미지 분류 (Top-1 Acc.) | SimCLR (ResNet-50)   | **자기지도 (대조)**        | 76.5%              | ResNet-50           | 76.5%              | 40   |
| **ImageNet**                 | 이미지 분류 (Top-1 Acc.) | MAE (ViT-Huge)       | **자기지도 (재구성)**      | 87.8%              | ViT-Huge            | (비교 데이터 필요) | 47   |
| **COCO**                     | 객체 탐지 (AP)           | MoCo (ResNet-50)     | **자기지도 (대조)**        | **지도 학습 능가** | ResNet-50           | (베이스라인)       | 41   |
| **GLUE**                     | 자연어 이해 (Avg. Score) | BERT-Large           | **자기지도 (MLM)**         | 80.5               | (Pre-BERT SOTA)     | ~70                | 77   |
| **GLUE**                     | 자연어 이해 (Avg. Score) | DeBERTaV3-Large      | **자기지도 (MLM 변형)**    | 91.4               | (인간 성능)         | 87.1               | 77   |
| **LibriSpeech (test-clean)** | 음성 인식 (WER)          | HuBERT (Libri-Light) | **자기지도 (마스크 예측)** | 1.8                | (기존 지도 학습)    | > 2.0              | 95   |
| **LibriSpeech (test-other)** | 음성 인식 (WER)          | HuBERT (Libri-Light) | **자기지도 (마스크 예측)** | 2.9                | (기존 지도 학습)    | > 4.0              | 84   |


자기지도학습은 의심할 여지 없이 AI 분야에 혁신을 가져왔지만, 만능 해결책은 아니다. 이 패러다임이 성숙해감에 따라, 그 이면에 존재하는 여러 도전 과제와 근본적인 한계들이 명확해지고 있다. 이러한 문제들을 깊이 있게 이해하는 것은 SSL의 현재를 정확히 진단하고 미래 발전 방향을 모색하는 데 필수적이다.


SSL의 눈부신 성공은 대규모 신경망 모델과 수십억, 수백억 개의 샘플로 이루어진 방대한 데이터셋을 전제로 한다.96 이는 필연적으로 막대한 계산 자원(computational resources)의 소모를 동반한다.

예를 들어, SimCLR와 같은 대조 학습 모델은 효과적인 부정 샘플을 확보하기 위해 4096개 이상의 매우 큰 배치 크기를 요구하는데, 이는 여러 개의 고성능 GPU나 TPU를 동시에, 그리고 장시간 사용해야만 감당할 수 있는 규모다.98 BERT-Base 모델을 처음부터 사전 학습하는 데에도 과거에는 16개의 TPU 칩으로 4일이 소요되었으며, 이는 상당한 금전적 비용을 의미한다.81 GPT-3와 같은 초거대 모델의 학습 비용은 수백만 달러에 달하는 것으로 알려져 있다.96

이러한 막대한 자원 요구는 심각한 진입 장벽으로 작용한다. 자본과 인프라가 풍부한 소수의 거대 기술 기업이나 최상위 연구 기관만이 최첨단 SSL 모델 개발을 주도하게 되면서, 학계나 중소 규모의 연구 그룹에서는 이들의 연구를 재현하거나 새로운 아이디어를 검증하기조차 어려운 상황이 발생하고 있다.10 이는 연구의 다양성을 저해하고 기술 발전의 방향이 소수에 의해 결정될 수 있다는 우려를 낳는다. 따라서 계산 효율성을 높이고, 더 적은 자원으로도 효과적인 SSL 모델을 학습할 수 있는 방법을 개발하는 것은 이 분야의 지속 가능한 발전을 위한 핵심 과제 중 하나다.


모델이 학습 데이터와 다른 분포를 가진 새로운 데이터, 즉 분포 외(Out-of-Distribution, OOD) 데이터에 대해서도 얼마나 잘 작동하는지는 AI 시스템의 신뢰성과 안전성에 직결되는 중요한 문제다. SSL은 이 문제에 대해 양면적인 특성을 보인다.

긍정적인 측면에서, SSL은 지도 학습에 비해 데이터 불균형(class imbalance) 문제에 더 강건한(robust) 경향을 보인다.100 지도 학습 모델은 레이블이 많은 다수 클래스(majority class)의 특징에 과적합되기 쉬운 반면, SSL은 레이블에 의해 학습이 직접적으로 편향되지 않는다. 따라서 다수 클래스 데이터로부터 레이블과 직접적인 관련은 없지만 다른 과제에 유용하게 전이될 수 있는 풍부하고 다양한 특징들을 학습할 수 있다. 이렇게 학습된 일반적인 특징들은 데이터가 거의 없는 소수 클래스(minority class)를 분류하는 데 도움을 주어, 전체적인 성능 저하를 완화한다.100

그러나 부정적인 측면도 존재한다. SSL 모델은 레이블이 없는 데이터로부터 패턴을 학습하는 과정에서, 데이터에 존재하는 피상적인 상관관계(spurious correlation)를 본질적인 특징으로 오인하여 학습할 위험이 있다.101 예를 들어, 특정 배경에서만 자주 나타나는 객체를 학습한 모델은, 배경이 달라지는 OOD 상황에서는 해당 객체를 제대로 인식하지 못할 수 있다. 이는 모델의 일반화 성능을 심각하게 저해하는 요인이 된다. 이러한 문제를 해결하기 위해, 최근에는 인과관계 추론(causal inference)의 개념을 도입하여 데이터 생성 과정의 근본적인 인과 구조를 모델링하려는 연구가 활발히 진행되고 있다.102 이 접근법은 피상적인 상관관계가 아닌, 환경 변화에도 변하지 않는 불변 특징(invariant features)을 학습함으로써 OOD 일반화 성능을 근본적으로 향상시키는 것을 목표로 한다.


SSL의 가장 큰 기대효과 중 하나는 레이블 효율성(label efficiency)을 높여, 소량의 레이블 데이터만으로도 높은 성능을 달성하는 것이다. 이는 특히 레이블 확보가 극도로 어려운 소량 데이터 학습(Few-Shot Learning, FSL) 시나리오에서 큰 잠재력을 가진다.

여러 연구에 따르면, 기존의 FSL 방법론에 SSL을 보조 과제(auxiliary task)로 추가할 경우, 지도 신호가 부족한 상황에서 모델이 데이터 자체로부터 추가적인 의미 정보를 학습하게 되어 FSL 성능이 크게 향상되는 것으로 나타났다.104 한 연구에서는 SSL을 통해 FSL의 상대적 오류율이 4%에서 27%까지 감소했으며, 특히 학습에 사용할 수 있는 레이블 데이터가 적거나(예: 1-shot learning), 과제 자체가 더 어려울수록 SSL의 기여도가 더 커지는 경향을 보였다.105 이는 SSL이 부족한 지도 신호를 보완하는 효과적인 수단임을 보여준다.

하지만 여기에는 중요한 전제 조건이 따른다. SSL 사전 학습에 사용되는 비레이블 데이터의 도메인과, FSL 후속 과제의 데이터 도메인이 일치하거나 유사해야 한다는 점이다. 만약 두 도메인 간의 분포 차이가 클 경우(예: 자연 이미지로 SSL 사전 학습 후, 의료 영상 FSL 과제에 적용), SSL로 학습된 표현이 오히려 후속 과제에 방해가 되어 성능이 저하될 수 있다.104 이는 무분별한 비레이블 데이터의 활용이 항상 긍정적인 결과를 가져오는 것은 아니며, 후속 과제와의 관련성을 고려한 데이터 선택 전략이 중요함을 시사한다.


SSL이 직면한 가장 심각하고 시급한 과제는 바로 사회적 편향(social bias)과 관련된 윤리적 문제다. 대부분의 대규모 SSL 모델은 정제되지 않은 방대한 인터넷 웹 데이터(예: Reddit 댓글, 위키피디아, 뉴스 기사)를 학습 데이터로 사용한다.106 이 데이터에는 인종, 성별, 국적, 종교, 성적 지향 등에 대한 우리 사회의 뿌리 깊은 편견, 고정관념, 혐오 표현이 그대로 녹아 있다.

모델은 이러한 편향된 데이터를 무비판적으로 학습하여, 그 편견을 그대로 재현하거나 심지어 증폭시킬 수 있다.108 실제로 AI 기반 채용 도구가 여성 지원자를 차별하거나, 이미지 캡셔닝 모델이 특정 인종에 대해 부정적인 설명을 생성하는 등 수많은 사례가 보고되었다.106 한 연구에서는 저명한 이미지-언어 모델인 CLIP이 흑인 이미지를 '고릴라', '침팬지' 등 비인간으로 잘못 분류하는 비율이 다른 인종에 비해 월등히 높게 나타나 심각한 우려를 낳았다.106

이 문제는 SSL의 본질적인 특성 때문에 더욱 해결하기 어렵다. SSL의 가장 큰 장점인 '인간의 개입 없는 대규모 학습'은 동시에 가장 큰 윤리적 아킬레스건이 되는 '감독의 역설(Paradox of Unsupervision)'을 만들어낸다. 지도 학습에서는 데이터 레이블링 과정에서 인간이 데이터를 검토하고 편향을 인지하거나 완화할 수 있는 최소한의 기회가 존재한다. 그러나 SSL은 확장성을 위해 바로 이 인간의 감독 과정을 의도적으로 배제한다. 그 결과, 데이터에 내재된 사회적 편견이 아무런 필터링 없이 대규모로, 그리고 자동화된 방식으로 모델에 주입된다. 즉, '감독'의 부재가 확장성(scalability)을 가져다주었지만, 동시에 책임성(accountability)의 부재를 낳은 것이다. 따라서 미래 SSL 연구의 핵심 과제는 확장성을 유지하면서도, 데이터 수집 단계에서의 필터링, pretext task 설계의 공정성 고려, 학습된 표현 공간의 편향성 분석 및 완화 등 학습의 전 과정에 걸쳐 '윤리적 감독'을 내재화하는 방법을 찾는 것이 될 것이다.


자기지도학습은 지난 몇 년간 괄목할 만한 경험적 성공을 거두며 AI 분야의 핵심 패러다임으로 자리 잡았다. 그러나 여전히 많은 부분이 '어떻게' 작동하는지에 대한 직관과 실험에 의존하고 있으며, '왜' 작동하는지에 대한 근본적인 이해는 부족한 실정이다. SSL의 지속 가능한 발전을 위해서는 견고한 이론적 토대를 구축하고, 현재의 한계를 극복하기 위한 차세대 연구 방향을 모색하는 것이 필수적이다.


현재 SSL의 성공은 대부분 경험적(empirical) 성과에 기반하고 있으며, 왜 특정 pretext task, 데이터 증강, 또는 모델 아키텍처가 더 나은 성능을 보이는지에 대한 이론적 설명은 아직 초기 단계에 머물러 있다.110 이러한 이론적 공백을 메우기 위해 여러 이론적 프레임워크가 제안되고 있으며, 그중 정보 이론(information theory)과 인과관계 추론(causal inference)이 가장 유망한 접근법으로 주목받고 있다.

- **정보 이론적 관점:** 이 접근법은 SSL을 정보량의 관점에서 분석한다. 대표적인 예로 정보 병목(Information Bottleneck) 원리를 적용하는 것이다.113 이 관점에서 좋은 표현 

  $Z$는 원본 데이터 $X$의 다른 뷰(view) $X'$에 대한 예측에 필요한 정보는 최대한 보존하면서($I(Z; X')$ 최대화), 뷰에만 특화된 불필요한 정보는 최대한 압축($I(Z; X)$ 최소화)해야 한다. 즉, 불변성(invariance)과 충분성(sufficiency) 사이의 최적의 균형을 찾는 과정으로 SSL을 해석할 수 있다.115 또한, Wang & Isola는 널리 사용되는 대조 손실(InfoNCE)을 두 가지 핵심 요소, 즉 긍정 쌍의 표현을 일치시키는 **정렬(alignment)**과 표현 벡터들이 잠재 공간에 고르게 분포되도록 하는 **균일성(uniformity)**으로 분해하여 분석했다.116 이러한 이론적 분석은 새로운 손실 함수를 설계하고 SSL의 작동 원리를 더 깊이 이해하는 데 중요한 통찰을 제공한다.

- **인과관계 추론적 관점:** SSL 모델이 분포 외(OOD) 데이터에 취약한 이유는 학습 데이터에 존재하는 피상적인 상관관계(spurious correlation)를 학습하기 때문이다. 인과관계 추론은 이러한 상관관계를 넘어 데이터 생성 과정의 근본적인 인과 구조를 파악하고, 환경 변화에도 변하지 않는 불변 특징(invariant features)을 학습하는 것을 목표로 한다.101 예를 들어, '눈 배경'과 '늑대' 사이의 강한 상관관계를 학습하는 대신, 배경과 무관하게 늑대를 인식할 수 있는 본질적인 특징을 학습하도록 유도하는 것이다. 이는 SSL 모델의 견고성과 일반화 성능을 한 단계 끌어올릴 수 있는 핵심적인 연구 방향으로, 보다 신뢰할 수 있는 AI 시스템 구축에 필수적이다.


현재의 SOTA SSL 모델들은 막대한 양의 데이터와 계산 자원을 전제로 하고 있다. 이는 SSL의 광범위한 적용을 가로막는 주요 장벽이므로, 미래 연구의 중요한 축은 효율성을 극대화하는 것이다.

- **데이터 효율성 (Data Efficiency):** 더 적은 양의 비레이블 데이터로도 높은 성능의 표현을 학습하는 능력이다. 이는 특히 데이터 수집 자체가 어려운 전문 분야(예: 의료, 과학)나 저자원 언어(low-resource language) 환경에서 SSL을 적용하는 데 매우 중요하다.117 데이터 증강 기법을 최적화하거나, 더 정보량이 많은 pretext task를 설계하거나, 소량의 데이터에서 핵심 구조를 효과적으로 추출하는 새로운 학습 알고리즘을 개발하는 연구가 필요하다.
- **계산 효율성 (Computational Efficiency):** 더 적은 GPU 시간과 메모리를 사용하여 모델을 학습시키는 것을 목표로 한다. MoCo가 큐를 이용해 배치 크기 의존성을 줄인 것이나, MAE가 비대칭 구조로 인코더의 계산량을 획기적으로 줄인 것이 대표적인 예다. Barlow Twins나 BYOL과 같은 비대조 학습 방법론은 대규모 부정 샘플 계산이 필요 없어 계산적으로 더 효율적인 대안을 제시한다.120 앞으로 모델 아키텍처 최적화, 경량화된 pretext task 개발, 효율적인 최적화 알고리즘 연구 등을 통해 SSL의 계산 비용을 낮추는 노력이 계속될 것이다.


인간은 시각, 청각, 언어 등 여러 감각(modality)을 통합하여 세상을 입체적으로 이해한다. 현재 대부분의 SSL 연구는 단일 모달리티(unimodal) 데이터에 집중되어 있지만, AI가 인간 수준의 이해에 도달하기 위해서는 다중모드(multimodal) 학습으로의 확장이 필연적이다.121

다중모드 SSL은 이미지-텍스트, 비디오-오디오, 음성-텍스트 등 여러 모달리티가 짝을 이룬 데이터를 활용한다. 이 접근법의 가장 큰 장점은 **한 모달리티가 다른 모달리티에게 자연스러운 감독 신호를 제공**한다는 점이다. 예를 들어, 비디오의 시각적 프레임과 그에 해당하는 오디오는 서로를 예측하는 pretext task의 입력과 정답이 될 수 있다.

**CLIP**은 이미지와 텍스트 캡션 간의 관계를 대조 학습으로 학습하여 다중모드 SSL의 가능성을 폭발적으로 보여준 대표적인 성공 사례다.123 최근에는 비디오, 오디오, 텍스트를 하나의 Transformer 모델로 동시에 처리하여 모달리티 간의 복잡한 상호 관계를 학습하는 **VATT(Video-Audio-Text Transformer)**와 같은 모델도 등장했다.123 이러한 다중모드 SSL은 각 모달리티가 가진 정보를 상호 보완적으로 활용하여, 단일 모달리티만으로는 학습하기 어려운 더 풍부하고, 추상적이며, 잘 정렬된(well-aligned) 표현을 학습할 수 있게 한다.121 이는 향후 더 정교한 의미 이해와 생성 능력을 갖춘 AI 모델 개발의 핵심적인 방향이 될 것이다.


자기지도학습은 지난 10년간 기계학습 분야에서 가장 중요한 패러다임 전환 중 하나를 이끌었다. 레이블 데이터의 고질적인 병목 현상을 해결하고, 세상에 존재하는 무한에 가까운 비정형 데이터를 AI 모델의 귀중한 학습 자원으로 전환시킴으로써, 인공지능의 규모와 능력을 전례 없는 수준으로 끌어올렸다. BERT, GPT, SimCLR, MAE와 같은 선구적인 모델들은 SSL이 특정 과제에서 지도 학습을 능가할 수 있음을 증명했고, 이는 파운데이션 모델의 등장을 촉발하며 AI 개발의 중심축을 '개별 과제 최적화'에서 '범용 표현 학습 후 적응'으로 완전히 바꾸어 놓았다.

앞으로 자기지도학습은 견고한 이론적 토대를 마련하고, 데이터 및 계산 효율성을 극대화하며, 인간의 인식과 유사한 다중모드 학습으로 확장해 나가는 방향으로 발전할 것이다. 이러한 발전은 단순히 더 정확한 모델을 만드는 것을 넘어, AI가 특정 작업에 특화된 '도구'의 역할을 넘어, 세상에 대한 보다 근본적이고 종합적인 이해를 갖춘 '지능'으로 진화하는 데 결정적인 기여를 할 것이다.

궁극적으로 자기지도학습은 인공 일반 지능(Artificial General Intelligence, AGI)으로 가는 가장 유력한 경로 중 하나를 제시한다. 인간의 아기가 명시적인 정답 없이 세상을 관찰하고, 예측하고, 상호작용하며 스스로 학습하는 것처럼 1, SSL은 AI가 데이터의 내재적 구조로부터 세상이 작동하는 방식에 대한 '상식(common sense)'을 스스로 구축하게 하는 메커니즘을 제공한다. 이는 수십억 개의 레이블된 예제를 암기하는 방식으로는 도달하기 어려운, 보다 근본적인 형태의 학습이다. 미래의 진정한 AI는 레이블이라는 제한된 창을 통해 세상을 보는 시스템이 아니라, SSL을 통해 세상의 방대한 관찰 데이터로부터 스스로 규칙과 표현을 터득한 시스템일 가능성이 높다. 따라서 자기지도학습 연구의 진전은 곧 인공지능의 미래를 가늠하는 바로미터가 될 것이며, 이 패러다임은 앞으로 다가올 AI 시대의 핵심 엔진으로서 그 역할을 계속해 나갈 것이다.


1. Self-supervised Learning: A Succinct Review - PMC - PubMed Central, 8월 24, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC9857922/
2. What's the difference between supervised and unsupervised machine learning - AWS, 8월 24, 2025에 액세스, https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/
3. 지도 학습이란 무엇인가요? - IBM, 8월 24, 2025에 액세스, https://www.ibm.com/kr-ko/think/topics/supervised-learning
4. 지도 학습과 비지도 학습 비교 - 기계 학습 알고리즘 간의 차이점 - AWS, 8월 24, 2025에 액세스, https://aws.amazon.com/ko/compare/the-difference-between-machine-learning-supervised-and-unsupervised/
5. Self-Supervised Learning (SSL) - GeeksforGeeks, 8월 24, 2025에 액세스, https://www.geeksforgeeks.org/machine-learning/self-supervised-learning-ssl/
6. Self-Supervised Learning and Its Applications - neptune.ai, 8월 24, 2025에 액세스, https://neptune.ai/blog/self-supervised-learning
7. A survey of the impact of self-supervised pretraining for diagnostic tasks in medical X-ray, CT, MRI, and ultrasound - PubMed Central, 8월 24, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10998380/
8. What Is Self-Supervised Learning? - IBM, 8월 24, 2025에 액세스, https://www.ibm.com/think/topics/self-supervised-learning
9. 비지도학습 - 나무위키, 8월 24, 2025에 액세스, [https://namu.wiki/w/%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5](https://namu.wiki/w/비지도학습)
10. Self-Supervised Learning (SSL): Future of Scalable, Multimodal, and AI - E-SPIN Group, 8월 24, 2025에 액세스, https://www.e-spincorp.com/self-supervised-learning-ai-future/
11. Self-Supervised Learning Algorithms - Meegle, 8월 24, 2025에 액세스, https://www.meegle.com/en_us/topics/algorithm/self-supervised-learning-algorithms
12. Self-supervised learning - Wikipedia, 8월 24, 2025에 액세스, https://en.wikipedia.org/wiki/Self-supervised_learning
13. www.kisdi.re.kr, 8월 24, 2025에 액세스, [https://www.kisdi.re.kr/report/view.do?key=m2101113025377&masterId=4333446&arrMasterId=4333446&artId=554256#:~:text=%EC%9E%90%EA%B8%B0%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5%EC%9D%B4%EB%9E%80%20%EB%8D%B0%EC%9D%B4%ED%84%B0,%EC%9D%B4%EC%9A%A9%ED%95%98%EB%8A%94%20%EB%B0%A9%EB%B2%95%EC%9D%84%20%EC%9D%98%EB%AF%B8%ED%95%9C%EB%8B%A4.](https://www.kisdi.re.kr/report/view.do?key=m2101113025377&masterId=4333446&arrMasterId=4333446&artId=554256#:~:text=자기지도학습이란 데이터,이용하는 방법을 의미한다.)
14. Breaking Down Self-Supervised Learning: Concepts, Comparisons, and Examples - Wandb, 8월 24, 2025에 액세스, https://wandb.ai/mostafaibrahim17/ml-articles/reports/Breaking-Down-Self-Supervised-Learning-Concepts-Comparisons-and-Examples--Vmlldzo2MzgwNjIx
15. Self-Supervised Learning: Definition, Tutorial & Examples - V7 Labs, 8월 24, 2025에 액세스, https://www.v7labs.com/blog/self-supervised-learning-guide
16. 머신러닝 기초 (지도, 비지도, 자기지도), 8월 24, 2025에 액세스, https://dodghek.tistory.com/90
17. Self-Supervised Learning (자기지도학습), 8월 24, 2025에 액세스, [https://www.kim2kie.com/res/html/0_formula/00%20AI/Self-Supervised.html](https://www.kim2kie.com/res/html/0_formula/00 AI/Self-Supervised.html)
18. Survey on Self-Supervised Learning: Auxiliary Pretext Tasks and Contrastive Learning Methods in Imaging - PMC - PubMed Central, 8월 24, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC9029566/
19. 자기 지도 학습(Self-Supervised Learning) - 아는 것의 미학 - 티스토리, 8월 24, 2025에 액세스, https://applepy.tistory.com/103
20. 자기 지도 학습이란 무엇인가요? - IBM, 8월 24, 2025에 액세스, https://www.ibm.com/kr-ko/think/topics/self-supervised-learning
21. [LLM/개념] 자기지도 학습(Self-Supervised Learning) 이란? - 내일은분석왕 - 티스토리, 8월 24, 2025에 액세스, https://datascience-hyemin.tistory.com/101
22. Self-Supervised Learning (SSL) Overview - Towards Data Science, 8월 24, 2025에 액세스, https://towardsdatascience.com/self-supervised-learning-ssl-overview-8a7f24740e40/
23. [D] Help me understand self-supervised learning : r/MachineLearning - Reddit, 8월 24, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/q0cex6/d_help_me_understand_selfsupervised_learning/
24. How is self-supervised learning different from unsupervised learning? - Milvus, 8월 24, 2025에 액세스, https://milvus.io/ai-quick-reference/how-is-selfsupervised-learning-different-from-unsupervised-learning
25. What is the difference between self-supervised and unsupervised learning?, 8월 24, 2025에 액세스, https://ai.stackexchange.com/questions/40341/what-is-the-difference-between-self-supervised-and-unsupervised-learning
26. What Is Semi-Supervised Learning? - IBM, 8월 24, 2025에 액세스, https://www.ibm.com/think/topics/semi-supervised-learning
27. What is the relation between semi-supervised and self-supervised visual representation learning? - Artificial Intelligence Stack Exchange, 8월 24, 2025에 액세스, https://ai.stackexchange.com/questions/12266/what-is-the-relation-between-semi-supervised-and-self-supervised-visual-represen
28. www.ibm.com, 8월 24, 2025에 액세스, [https://www.ibm.com/think/topics/semi-supervised-learning#:~:text=Semi%2Dsupervised%20learning%20vs%20self,like%20autoencoders%20are%20truly%20unsupervised.](https://www.ibm.com/think/topics/semi-supervised-learning#:~:text=Semi-supervised learning vs self,like autoencoders are truly unsupervised.)
29. What the differences between self-supervised/semi-supervised in NLP?, 8월 24, 2025에 액세스, https://datascience.stackexchange.com/questions/94951/what-the-differences-between-self-supervised-semi-supervised-in-nlp
30. Autoencoders 101: Decoding the Power of Self-Supervised Learning | by Jim Canary, 8월 24, 2025에 액세스, https://medium.com/@jimcanary/autoencoders-101-decoding-the-power-of-self-supervised-learning-356ee59f3db8
31. The Basic Concept of Autoencoder — The Self-supervised Deep Learning - Medium, 8월 24, 2025에 액세스, https://medium.com/@soumallya160/the-basic-concept-of-autoencoder-the-self-supervised-deep-learning-454e75d93a04
32. Autoencoders and Self-supervised Learning - COMP6248 Differentiable Programming (and Deep Learning) - University of Southampton, 8월 24, 2025에 액세스, http://comp6248.ecs.soton.ac.uk/handouts/autoencoders-handouts.pdf
33. How to differentiate Auto Encoder techniques from Self Supervised Learning?, 8월 24, 2025에 액세스, https://stats.stackexchange.com/questions/434224/how-to-differentiate-auto-encoder-techniques-from-self-supervised-learning
34. COMP 451 – Fundamentals of Machine Learning Lecture 25 --- Autoencoders and self-supervision - McGill University, 8월 24, 2025에 액세스, https://cs.mcgill.ca/~wlh/comp451/files/lecture_25_slides.pdf
35. Self-Supervised Learning of Pretext-Invariant Representations - CVF Open Access, 8월 24, 2025에 액세스, https://openaccess.thecvf.com/content_CVPR_2020/papers/Misra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.pdf
36. Pretext Tasks Selection for Multitask Self-Supervised Audio Representation Learning - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2107.00594
37. A Survey on Contrastive Self-Supervised Learning - MDPI, 8월 24, 2025에 액세스, https://www.mdpi.com/2227-7080/9/1/2
38. The Illustrated SimCLR Framework - Amit Chaudhary, 8월 24, 2025에 액세스, https://amitness.com/posts/simclr
39. Easily Explained: Momentum Contrast for Unsupervised Visual Representation Learning | by Hey Amit | Medium, 8월 24, 2025에 액세스, https://medium.com/@heyamit10/easily-explained-momentum-contrast-for-unsupervised-visual-representation-learning-10bf51e08fb1
40. SimCLR Explained: The ELI5 Guide for Engineers - Lightly, 8월 24, 2025에 액세스, https://www.lightly.ai/blog/simclr
41. Momentum Contrast for Unsupervised Visual ... - CVF Open Access, 8월 24, 2025에 액세스, https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf
42. What are masked language models? | IBM, 8월 24, 2025에 액세스, https://www.ibm.com/think/topics/masked-language-model
43. BERT (language model) - Wikipedia, 8월 24, 2025에 액세스, https://en.wikipedia.org/wiki/BERT_(language_model)
44. Understanding NLP Algorithms: The Masked Language Model - Coursera, 8월 24, 2025에 액세스, https://www.coursera.org/articles/masked-language-model
45. Mastering Masked Language Models: Techniques, Comparisons, and Best Practices | by Atharv Yeolekar | Medium, 8월 24, 2025에 액세스, https://medium.com/@atharv6f_47401/mastering-masked-language-models-techniques-comparisons-and-best-practices-33cf061a1693
46. natural language - BERT masking scheme - Cross Validated, 8월 24, 2025에 액세스, https://stats.stackexchange.com/questions/464201/bert-masking-scheme
47. Masked Autoencoders Are Scalable Vision ... - CVF Open Access, 8월 24, 2025에 액세스, https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf
48. How to Understand Masked Autoencoders - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2202.03670
49. Understanding Masked Autoencoders From a Local Contrastive Perspective - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2310.01994v2
50. Masked autoencoder (MAE) for visual representation learning. Form the author of ResNet. - Michał Chromiak's blog, 8월 24, 2025에 액세스, https://mchromiak.github.io/articles/2021/Nov/14/Masked-Autoencoders-Are-Scalable-Vision-Learners/
51. Masked AutoEncoders. Masked AutoEncoder is a simple… | by Prakash Jay | Medium, 8월 24, 2025에 액세스, https://medium.com/@14prakash/masked-autoencoders-9e0f7a4a2585
52. MAE/SimMIM for Pre-Training Like a Masked Language Model | by Akihiro FUJII | Medium, 8월 24, 2025에 액세스, https://akichan-f.medium.com/mae-simmim-for-pre-training-like-a-masked-language-model-9b42579e25a9
53. Contrastive Learning - SimCLR and BYOL (With Code Example) - LearnOpenCV, 8월 24, 2025에 액세스, https://learnopencv.com/contrastive-learning-simclr-and-byol-with-code-example/
54. [2501.17683] Temperature-Free Loss Function for Contrastive Learning - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/2501.17683
55. SINCERE: Supervised Information Noise-Contrastive Estimation REvisited - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2309.14277
56. Paper explained: Momentum Contrast for Unsupervised Visual Representation Learning, 8월 24, 2025에 액세스, https://towardsdatascience.com/paper-explained-momentum-contrast-for-unsupervised-visual-representation-learning-ff2b0e08acfb/
57. Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations, 8월 24, 2025에 액세스, https://sthalles.github.io/simple-self-supervised-learning/
58. Temperature-Free Loss Function for Contrastive Learning - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2501.17683v1
59. SimCLR Explained in Simple Terms. SimCLR explained simply in 1 sentence… | by Jeffrey Boschman | One Minute Machine Learning | Medium, 8월 24, 2025에 액세스, https://medium.com/one-minute-machine-learning/simclr-explained-in-simple-terms-3fa69af45ff9
60. A Simple Framework for Contrastive Learning of Visual ..., 8월 24, 2025에 액세스, https://proceedings.mlr.press/v119/chen20j/chen20j.pdf
61. SimCLR — MMSelfSup 1.0.0 documentation, 8월 24, 2025에 액세스, https://mmselfsup.readthedocs.io/en/latest/papers/simclr.html
62. Paper digest: Momentum Contrast for Unsupervised Visual Representation Learning MoCo v1 & v2 by Kwonjoon Lee et al. explained in 5 minutes : r/deeplearning - Reddit, 8월 24, 2025에 액세스, https://www.reddit.com/r/deeplearning/comments/orc7pw/paper_digest_momentum_contrast_for_unsupervised/
63. 39: MoCO v1 & v2 Explained - Casual GAN Papers, 8월 24, 2025에 액세스, https://www.casualganpapers.com/self-supervised-representation-learning-encoder/MoCo-explained.html
64. Bootstrap Your Own Latent A New Approach to Self ... - NIPS, 8월 24, 2025에 액세스, https://papers.nips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf
65. BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2103.06695
66. Barlow Twins: Self-Supervised Learning via Redundancy Reduction - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2103.03230
67. Redundancy Reduction Twins Network: A Training framework for Multi-output Emotion Regression - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2206.09142
68. Barlow Twins paper Explained - A blog by Pramesh Gautam, 8월 24, 2025에 액세스, https://pmgautam.com/posts/barlow-twins-explanation.html
69. Evaluation of Barlow Twins and VICReg self-supervised learning for sound patterns of bird and anuran species - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2312.11240v1
70. Benchmarking Self-Supervised Contrastive Learning Methods for Image-Based Plant Phenotyping - PMC, 8월 24, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10079263/
71. Self-Supervised Learning of Intertwined Content and Positional Features for Object Detection - ICML 2025, 8월 24, 2025에 액세스, https://icml.cc/virtual/2025/poster/45621
72. [2410.07442] Self-Supervised Learning for Real-World Object Detection: a Survey - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/2410.07442
73. What Are Word Embeddings? | IBM, 8월 24, 2025에 액세스, https://www.ibm.com/think/topics/word-embeddings
74. Word2vec - Wikipedia, 8월 24, 2025에 액세스, https://en.wikipedia.org/wiki/Word2vec
75. Word Embeddings & Self-Supervised Learning, Explained - KDnuggets, 8월 24, 2025에 액세스, https://www.kdnuggets.com/2019/01/burkov-self-supervised-learning-word-embeddings.html
76. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/abs/1810.04805
77. A Conservative Estimate of Human Performance on the GLUE Benchmark - Nikita Nangia, 8월 24, 2025에 액세스, https://woollysocks.github.io/assets/GLUE_Human_Baseline.pdf
78. A Robustly Optimized BERT Pre-training Approach with Post-training, 8월 24, 2025에 액세스, http://www.cips-cl.org/static/anthology/CCL-2021/CCL-21-108.pdf
79. DEBERTAV3: DEBERTA ELECTRA-STYLE PRE-TRAINING - OpenReview, 8월 24, 2025에 액세스, https://openreview.net/pdf?id=sE7-XhLxHA
80. Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2410.11020v3
81. MosaicBERT: Pretraining BERT from Scratch for $20 | Databricks Blog, 8월 24, 2025에 액세스, https://www.databricks.com/blog/mosaicbert
82. Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0 - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2508.08110v1
83. Recognition of vocal emotions with the adjusted version of Wav2vec 2.0/HuBERT - Zaion, 8월 24, 2025에 액세스, https://zaion.ai/en/reconnaissance-des-emotions-vocales-avec-la-version-ajustee-de-wav2vec-2-0-hubert/
84. LibriSpeech test-other Benchmark (Speech Recognition) - Papers With Code, 8월 24, 2025에 액세스, https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-other?p=hubert-self-supervised-speech-representation
85. arXiv:2111.02735v3 [cs.CL] 3 Oct 2022, 8월 24, 2025에 액세스, https://arxiv.org/pdf/2111.02735
86. GPT-4 Technical Report - OpenAI, 8월 24, 2025에 액세스, https://cdn.openai.com/papers/gpt-4.pdf
87. How self-supervised learning revolutionized natural language processing and gen AI, 8월 24, 2025에 액세스, https://stackoverflow.blog/2025/04/28/how-self-supervised-language-revolutionized-natural-language-processing-and-gen-ai/
88. GPT-4 - OpenAI, 8월 24, 2025에 액세스, https://openai.com/index/gpt-4-research/
89. GPT-3 vs GPT-4: A Detailed Comparison of Capabilities - Accubits Blog, 8월 24, 2025에 액세스, https://blog.accubits.com/gpt-3-vs-gpt-4-a-detailed-comparison-of-capabilities/
90. A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4 - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2310.12321
91. OpenAI's DALL-E and CLIP 101: a brief introduction | by David Pereira | TDS Archive, 8월 24, 2025에 액세스, https://medium.com/data-science/openais-dall-e-and-clip-101-a-brief-introduction-3a4367280d4e
92. How DALL-E 2 Actually Works - AssemblyAI, 8월 24, 2025에 액세스, https://www.assemblyai.com/blog/how-dall-e-2-actually-works
93. Towards understanding CLIP model. What is CLIP? | by Chini - Medium, 8월 24, 2025에 액세스, https://medium.com/@chinihermann25/towards-understanding-clip-model-d19fcd0c175a
94. Prepping Data for Self-Supervised Learning: Labeling Less, Learning More - Keymakr, 8월 24, 2025에 액세스, https://keymakr.com/blog/prepping-data-for-self-supervised-learning-labeling-less-learning-more/
95. LibriSpeech test-clean Benchmark (Speech Recognition) - Papers With Code, 8월 24, 2025에 액세스, https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean?p=wav2vec-2-0-a-framework-for-self-supervised
96. What is the trade-off between computational cost and performance in SSL? - Milvus, 8월 24, 2025에 액세스, https://milvus.io/ai-quick-reference/what-is-the-tradeoff-between-computational-cost-and-performance-in-ssl
97. On Pretraining Data Diversity for Self-Supervised Learning - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2403.13808v3
98. What challenges are faced when implementing self-supervised learning? - Milvus, 8월 24, 2025에 액세스, https://milvus.io/ai-quick-reference/what-challenges-are-faced-when-implementing-selfsupervised-learning
99. Workshop on Computer Vision in the Wild 2025, 8월 24, 2025에 액세스, https://computer-vision-in-the-wild.github.io/cvpr-2025/
100. SELF-SUPERVISED LEARNING IS MORE ROBUST TO DATASET IMBALANCE - OpenReview, 8월 24, 2025에 액세스, https://openreview.net/pdf?id=4AZz9osqrar
101. On the Out-of-Distribution Generalization of Self-Supervised Learning - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2505.16675v1
102. On the Out-of-Distribution Generalization of Self-Supervised Learning | OpenReview, 8월 24, 2025에 액세스, https://openreview.net/forum?id=22ywev7zMt
103. Out-of-Distribution Generalization, 8월 24, 2025에 액세스, https://out-of-distribution-generalization.com/
104. When Does Self-Supervision Improve Few-Shot Learning? | fsl-ssl - Wandb, 8월 24, 2025에 액세스, https://wandb.ai/meta-learners/fsl-ssl/reports/When-Does-Self-Supervision-Improve-Few-Shot-Learning---Vmlldzo5MDA5NzA
105. When Does Self-supervision Improve Few-shot Learning?, 8월 24, 2025에 액세스, https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123520630.pdf
106. Bias and Ethical Concerns in Machine Learning - GeeksforGeeks, 8월 24, 2025에 액세스, https://www.geeksforgeeks.org/machine-learning/bias-and-ethical-concerns-in-machine-learning/
107. 2022 Volume 4 Bias and Ethical Concerns in Machine Learning - ISACA, 8월 24, 2025에 액세스, https://www.isaca.org/resources/isaca-journal/issues/2022/volume-4/bias-and-ethical-concerns-in-machine-learning
108. brainly.com, 8월 24, 2025에 액세스, [https://brainly.com/question/59860147#:~:text=Self%2Dsupervised%20learning%20is%20a,present%20in%20the%20training%20data.](https://brainly.com/question/59860147#:~:text=Self-supervised learning is a,present in the training data.)
109. Ethical Principles for Web Machine Learning - W3C, 8월 24, 2025에 액세스, https://www.w3.org/TR/webmachinelearning-ethics/
110. Self-Supervised Learning Principles Challenges and Emerging Directions - Preprints.org, 8월 24, 2025에 액세스, https://www.preprints.org/manuscript/202502.1894/v1
111. NeurIPS 2024 Workshop: Self-Supervised Learning - Theory and Practice, 8월 24, 2025에 액세스, https://sslneurips2024.github.io/
112. Workshop: Self-Supervised Learning - NeurIPS 2020, 8월 24, 2025에 액세스, https://neurips.cc/virtual/2020/protected/workshop_16146.html
113. To Compress or Not to Compress—Self-Supervised Learning and Information Theory: A Review - MDPI, 8월 24, 2025에 액세스, https://www.mdpi.com/1099-4300/26/3/252
114. To Compress or Not to Compress—Self-Supervised Learning and Information Theory: A Review - NYU Scholars, 8월 24, 2025에 액세스, https://nyuscholars.nyu.edu/en/publications/to-compress-or-not-to-compressself-supervised-learning-and-inform
115. Self-supervised Learning from a Multi-view Perspective - OpenReview, 8월 24, 2025에 액세스, https://openreview.net/forum?id=-bdp_8Itjwp
116. Matrix Information Theory for Self-Supervised Learning, 8월 24, 2025에 액세스, https://icml.cc/media/icml-2024/Slides/32737.pdf
117. A Survey of Data-Efficient Graph Learning - IJCAI, 8월 24, 2025에 액세스, https://www.ijcai.org/proceedings/2024/0896.pdf
118. A Survey on Self-Supervised Learning: Algorithms, Applications, and Future Trends, 8월 24, 2025에 액세스, https://www.computer.org/csdl/journal/tp/2024/12/10559458/1XR0ep31Wr6
119. A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends - arXiv, 8월 24, 2025에 액세스, https://arxiv.org/html/2301.05712v4
120. Resource Efficient Self-Supervised Learning for Speech Recognition - OpenReview, 8월 24, 2025에 액세스, https://openreview.net/forum?id=L9pW5fknjO
121. Self-supervised multimodal learning - Edinburgh Research Explorer, 8월 24, 2025에 액세스, https://www.research.ed.ac.uk/files/525233072/ZongEtalIEEETPAMI2024Self-supervisedMultimodalLearning.pdf
122. A Review of the Applications of Self-Supervised Learning in Multimodal Models, 8월 24, 2025에 액세스, https://madison-proceedings.com/index.php/aetr/article/view/3924
123. ys-zong/awesome-self-supervised-multimodal-learning - GitHub, 8월 24, 2025에 액세스, https://github.com/ys-zong/awesome-self-supervised-multimodal-learning
124. A Review of the Applications of Self-Supervised Learning in Multimodal Models - Madison Academic Press, 8월 24, 2025에 액세스, https://madison-proceedings.com/index.php/aetr/article/download/3924/3947/7997

