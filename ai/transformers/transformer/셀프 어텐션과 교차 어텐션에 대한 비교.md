# 셀프 어텐션과 교차 어텐션에 대한 비교


순환 신경망(Recurrent Neural Network, RNN)과 그 변형인 장단기 기억(Long Short-Term Memory, LSTM) 모델은 오랫동안 순차적 데이터 처리의 표준으로 자리 잡아왔다. 이들 모델은 시퀀스의 정보를 순차적으로 처리하며 내부 상태(hidden state)를 갱신하는 방식으로 작동한다. 그러나 이러한 순차적 처리 방식은 두 가지 근본적인 한계를 내포했다. 첫째, 각 타임스텝의 계산이 이전 타임스텝의 결과에 의존하므로 병렬 처리가 불가능하여 계산 병목 현상을 야기했다.1 둘째, 시퀀스가 길어질수록 초기의 정보가 뒤쪽으로 전달되면서 희석되거나 소실되는 '장거리 의존성(long-range dependency)' 문제가 발생했으며, 이는 그래디언트 소실(vanishing gradients) 문제와 직결되었다.2

이러한 한계를 극복하기 위해 등장한 개념이 바로 어텐션(Attention) 메커니즘이다.4 초기의 어텐션은 RNN 기반의 인코더-디코더 구조에서 디코더가 매 타임스텝마다 인코더의 모든 출력 시퀀스를 직접 참조하여, 현재 예측에 가장 관련성이 높은 정보에 '집중'하도록 가중치를 부여하는 방식으로 작동했다.5 이는 인코더의 모든 정보를 하나의 고정된 크기의 컨텍스트 벡터로 압축하면서 발생하는 정보 손실 문제를 효과적으로 해결하는 결정적 전환점이었다.6 이 시점에서 어텐션은 RNN이라는 주된 연산 엔진을 보조하는 강력한 '정렬 도구(alignment tool)'로서 기능했다.

그러나 2017년 발표된 "Attention Is All You Need" 논문은 어텐션의 역할을 근본적으로 재정의했다.8 이 논문은 순환 구조를 완전히 배제하고 오직 어텐션 메커니즘에만 의존하는 '트랜스포머(Transformer)' 아키텍처를 제안했다.3 이는 어텐션을 보조적인 도구가 아닌, 정보 간의 관계를 모델링하고 표현을 계산하는 핵심적인 '연산 엔진(computational engine)'으로 격상시킨 패러다임의 전환이었다. 이 가설, 즉 어텐션만으로 순환 구조 없이도 충분히 시퀀스 데이터를 모델링할 수 있다는 아이디어는 당시의 통념에 반하는 혁신적인 것이었다.10 트랜스포머 아키텍처의 성공은 이 가설의 타당성을 입증했으며, 그 중심에는 두 개의 핵심적인 어텐션 기둥이 있다. 바로 시퀀스 내부의 관계를 심층적으로 모델링하는 **셀프 어텐션(Self-Attention)**과 두 개의 독립된 시퀀스 간의 관계를 연결하는 **교차 어텐션(Cross-Attention)**이다.

본 보고서는 현대 인공지능 모델의 근간을 이루는 이 두 메커니즘을 심층적으로 해부하고 비교 고찰하는 것을 목표로 한다. 개념적 토대부터 수학적 원리, 아키텍처 내에서의 역할과 기능적 차이, 그리고 확장된 응용 분야에 이르기까지 다각적인 분석을 통해 두 어텐션의 본질과 상호작용을 명확히 규명하고, 이를 통해 복잡한 AI 시스템의 작동 원리에 대한 근본적인 이해를 제공하고자 한다.



셀프 어텐션은 이름에서 알 수 있듯이, 하나의 시퀀스 내에 존재하는 모든 요소(토큰)들이 서로를 직접 참조(self-reference)하여 각 요소의 의미를 문맥에 맞게 동적으로 재구성하는 메커니즘이다.2 이는 문장이나 데이터 시퀀스 내의 각 요소가 고립된 의미를 갖는 것이 아니라, 주변 요소들과의 상호작용 속에서 비로소 완전한 의미를 갖게 된다는 언어학적 직관을 모델링한다.

예를 들어, "The animal didn't cross the street because **it** was too tired"라는 문장을 생각해보자.13 인간은 'it'이라는 대명사가 문맥상 'animal'을 지칭한다는 것을 쉽게 파악한다. 셀프 어텐션은 모델이 이러한 문법적, 의미적 관계를 데이터로부터 직접 학습하도록 설계되었다. 'it'이라는 토큰의 표현을 계산할 때, 모델은 문장 내의 다른 모든 단어, 즉 'The', 'animal', 'didn't', 'cross' 등과의 관련성을 계산한다. 이 과정에서 'it'과 'animal' 사이의 관련성 점수가 높게 계산되고, 결과적으로 'animal'의 의미 정보가 'it'의 새로운 표현에 강하게 반영된다. 이처럼 셀프 어텐션은 각 단어가 문장 전체라는 하나의 '회의'에 참여하여, 다른 모든 단어와의 관계를 종합적으로 따져보고 자신의 '역할'과 '의미'를 스스로 갱신하는 과정에 비유할 수 있다.14

이러한 자기 참조 과정은 시퀀스 내 모든 토큰에 대해 동시에 일어나며, 결과적으로 생성되는 새로운 표현 벡터들은 초기 임베딩 벡터보다 훨씬 풍부한 문맥 정보를 내포하게 된다. 이는 단순히 단어 간의 관계를 파악하는 것을 넘어, 각 토큰을 위한 동적인 문맥 벡터를 생성하는 과정으로 이해할 수 있다. 즉, 셀프 어텐션은 시퀀스 자체를 일종의 미분 가능한 키-밸류 메모리 네트워크(differentiable key-value memory network)로 취급한다. 각 토큰은 자신의 정보를 질의(query)하여, 시퀀스 전체로 구성된 메모리 뱅크로부터 가장 관련성 높은 정보들을 '소프트'하게 검색하고 가중합하여 자신의 표현을 업데이트한다. 이 관점은 셀프 어텐션을 단순한 '관계 모델러'를 넘어, 시퀀스 내부에서 작동하는 동적이고 내용 기반의 정보 검색 시스템으로 이해하는 더 깊은 통찰을 제공한다.


셀프 어텐션의 계산 과정은 'Scaled Dot-Product Attention'이라는 함수를 통해 수학적으로 정형화된다. 이 과정은 여러 단계로 나눌 수 있다.


먼저, 입력 문장은 토크나이저(tokenizer)에 의해 개별적인 토큰(token) 시퀀스로 분할된다. 각 토큰은 사전 훈련된 임베딩 모델을 통해 고차원의 벡터, 즉 임베딩 벡터(embedding vector)로 변환된다.15 트랜스포머는 순환 구조가 없기 때문에 단어의 순서 정보를 별도로 주입해야 하는데, 이를 위해 각 위치별로 고유한 값을 가지는 위치 인코딩(Positional Encoding) 벡터를 생성하여 임베딩 벡터에 더해준다.10 이렇게 준비된 입력 벡터들의 시퀀스는 행렬 $X \in \mathbb{R}^{n \times d_{model}}$로 표현할 수 있으며, 여기서 $n$은 시퀀스의 길이, $d_{model}$은 임베딩 벡터의 차원을 의미한다.


셀프 어텐션은 동일한 입력 시퀀스 행렬 $X$로부터 세 개의 서로 다른 벡터 시퀀스, 즉 쿼리(Query), 키(Key), 밸류(Value)를 생성하는 것으로 시작한다. 이는 각각 학습 가능한 가중치 행렬 $W_Q \in \mathbb{R}^{d_{model} \times d_k}$, $W_K \in \mathbb{R}^{d_{model} \times d_k}$, $W_V \in \mathbb{R}^{d_{model} \times d_v}$를 입력 행렬 $X$에 곱함으로써 이루어진다.18
$$
Q = XW_Q \\
K = XW_K \\
V = XW_V
$$
여기서 $Q, K \in \mathbb{R}^{n \times d_k}$이고 $V \in \mathbb{R}^{n \times d_v}$이다. 일반적으로 $d_k = d_v = d_{model} / h$로 설정되며, $h$는 멀티헤드 어텐션의 헤드 수이다. 이 세 벡터는 은유적으로 다음과 같은 역할을 수행한다.

- **쿼리(Q):** 현재 처리 중인 토큰이 다른 토큰들에게 "나는 이런 정보를 찾고 있다"고 알리는 '질의' 또는 '정보 요청 주체'이다.16
- **키(K):** 각 토큰이 자신을 식별하고 쿼리와의 관련성을 측정하는 데 사용되는 '꼬리표(label)' 또는 '색인'이다.16
- **밸류(V):** 각 토큰이 실제로 담고 있는 의미론적 정보, 즉 '내용'이다.16

중요한 점은 Q, K, V가 모두 동일한 입력 $X$에서 파생되었지만, 서로 다른 학습 가능한 가중치 행렬을 통해 각각 다른 의미 공간으로 투영(projection)된다는 것이다. 이를 통해 모델은 다양한 관점에서 토큰 간의 관계를 유연하게 학습할 수 있다.13


다음으로, 특정 쿼리(한 토큰)가 모든 키(다른 모든 토큰)와 얼마나 연관되어 있는지를 측정하기 위해 어텐션 스코어를 계산한다. 이는 쿼리 행렬 $Q$와 키 행렬의 전치 $K^T$를 내적(dot product)하여 구한다.13

$i$번째 쿼리 벡터 $q_i$와 $j$번째 키 벡터 $k_j$의 내적은 두 벡터 간의 유사도(similarity)를 나타내는 척도로 작용한다. 두 벡터가 유사한 방향을 가리킬수록 내적 값은 커지고, 직교에 가까울수록 0에 가까워진다.15


키 벡터의 차원 $d_k$가 커질수록 내적 값의 분산이 커져 매우 큰 값을 가질 수 있다. 이 값이 그대로 소프트맥스 함수에 입력되면, 그래디언트가 거의 0에 가까워지는 포화(saturation) 영역으로 들어가 학습이 불안정해질 수 있다. 이러한 문제를 방지하기 위해, 계산된 어텐션 스코어를 키 벡터 차원의 제곱근 $\sqrt{d_k}$로 나누어주는 스케일링 과정을 거친다.1 이것이 'Scaled' Dot-Product Attention이라는 이름이 붙은 이유이다.


스케일링된 어텐션 스코어 행렬에 소프트맥스 함수를 행(row) 단위로 적용한다. 이를 통해 각 행의 합이 1이 되는 확률 분포, 즉 '어텐션 가중치(Attention Weights)'를 얻는다.19

$i$번째 행의 $j$번째 값은, $i$번째 토큰의 표현을 계산할 때 $j$번째 토큰에 얼마나 많은 '주의'를 기울여야 하는지를 나타내는 가중치가 된다.5


마지막으로, 계산된 어텐션 가중치 행렬을 밸류 행렬 $V$에 곱한다. 이는 각 토큰에 대해 다른 모든 토큰의 밸류 벡터들을 어텐션 가중치에 따라 가중합(weighted sum)하는 것과 같다.20 이 과정을 통해 관련성이 높은 토큰의 정보(Value)는 보존 및 강화되고, 관련성이 낮은 토큰의 정보는 약화되어, 문맥 정보가 풍부하게 반영된 최종 출력 행렬 $Z \in \mathbb{R}^{n \times d_v}$가 생성된다.


위의 모든 과정을 하나의 수식으로 통합하면 다음과 같다.10
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$


셀프 어텐션은 트랜스포머 아키텍처의 인코더와 디코더 모두에서 핵심적인 역할을 수행하지만, 그 작동 방식에는 미묘한 차이가 있다.


인코더에 사용되는 셀프 어텐션은 입력 시퀀스 내의 모든 토큰이 다른 모든 토큰을 제한 없이 참조할 수 있다. 즉, 특정 토큰의 표현을 계산할 때 자신을 포함한 시퀀스 전체의 과거와 미래 정보를 모두 활용한다. 이러한 양방향(bi-directional) 문맥 학습 능력 덕분에 인코더는 입력 문장 전체의 구조와 의미를 깊이 있게 이해하고, 매우 풍부한 문맥적 표현을 생성할 수 있다.3 BERT와 같은 인코더 전용 모델의 강력한 성능은 이러한 양방향 셀프 어텐션에 기반한다.


디코더의 첫 번째 서브층에 위치하는 셀프 어텐션은 '마스크드 셀프 어텐션(Masked Self-Attention)'이라고 불린다.26 디코더는 주로 번역이나 텍스트 생성과 같이 순차적으로 결과를 만들어내는 자기회귀적(auto-regressive) 작업을 수행한다. 따라서, 

$t$ 시점의 단어를 예측할 때 $t$ 시점 이후의 미래 정보를 참조해서는 안 된다. 이를 방지하기 위해 마스킹 기법이 사용된다. 어텐션 스코어를 계산한 후, 소프트맥스 함수를 적용하기 전에 현재 위치보다 미래에 있는 위치에 해당하는 스코어 값들을 매우 작은 음수(예: $-\infty$)로 바꾸어버린다.27 이렇게 하면 소프트맥스 함수를 통과했을 때 해당 위치의 어텐션 가중치가 0에 가까워져 미래 정보를 참조하는 것을 원천적으로 차단한다. 이 마스킹 덕분에 디코더의 셀프 어텐션은 단방향(uni-directional) 문맥만을 학습하게 되며, 모델의 자기회귀적 속성을 보존할 수 있다.24


셀프 어텐션은 기존의 순환 모델에 비해 여러 가지 뚜렷한 장점을 가지지만, 동시에 명확한 한계점도 존재한다.


- **병렬 처리:** 셀프 어텐션은 각 토큰에 대한 계산이 다른 토큰의 계산 결과에 의존하지 않기 때문에, 시퀀스 내 모든 토큰에 대한 어텐션 계산을 동시에 수행할 수 있다. 이는 RNN의 순차적 계산($O(n)$)과 대조적으로, GPU와 같은 병렬 처리 장치를 최대한 활용하여 학습 및 추론 속도를 획기적으로 향상시킨다.2
- **장거리 의존성 포착:** 시퀀스 내 임의의 두 토큰 간의 정보 전달 경로 길이가 $O(1)$로 일정하다. 즉, 아무리 멀리 떨어진 토큰이라도 단 한 번의 어텐션 계산으로 직접적인 상호작용이 가능하다. 이는 경로 길이가 시퀀스 길이에 비례하여 길어지는($O(n)$) RNN에 비해 장거리 의존성을 포착하는 데 훨씬 효과적이다.2


- **계산 복잡도:** 셀프 어텐션의 가장 큰 단점은 시퀀스의 길이 $n$에 대해 모든 토큰 쌍 간의 상호작용을 계산해야 한다는 점이다. 이로 인해 어텐션 스코어 행렬의 크기가 $n \times n$이 되어, 계산량과 메모리 사용량이 시퀀스 길이의 제곱에 비례($O(n^2)$)하여 증가한다. 이 때문에 매우 긴 시퀀스(예: 긴 문서, 고해상도 이미지)를 처리하는 데에는 실질적인 한계가 존재한다.3



교차 어텐션은 셀프 어텐션과 달리, 두 개의 **서로 다른** 시퀀스 사이의 관계를 모델링하는 데 특화된 메커니즘이다.33 그 핵심 기능은 하나의 시퀀스(쿼리 시퀀스)가 다른 시퀀스(키-밸류 시퀀스)의 정보 중 어떤 부분에 '집중'하고 그 정보를 가져와야 할지를 학습하는 것이다.

이 과정은 다양한 비유를 통해 이해할 수 있다. 기계 번역의 경우, 이는 마치 통역사가 소스 언어 문장(예: 한국어)을 들으면서 타겟 언어 문장(예: 영어)을 한 단어씩 생성할 때, 현재 생성할 영어 단어와 가장 관련성이 높은 한국어 단어를 찾아 연결하는 과정과 유사하다.14 또 다른 관점에서는, 하나의 정보 소스를 '질의(Query)'로 사용하여, 다른 정보 소스로 구성된 일종의 '데이터베이스'(Key-Value 쌍)에서 관련 정보를 '검색'하고 '통합'하는 과정으로 볼 수 있다.14

이처럼 교차 어텐션의 본질은 이종(heterogeneous) 정보 간의 정렬(alignment)과 융합(fusion)에 있다. 이는 단순히 두 정보를 합치는 것을 넘어, 한 정보가 다른 정보에 의해 조건화(conditioning)되거나 안내(guiding)되는, 보다 정교한 상호작용을 가능하게 한다. 예를 들어, 텍스트 프롬프트에 따라 이미지를 생성하는 모델에서 교차 어텐션은 이미지 생성 과정이 텍스트의 의미에 의해 '조건화'되도록 하는 핵심 장치이다. 이처럼 교차 어텐션은 한 정보의 맥락 안에서 다른 정보를 해석하고 활용하는, 일반화된 조건부 처리 메커니즘으로 기능한다. 이는 현대 AI 시스템이 외부 지식을 활용하거나, 여러 양식의 데이터를 통합하여 복잡한 작업을 수행하는 능력의 근간을 이룬다.


교차 어텐션이 사용하는 근본적인 계산 공식은 셀프 어텐션의 Scaled Dot-Product Attention과 완전히 동일하다.
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
그러나 **결정적인 차이점은 쿼리(Q), 키(K), 밸류(V) 벡터의 출처**에 있다.26 셀프 어텐션에서는 이 세 가지가 모두 동일한 시퀀스에서 파생되지만, 교차 어텐션에서는 다음과 같이 서로 다른 두 시퀀스에서 생성된다.

- **쿼리(Q):** 정보를 필요로 하고, 다른 시퀀스를 '참조하려는' 시퀀스 A에서 생성된다. 예를 들어, 트랜스포머 디코더의 이전 층 출력에서 파생된다.
- **키(K)와 밸류(V):** 정보의 원천이 되며, '참조되는' 시퀀스 B에서 생성된다. 예를 들어, 트랜스포머 인코더의 최종 출력에서 파생된다.

이러한 비대칭적 구조를 통해 시퀀스 A의 각 요소는 시퀀스 B의 모든 요소와의 관련성을 계산할 수 있다. 그 결과로 얻어지는 어텐션 가중치는 시퀀스 A의 현재 요소가 시퀀스 B의 어떤 요소들로부터 정보를 가져와야 하는지를 결정한다. 최종적으로 이 가중치를 시퀀스 B에서 파생된 밸류(V)에 적용하여 가중합을 구함으로써, 시퀀스 A는 시퀀스 B의 가장 관련성 높은 정보를 선택적으로 통합하여 자신의 표현을 업데이트하게 된다.14


트랜스포머의 표준 인코더-디코더 아키텍처에서 교차 어텐션은 인코더와 디코더를 연결하는 핵심적인 '교량' 역할을 수행하며, '인코더-디코더 어텐션'이라는 이름으로 불린다. 이 메커니즘은 디코더의 각 층에 포함된 세 개의 서브층 중 두 번째에 위치한다.6

이 층에서의 정보 흐름은 다음과 같다.

1. **쿼리(Q)의 출처:** 디코더의 바로 아래에 있는 첫 번째 서브층, 즉 마스크드 셀프 어텐션 층의 출력에서 쿼리 벡터가 생성된다. 이 쿼리는 "지금까지 생성된 타겟 시퀀스의 문맥을 고려할 때, 소스 시퀀스로부터 어떤 정보가 필요한가?"라는 질문을 담고 있다.
2. **키(K)와 밸류(V)의 출처:** 인코더 스택 전체를 통과한 최종 출력 행렬로부터 키와 밸류 벡터가 생성된다. 이 K-V 쌍은 소스 시퀀스 전체에 대한 풍부한 문맥 정보를 담고 있는 '지식 저장소' 역할을 한다.26

따라서, 디코더는 타겟 시퀀스의 다음 토큰을 생성하는 매 단계마다, 인코더-디코더 어텐션을 통해 소스 시퀀스 전체의 정보를 다시 참조한다. 이를 통해 현재 생성하려는 단어와 가장 관련성이 높은 소스 시퀀스의 부분에 높은 가중치를 부여하고, 그 정보를 활용하여 보다 정확하고 문맥에 맞는 출력을 생성할 수 있다.4


교차 어텐션의 개념은 텍스트-텍스트 관계를 모델링하는 기계 번역이나 텍스트 요약을 넘어, 서로 다른 종류의 데이터 양식(modality)을 융합하는 멀티모달 AI 분야에서 핵심적인 기술로 자리 잡았다.33 두 개의 서로 다른 정보 소스를 연결하는 교차 어텐션의 본질적인 특성은 이종 데이터 간의 상호작용을 모델링하는 데 매우 효과적이기 때문이다.

- **이미지 캡셔닝 (Image Captioning):** 이 태스크에서 모델은 이미지를 보고 그 내용을 설명하는 텍스트를 생성해야 한다. 교차 어텐션은 이미지 특징과 텍스트 생성을 연결하는 데 사용된다. 먼저, CNN이나 Vision Transformer(ViT)와 같은 비전 인코더가 이미지로부터 특징 벡터들의 집합을 추출한다. 이 이미지 특징들이 교차 어텐션의 키(K)와 밸류(V)가 된다. 텍스트를 생성하는 디코더는 매 단어를 생성할 때마다 자신의 현재 상태를 쿼리(Q)로 사용하여 이미지 특징들에 어텐션을 수행한다. 이를 통해 캡션의 특정 단어(예: 'dog')를 생성할 때 이미지 내의 '개' 영역에 집중할 수 있다.34
- **텍스트-이미지 생성 (Text-to-Image Generation):** Stable Diffusion과 같은 모델에서 교차 어텐션은 텍스트 프롬프트의 의미를 이미지 생성 과정에 주입하는 역할을 한다. 텍스트 프롬프트(예: "a red apple on a table")는 텍스트 인코더를 통해 임베딩 시퀀스로 변환되어 키(K)와 밸류(V)를 형성한다. 이미지를 생성하는 U-Net 아키텍처의 각 층에서는 생성 중인 이미지의 잠재 표현(latent representation)이 쿼리(Q)가 되어 텍스트 K-V 쌍에 어텐션을 수행한다. 이 과정을 통해 'red'라는 단어는 이미지의 사과 영역 색상에, 'table'이라는 단어는 배경 영역의 형태에 영향을 미치게 된다.46
- **다양한 모달리티 융합:** 이러한 원리는 다른 멀티모달 태스크에도 동일하게 적용된다. 음성 인식에서는 음성 신호의 특징(K, V)과 생성된 텍스트(Q)를 연결하고, 비디오 요약에서는 비디오 프레임들의 특징(K, V)과 요약 텍스트(Q)를 연결한다. 또한, 로보틱스에서는 시각 정보(K, V)와 로봇의 행동 계획(Q)을 결합하는 등, 한 모달리티의 정보를 다른 모달리티에 조건화(conditioning)하는 거의 모든 멀티모달 문제에서 교차 어텐션은 핵심적인 기술로 활용된다.14


셀프 어텐션과 교차 어텐션은 동일한 수학적 연산(Scaled Dot-Product Attention)을 기반으로 하지만, 그들의 목적, 정보 흐름, 그리고 아키텍처 내 역할은 근본적으로 다르다. 이 두 메커니즘의 차이점을 명확히 이해하는 것은 트랜스포머 기반 모델의 작동 방식을 이해하는 데 필수적이다.


두 메커니즘을 구분하는 가장 근본적이고 명확한 기준은 어텐션 계산에 사용되는 쿼리(Q), 키(K), 밸류(V) 벡터의 출처이다.

- **셀프 어텐션:** Q, K, V가 모두 **동일한 단일 시퀀스**로부터 파생된다. 정보의 흐름이 시퀀스 **내부에서 순환**하며, 각 요소가 다른 모든 요소를 참조하여 자신의 표현을 정교화하는 '자기 참조(self-referential)' 구조를 가진다. 이는 시퀀스 내부(Intra-sequence)의 관계를 모델링하는 데 초점을 맞춘다.3
- **교차 어텐션:** Q는 하나의 시퀀스(시퀀스 A)에서, K와 V는 **전혀 다른 시퀀스**(시퀀스 B)에서 파생된다. 정보의 흐름이 시퀀스 A에서 시퀀스 B로 향하는 단방향의 '외부 참조(external-referential)' 형태를 띤다. 이는 시퀀스 간(Inter-sequence)의 관계를 모델링하는 데 사용된다.4

이러한 근원적인 차이를 요약하면 다음 표와 같다.

| 항목 (Item)         | 셀프 어텐션 (Self-Attention)                             | 교차 어텐션 (Cross-Attention)                           |
| ------------------- | -------------------------------------------------------- | ------------------------------------------------------- |
| **핵심 목적**       | 단일 시퀀스 내 요소 간의 문맥적 관계 학습 및 표현 정교화 | 두 개의 서로 다른 시퀀스 간의 정보 정렬, 융합 및 조건화 |
| **Query 출처**      | 현재 처리 중인 시퀀스 $X$                                | 시퀀스 $A$ (예: 디코더의 이전 출력)                     |
| **Key, Value 출처** | 현재 처리 중인 시퀀스 $X$                                | 시퀀스 $B$ (예: 인코더의 최종 출력)                     |
| **수식적 표현**     | `Attention(XW_Q, XW_K, XW_V)`                            | `Attention(X_A W_Q, X_B W_K, X_B W_V)`                  |
| **주요 위치**       | 인코더 전체 층, 디코더의 첫 번째 서브층                  | 디코더의 두 번째 서브층 (인코더-디코더 어텐션)          |
| **정보 흐름**       | 내부 순환적 (Intra-sequence)                             | 외부 참조적 / 방향성 (Inter-sequence / Directional)     |
| **주요 기능**       | 내부 문맥화 (Internal Contextualization)                 | 외부 정보 정렬 및 융합 (External Alignment & Fusion)    |
| **대표 예시**       | BERT 인코더, GPT 디코더 (마스크드)                       | 기계 번역 디코더, 이미지 캡셔닝, RAG                    |


Q, K, V의 출처 차이는 두 메커니즘의 기능적 목적의 분화로 이어진다.


셀프 어텐션의 주된 목표는 시퀀스 내 각 요소의 표현을 문장 전체의 포괄적인 문맥을 반영하여 더욱 풍부하고 정교하게 만드는 것이다. 이는 시퀀스에 대한 '이해의 깊이'를 더하는 과정이라 할 수 있다.2 예를 들어, 문장 내에서 동음이의어의 의미를 명확히 하거나, 대명사가 지칭하는 대상을 찾거나, 복잡한 문법 구조를 파악하는 등 시퀀스 내부의 복잡한 의존성을 포착하는 데 핵심적인 역할을 한다.13


교차 어텐션의 주된 목표는 두 개의 독립적인 시퀀스 간의 관련성을 파악하고, 한 시퀀스의 정보를 다른 시퀀스에 효과적으로 전달하고 통합하는 것이다. 이는 '정보의 연결'과 '지식의 확장'을 담당하는 과정이다.4 기계 번역에서 소스 언어의 단어와 타겟 언어의 단어를 정렬하거나, 이미지 캡셔닝에서 이미지의 특정 영역과 캡션의 특정 단어를 정렬하는 등, 이종 정보 간의 의미론적 매핑을 수행하는 것이 주요 기능이다.34


표준적인 인코더-디코더 트랜스포머 모델에서 셀프 어텐션과 교차 어텐션은 독립적으로 작동하는 것이 아니라, 하나의 목표를 위해 유기적으로 연결된 정교한 파이프라인을 형성한다.

1. **1단계 (인코더의 심층 문맥화):** 입력된 소스 시퀀스는 먼저 인코더 스택으로 들어간다. 인코더의 여러 층에 쌓인 **셀프 어텐션**은 소스 시퀀스를 반복적으로 처리하며, 각 토큰에 대한 깊고 풍부한 문맥적 표현(context-rich vectors)을 생성한다. 이 과정은 소스 시퀀스 자체에 대한 철저한 이해를 구축하는 단계이다.8
2. **2단계 (정보 전달):** 인코더 스택의 최종 출력, 즉 소스 시퀀스의 모든 토큰에 대한 최종적인 문맥적 표현의 집합은 디코더의 모든 층으로 전달된다. 이 출력은 이후 디코더에서 수행될 교차 어텐션을 위한 **키(K)와 밸류(V)의 재료**가 된다.41
3. **3단계 (디코더의 정보 융합):** 디코더는 타겟 시퀀스 생성을 시작한다. 각 디코더 층에서, 먼저 **마스크드 셀프 어텐션**이 지금까지 생성된 타겟 시퀀스의 내부 문맥을 파악한다. 그 결과로 나온 표현은 **교차 어텐션**의 **쿼리(Q)**가 되어, 인코더로부터 전달받은 K와 V에 질의를 보낸다. 이 교차 어텐션 과정을 통해, 디코더는 현재 생성하려는 타겟 토큰에 가장 중요한 소스 시퀀스의 정보가 무엇인지 파악하고, 그 정보를 자신의 표현에 통합한다.6

이러한 상호작용을 통해 두 메커니즘은 명확한 역할을 분담하며 시너지를 창출한다. 즉, **인코더의 셀프 어텐션이 고품질의 '정보 소스(K, V)'를 만드는 역할을 하면, 디코더의 교차 어텐션은 그 정보 소스를 효과적으로 '활용'하여 타겟 시퀀스를 생성하는 역할을 한다.** 셀프 어텐션을 통해 소스 문장에 대한 이해의 '질'이 높아질수록, 교차 어텐션은 그 질 높은 정보를 바탕으로 더욱 정확하고 문맥에 맞는 '연결'을 수행할 수 있게 되는 상호보완적 관계가 성립된다.


셀프 어텐션과 교차 어텐션은 트랜스포머 아키텍처를 구성하는 두 개의 핵심 기둥으로서, 서로 경쟁하는 관계가 아닌, 현대 AI 모델의 정교하고 다층적인 정보 처리를 가능하게 하는 상호보완적인 파트너 관계에 있다. 이 두 메커니즘의 기능적 분화와 유기적 결합은 트랜스포머가 기존의 순차적 모델들을 뛰어넘는 성능을 달성할 수 있었던 근본적인 동력이다.

**셀프 어텐션**은 하나의 정보 소스를 깊이 있게 파고들어 그 내부의 구조, 문맥, 그리고 요소 간의 미묘한 관계를 이해하는 **'심층 분석가'**의 역할을 수행한다. 이는 모델이 입력된 데이터의 의미를 스스로 내재화하고 풍부한 표현을 구축하는 능력, 즉 '스스로 생각하는' 능력의 기반이 된다. 시퀀스 내부의 모든 정보를 동시에 고려하는 병렬적 특성과 장거리 의존성을 효과적으로 포착하는 능력은 정보 표현의 질을 극대화한다.

반면, **교차 어텐션**은 서로 다른 정보 소스들을 연결하고, 하나의 맥락에서 다른 맥락의 지식을 선별적으로 가져와 융합하는 **'외부 정보 전문가' 또는 '통역사'**의 역할을 수행한다.14 이는 모델이 하나의 양식(modality)에 국한되지 않고, 외부 세계의 다양한 정보(텍스트, 이미지, 음성 등)와 소통하며 지식을 확장하고, 특정 조건에 맞는 결과를 생성하는 능력의 기반이 된다.

결론적으로, 셀프 어텐션이 정보의 '깊이'를 더하고, 교차 어텐션이 정보의 '넓이'를 확장하는 방식으로 이 두 메커니즘은 정교하게 협력한다. 셀프 어텐션을 통해 깊이 있게 이해된 고품질의 정보 표현은 교차 어텐션을 통해 다른 정보와 효과적으로 연결되고 활용된다. 이들의 정교한 조합은 트랜스포머 아키텍처가 기계 번역, 텍스트 생성과 같은 전통적인 NLP 작업을 넘어, 멀티모달 융합, 검색 증강 생성(RAG) 등 현대 AI의 가장 도전적인 과제들에서 전례 없는 성공을 거둘 수 있었던 핵심 요인이다. 따라서 셀프 어텐션과 교차 어텐션의 원리, 차이점, 그리고 상호작용을 명확히 이해하는 것은 복잡한 AI 시스템을 분석하고, 설계하며, 그 잠재력을 최대한 활용하기 위한 필수적인 첫걸음이라 할 수 있다.


1. The Detailed Explanation of Self-Attention in Simple Words | by Maninder Singh | Medium, 8월 20, 2025에 액세스, https://medium.com/@manindersingh120996/the-detailed-explanation-of-self-attention-in-simple-words-dec917f83ef3
2. 딥러닝의 핵심, 셀프 어텐션 매커니즘이란? - Duky - 티스토리, 8월 20, 2025에 액세스, https://dukymood.tistory.com/109
3. Attention Is All You Need - arXiv, 8월 20, 2025에 액세스, https://arxiv.org/html/1706.03762v7
4. Transformer의 큰 그림 이해: 기술적 복잡함 없이 핵심 아이디어 파악하기 - Medium, 8월 20, 2025에 액세스, [https://medium.com/@hugmanskj/transformer%EC%9D%98-%ED%81%B0-%EA%B7%B8%EB%A6%BC-%EC%9D%B4%ED%95%B4-%EA%B8%B0%EC%88%A0%EC%A0%81-%EB%B3%B5%EC%9E%A1%ED%95%A8-%EC%97%86%EC%9D%B4-%ED%95%B5%EC%8B%AC-%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4-%ED%8C%8C%EC%95%85%ED%95%98%EA%B8%B0-5e182a40459d](https://medium.com/@hugmanskj/transformer의-큰-그림-이해-기술적-복잡함-없이-핵심-아이디어-파악하기-5e182a40459d)
5. 어텐션 메커니즘이란 무엇인가요? - IBM, 8월 20, 2025에 액세스, https://www.ibm.com/kr-ko/think/topics/attention-mechanism
6. Transfomer 기본 개념 정리 - 벌꿀오소리의 공부 일지, 8월 20, 2025에 액세스, https://yeong-jin-data-blog.tistory.com/entry/Tranfomer
7. Self - Attention in NLP - GeeksforGeeks, 8월 20, 2025에 액세스, https://www.geeksforgeeks.org/nlp/self-attention-in-nlp/
8. Attention is All you Need - NIPS, 8월 20, 2025에 액세스, https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
9. 트랜스포머(인공신경망) - 나무위키, 8월 20, 2025에 액세스, [https://namu.wiki/w/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8(%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D)](https://namu.wiki/w/트랜스포머(인공신경망))
10. Attention Is All You Need - Wikipedia, 8월 20, 2025에 액세스, https://en.wikipedia.org/wiki/Attention_Is_All_You_Need
11. Transformer (deep learning architecture) - Wikipedia, 8월 20, 2025에 액세스, https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)
12. datasciencebeehive.tistory.com, 8월 20, 2025에 액세스, [https://datasciencebeehive.tistory.com/126#:~:text=Self%2DAttention%EC%9D%80%20%EC%9E%85%EB%A0%A5%20%EC%8B%9C%ED%80%80%EC%8A%A4,%EB%A5%BC%20%EA%B2%B0%EC%A0%95%ED%95%A0%20%EC%88%98%20%EC%9E%88%EA%B2%8C%20%ED%95%A9%EB%8B%88%EB%8B%A4.](https://datasciencebeehive.tistory.com/126#:~:text=Self-Attention은 입력 시퀀스,를 결정할 수 있게 합니다.)
13. 4-1. Transformer(Self Attention) [초등학생도 이해하는 자연어처리] - 코딩 오페라, 8월 20, 2025에 액세스, https://codingopera.tistory.com/43
14. 13화 LLM은 어떻게 최신 웹 검색 결과를 이해할까? - 브런치, 8월 20, 2025에 액세스, https://brunch.co.kr/@mentorsapiens/96
15. [트렌스포머 모델 이해하기] Self-Attention에서 Q, K, V(Query, Key, Value)의 의미, 8월 20, 2025에 액세스, https://cn-c.tistory.com/68
16. What is self-attention? | IBM, 8월 20, 2025에 액세스, https://www.ibm.com/think/topics/self-attention
17. 예제로 보는 트랜스포머/어텐션 (Attention is All You Need) - 컴퓨터와 수학, 몽상 조금, 8월 20, 2025에 액세스, https://skyil.tistory.com/256
18. Self Attention - 셀프 어텐션 동작 원리, 8월 20, 2025에 액세스, https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/
19. [NLP] Transformer (3) : Self-Attention & Multi-Head Attention - velog, 8월 20, 2025에 액세스, https://velog.io/@judy_choi/NLP-Transformer-3-Self-Attention-Multi-Head-Attention
20. Self Attention에 대해 공부, 8월 20, 2025에 액세스, https://zayunsna.github.io/blog/2023-09-05-self_attention/
21. 어텐션 매커니즘(Attention Mechanism) - Hello, didi universe - 티스토리, 8월 20, 2025에 액세스, [https://didi-universe.tistory.com/entry/%EC%96%B4%ED%85%90%EC%85%98-%EB%A7%A4%EC%BB%A4%EB%8B%88%EC%A6%98Attention-Mechanism](https://didi-universe.tistory.com/entry/어텐션-매커니즘Attention-Mechanism)
22. [Deep Learning] Self-Attention 메커니즘 이해하기 📘🤖 - 데이터 AI 벌집 - 티스토리, 8월 20, 2025에 액세스, https://datasciencebeehive.tistory.com/126
23. 딥러닝 트랜스포머 셀프어텐션, Transformer, self attention - YouTube, 8월 20, 2025에 액세스, https://www.youtube.com/watch?v=DdpOpLNKRJs
24. Encoder vs. Decoder in Transformers: Unpacking the Differences | by Hassaan Idrees, 8월 20, 2025에 액세스, https://medium.com/@hassaanidrees7/encoder-vs-decoder-in-transformers-unpacking-the-differences-9e6ddb0ff3c5
25. [Transformer] Transformer 트랜스포머 모델: 인코더와 디코더의 자세한 순서별 설명, 8월 20, 2025에 액세스, https://datasciencebeehive.tistory.com/118
26. Encoders and Decoders in Transformer Models ..., 8월 20, 2025에 액세스, https://machinelearningmastery.com/encoders-and-decoders-in-transformer-models/
27. 트랜스포머(Transformer) 모델 / 디코더, 8월 20, 2025에 액세스, https://zorba-blog.tistory.com/28
28. 트랜스포머 시리즈 마지막편 트랜스포머의 구조 - 디코더, 8월 20, 2025에 액세스, https://insoo-hwang.tistory.com/34
29. 트랜스포머(Transformer) 파헤치기—3. Decoder & Masked Attention, 8월 20, 2025에 액세스, https://www.blossominkyung.com/deeplearning/transfomer-last
30. [NLP] Transformer : Masked Multi-Head Attention - part3 - Learn by doing - 티스토리, 8월 20, 2025에 액세스, https://acdongpgm.tistory.com/221
31. Introduction of Self-Attention Layer in Transformer | by Neil Wu | LSC PSD | Medium, 8월 20, 2025에 액세스, https://medium.com/lsc-psd/introduction-of-self-attention-layer-in-transformer-fc7bff63f3bc
32. 5 Attention Mechanism Insights Every AI Developer Should Know - Shelf.io, 8월 20, 2025에 액세스, https://shelf.io/blog/attention-mechanism/
33. Cross Attention Module, 8월 20, 2025에 액세스, [https://vds.sogang.ac.kr/wp-content/uploads/2023/01/2022%ED%95%98%EA%B3%84%EC%84%B8%EB%AF%B8%EB%82%98_%EC%9C%A0%ED%98%84%EC%9A%B0.pdf](https://vds.sogang.ac.kr/wp-content/uploads/2023/01/2022하계세미나_유현우.pdf)
34. Cross-Attention vs Self-Attention Explained - AIML.com, 8월 20, 2025에 액세스, https://aiml.com/explain-cross-attention-and-how-is-it-different-from-self-attention/
35. Implement Self-Attention and Cross-Attention in Pytorch | by Hey Amit - Medium, 8월 20, 2025에 액세스, https://medium.com/@heyamit10/implement-self-attention-and-cross-attention-in-pytorch-cfe17ab0b3ee
36. 트랜스포머 인코더-디코더의 교차 주의에 대한 질문 : r/learnmachinelearning - Reddit, 8월 20, 2025에 액세스, https://www.reddit.com/r/learnmachinelearning/comments/14mtilm/question_about_cross_attention_in_transformer/?tl=ko
37. Cross-Attention Mechanism in Transformers - GeeksforGeeks, 8월 20, 2025에 액세스, https://www.geeksforgeeks.org/nlp/cross-attention-mechanism-in-transformers/
38. encoder-decoder attention - learnius, 8월 20, 2025에 액세스, https://learnius.com/llms/2+LLMs+and+Transformers/encoder-decoder+attention
39. 11.7. The Transformer Architecture — Dive into Deep Learning 1.0.3 documentation, 8월 20, 2025에 액세스, https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html
40. [액션파워 LAB] 트랜스포머(Transformer) 알아보기 | by ActionPower - Medium, 8월 20, 2025에 액세스, [https://actionpower.medium.com/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-transformer-%EC%95%8C%EC%95%84%EB%B3%B4%EA%B8%B0-1d595a208230](https://actionpower.medium.com/트랜스포머-transformer-알아보기-1d595a208230)
41. Deep Learning #7 Transformer - seungbeomdo - 티스토리, 8월 20, 2025에 액세스, https://seungbeomdo.tistory.com/49
42. Transformer Decoder : Encoder-Decoder Attention - 정리 - 티스토리, 8월 20, 2025에 액세스, https://better-tomorrow.tistory.com/entry/Transformer-Decoder-Encoder-Decoder-Attention
43. Attention is All You Need, 8월 20, 2025에 액세스, https://brunch.co.kr/@leadbreak/10
44. Self-Attention vs Cross-Attention: From Fundamentals to Applications | by Shawn | Medium, 8월 20, 2025에 액세스, https://medium.com/@hexiangnan/self-attention-vs-cross-attention-from-fundamentals-to-applications-4b065285f3f8
45. Attention is all you need, But which one ? | by Suparna - Medium, 8월 20, 2025에 액세스, https://medium.com/@ssuparnataneja/attention-is-all-you-need-but-which-one-14dea0e8c148
46. Why Cross-Attention is the Secret Sauce of Multimodal Models | by Jakub Strawa | Medium, 8월 20, 2025에 액세스, https://medium.com/@jakubstrawadev/why-cross-attention-is-the-secret-sauce-of-multimodal-models-f8ec77fc089b
47. Image Captioning using Transformers | by Mayank Keshari - Medium, 8월 20, 2025에 액세스, https://medium.com/@mayankkeshari34/image-captioning-using-transformers-627cd39c6b7a
48. LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models - arXiv, 8월 20, 2025에 액세스, https://arxiv.org/html/2502.02406v1
49. Multi-Modality Cross Attention Network for Image and Sentence Matching - CVF Open Access, 8월 20, 2025에 액세스, https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Multi-Modality_Cross_Attention_Network_for_Image_and_Sentence_Matching_CVPR_2020_paper.pdf
50. Cross-Attention-Based Multimodal Representation Fusion for Parametric Gait Adaptation in Complex Terrains - arXiv, 8월 20, 2025에 액세스, https://arxiv.org/html/2409.17262v2
51. Multimodal Recommendation System Based on Cross Self-Attention Fusion - MDPI, 8월 20, 2025에 액세스, https://www.mdpi.com/2079-8954/13/1/57
52. 트랜스포머(Transformer) (4) - 인코더와 디코더, 어텐션 - nongdevlog - 티스토리, 8월 20, 2025에 액세스, https://nongnongai.tistory.com/67