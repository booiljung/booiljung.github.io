# 크로스 어텐션 메커니즘

## 1.  어텐션 메커니즘의 패러다임

### 1.1  어텐션의 개념적 기원과 진화

어텐션(Attention) 메커니즘은 인간의 인지 과정에서 영감을 받은 딥러닝 기법이다. 인간은 방대한 양의 감각 정보 속에서 특정 순간에 가장 중요하고 관련성 높은 정보에 선택적으로 집중하는 능력을 지니고 있다.1 이러한 선택적 집중은 제한된 인지 자원을 효율적으로 사용하여 복잡한 문제를 해결하는 데 핵심적인 역할을 한다. 딥러닝 모델은 이 원리를 차용하여, 입력 데이터의 각 부분에 서로 다른 중요도, 즉 '가중치'를 부여함으로써 모델의 성능과 효율성을 극대화한다.1

어텐션 메커니즘이 딥러닝의 전면에 등장하게 된 배경에는 순환 신경망(Recurrent Neural Network, RNN)과 그 변형인 LSTM(Long Short-Term Memory) 기반의 순차-대-순차(Sequence-to-Sequence, Seq2Seq) 모델이 지닌 근본적인 한계가 있었다. 기계 번역이나 텍스트 요약과 같은 태스크에서 Seq2Seq 모델은 입력 시퀀스 전체를 고정된 크기의 단일 벡터, 즉 '문맥 벡터(context vector)'로 압축하는 인코더-디코더 구조를 사용했다.2 그러나 입력 시퀀스가 길어질수록 문맥 벡터에 모든 정보를 온전히 담기 어려워졌고, 특히 시퀀스 초반부의 정보가 소실되는 '정보 병목(information bottleneck)' 현상이 발생했다.1

이 문제를 해결하기 위한 돌파구로 2014년 Bahdanau 등에 의해 처음 제안된 것이 바로 어텐션 메커니즘이다.1 이 초기 모델은 디코더가 매 타임스텝에서 출력 단어를 생성할 때마다, 인코더의 모든 은닉 상태(hidden states)를 다시 참조하여 현재 생성할 단어와 가장 관련이 깊은 입력 부분에 높은 가중치를 부여했다. 이를 통해 모델은 더 이상 단 하나의 고정된 문맥 벡터에 의존하지 않고, 필요에 따라 입력 시퀀스의 특정 부분에 동적으로 '집중'할 수 있게 되었다. 이후 어텐션 메커니즘은 Bahdanau의 가법 어텐션(additive attention)에서 Vaswani 등이 2017년 "Attention is All You Need" 논문에서 제안한, 계산적으로 더 효율적인 스케일드 닷-프로덕트 어텐션(scaled dot-product attention)으로 발전하며 트랜스포머 아키텍처의 핵심 부품으로 자리 잡게 되었다.1

어텐션 메커니즘의 발전은 단순히 모델 성능 향상을 넘어, 딥러닝 모델의 작동 방식을 근본적으로 바꾸는 패러다임의 전환을 이끌었다. RNN이 시퀀스를 시간 순서에 따라 순차적으로 처리했던 것과 달리, 어텐션은 시퀀스 내 모든 요소 간의 관계를 한 번에 계산할 수 있는 병렬적, 관계 기반 처리 방식을 도입했다.1 순환(recurrence) 구조를 완전히 제거하고 어텐션만으로 모델을 구성한 트랜스포머는 이러한 병렬 처리의 잠재력을 극대화했다.2 이 구조적 변화는 GPU와 같은 병렬 컴퓨팅 하드웨어의 성능을 최대한 활용할 수 있는 길을 열었고, 이는 대규모 데이터셋을 이용한 거대 모델의 훈련을 현실적으로 만들었다. 결과적으로 어텐션의 등장은 단순한 기술 개선을 넘어, 딥러닝의 계산 패러다임을 재정의하고 '스케일링 법칙(scaling laws)'이 지배하는 현 시대 AI 연구의 방향을 결정지은 핵심적인 사건이 되었다.

### 1.2  크로스 어텐션의 정의와 핵심 역할

본 보고서의 핵심 주제인 크로스 어텐션(Cross-Attention)은 어텐션 메커니즘의 한 종류로, **두 개의 서로 다른 시퀀스 간의 정보 교량을 구축하는 메커니즘**으로 정의할 수 있다.3 이는 하나의 시퀀스가 다른 시퀀스의 정보를 선택적으로 참조하고, 정렬하며, 통합할 수 있게 하는 강력한 도구이다.6

크로스 어텐션의 역할은 인코더-디코더 구조에 빗대어 직관적으로 이해할 수 있다. 예를 들어, 영어 문장을 힌디어로 번역하는 기계 번역 시스템을 생각해보자. 인코더는 입력된 영어 문장을 읽고 그 문맥적 의미를 담은 표현(representation)의 집합을 생성한다. 디코더는 힌디어 번역문을 한 단어씩 생성해 나간다. 이때 디코더는 매 단어를 생성하는 순간마다 "지금 생성할 힌디어 단어에 가장 부합하는 영어 단어는 무엇인가?"라는 질문을 던진다. 크로스 어텐션은 바로 이 질문에 답하는 과정이다. 디코더는 자신의 현재 상태를 '질의(query)'로 삼아 인코더가 생성한 영어 문장 전체의 표현에 접근하여, 가장 관련성 높은 부분에 집중하고 그 정보를 바탕으로 다음 힌디어 단어를 생성한다.3 이처럼 크로스 어텐션은 디코더가 인코더의 출력에 동적으로 주의를 기울이게 함으로써 두 시퀀스 간의 의미적 연결을 수행한다.

이러한 특성은 단일 시퀀스 내부의 관계를 모델링하는 셀프 어텐션(self-attention)과의 근본적인 차이점을 드러낸다. 셀프 어텐션이 하나의 문장 내에서 단어들 간의 문법적, 의미적 관계를 파악하여 문맥을 이해하는 과정이라면, 크로스 어텐션은 두 개의 다른 문장(또는 이미지와 텍스트 같은 다른 종류의 데이터) 사이의 다리를 놓는 과정이다.4 이 차이점은 보고서 전반에 걸쳐 논의의 핵심적인 축을 이룰 것이다.

### 1.3  보고서의 구조와 탐구 주제

본 보고서는 크로스 어텐션에 대한 포괄적이고 심층적인 분석을 제공하는 것을 목표로 한다. 이를 위해 보고서는 다음과 같은 구조로 전개된다.

- **제2장**에서는 크로스 어텐션의 수학적 원리를 쿼리, 키, 밸류의 개념을 통해 설명하고, 스케일드 닷-프로덕트 어텐션의 계산 과정을 단계별로 분석한다.
- **제3장**에서는 크로스 어텐션과 셀프 어텐션의 근본적인 차이점을 Q, K, V의 출처, 목적, 아키텍처 내 활용 위치 등 다각도에서 비교 분석한다.
- **제4장**에서는 트랜스포머 인코더-디코더 아키텍처 내에서 크로스 어텐션이 정보 병목 현상을 해결하고 동적 정보 선택을 가능하게 하는 핵심 요소로서 어떻게 작동하는지 탐구한다.
- **제5장**에서는 멀티모달 학습, 디퓨전 모델, 그래프 신경망, 음성 처리 등 다양한 최신 응용 분야에서 크로스 어텐션이 어떻게 활용되는지 심층적으로 살펴본다.
- **제6장**에서는 크로스 어텐션의 연산 및 메모리 복잡도를 분석하고, Perceiver IO, LV-XAttn 등 이를 개선하기 위한 최신 효율화 기법들을 소개한다.
- **제7장**에서는 토큰 불균형 문제와 같은 내재적 실패 모드를 분석하고, 'Attention is Not Explanation'으로 대표되는 설명 가능성에 대한 학술적 논쟁을 조명한다.
- **마지막으로 제8장**에서는 크로스 어텐션의 핵심 기여를 요약하고, 현재의 한계를 극복하기 위한 연구 동향과 차세대 AI 아키텍처에서의 미래 역할을 전망하며 보고서를 마무리한다.

## 2.  크로스 어텐션의 작동 원리 및 수학적 기초

### 2.1  쿼리(Query), 키(Key), 밸류(Value)의 개념

크로스 어텐션 메커니즘을 이해하기 위한 첫걸음은 쿼리(Query, Q), 키(Key, K), 밸류(Value, V)라는 세 가지 핵심 요소를 파악하는 것이다. 이 개념들은 정보 검색 시스템의 작동 방식에 비유하여 효과적으로 설명할 수 있다.1

- **쿼리 (Query, Q):** 사용자가 검색 엔진에 입력하는 검색어와 같다. 이는 '내가 현재 알고 싶은 정보에 대한 질문' 또는 '관심의 초점'을 나타내는 벡터이다.11 크로스 어텐션에서는 한 시퀀스(예: 디코더의 현재 상태)가 다른 시퀀스로부터 어떤 정보를 가져올지를 결정하기 위해 생성하는 '질의 벡터'이다.3
- **키 (Key, K):** 검색 결과로 나타나는 여러 문서들의 제목이나 핵심 키워드에 해당한다. 이는 정보 저장소(예: 인코더의 출력)에 있는 각 항목들이 어떤 정보를 담고 있는지 나타내는 '색인 벡터'이다.1 쿼리는 모든 키들과의 유사도를 비교하여 어떤 정보가 자신의 질문과 가장 관련성이 높은지를 판단한다.
- **밸류 (Value, V):** 사용자가 제목(키)을 클릭했을 때 보게 되는 문서의 실제 내용이다. 이는 키와 연관된 '실제 정보 벡터'이다.1 쿼리와 키의 유사도 계산을 통해 얻어진 가중치는 이 밸류 벡터들에 적용되어, 최종적으로 어떤 정보를 얼마나 가져올지를 결정한다.

셀프 어텐션에서는 Q, K, V가 모두 동일한 입력 시퀀스로부터 파생되지만, 크로스 어텐션의 핵심적인 특징은 이들이 서로 다른 두 개의 시퀀스에서 비롯된다는 점이다.13 구체적으로, 

**쿼리(Q)는 정보를 필요로 하는 시퀀스(예: 디코더)에서 생성**되고, **키(K)와 밸류(V)는 정보의 원천이 되는 시퀀스(예: 인코더)에서 생성**된다.3

이 Q, K, V 벡터들은 단순히 입력 임베딩 그 자체가 아니라, 각각 별도의 학습 가능한 가중치 행렬 $W^Q$, $W^K$, $W^V$를 통해 선형 변환(linear transformation)된 결과물이다.14 예를 들어, 쿼리 시퀀스의 입력 행렬을 $X_{query}$라 하고 키/밸류 시퀀스의 입력 행렬을 $X_{source}$라 할 때, $Q$, $K$, $V$는 다음과 같이 계산된다.
$$
Q = X_{query}W^Q \\
K = X_{source}W^K \\
V = X_{source}W^V \\
$$
이 선형 변환 과정은 단순한 차원 맞추기를 넘어서는 중요한 의미를 지닌다. 이는 동일한 입력 벡터라 할지라도, 서로 다른 가중치 행렬을 통해 각기 다른 '표현 공간(representation space)'으로 투영(projection)됨을 의미한다. 즉, 모델은 학습을 통해 하나의 정보 덩어리를 목적에 맞게 '질문용 표현(Q)', '색인용 표현(K)', '내용용 표현(V)'이라는 세 가지 다른 역할로 분화시키는 법을 배운다.9 예를 들어, 

$W^Q$는 문맥상 질문을 던지는 데 중요한 특징을 추출하도록, $W^K$는 그 질문에 답이 될 만한 특징을 부각하도록 학습될 수 있다. 이처럼 선형 변환은 어텐션이 단순한 벡터 유사도 계산을 넘어, 학습된 '관점'에 따라 동적으로 관계를 설정하는 고차원적인 메커니즘으로 작동하게 하는 근본적인 장치이다.

### 2.2  스케일드 닷-프로덕트 어텐션

트랜스포머에서 사용되는 표준적인 어텐션 계산 방식은 '스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)'이다. 크로스 어텐션 역시 이 방식을 따르며, 계산 과정은 다음과 같은 4개의 논리적 단계로 구성된다.1

1. **유사도 계산 (Similarity Calculation):** 첫 단계는 쿼리가 각각의 키와 얼마나 유사한지를 측정하는 것이다. 이는 쿼리 행렬 $Q$와 키 행렬의 전치 $K^T$를 행렬 곱(dot product)하여 계산된다. 결과적으로 생성되는 행렬의 각 원소 $(i, j)$는 $i$번째 쿼리와 $j$번째 키 사이의 유사도 점수(attention score)를 나타낸다.
2. **스케일링 (Scaling):** 키(그리고 쿼리) 벡터의 차원 $d_k$가 클 경우, 내적 값의 크기가 매우 커질 수 있다. 이 값이 너무 크면 뒤따르는 소프트맥스 함수의 입력값으로 들어갔을 때, 함수의 기울기(gradient)가 거의 0에 가까워지는 영역으로 밀려나 학습이 불안정해지는 문제가 발생한다. 이를 방지하기 위해, 계산된 유사도 점수 행렬 전체를 $d_k$의 제곱근($\sqrt{d_k}$)으로 나누어준다. 이 스케일링 과정은 학습의 안정성을 확보하는 데 중요한 역할을 한다.1
3. **소프트맥스 (Softmax):** 스케일링된 점수 행렬에 소프트맥스(softmax) 함수를 행(row) 단위로 적용한다. 이 과정을 통해 각 행의 모든 값들은 0과 1 사이의 값으로 정규화되며, 각 행의 합은 1이 된다. 이렇게 생성된 행렬이 바로 '어텐션 가중치(attention weights)' 행렬이다. $i$번째 행은 $i$번째 쿼리가 모든 키(그리고 그에 해당하는 밸류)에 대해 얼마나 '집중'해야 하는지를 나타내는 확률 분포로 해석할 수 있다.1
4. **가중합 (Weighted Sum):** 마지막으로, 계산된 어텐션 가중치 행렬을 밸류 행렬 $V$와 곱한다. 이 연산은 각 쿼리에 대해 모든 밸류 벡터들의 가중 평균(weighted average)을 구하는 것과 같다. 어텐션 가중치가 높은 밸류 벡터는 최종 결과에 더 큰 영향을 미치고, 가중치가 낮은 밸류 벡터의 영향력은 줄어든다. 이렇게 계산된 최종 행렬이 어텐션 계층의 출력이 된다.1

이 전체 과정은 다음의 간결한 수식으로 표현된다.13
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

### 2.3  멀티-헤드 어텐션으로의 확장

단일 어텐션 메커니즘은 한 번에 한 가지 종류의 관계만을 학습하는 경향이 있다. 예를 들어, 어떤 어텐션은 주로 구문적 관계에 집중하고, 다른 어텐션은 의미적 유사성에 집중할 수 있다. 모델이 이처럼 다양한 관점의 관계를 동시에 포착할 수 있도록 고안된 것이 '멀티-헤드 어텐션(Multi-Head Attention)'이다.18

멀티-헤드 어텐션의 핵심 아이디어는 Q, K, V를 $h$개의 작은 조각(head)으로 나누어, 각각에 대해 독립적으로 스케일드 닷-프로덕트 어텐션을 병렬적으로 수행하는 것이다.13 구체적인 과정은 다음과 같다.

1. **선형 투영 (Linear Projection):** 먼저, 전체 차원의 Q, K, V 행렬을 각각 $h$개의 서로 다른 학습 가능한 선형 투영(가중치 행렬 $W_i^Q, W_i^K, W_i^V$)을 통해 더 낮은 차원의 Q, K, V로 변환한다. 이 과정을 통해 $h$개의 '헤드'가 생성된다.
2. **병렬 어텐션 계산 (Parallel Attention Calculation):** $h$개의 헤드 각각에 대해 독립적으로 스케일드 닷-프로덕트 어텐션을 수행한다. 각 헤드는 입력의 서로 다른 표현 부분 공간(representation subspace)에 주의를 기울이게 되어, 다양한 종류의 관계를 학습할 수 있다.13
3. **결합 및 최종 투영 (Concatenation and Final Projection):** $h$개 헤드의 출력 결과들을 모두 연결(concatenate)하여 다시 원래의 차원으로 합친다. 그 후, 또 다른 학습 가능한 가중치 행렬 $W^O$를 사용하여 최종 선형 투영을 거쳐 멀티-헤드 어텐션 계층의 최종 출력을 생성한다.

이 과정은 다음 수식으로 요약될 수 있다.10
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
\\
\text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$
멀티-헤드 구조는 모델의 표현력을 크게 향상시킨다. 각 헤드가 특정 역할(예: 주어-동사 관계 추적, 대명사 해석, 동의어 관계 파악 등)에 전문화될 수 있는 일종의 '앙상블' 효과를 제공함으로써, 모델이 입력 데이터에 대한 더 풍부하고 다각적인 이해를 형성하도록 돕는다.13

## 3.  셀프 어텐션과의 비교 분석

크로스 어텐션의 본질을 더 깊이 이해하기 위해서는 그 사촌 격인 셀프 어텐션(Self-Attention)과의 비교가 필수적이다. 두 메커니즘은 동일한 스케일드 닷-프로덕트 계산 방식을 공유하지만, 그 목적과 작동 방식에서 근본적인 차이를 보인다.

### 3.1  Q, K, V의 출처: 근본적 차이

두 어텐션 메커니즘의 가장 핵심적이고 근본적인 차이는 쿼리(Q), 키(K), 밸류(V) 벡터가 어디에서 오는가에 있다.4

- **셀프 어텐션 (Self-Attention):** 이름에서 알 수 있듯이, '자기 자신'에게 주의를 기울이는 방식이다. Q, K, V가 모두 **동일한 단일 입력 시퀀스**에서 파생된다.3 예를 들어, "The cat sat on the mat"이라는 문장을 처리할 때, "sat"이라는 단어의 문맥적 의미를 파악하기 위해 "The", "cat", "on", "the", "mat" 등 문장 내 모든 단어(자기 자신 포함)와의 관계를 계산한다. 이 과정은 시퀀스 내부의 요소들이 서로를 참조하여 문맥적 표현을 구축하는 과정이다.9
- **크로스 어텐션 (Cross-Attention):** 두 개의 서로 다른 시퀀스를 '교차'하여 주의를 기울이는 방식이다. **Q는 하나의 시퀀스(보통 타겟 시퀀스)에서, K와 V는 다른 시퀀스(소스 시퀀스)에서 파생**된다.3 예를 들어, 독일어 문장 "Der Hund"를 영어로 번역하는 과정에서 "dog"라는 단어를 생성할 때, 디코더는 "dog"에 해당하는 쿼리를 생성하여 인코딩된 소스 문장 "Der Hund"에서 파생된 키와 밸류를 참조한다. 이 과정에서 "Hund"라는 단어에 높은 어텐션 가중치가 부여되어 번역의 정확성을 높인다.9

### 3.2  목적의 차이: 내부 문맥 형성 vs. 외부 정보 통합

Q, K, V의 출처 차이는 두 메커니즘의 근본적인 목적 차이로 이어진다.

- **셀프 어텐션의 목적:** 시퀀스 **내부의 문맥을 형성(Internal Contextualization)**하는 것이다. 각 토큰이 시퀀스 내 다른 모든 토큰과의 관계를 파악하게 함으로써, 문맥에 따라 의미가 달라지는 다의어 문제를 해결하고, 문장 내의 장거리 의존성(long-range dependencies)을 효과적으로 포착한다.4 최종적으로 각 토큰은 전체 시퀀스의 정보를 압축한 풍부한 문맥적 표현(context-rich representation)을 갖게 된다. 이는 일종의 '자기 이해(self-understanding)' 과정에 비유할 수 있다.9
- **크로스 어텐션의 목적:** **외부 정보를 통합(External Information Integration)**하는 것이다. 이는 두 개의 서로 다른 정보 스트림 간의 정렬(alignment), 융합(fusion), 그리고 접지(grounding)를 수행하는 데 사용된다.3 기계 번역에서는 소스 언어와 타겟 언어 간의 단어 정렬을, 이미지 캡셔닝에서는 텍스트와 이미지 영역 간의 접지를 수행한다. 이는 '외부 참조(external reference)'를 통해 한 정보를 다른 정보의 맥락에서 해석하고 통합하는 과정이다.9

### 3.3  아키텍처 내 활용 위치

이러한 목적의 차이는 트랜스포머와 같은 표준 아키텍처 내에서 두 메커니즘이 배치되는 위치의 차이로 나타난다.

- **셀프 어텐션:** 주로 트랜스포머의 **인코더** 블록 전체와 **디코더** 블록의 첫 번째 하위 계층에서 사용된다.4 인코더에서는 입력 시퀀스 전체의 문맥적 관계를 파악하는 데 사용되며, 디코더의 첫 번째 하위 계층에서는 이미 생성된 출력 시퀀스의 내부적인 문법적, 의미적 일관성을 유지하기 위해 사용된다(이때 미래 토큰을 보지 못하도록 마스킹이 적용된다).9
- **크로스 어텐션:** 트랜스포머의 **디코더** 블록의 두 번째 하위 계층에서만 사용된다.4 이 위치는 디코더가 자신의 내부 문맥(셀프 어텐션을 통해 파악한)을 바탕으로, 인코더가 처리한 전체 입력 시퀀스의 정보를 효과적으로 활용할 수 있게 하는 전략적인 배치이다. 이 계층이 바로 인코더와 디코더를 연결하는 핵심적인 '다리' 역할을 수행하는 곳이다.8

이러한 두 메커니즘의 관계는 독립적인 모듈이 아니라, 트랜스포머 디코더 내에서 상호 보완적인 역할을 수행하는 '협력적 메커니즘'으로 이해해야 한다. 디코더가 고품질의 결과물을 생성하기 위해서는 이 둘의 조화가 필수적이다. 첫 번째 하위 계층인 마스크드 셀프 어텐션은 "지금까지 내가 무슨 말을 생성했는가?"를 파악하여 내부적인 일관성을 유지하게 해준다. 만약 이 과정이 없다면, 디코더는 소스 문장의 단어들을 참조할 수는 있지만, 이미 생성한 단어들과의 문맥적 연결을 고려하지 못해 문법적으로 어색하거나 부자연스러운 문장을 만들 위험이 있다. 그 다음, 두 번째 하위 계층인 크로스 어텐션은 "이 일관성을 유지하면서, 원문(소스)의 어떤 부분을 참조하여 다음 말을 해야 하는가?"를 결정하여 외부 정보와의 정합성을 보장한다. 만약 이 과정이 없다면, 디코더는 문법적으로는 완벽하지만 소스 문장의 내용과는 전혀 관련 없는 문장을 생성하는 '자유로운 글짓기'를 하게 될 것이다. 따라서, 셀프 어텐션이 '내부 문맥'을, 크로스 어텐션이 '외부 문맥'을 담당하며, 이 둘의 순차적이고 협력적인 작동이 고품질의 Seq2Seq 생성을 가능하게 하는 핵심 원리이다.3

### 3.4  핵심 비교 테이블

셀프 어텐션과 크로스 어텐션의 주요 차이점을 요약하면 다음 표와 같다.

| 특징 (Aspect)        | 셀프 어텐션 (Self-Attention)                                 | 크로스 어텐션 (Cross-Attention)                              |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **입력 유형**        | 단일 시퀀스 (Single Sequence)                                | 두 개의 서로 다른 시퀀스 (Two different sequences)           |
| **Q, K, V 출처**     | Q, K, V 모두 동일한 시퀀스에서 파생                          | Q는 타겟 시퀀스, K와 V는 소스 시퀀스에서 파생                |
| **주요 목적**        | 시퀀스 **내부**의 문맥적 관계 및 의존성 포착 (Contextual Encoding) | 두 시퀀스 **간의** 관계 정렬 및 정보 융합 (Alignment & Fusion) |
| **주요 기능**        | 각 토큰이 전체 시퀀스를 인식하여 문맥적으로 풍부한 임베딩 생성 | 타겟 시퀀스 생성 시 소스 시퀀스의 관련 부분에 집중           |
| **주요 사용 위치**   | 인코더 전체 계층, 디코더의 첫 번째 하위 계층                 | 디코더의 두 번째 하위 계층 (인코더-디코더 어텐션)            |
| **대표적 사용 사례** | BERT (문맥 이해), GPT (텍스트 생성), ViT (이미지 내 공간 관계) | 기계 번역, 텍스트 요약, 이미지 캡셔닝, VQA, 텍스트-이미지 생성 |
| **연산 복잡도**      | $O(N^2 \cdot d)$ (N: 시퀀스 길이, d: 임베딩 차원)            | $O(M \cdot N \cdot d)$ (M: 타겟 시퀀스 길이, N: 소스 시퀀스 길이) |

이 표는 두 메커니즘의 핵심적인 차원들을 명확하게 대비시켜, 복잡한 개념을 구조적으로 이해하는 데 도움을 준다.4

## 4.  트랜스포머 아키텍처의 핵심 요소로서의 크로스 어텐션

### 4.1  인코더-디코더 구조와 정보의 흐름

크로스 어텐션은 트랜스포머 아키텍처, 특히 인코더-디코더 구조에서 정보의 흐름을 혁신적으로 바꾼 핵심 요소이다. 이를 이해하기 위해서는 먼저 전통적인 RNN 기반 인코더-디코더 모델의 한계를 되짚어볼 필요가 있다. 기존 모델들은 인코더가 입력 시퀀스(예: "나는 학생입니다") 전체를 순차적으로 처리한 후, 모든 정보를 고정된 크기의 단일 문맥 벡터(context vector) 하나로 압축했다.2 디코더는 오직 이 하나의 벡터에만 의존하여 출력 시퀀스(예: "I am a student")를 생성해야 했다. 이 방식은 입력 시퀀스가 길어질수록 초반부의 중요한 정보가 소실되거나 희석되는 심각한 '정보 병목' 현상을 초래했다.1

트랜스포머는 이 문제를 근본적으로 해결했다. 트랜스포머의 인코더는 셀프 어텐션을 통해 입력 시퀀스의 각 토큰에 대한 문맥화된 표현(contextualized representation)을 계산한다. 중요한 점은, 인코더가 최종적으로 출력하는 것이 단일 벡터가 아니라, 입력 시퀀스의 길이와 동일한 개수의 문맥화된 벡터들의 '집합', 즉 행렬이라는 것이다.2 이 행렬에는 입력 시퀀스의 모든 부분에 대한 풍부한 정보가 위치 정보와 함께 보존되어 있다.

바로 이 지점에서 크로스 어텐션이 결정적인 '다리' 역할을 수행한다. 디코더는 매 타임스텝에서 다음 토큰을 생성할 때마다, 크로스 어텐션 메커니즘을 통해 인코더가 출력한 이 표현 행렬 전체에 직접 접근할 수 있다.6 디코더는 자신의 현재 상태를 쿼리로 삼아, 인코더 출력 전체(키와 밸류)를 훑어보고 현재 예측에 가장 필요한 정보를 동적으로 선택하여 가져온다. 이로써 정보는 더 이상 하나의 좁은 파이프라인을 통과할 필요 없이, 필요할 때마다 원본 소스에서 직접 참조될 수 있게 되어 정보 병목 현상이 근본적으로 해소되었다.1

### 4.2  디코딩 과정에서의 동적 정보 선택

트랜스포머 디코더가 크로스 어텐션을 통해 정보를 동적으로 선택하는 과정은 매우 정교하게 설계되어 있다. 기계 번역을 예로 들어, "나는 배드민턴을 치러 간다"라는 문장을 "I am going to play badminton"으로 번역하는 과정을 단계별로 살펴보자.7

1. **디코더의 내부 문맥 파악:** 디코더가 "I am going to play"까지 생성했다고 가정하자. 디코더는 먼저 이 생성된 시퀀스를 입력으로 받아 '마스크드 셀프 어텐션'을 수행한다. 이를 통해 "play"라는 동사 다음에는 운동 이름과 같은 명사가 올 확률이 높다는 등의 내부적인 문맥과 문법적 구조를 파악한다.8
2. **쿼리 생성:** 셀프 어텐션 계층을 통과한 "play"의 표현 벡터는 다음 계층인 크로스 어텐션의 **쿼리(Q)**가 된다. 이 쿼리 벡터는 "나는 지금 운동과 관련된 단어를 생성해야 하는데, 원본 한국어 문장에서 어떤 운동에 대한 정보가 있는가?"라는 의미론적 질문을 인코딩하고 있다.7
3. **키와 밸류 제공:** 한편, 인코더는 이미 원본 한국어 문장 "나는 배드민턴을 치러 간다"를 처리하여 각 단어에 대한 문맥화된 표현 벡터들의 집합을 만들어 두었다. 이 전체 집합이 크로스 어텐션의 **키(K)와 밸류(V)**로 제공된다.13
4. **어텐션 스코어 계산 및 정보 추출:** 디코더의 쿼리("운동 관련 정보?")는 인코더의 모든 키("나", "배드민턴", "치러", "간다" 등)와 유사도를 계산한다. 이 과정에서 "배드민턴"이라는 키가 가장 높은 유사도 점수를 얻게 되고, 따라서 가장 높은 어텐션 가중치를 부여받는다.7
5. **다음 토큰 예측:** 이 높은 가중치는 "배드민턴"에 해당하는 밸류 벡터에 곱해진다. 결과적으로, 디코더는 원본 문장에서 "배드민턴"이라는 핵심 정보를 담은 문맥 벡터를 생성하게 된다. 이 정보를 바탕으로 디코더의 최종 출력 계층은 다음 토큰으로 "badminton"을 예측할 확률을 높인다.7

이 과정은 디코더가 다음 토큰을 생성할 때마다 반복된다. 이를 통해 디코더는 단순히 순서대로 정보를 처리하는 것이 아니라, 매 순간 가장 필요한 정보를 소스 시퀀스로부터 능동적으로 찾아와 활용하는 동적인 정보 선택 능력을 갖추게 된다.

### 4.3  Seq2Seq 태스크에서의 의의

크로스 어텐션의 이러한 동적 정보 선택 능력은 다양한 Seq2Seq 태스크에서 혁신적인 성능 향상을 가져왔다.

- **기계 번역 (Machine Translation):** 크로스 어텐션은 소스 언어와 타겟 언어 간의 복잡하고 비선형적인 단어 정렬(alignment)을 학습할 수 있게 해준다. 예를 들어, 어순이 완전히 다른 언어(예: 영어 SVO, 한국어 SOV)를 번역할 때, 디코더는 문법적 구조에 얽매이지 않고 의미적으로 대응되는 소스 단어를 정확히 찾아 참조할 수 있다.3
- **텍스트 요약 (Text Summarization):** 긴 원문에서 중요한 내용은 여러 문단에 흩어져 있을 수 있다. 크로스 어텐션은 요약문을 생성하는 디코더가 원문의 특정 한 부분만이 아니라, 여러 핵심 문장이나 구절에 동시에 주의를 기울여 종합적인 요약을 생성하도록 돕는다.6
- **질의응답 (Question Answering):** 주어진 질문(Question)을 쿼리로, 참조해야 할 긴 본문(Context)을 키와 밸류로 사용하는 구조를 통해, 모델은 질문에 대한 답을 담고 있는 본문의 특정 구절이나 문장을 정확하게 찾아내는 데 크로스 어텐션을 핵심적으로 활용한다.6

이처럼 크로스 어텐션은 두 시퀀스 간의 유연하고 동적인 상호작용을 가능하게 함으로써, 기존 모델들이 어려움을 겪었던 복잡한 매핑 문제를 해결하는 강력한 도구로 자리매김했다.

더 나아가, 크로스 어텐션은 모델의 작동 방식을 엿볼 수 있는 '해석 가능성(interpretability)'의 창구를 제공한다는 점에서 중요한 의의를 갖는다. 어텐션 가중치 행렬을 시각화(heatmap)하면, 디코더가 특정 출력 단어를 생성할 때 어떤 입력 단어에 집중했는지를 직관적으로 확인할 수 있다.9 예를 들어, 번역 모델의 어텐션 맵은 소스 언어와 타겟 언어 단어 간의 정렬 관계를 명확하게 보여준다. 이러한 시각화는 모델이 왜 특정 예측을 했는지에 대한 단서를 제공하며, 오류 분석 및 디버깅에 매우 유용하다.6 하지만 이 직관적인 '설명'이 모델의 실제 인과적 추론 과정을 얼마나 충실하게 반영하는지에 대해서는 깊은 학술적 논쟁이 존재하며, 이는 'Attention is Not Explanation'이라는 주제로 이어졌다. 이처럼 크로스 어텐션은 성능 향상과 더불어 모델 해석 가능성에 대한 중요한 화두를 던졌다는 양면성을 지닌다. 이 논쟁은 제7장에서 더 상세히 다룰 것이다.

## 5.  핵심 응용 분야별 심층 분석

크로스 어텐션의 본질은 '질의 기반 정보 필터링 및 통합' 메커니즘이다. 이 추상적이고 강력한 기능 덕분에, 크로스 어텐션은 특정 도메인에 국한되지 않고, 서로 다른 두 정보 소스 간의 유의미한 상호작용이 필요한 거의 모든 문제에 적용될 수 있는 '범용 인터페이스(universal interface)'로 자리 잡았다. 기계 번역과 같은 전통적인 NLP 태스크를 넘어, 멀티모달 학습, 이미지 생성, 그래프 분석 등 다양한 AI 분야에서 혁신을 이끌고 있다.

### 5.1  멀티모달 학습의 교두보

현실 세계의 정보는 텍스트, 이미지, 소리 등 다양한 양식(modality)으로 존재한다. 멀티모달 학습은 이러한 이질적인 데이터를 통합적으로 이해하고 처리하는 것을 목표로 하며, 크로스 어텐션은 이 분야의 핵심 기술, 즉 '교두보' 역할을 수행한다.14 크로스 어텐션은 서로 다른 임베딩 공간에 존재하는 각 모달리티의 표현을 효과적으로 정렬(align)하고 융합(fuse)하는 다리를 놓는다.17

- **이미지 캡셔닝 및 시각 질의응답 (VQA):** 이 분야는 크로스 어텐션의 멀티모달 적용 가능성을 명확하게 보여주는 고전적인 예시다.
  - **작동 원리:** 모델은 먼저 이미지를 CNN이나 Vision Transformer(ViT)와 같은 비전 인코더를 통해 여러 개의 지역적 특징 벡터(예: 이미지 패치 임베딩)의 집합으로 변환한다. 텍스트(생성 중인 캡션 또는 주어진 질문)는 텍스트 인코더를 통해 토큰 임베딩으로 변환된다.4
  - **정보 융합:** 캡션의 다음 단어를 생성하거나 질문에 답하기 위해, 텍스트 임베딩이 **쿼리(Q)**가 되고, 이미지 특징 벡터들의 집합이 **키(K)와 밸류(V)**가 된다. 모델은 텍스트 쿼리를 통해 이미지의 어떤 영역이 현재 문맥과 가장 관련이 깊은지 '질문'한다. 예를 들어, "잔디밭 위의 개"라는 캡션을 생성하는 중 "개"라는 단어를 예측할 차례라면, "개"에 해당하는 쿼리는 이미지에서 실제 개의 모습이 있는 패치(키)에 높은 어텐션 가중치를 부여하게 된다. 이 가중치를 통해 해당 이미지 영역의 정보(밸류)가 디코더로 전달되어 "개"라는 단어가 생성된다.9
- **최신 멀티모달 모델 (CLIP, Flamingo, GPT-4V):** 이들 대규모 멀티모달 모델들은 크로스 어텐션 또는 그와 유사한 상호작용 메커니즘을 더욱 정교하게 활용하여 텍스트와 이미지 간의 깊은 의미론적 연결을 학습한다.
  - **CLIP**은 직접적인 생성 모델은 아니지만, 학습 과정에서 이미지와 텍스트 쌍의 유사도를 최대화하기 위해 두 모달리티의 표현을 같은 공간으로 정렬하는데, 이는 크로스 어텐션의 정렬(alignment) 개념과 맞닿아 있다.9
  - **Flamingo**와 같은 모델은 사전 학습된 강력한 대규모 언어 모델(LLM)을 기반으로, 중간중간에 새로운 '게이트가 있는 크로스 어텐션(gated cross-attention)' 레이어를 삽입한다. 이 레이어에서 텍스트(LLM의 은닉 상태)가 쿼리가 되고, 이미지 인코더로부터 추출된 시각적 특징이 키와 밸류가 되어, 언어 모델이 텍스트를 생성하는 과정에서 시각적 정보를 자연스럽게 참조할 수 있도록 한다.23

### 5.2  디퓨전 모델에서의 조건부 생성

Stable Diffusion, DALL-E, Midjourney와 같은 텍스트-이미지 생성 디퓨전 모델의 경이로운 성능 뒤에는 크로스 어텐션이 '조건(conditioning)'을 주입하는 핵심 메커니즘으로 작동하고 있다.23 디퓨전 모델은 순수한 노이즈로부터 점진적으로 노이즈를 제거(denoising)하여 이미지를 생성하는데, 이때 크로스 어텐션은 매 단계에서 텍스트 프롬프트의 의미가 생성 과정에 반영되도록 안내하는 '조종간' 역할을 한다.

- **작동 원리:**
  - **쿼리 (Query, Q):** 노이즈 제거 과정에 있는 U-Net 아키텍처의 중간 단계 이미지 잠재 표현(latent image features)이 쿼리가 된다. 이 쿼리는 이미지의 각 공간적 위치(픽셀 그룹)에 대한 표현이다.23
  - **키 (Key, K)와 밸류 (Value, V):** 사용자가 입력한 텍스트 프롬프트(예: "a red apple on the table")는 CLIP과 같은 강력한 텍스트 인코더를 통해 일련의 토큰 임베딩으로 변환된다. 이 임베딩 시퀀스가 키와 밸류의 소스가 된다.23
  - **조건 주입 과정:** 노이즈 제거 U-Net의 각 블록에서, 이미지 잠재 공간의 각 위치(쿼리)는 텍스트 프롬프트의 모든 단어(키)와 어텐션 스코어를 계산한다. 이는 "이 픽셀 영역은 프롬프트의 어떤 단어와 가장 관련이 있는가?"라고 묻는 것과 같다. "빨간 사과" 프롬프트가 주어졌을 때, 이미지의 특정 영역은 'red'와 'apple' 토큰에 높은 어텐션 가중치를 부여받게 된다. 이 가중치를 통해 'red'와 'apple'의 의미 정보(밸류)가 해당 이미지 영역의 표현에 주입되고, U-Net은 이 정보를 바탕으로 다음 단계에서 해당 영역을 붉은색의 사과 형태로 다듬어 나간다.23

이 메커니즘은 U-Net의 여러 해상도 레벨에서 반복적으로 적용된다. 초기 단계에서는 이미지의 전반적인 구도와 색감(예: '푸른 하늘')을 결정하는 데 사용되고, 후기 단계에서는 더 세부적인 객체의 형태와 질감(예: '사과의 광택')을 묘사하는 데 사용된다. 이처럼 크로스 어텐션은 텍스트의 추상적인 의미를 이미지의 구체적인 픽셀 수준까지 정교하게 연결하는 핵심적인 역할을 수행한다.23

### 5.3  그래프 신경망(GNN)으로의 확장

그래프 구조 데이터는 노드(개체)와 엣지(관계)로 구성되며, 소셜 네트워크, 분자 구조, 지식 그래프 등 다양한 분야에서 활용된다. 그래프 신경망(GNN)은 이러한 데이터를 처리하는 데 특화된 모델이다. 최근 연구에서는 크로스 어텐션을 GNN에 도입하여 노드의 고유 특징(node features)과 그래프의 구조적 정보(topological information)라는 두 가지 다른 정보 소스를 효과적으로 융합하려는 시도가 이루어지고 있다.27

- **GTAT (Graph Topology Attention Networks) 사례:**
  - **두 가지 정보 소스:** GTAT는 각 노드에 대해 두 가지 종류의 표현을 사용한다. 하나는 노드 자체의 속성을 나타내는 '노드 특징 표현'이고, 다른 하나는 그래프 내에서 노드의 국소적 연결 구조(예: 노드가 삼각형의 일부인지, 사슬의 끝에 있는지 등)를 나타내는 '위상 특징 표현'이다. 이 위상 특징은 그래프렛 차수 벡터(Graphlet Degree Vector, GDV)와 같은 기법으로 추출된다.27
  - **그래프 크로스 어텐션 (GCA) 레이어:** GTAT의 핵심은 GCA 레이어이다. 이 레이어는 두 표현을 '교차'하여 업데이트한다. 즉, 한 노드의 **노드 특징을 업데이트**할 때는 이웃 노드들과의 **위상적 유사도**에 기반한 어텐션 가중치를 사용한다. 반대로, **위상 특징을 업데이트**할 때는 이웃 노드들과의 **특징적 유사도**에 기반한 어텐션 가중치를 사용한다.27
  - **효과:** 이 교차 업데이트 방식을 통해, 특징이 유사한 노드들은 서로의 위상 정보를 공유하게 되고, 위상적으로 유사한 위치에 있는 노드들은 서로의 특징 정보를 공유하게 된다. 이는 노드의 의미(feature)와 구조적 역할(topology)이 상호 보완적으로 학습되도록 유도하여, 최종적인 노드 표현력을 크게 향상시킨다.27

### 5.4  음성 처리 및 강화학습

- **음성 처리 (OpenAI Whisper):** OpenAI의 Whisper는 매우 정확한 음성-텍스트 변환(Speech-to-Text) 성능으로 잘 알려져 있으며, 그 구조의 핵심에는 트랜스포머 인코더-디코더와 크로스 어텐션이 있다.29
  - **인코더:** 입력된 음성 신호를 30초 단위로 잘라 멜-주파수 스펙트로그램(MFC)이라는 시각적 표현으로 변환하고, 이를 처리하여 음성 특징의 시퀀스를 생성한다.30
  - **디코더:** 텍스트 토큰을 하나씩 생성해 나갈 때, 크로스 어텐션을 통해 인코더가 출력한 전체 오디오 특징 시퀀스를 참조한다.30 예를 들어, 특정 단어를 전사(transcribe)해야 할 때, 디코더는 해당 단어가 발음된 오디오의 특정 시간대 구간에 높은 어텐션을 부여하여 정확한 텍스트를 생성한다. Whisper의 크로스 어텐션은 시간적 정렬이 잘 이루어지는 특성을 보여, 스트리밍 음성 인식 연구에도 활용되고 있다.32
- **강화학습 (Reinforcement Learning):** 강화학습에서 에이전트는 환경과의 상호작용을 통해 보상을 최대화하는 행동 정책을 학습한다. 어텐션, 특히 크로스 어텐션은 에이전트의 의사결정 과정과 학습 메커니즘을 정교화하는 데 사용될 수 있다.
  - **의사결정에서의 선택적 집중:** 시각적 입력(환경 상태)을 받는 에이전트의 경우, 어텐션을 사용하여 현재 과제 해결에 중요한 화면의 특정 영역(예: 다가오는 적, 획득해야 할 아이템)에 집중하고, 관련 없는 배경 정보는 무시할 수 있다. 이는 에이전트가 더 효율적이고 강건한 정책을 학습하도록 돕는다.34 일부 연구에서는 에이전트가 "어디를 볼 것인가(where)"와 "무엇을 볼 것인가(what)"를 분리하여 쿼리하는 방식으로 어텐션을 활용하기도 한다.34
  - **자기 지도 보상 생성:** 대규모 언어 모델(LLM)을 강화학습으로 미세 조정할 때, 생성된 응답이 원래 프롬프트에 얼마나 충실한지를 평가하는 보상 신호를 사람이 일일이 제공하기는 어렵다. 한 연구에서는 크로스 어텐션 가중치를 분석하여 이 보상 신호를 자동으로 생성하는 방법을 제안했다.35 생성된 응답(쿼리)이 원래 프롬프트(키, 밸류)의 모든 중요한 부분을 적절히 참조했는지(coverage), 특정 부분에만 과도하게 집중하지는 않았는지(focus) 등을 어텐션 분포를 통해 측정하여, 이를 보상 함수로 활용하는 것이다.35

이처럼 크로스 어텐션의 범용성은 다양한 AI 분야에서 새로운 가능성을 열어주고 있다. 이는 크로스 어텐션이 특정 모달리티를 위한 기술이 아니라, 이종(heterogeneous) 데이터 소스를 연결하는 근본적인 연산 프리미티브(computational primitive)임을 보여준다. 앞으로 로보틱스, 신약 개발, 금융 분석 등 더욱 복잡한 데이터를 다루는 분야에서 크로스 어텐션의 역할은 더욱 중요해질 것이다.

## 6.  성능, 효율성, 그리고 최적화

크로스 어텐션은 강력한 성능을 제공하지만, 그 이면에는 상당한 계산 비용이라는 문제가 존재한다. 특히 입력 시퀀스의 길이가 길어질수록 연산량과 메모리 사용량이 급격히 증가하여 실제 응용에서 병목이 되기도 한다. 이러한 한계를 극복하기 위해 다양한 효율적 어텐션 기법들이 활발히 연구되고 있다.

### 6.1  연산 및 메모리 복잡도 분석

크로스 어텐션의 계산적 부담을 이해하기 위해서는 시간 복잡도와 공간 복잡도를 분석해야 한다.

- **시간 복잡도 (Time Complexity):** 크로스 어텐션의 주요 연산은 쿼리 행렬 $Q$ (크기 $M \times d$)와 키 행렬 $K^T$ (크기 $d \times N$)의 행렬 곱이다. 이 연산의 복잡도는 $O(M \cdot N \cdot d)$이다. 여기서 $M$은 쿼리 시퀀스의 길이, $N$은 키/밸류 시퀀스의 길이, $d$는 임베딩 차원을 의미한다.22 이후 소프트맥스와 밸류 행렬과의 곱셈 역시 동일한 복잡도를 가진다. 따라서 크로스 어텐션의 전체 시간 복잡도는 $O(M \cdot N \cdot d)$로 결정된다.
- **메모리 복잡도 (Space Complexity):** 가장 큰 메모리 요구 사항은 $M \times N$ 크기의 어텐션 스코어 행렬을 저장하는 데서 발생한다. 따라서 메모리 복잡도는 $O(M \cdot N)$에 비례한다.6

이 복잡도는 셀프 어텐션의 복잡도 $O(N^2 \cdot d)$와 비교하여 그 특성을 파악할 수 있다. 만약 두 시퀀스의 길이가 비슷하다면($M \approx N$), 두 메커니즘의 복잡도는 거의 같다. 그러나 $M$과 $N$의 크기가 비대칭적인 경우, 상황이 달라진다. 예를 들어, 멀티모달 모델에서 텍스트 프롬프트(쿼리, $M$이 작음)가 고해상도 이미지의 패치(키/밸류, $N$이 매우 큼)를 참조하는 경우, $M \cdot N$ 항이 매우 커져 크로스 어텐션이 계산 병목이 될 수 있다.38 반대로, Perceiver와 같이 매우 긴 입력 시퀀스($M$)가 짧은 잠재 벡터($N$)를 참조하는 경우에는 셀프 어텐션보다 훨씬 효율적일 수 있다.9

### 6.2  효율적 어텐션 기법 연구

표준 어텐션의 이차적 복잡도 문제를 해결하기 위해, 다양한 효율화 기법들이 제안되었다. 이들은 크게 어텐션 계산 방식을 근사하거나, 계산 구조를 최적화하는 방향으로 나뉜다.

- **Perceiver IO:** 이 아키텍처는 어텐션 계산의 병목 현상을 역으로 이용하여 효율성을 달성하는 독창적인 접근법을 취한다. 매우 긴 입력 시퀀스(예: 이미지 픽셀, 수십만 개)를 직접 처리하는 대신, 고정된 크기의 훨씬 작은 '잠재 배열(latent array)'(예: 512개)을 도입한다.
  - **작동 방식:** 입력 시퀀스 전체(K, V)와 이 잠재 배열(Q) 간에 크로스 어텐션을 수행하여, 방대한 입력 정보를 이 작은 잠재 배열로 압축한다. 그 후, 이 압축된 잠재 배열 내에서만 깊은 셀프 어텐션을 수행한다. 이를 통해 연산 복잡도가 입력 길이에 대해 이차적이 아닌 선형($O(M)$)으로 줄어든다.39 Perceiver IO는 이 구조를 출력단에도 적용하여, 임의의 크기와 형태의 출력을 생성할 수 있는 유연성까지 확보했다.41
- **LV-XAttn (Long Visual inputs Cross-Attention):** 이 기법은 멀티모달 LLM이 비디오와 같은 매우 긴 시각적 입력을 처리할 때 발생하는 분산 컴퓨팅 환경에서의 통신 병목을 해결하는 데 초점을 맞춘다.
  - **작동 방식:** 기존의 분산 어텐션은 큰 키(K)와 밸류(V) 행렬을 GPU 간에 주고받아야 해 막대한 통신 오버헤드가 발생했다. LV-XAttn은 멀티모달 크로스 어텐션에서 쿼리(텍스트)는 작고 키/밸류(시각)는 매우 크다는 비대칭성에 주목한다. 각 GPU는 거대한 K, V 행렬의 일부를 로컬 메모리에 그대로 유지하고, 크기가 훨씬 작은 쿼리(Q) 행렬만 GPU 간에 순환 전송(ring-communication)한다. 이 방식을 통해 통신량을 극적으로 줄여 전체 학습 및 추론 속도를 크게 향상시킨다.38
- **Tgate (Temporally Gating the Attention):** 이 기법은 텍스트-이미지 디퓨전 모델의 '추론' 과정 최적화에 특화되어 있다.
  - **작동 방식:** 연구자들은 디퓨전 모델의 노이즈 제거 과정에서, 크로스 어텐션의 출력(즉, 텍스트 조건이 이미지에 미치는 영향)이 초기 몇 단계('semantics-planning' 단계) 이후에는 거의 변하지 않고 특정 값으로 수렴한다는 사실을 발견했다. Tgate는 이 관찰에 기반하여, 수렴이 일어난 시점 이후로는 더 이상 크로스 어텐션을 계산하지 않고, 이전에 계산된 결과를 캐싱(caching)하여 그대로 재사용한다. 이는 훈련 과정 없이(training-free) 추론 속도를 10~50%까지 향상시키는 매우 효과적인 방법이다.46
- **PCAN (Prototypical Cross-Attention Network):** 비디오 객체 추적 및 분할(MOTS)과 같이 방대한 시공간 데이터를 다루는 문제에 적용된다.
  - **작동 방식:** 과거의 모든 비디오 프레임에서 추출한 특징 벡터들은 엄청난 양의 데이터를 형성한다. PCAN은 이 방대한 데이터를 가우시안 혼합 모델(GMM)과 같은 클러스터링 기법을 사용하여 소수의 대표적인 '프로토타입(prototypes)'으로 압축하고 요약한다. 그 후, 현재 프레임은 이 수많은 원본 특징 벡터들 대신, 단 몇 개에서 몇십 개의 프로토타입에 대해서만 크로스 어텐션을 수행한다. 이를 통해 연산량을 획기적으로 줄이면서도 장기적인 시간적 문맥을 효과적으로 활용할 수 있게 된다.49

이러한 효율화 연구들은 '정확한(exact) 어텐션'을 유지하려는 방향과 '근사(approximation) 어텐션'을 허용하는 방향으로 나뉜다는 점에서 중요한 시사점을 제공한다. LV-XAttn은 분산 처리 방식을 최적화할 뿐, 최종 계산 결과는 원래의 어텐션과 동일한 '정확한' 어텐션이다.38 이는 결과의 재현성과 신뢰성이 중요한 모델 학습(training) 시나리오에 적합하다. 반면, PCAN이나 Perceiver는 클러스터링이나 병목을 통해 정보를 요약/압축하므로 일부 정보 손실을 감수하는 '근사' 어텐션에 해당한다.39 이는 추론(inference) 속도가 매우 중요하고 약간의 성능 저하를 감수할 수 있는 실시간 애플리케이션에 더 적합할 수 있다. Tgate는 이 두 범주 사이에 위치하여, 추론의 특정 단계에서는 정확한 어텐션을, 다른 단계에서는 계산을 생략하는 '조건부 정확' 어텐션으로 볼 수 있다.46 이는 단순히 '빠르게 만드는 것'을 넘어, '어떤 성능-효율 트레이드오프를 선택할 것인가'라는 더 깊은 설계 철학의 문제와 연결되며, 개발자가 자신의 애플리케이션에 맞는 최적의 균형점을 찾아야 함을 의미한다.

### 6.3 효율적 어텐션 기법 비교 테이블

다양한 효율적 크로스 어텐션 기법들의 핵심 아이디어와 적용 분야를 요약하면 다음과 같다.

| 기법 (Method)    | 핵심 아이디어 (Core Idea)                                    | 주요 적용 분야 (Primary Application)  | 복잡도 개선 (Complexity Improvement)                         |
| ---------------- | ------------------------------------------------------------ | ------------------------------------- | ------------------------------------------------------------ |
| **Perceiver IO** | 고정된 크기의 잠재 배열을 '병목'으로 활용하여 입력 정보를 압축 | 임의의 길고 다양한 모달리티 입력 처리 | $O(MNd) \rightarrow O(Md) + O(N^2 d)$ (단, $N \ll M$)        |
| **LV-XAttn**     | 분산 환경에서 작은 Q만 통신하고 큰 K, V는 로컬에 유지        | 긴 시각적 입력을 받는 멀티모달 LLM    | 통신 오버헤드 최소화 (연산 복잡도 자체는 동일)               |
| **Tgate**        | 디퓨전 추론 후반부의 수렴된 어텐션 결과를 캐싱 및 재사용     | 텍스트-이미지 디퓨전 모델 추론 가속   | 후반부 어텐션 연산 생략으로 실질적 추론 시간 단축            |
| **PCAN**         | 시공간 특징을 소수의 '프로토타입'으로 클러스터링 후 어텐션 수행 | 비디오 객체 추적 및 분할 (MOTS)       | $O(HWT \cdot d) \rightarrow O(N_{proto} \cdot d)$ 로 연산량 감소 |

## 7.  근본적 한계와 학술적 논쟁

크로스 어텐션은 현대 AI의 발전에 지대한 공헌을 했지만, 만능 해결책은 아니다. 이 메커니즘은 계산적 비용 외에도 데이터의 특성이나 태스크의 복잡성에 따라 성능이 저하되는 내재적인 실패 모드를 가지고 있으며, 그 작동 방식의 해석을 둘러싼 깊은 학술적 논쟁의 중심에 있기도 하다.

### 7.1  내재적 실패 모드

- **토큰 불균형으로 인한 어텐션 억제 (Suppression due to Token Imbalance):** 멀티모달 디퓨전 트랜스포머(MM-DiT)와 같은 아키텍처에서는 시각 토큰과 텍스트 토큰을 함께 처리한다. 이때, 일반적으로 고해상도 이미지에서 추출된 시각 토큰의 수는 텍스트 프롬프트에서 나온 텍스트 토큰의 수보다 압도적으로 많다 (예: 수천 개 vs. 수십 개). 표준 어텐션 메커니즘의 소프트맥스 함수는 모든 토큰에 대한 유사도 점수를 합산하여 정규화하는데, 시각 토큰의 수가 너무 많으면 소프트맥스의 분모가 시각 토큰 간의 유사도(셀프 어텐션)에 의해 지배된다. 결과적으로 텍스트 토큰에 대한 유사도(크로스 어텐션)의 상대적 영향력이 희석되어, 시각 토큰이 텍스트 토큰에 충분히 '집중'하지 못하는 현상이 발생한다. 이는 생성된 이미지가 텍스트 프롬프트의 세부 사항을 제대로 반영하지 못하는 '의미론적 불일치(semantic discrepancies)'의 주요 원인이 된다.26 TACA(Temperature-Adjusted Cross-modal Attention)와 같은 연구는 이 문제를 해결하기 위해, 소프트맥스 계산 시 시각-시각 어텐션과 시각-텍스트 어텐션에 서로 다른 온도 스케일링(temperature scaling)을 적용하여 크로스 어텐션의 영향력을 동적으로 강화하는 방법을 제안했다.26
- **이질적 데이터 융합의 어려움 (Challenges in Fusing Heterogeneous Data):** 텍스트, 이미지, 오디오 등 서로 다른 모달리티는 각기 다른 통계적 특성, 정보 밀도, 노이즈 패턴을 가진다. 크로스 어텐션을 사용하여 이들을 단순히 융합할 경우, 특정 모달리티에 포함된 노이즈나 중복된 정보가 다른 모달리티의 표현을 오염시키거나, 모달리티 간의 근본적인 의미적 불일치(semantic mismatch)로 인해 효과적인 학습이 저해될 수 있다.51 예를 들어, 멀티모달 감성 분석에서 텍스트는 명확한 감성을 표현하지만, 시각이나 음성 정보는 모호하거나 중립적인 경우가 많다. 이 경우, 약한 모달리티의 정보가 강한 모달리티의 신호를 방해할 수 있다. TCAN(Text-oriented Cross-Attention Network)과 같은 연구는 텍스트 모달리티가 일반적으로 가장 풍부한 의미 정보를 담고 있다는 관찰에 기반하여, 텍스트를 중심으로 다른 모달리티의 정보를 필터링하고 융합하는 비대칭적 구조와 게이트 메커니즘을 제안하여 이 문제를 완화하고자 한다.51
- **구성적 일반화 실패 (Failure in Compositional Generalization):** 크로스 어텐션을 포함한 현재의 딥러닝 모델들은 개별적인 개념이나 객체('사과', '상자', '파란색')를 학습하는 데는 뛰어나지만, 이들을 새로운 방식으로 조합하여 '파란 사과'나 '상자 위의 사과'와 같은 관계적, 구성적 개념을 이해하고 생성하는 데 어려움을 겪는 경우가 많다. 한 연구에서는 멀티모달 LLM이 '사과'와 '상자'에 대한 원자적 지식은 가지고 있음에도 불구하고, '깨지기 쉬운 상자 위에 무거운 사과를 놓지 말라'는 식의 물리적 추론을 요구하는 과제에서 실패하는 사례를 보였다.53 이는 크로스 어텐션이 표면적인 패턴이나 연관성은 잘 포착하지만, 객체 간의 논리적, 인과적, 공간적 관계를 깊이 있게 추론하는 데에는 근본적인 한계가 있을 수 있음을 시사한다.

이러한 실패 모드들은 크로스 어텐션이 '모든 키(key)는 동등하게 접근 가능하다'는 암묵적 가정 하에 작동하는 도구임을 보여준다. 토큰 불균형이나 모달리티 이질성과 같은 현실 세계 데이터의 구조적, 통계적 불균형 문제 앞에서 이 가정은 취약점을 드러낸다. 이는 미래의 어텐션 연구가 단순한 정보 연결을 넘어, 데이터의 분포와 구조적 차이까지 고려하는 더욱 정교한 '문맥 인식적(context-aware)' 융합 메커니즘으로 발전해야 함을 시사한다.

### 7.2  설명 가능성에 대한 논쟁: "Attention is Not Explanation"

크로스 어텐션이 제공하는 가장 매력적인 부가 기능 중 하나는 모델의 '설명 가능성(explainability)' 또는 '해석 가능성(interpretability)'에 대한 잠재력이었다. 어텐션 가중치를 시각화하면 모델이 예측을 위해 입력의 어느 부분에 '집중'했는지를 보여주는 것처럼 보였기 때문이다.

- **초기 낙관론:** 어텐션 히트맵은 기계 번역에서 단어 정렬을 보여주거나, 이미지 캡셔닝에서 특정 단어와 이미지 영역을 연결하는 등, 모델의 내부 작동에 대한 직관적인 통찰을 제공했다. 많은 연구자들이 이를 모델의 의사결정 과정을 설명하는 강력하고 충실한(faithful) 도구로 간주했다.54
- **Jain & Wallace (2019)의 비판:** 2019년, Jain과 Wallace는 "Attention is Not Explanation"이라는 도발적인 제목의 논문을 통해 이러한 낙관론에 정면으로 도전했다.54 그들의 핵심 주장은 다음과 같다.
  1. **낮은 상관관계:** 어텐션 가중치의 분포는 그래디언트 기반 특징 중요도나 Leave-One-Out과 같은 다른 특징 중요도 측정 방식과 상관관계가 매우 낮았다. 만약 어텐션이 정말로 중요한 입력을 가리킨다면, 다른 중요도 측정 방식과 어느 정도 일치해야 하지만 그렇지 않았다.
  2. **대체 가능성:** 모델의 최종 예측을 거의 바꾸지 않으면서도 원래의 어텐션 분포와는 매우 다른 '적대적 어텐션(adversarial attention)' 분포를 만들 수 있었다. 이는 특정 어텐션 분포가 예측의 유일하거나 필수적인 '원인'이 아닐 수 있음을 시사했다.
- **Wiegreffe & Pinter (2019)의 반론:** 이에 대해 Wiegreffe와 Pinter는 "Attention is Not *Not* Explanation"이라는 논문으로 반론을 제기했다.54 그들은 Jain과 Wallace의 실험이 어텐션 가중치만 인위적으로 변경하여 모델이 학습한 정상적인 작동 방식을 깨뜨리는 부자연스러운 개입이라고 비판했다. 그들의 주장은 어텐션이 예측에 대한 '유일하고 완전한' 설명은 아닐지라도, 모델의 내부 상태에 대한 '유용하고 의미 있는 신호'를 제공할 수 있으며, 다른 해석 도구와 함께 사용될 때 가치가 있다는 것이었다.
- **현재의 실용적 합의:** 이 논쟁을 거치면서 학계의 시각은 더 신중하고 다각적인 방향으로 수렴했다. 현재의 실용적인 합의는 **어텐션은 모델 내부를 들여다보는 여러 유용한 도구 중 하나일 뿐, 그 자체로 완전하고 충실한 설명으로 간주해서는 안 된다**는 것이다.54 어텐션은 상관관계를 보여줄 수는 있지만, 인과관계를 보장하지는 않는다. 따라서 어텐션 시각화는 가설을 세우거나 모델의 행동을 탐색하는 '탐정의 단서'로 활용하되, LIME, SHAP, 통합 그래디언트, 또는 최근의 회로 분석(circuit analysis)과 같은 다른 설명 가능성 기법과 함께 교차 검증하여 사용하는 것이 바람직한 접근법으로 여겨진다.

## 8.  결론 및 미래 전망

### 8.1. 크로스 어텐션의 핵심 기여와 의의 요약

크로스 어텐션은 지난 10년간 딥러닝, 특히 자연어 처리와 멀티모달 AI 분야에서 가장 영향력 있는 기술 중 하나로 자리매김했다. 본 보고서에서 심층적으로 분석한 바와 같이, 크로스 어텐션의 핵심 기여와 의의는 다음과 같이 요약할 수 있다.

첫째, 크로스 어텐션은 RNN 기반 Seq2Seq 모델의 고질적인 '정보 병목' 문제를 해결하고, 트랜스포머 아키텍처의 등장을 가능하게 함으로써 순차 데이터 처리의 패러다임을 근본적으로 바꾸었다. 고정된 크기의 문맥 벡터 대신, 디코더가 인코더의 모든 출력에 동적으로 접근할 수 있게 함으로써 정보 손실 없이 장거리 의존성을 모델링할 수 있는 길을 열었다.

둘째, 크로스 어텐션은 텍스트, 이미지, 오디오 등 서로 다른 양식(modality)의 데이터를 연결하는 강력한 '교량' 역할을 수행하며 멀티모달 AI 시대를 본격적으로 열었다.9 이종 데이터 간의 정렬, 융합, 접지를 가능하게 함으로써, 기계가 인간처럼 다양한 정보를 통합적으로 이해하고 추론하는 능력에 한 걸음 더 다가서게 했다.

셋째, 크로스 어텐션의 본질은 특정 도메인에 국한되지 않는 '질의 기반의 동적 정보 라우팅' 메커니즘이다. 이러한 범용성 덕분에 기계 번역에서 시작된 이 기술은 텍스트-이미지 생성, 시각 질의응답, 그래프 분석, 음성 인식 등 상상하기 어려웠던 다양한 AI 응용 분야로 성공적으로 확장될 수 있었다.

### 8.1  현재의 한계를 극복하기 위한 연구 동향

크로스 어텐션의 성공에도 불구하고, 계산 효율성, 구조적 이해 능력, 융합 방식의 정교함 등 여러 측면에서 여전히 극복해야 할 한계가 존재한다. 이를 해결하기 위한 연구는 다음과 같은 방향으로 활발히 진행되고 있다.

- **효율성 개선:** 표준 어텐션의 이차적 복잡도를 해결하기 위한 연구는 계속해서 중요한 주제이다. Linformer, Performer와 같이 전체 어텐션 행렬을 근사하여 선형 복잡도를 달성하는 기법이나, Longformer와 같이 미리 정의된 희소 패턴(sparse pattern)을 통해 계산량을 줄이는 희소 어텐션 연구가 지속되고 있다.9 또한, Perceiver IO나 PCAN처럼 정보 병목이나 프로토타입을 활용하여 계산 구조 자체를 바꾸려는 시도도 중요한 흐름을 형성하고 있다.
- **구조적 및 고차원적 이해:** 표준 어텐션은 기본적으로 입력 요소 쌍(pair) 간의 관계를 모델링하는 데 최적화되어 있다. 이로 인해 세 개 이상의 요소가 얽힌 복잡한 관계를 파악하는 데는 한계가 있을 수 있다.57 GNN과의 결합을 통해 명시적인 그래프 구조 정보를 활용하거나 58, 어텐션 메커니즘 자체를 고차원 텐서 연산으로 확장하려는 연구는 이러한 한계를 극복하려는 시도이다.
- **융합 방식의 정교화:** 단순한 어텐션 가중합을 넘어, 더 유연하고 동적인 정보 통합 방식으로의 발전이 이루어지고 있다. 입력의 특성에 따라 동적으로 다른 전문가 네트워크를 활성화하는 혼합 전문가 모델(Mixture-of-Experts), 여러 어텐션 모듈을 레고 블록처럼 조립하여 사용하는 구성 가능한 어텐션(Composable Attention), 그리고 외부 지식 베이스나 메모리 뱅크를 동적으로 참조하는 메모리 증강 어텐션(Memory-Augmented Attention) 등은 크로스 어텐션이 더 복잡한 추론을 수행할 수 있도록 진화하는 방향을 보여준다.9

### 8.2  차세대 AI 아키텍처에서의 역할 예측

크로스 어텐션은 트랜스포머 아키텍처의 핵심 부품을 넘어, 미래 AI 시스템을 구성하는 더욱 근본적인 '연산 프리미티브(fundamental primitive)'로서 그 중요성이 더욱 커질 것이다.59

현재의 크로스 어텐션이 주로 정적인 데이터셋(예: 이미지-텍스트 쌍) 간의 관계를 학습하는 데 초점을 맞추고 있다면, 미래의 크로스 어텐션은 '동적인 세계와의 상호작용'을 모델링하는 방향으로 진화할 것이다. 로보틱스, 증강/가상현실(AR/VR), 지능형 비서와 같은 차세대 AI 에이전트 시스템에서 크로스 어텐션은 핵심적인 역할을 수행할 것으로 예측된다. 이러한 시스템에서 크로스 어텐션은 실시간으로 변화하는 환경(센서 데이터 시퀀스)과 에이전트의 내부 목표(의도 시퀀스)를 연결하고, 필요에 따라 외부 도구(계산기, 검색 엔진, API)를 동적으로 '쿼리'하여 그 결과를 통합하는 역할을 맡게 될 것이다.6

이러한 관점에서 볼 때, 크로스 어텐션은 단순히 두 데이터 덩어리를 연결하는 기술을 넘어, AI 에이전트가 세상을 인식하고, 정보를 탐색하며, 목표 지향적인 행동을 생성하는 근본적인 '인지 루프(cognitive loop)'를 구현하는 핵심 엔진으로 발전할 것이다. 어텐션 메커니즘 자체도 현재의 형태에 머무르지 않고, 상태 공간 모델(State Space Models)과 같은 새로운 아키텍처와 결합되거나, 그 자체로 더욱 효율적이고 강력한 형태로 진화하며 미래 AI 기술의 근간을 이룰 것이다.56 크로스 어텐션은 단순한 딥러닝 컴포넌트에서 범용 인공지능(AGI)으로 가는 길목의 중요한 아키텍처적 초석이 될 잠재력을 지니고 있다.

#### **참고 자료**

1. What is an attention mechanism? | IBM, 8월 16, 2025에 액세스, https://www.ibm.com/think/topics/attention-mechanism
2. Transformer (deep learning architecture) - Wikipedia, 8월 16, 2025에 액세스, https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)
3. Cross Attention in Transformer - Medium, 8월 16, 2025에 액세스, https://medium.com/@sachinsoni600517/cross-attention-in-transformer-f37ce7129d78
4. Cross-Attention vs Self-Attention Explained - AIML.com, 8월 16, 2025에 액세스, https://aiml.com/explain-cross-attention-and-how-is-it-different-from-self-attention/
5. medium.com, 8월 16, 2025에 액세스, [https://medium.com/@hexiangnan/self-attention-vs-cross-attention-from-fundamentals-to-applications-4b065285f3f8#:~:text=Self%2Dattention%20and%20cross%2Dattention%20are%20fundamental%20tools%20in%20the,alignment%2C%20fusion%2C%20and%20grounding.](https://medium.com/@hexiangnan/self-attention-vs-cross-attention-from-fundamentals-to-applications-4b065285f3f8#:~:text=Self-attention and cross-attention are fundamental tools in the,alignment%2C fusion%2C and grounding.)
6. Cross-Attention Mechanism in Transformers - GeeksforGeeks, 8월 16, 2025에 액세스, https://www.geeksforgeeks.org/nlp/cross-attention-mechanism-in-transformers/
7. Why do the values in the cross attentional mechanism within a transformer come from the encoder and not from the decoder? - AI Stack Exchange, 8월 16, 2025에 액세스, https://ai.stackexchange.com/questions/38340/why-do-the-values-in-the-cross-attentional-mechanism-within-a-transformer-come-f
8. Unraveling Transformers: A Deep Dive into Self-Attention and Cross-Attention Mechanisms | by Abhinav Bharti | Medium, 8월 16, 2025에 액세스, https://medium.com/@abhinavbhartigoml/unraveling-transformers-a-deep-dive-into-self-attention-and-3e37dc875bea
9. Self-Attention vs Cross-Attention: From Fundamentals to Applications | by Shawn | Medium, 8월 16, 2025에 액세스, https://medium.com/@hexiangnan/self-attention-vs-cross-attention-from-fundamentals-to-applications-4b065285f3f8
10. What exactly are keys, queries, and values in attention mechanisms? - Cross Validated, 8월 16, 2025에 액세스, https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms
11. [D] Attention Mystery: Which Is Which - q, k, or v? : r/MachineLearning - Reddit, 8월 16, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/19ewfm9/d_attention_mystery_which_is_which_q_k_or_v/
12. LLM Transformer Model Visually Explained - Polo Club of Data Science, 8월 16, 2025에 액세스, https://poloclub.github.io/transformer-explainer/
13. Stefan's Blog - Understanding Transformers and Attention, 8월 16, 2025에 액세스, https://stefanbschneider.github.io/blog/posts/understanding-transformers-attention/
14. what is the cross attention? : r/deeplearning - Reddit, 8월 16, 2025에 액세스, https://www.reddit.com/r/deeplearning/comments/nf08zz/what_is_the_cross_attention/
15. How to obtain Key, Value and Query in Attention and Multi-Head-Attention - Cross Validated, 8월 16, 2025에 액세스, https://stats.stackexchange.com/questions/574871/how-to-obtain-key-value-and-query-in-attention-and-multi-head-attention
16. 11.1. Queries, Keys, and Values — Dive into Deep Learning 1.0.3 documentation, 8월 16, 2025에 액세스, https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html
17. Cross-Modal Attention. What is Cross-Attention and How it… | by ..., 8월 16, 2025에 액세스, https://abdulkaderhelwan.medium.com/cross-modal-attention-efa7a774c80a
18. What are the Different Types of Attention Mechanisms? - Analytics Vidhya, 8월 16, 2025에 액세스, https://www.analyticsvidhya.com/blog/2024/01/different-types-of-attention-mechanisms/
19. Building a Transformer (Cross-Attention and MHA Explained) | Eva Koroleva - GitHub Pages, 8월 16, 2025에 액세스, https://xmarva.github.io/blog/2025/building-a-transformer/
20. The Transformer Revolution: How "Attention Is All You Need" Reshaped Modern AI - Introl, 8월 16, 2025에 액세스, https://introl.com/blog/the-transformer-revolution-how-attention-is-all-you-need-reshaped-modern-ai
21. Multi-scale cross-attention transformer encoder for event classification - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2401.00452v2
22. Paying Attention to Attention: A compilation | by Vijayasri Iyer | Medium, 8월 16, 2025에 액세스, https://vijayasriiyer.medium.com/paying-attention-to-attention-a-compilation-77b16e8a196d
23. Why Cross-Attention is the Secret Sauce of Multimodal Models | by ..., 8월 16, 2025에 액세스, https://medium.com/@jakubstrawadev/why-cross-attention-is-the-secret-sauce-of-multimodal-models-f8ec77fc089b
24. How Multimodal Learning is Used in Generative AI - DigitalOcean, 8월 16, 2025에 액세스, https://www.digitalocean.com/community/tutorials/multimodal-learning-generative-ai
25. CAST: Cross Attention based multimodal fusion of Structure and Text for materials property prediction - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2502.06836v1
26. TACA: Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2506.07986v1
27. (PDF) GTAT: empowering graph neural networks with cross attention, 8월 16, 2025에 액세스, https://www.researchgate.net/publication/388823279_GTAT_empowering_graph_neural_networks_with_cross_attention
28. Graph-to-Text Generation with Bidirectional Dual Cross-Attention and Concatenation - MDPI, 8월 16, 2025에 액세스, https://www.mdpi.com/2227-7390/13/6/935
29. Whisper - Hugging Face, 8월 16, 2025에 액세스, https://huggingface.co/docs/transformers/model_doc/whisper
30. OpenAI Whisper - Converting Speech to Text - GeeksforGeeks, 8월 16, 2025에 액세스, https://www.geeksforgeeks.org/data-science/openai-whisper-converting-speech-to-text/
31. Whisper: Functionality and Finetuning | by Okezie Okoye - Medium, 8월 16, 2025에 액세스, https://medium.com/@okezieowen/whisper-functionality-and-finetuning-ba7f9444f55a
32. How does the cross attention work? · openai whisper · Discussion #943 - GitHub, 8월 16, 2025에 액세스, https://github.com/openai/whisper/discussions/943
33. Simul-Whisper: Attention-Guided Streaming Whisper with Truncation Detection - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2406.10052v1
34. Towards Interpretable Reinforcement Learning Using Attention Augmented Agents, 8월 16, 2025에 액세스, http://papers.neurips.cc/paper/9400-towards-interpretable-reinforcement-learning-using-attention-augmented-agents.pdf
35. A Self-Supervised Reinforcement Learning Approach for Fine-Tuning Large Language Models Using Cross-Attention Signals - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2502.10482v2
36. Attention is all you need, But which one ? | by Suparna - Medium, 8월 16, 2025에 액세스, https://medium.com/@ssuparnataneja/attention-is-all-you-need-but-which-one-14dea0e8c148
37. Complexity of transformer attention network : r/LanguageTechnology - Reddit, 8월 16, 2025에 액세스, https://www.reddit.com/r/LanguageTechnology/comments/9gulm9/complexity_of_transformer_attention_network/
38. LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2502.02406v1
39. Perceiver - Wikipedia, 8월 16, 2025에 액세스, https://en.wikipedia.org/wiki/Perceiver
40. PERCEIVER IO: A GENERAL ARCHITECTURE FOR ... - OpenReview, 8월 16, 2025에 액세스, https://openreview.net/pdf?id=fILj7WpI-g
41. The Annotated Perceiver. A detailed PyTorch tutorial for the… | by ..., 8월 16, 2025에 액세스, https://medium.com/@curttigges/the-annotated-perceiver-74752113eefb
42. Perceiver IO: A General Architecture for Structured Inputs & Outputs | OpenReview, 8월 16, 2025에 액세스, https://openreview.net/forum?id=fILj7WpI-g
43. [2502.02406] LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/abs/2502.02406
44. LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models - ICML 2025, 8월 16, 2025에 액세스, https://icml.cc/virtual/2025/poster/44230
45. LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models | OpenReview, 8월 16, 2025에 액세스, https://openreview.net/forum?id=kuIwMEHXMT
46. Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2404.02747v1
47. Faster Diffusion via Temporal Attention Decomposition - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2404.02747v2
48. [2404.02747] Faster Diffusion via Temporal Attention Decomposition - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/abs/2404.02747
49. Prototypical Cross-Attention Networks for Multiple Object ... - NIPS, 8월 16, 2025에 액세스, https://papers.nips.cc/paper/2021/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf
50. Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation - Lei Ke, 8월 16, 2025에 액세스, https://www.kelei.site/poster/pcan_poster.pdf
51. TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2404.04545v3
52. Multi-modal Semantic Understanding with Contrastive Cross-modal Feature Alignment, 8월 16, 2025에 액세스, https://arxiv.org/html/2403.06355v1
53. Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/abs/2402.15654
54. Is Attention Interpretable in Transformer-Based Large Language ..., 8월 16, 2025에 액세스, https://huggingface.co/blog/royswastik/is-attention-interpretable
55. Attention Meets Post-hoc Interpretability: A Mathematical Perspective - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2402.03485v2
56. (PDF) Future Directions in Attention Research: Beyond Transformers - ResearchGate, 8월 16, 2025에 액세스, https://www.researchgate.net/publication/392364353_Future_Directions_in_Attention_Research_Beyond_Transformers
57. NeurIPS Poster Representational Strengths and Limitations of Transformers, 8월 16, 2025에 액세스, https://neurips.cc/virtual/2023/poster/72920
58. Transformers meet Neural Algorithmic Reasoners - OpenReview, 8월 16, 2025에 액세스, https://openreview.net/forum?id=fk4czNKXPC
59. AI's "Secret Sauce": Attention & Transformer Models Explained - YouTube, 8월 16, 2025에 액세스, https://www.youtube.com/watch?v=9fKJaGLzLlY
60. Attention Mechanisms in Deep Learning: Beyond Transformers Explained | Graph AI, 8월 16, 2025에 액세스, https://www.graphapp.ai/blog/attention-mechanisms-in-deep-learning-beyond-transformers-explained