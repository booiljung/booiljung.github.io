# 선형화 어텐션



2017년 Vaswani 등이 발표한 "Attention Is All You Need" 논문은 자연어 처리(NLP)를 비롯한 시퀀스 처리 분야에 혁명적인 변화를 가져왔다.1 이 논문에서 제안된 트랜스포머(Transformer) 아키텍처는 기존의 순환 신경망(RNN)이나 합성곱 신경망(CNN) 기반 모델이 가졌던 순차적 계산의 한계를 극복하고, 병렬 처리를 극대화하여 훈련 시간을 단축하면서도 뛰어난 성능을 달성했다.4 트랜스포머의 성공은 전적으로 셀프 어텐션(Self-Attention)이라는 메커니즘에 기반한다.

셀프 어텐션은 입력 시퀀스 내의 모든 단어(토큰) 쌍 간의 관계를 동시에 계산하여, 각 단어가 문맥 속에서 어떤 다른 단어들과 연관되어 있는지를 동적으로 파악하는 메커니즘이다.1 이는 문장의 특정 단어가 멀리 떨어진 다른 단어와 가지는 의미적, 통사적 의존성, 즉 전역적 의존성(global dependencies)을 효과적으로 포착할 수 있게 한다.2 기존 RNN이 은닉 상태(hidden state)를 통해 순차적으로 정보를 전달하면서 발생하는 장기 의존성 문제(long-range dependency problem)를 근본적으로 해결한 것이다.6

셀프 어텐션의 핵심 연산은 스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)으로, 각 토큰에 대해 세 가지 벡터, 즉 쿼리(Query, Q), 키(Key, K), 값(Value, V)을 생성하여 계산된다.9 쿼리는 현재 토큰이 다른 토큰들에게 던지는 질문으로, 키는 각 토큰이 가지고 있는 고유한 특성으로, 값은 각 토큰이 전달하고자 하는 정보로 비유할 수 있다. 특정 쿼리 벡터는 모든 키 벡터와 내적(dot product)을 수행하여 유사도 점수(similarity score)를 계산한다. 이 점수가 높을수록 해당 키를 가진 토큰이 현재 쿼리 토큰과 높은 연관성을 가짐을 의미한다. 계산된 점수는 키 벡터 차원(

$d_k$)의 제곱근으로 나누어 스케일링되는데, 이는 내적 값이 지나치게 커져 소프트맥스(softmax) 함수의 경사도(gradient)가 소실되는 것을 방지하기 위함이다.9 마지막으로, 스케일링된 점수에 소프트맥스 함수를 적용하여 합이 1이 되는 어텐션 가중치(attention weights)를 얻고, 이를 각 키에 해당하는 값 벡터에 곱하여 가중합(weighted sum)을 구한다. 이 가중합이 바로 해당 쿼리 토큰에 대한 문맥 정보가 풍부하게 반영된 새로운 표현(representation)이 된다.10 이 과정은 다음 수식으로 요약된다.2
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
트랜스포머의 성공은 이처럼 모든 토큰 쌍의 관계를 한 번에 병렬적으로 계산할 수 있는 능력에 크게 의존한다. RNN이 시퀀스를 하나씩 순차적으로 처리해야 했던 것과 달리, 트랜스포머는 GPU의 병렬 연산 능력을 최대한 활용하여 학습 속도를 획기적으로 개선했다.7 그러나 이러한 아키텍처적 선택은 필연적으로 막대한 계산 비용을 수반하게 되는데, 이는 트랜스포머의 가장 큰 장점이 동시에 가장 치명적인 단점의 원인이 되는 내재적 딜레마를 형성한다. 즉, 병렬화를 가능하게 한 '모든 쌍을 한 번에 계산한다'는 개념 자체가 계산 복잡도의 근본적인 원인이 되는 것이다.


트랜스포머의 셀프 어텐션 메커니즘은 입력 시퀀스의 길이 $N$에 대해 시간 복잡도와 공간 복잡도가 제곱에 비례하여 증가하는, 이른바 이차 복잡도(quadratic complexity) 문제를 가진다.12 이 문제의 근원은 어텐션 가중치를 계산하기 위해 $N \times N$ 크기의 어텐션 스코어 행렬, 즉 $QK^T$를 명시적으로 계산하고 메모리에 저장해야 한다는 점에 있다.15

구체적으로 계산 과정을 살펴보면, $N$개의 토큰이 각각 $d$차원의 벡터로 표현될 때, 쿼리 행렬 $Q$와 키 행렬 $K$는 $(N, d)$ 크기를 가진다. 어텐션 스코어를 계산하기 위한 첫 단계인 $QK^T$ 행렬 곱셈은 $(N, d)$ 크기의 행렬과 $(d, N)$ 크기의 행렬(K의 전치)을 곱하는 연산이다. 이 행렬 곱셈에는 총 $N \times d \times N = N^2d$ 만큼의 곱셈 연산이 필요하다. 따라서, 이 연산의 시간 복잡도는 $O(N^2d)$가 된다.14 이후 $N \times N$ 스코어 행렬에 소프트맥스를 적용하고, 이를 $(N, d)$ 크기의 값 행렬 $V$와 곱하는 과정에서도 $O(N^2d)$의 복잡도가 발생한다. $d$는 모델의 하이퍼파라미터로 고정된 값이므로, 전체 어텐션 연산의 복잡도는 시퀀스 길이 $N$에 의해 지배되며, 최종적으로 $O(N^2)$로 요약된다.12

이러한 이차 복잡도는 시퀀스 길이가 짧을 때는 문제가 되지 않지만, 길이가 수천, 수만 개를 넘어서는 현대의 AI 응용 분야에서는 심각한 병목으로 작용한다. 예를 들어, 긴 논문이나 법률 문서를 요약하는 태스크, 메가픽셀 단위의 고해상도 이미지를 처리하는 컴퓨터 비전 태스크, 수백만 염기쌍에 달하는 유전체 서열을 분석하는 생물정보학 태스크 등에서는 $N$의 값이 매우 커져 $N^2$에 해당하는 계산량과 메모리 요구량이 기하급수적으로 증가한다.14 이는 모델의 학습을 비현실적으로 만들거나, 제한된 GPU 메모리로 인해 처리 가능한 시퀀스 길이에 심각한 제약을 가하게 된다. 이 $O(N^2)$의 장벽은 트랜스포머 아키텍처의 적용 범위를 제한하는 가장 근본적인 한계로 인식되어 왔다.


$O(N^2)$의 한계를 극복하기 위한 노력은 트랜스포머가 발표된 직후부터 활발하게 진행되었다. 초기 연구들은 완전한 전역적 어텐션(full attention)을 포기하고, 일부 토큰 쌍 간의 상호작용만을 계산하는 방식으로 효율성을 확보하고자 했다. 대표적인 예로 희소 어텐션(Sparse Attention)이 있다.12 희소 어텐션은 미리 정의된 패턴에 따라 어텐션을 계산하는 방식으로, 모든 토큰 쌍이 아닌 일부 핵심적인 쌍에 대해서만 계산을 수행한다.

Longformer 모델은 슬라이딩 윈도우 어텐션(sliding window attention)과 전역 어텐션(global attention)을 결합한 희소 어텐션을 사용한다.12 각 토큰은 자신의 주변에 있는 고정된 크기의 윈도우 내 토큰들에만 어텐션을 수행하고, 문서의 시작(예: `<s>` 토큰)과 같이 중요한 일부 토큰에 대해서만 모든 토큰에 대한 전역 어텐션을 수행한다. 이를 통해 계산 복잡도를 $O(N)$으로 낮추면서도 중요한 전역 정보를 포착하려는 시도였다.12 BigBird 역시 무작위 어텐션, 윈도우 어텐션, 전역 어텐션을 조합하여 비슷한 효과를 얻고자 했다.14

그러나 이러한 희소 기반 접근법들은 근본적으로 모든 토큰 쌍 간의 상호작용을 포기한다는 한계를 가진다. 특정 패턴에 의존하기 때문에, 해당 패턴이 포착하지 못하는 중요한 장거리 의존성을 놓칠 위험이 있다. 이러한 배경 속에서, 전역적 상호작용의 표현력을 최대한 유지하면서도 계산 복잡도를 선형 시간($O(N)$)으로 줄이려는 새로운 연구 흐름이 등장했는데, 이것이 바로 '선형화 어텐션(Linearized Attention)'이다.20 선형화 어텐션은 어텐션 계산의 본질을 수학적으로 재해석하고, 행렬 연산의 순서를 변경하는 영리한 기법을 통해 $N \times N$ 행렬을 명시적으로 생성하지 않고도 동일하거나 근사적인 결과를 얻는 것을 목표로 한다. 이는 효율적 어텐션 연구의 패러다임을 '연산의 생략'에서 '연산의 재구성'으로 전환시킨 중요한 진전이라 할 수 있다.


선형화 어텐션의 핵심은 표준 셀프 어텐션의 계산 과정을 다른 수학적 형태로 변환하여, 시퀀스 길이에 대한 이차 복잡도를 피하는 데 있다. 이 변환의 중심에는 커널(Kernel) 방법론과 행렬 곱셈의 결합법칙(associative property)이라는 두 가지 강력한 수학적 도구가 자리 잡고 있다.


선형화 어텐션의 이론적 출발점은 표준 어텐션의 핵심 연산인 $\text{softmax}(QK^T)$를 일종의 커널 함수(kernel function)로 재해석하는 것이다.22 커널 함수 $\kappa(x, y)$는 두 벡터 $x$와 $y$ 사이의 유사도를 측정하는 함수로 정의된다. 이러한 관점에서, $i$번째 쿼리 벡터 $q_i$와 $j$번째 키 벡터 $k_j$ 사이의 스케일드 닷-프로덕트 어텐션 스코어는 $\kappa(q_i, k_j) = \exp(q_i k_j^T / \sqrt{d_k})$로 표현되는 커널로 볼 수 있다.25

더 나아가, Tsai 등(2019)의 연구는 이 $\exp$ 기반 커널이 가우시안 커널, 특히 방사 기저 함수(Radial Basis Function, RBF) 커널과 밀접한 관련이 있음을 보였다.22 벡터 간의 내적 $q_i k_j^T$는 유클리드 거리 제곱 $\|q_i - k_j\|^2$과 다음과 같은 관계를 가진다: $\|q_i - k_j\|^2 = \|q_i\|^2 + \|k_j\|^2 - 2q_i k_j^T$. 이를 이용하면 어텐션 스코어 항을 다음과 같이 분해할 수 있다.
$$
\text{exp}\left(\frac{q_i k_j^T}{\sqrt{d_k}}\right) \propto \underbrace{\text{exp}\left(-\frac{\|q_i - k_j\|^2_2}{2\sqrt{d_k}}\right)}_{\text{RBF Kernel Term}} \times \underbrace{\text{exp}\left(\frac{\|q_i\|^2_2 + \|k_j\|^2_2}{2\sqrt{d_k}}\right)}_{\text{Magnitude Term}}
$$
이 분해는 어텐션 스코어가 두 벡터 간의 거리에 기반한 유사도(RBF 커널 항)와 각 벡터의 크기(magnitude)에 기반한 중요도(Magnitude 항)의 곱으로 이루어져 있음을 보여준다.23 이러한 재해석은 $\text{softmax}$ 어텐션이 특정 커널의 한 형태로 이해될 수 있으며, 따라서 이론적으로 다른 종류의 커널 함수로 대체될 수 있다는 가능성을 열어주었다. 이는 어텐션의 본질인 '유사도 측정'이라는 기능은 유지하되, 그 구현 방식을 보다 계산적으로 효율적인 형태로 바꿀 수 있는 이론적 토대를 마련한 것이다.


전통적인 머신러닝, 특히 서포트 벡터 머신(SVM)에서 사용되는 커널 트릭(kernel trick)은 비선형적으로 분포된 데이터를 선형적으로 분리하기 위해 사용된다.26 이는 원래의 저차원 데이터 공간 $x$를 고차원의 특징 공간으로 매핑하는 함수 $\Phi(x)$를 도입하고, 이 고차원 공간에서의 내적 $\Phi(x)^T\Phi(y)$를 원래 공간에서 간단히 계산할 수 있는 커널 함수 $\kappa(x, y)$로 대체하는 방식이다.28 즉, 명시적으로 고차원 매핑을 수행하지 않고도 고차원 공간에서 작업하는 효과를 얻는 것이다.

선형화 어텐션은 이 아이디어를 역으로 적용하는 '역발상'에서 출발한다.30 즉, 고차원 공간의 내적을 저차원의 비선형 함수로 대체하는 대신, 계산이 복잡한 비선형 함수(즉, $\text{softmax}$ 커널)를 어떤 특징 맵(feature map) $\phi$를 통해 변환된 벡터들의 내적으로 근사하는 것이다.30 수학적으로 표현하면 다음과 같다.
$$
\kappa(q, k) = \text{softmax}(q^Tk) \approx \phi(q)^T\phi(k)
$$
여기서 $\phi$는 쿼리와 키 벡터를 새로운 특징 공간으로 매핑하는 함수이다. 이 특징 맵 $\phi$는 다양한 형태로 정의될 수 있다. 가장 간단한 형태는 $\text{elu}(x) + 1$이나 $\text{relu}(x)$와 같은 비선형 활성화 함수를 사용하는 것이다.32 이러한 함수들은 항상 양수 값을 반환하여 $\text{softmax}$의 확률적 해석과 유사한 속성을 유지하는 데 도움을 준다. 더 정교한 접근법으로는 Performer 모델에서 사용된 무작위 푸리에 특징(Random Fourier Features)이 있으며, 이는 $\text{softmax}$ 커널을 수학적으로 더 엄밀하게 근사한다.33

이처럼 $\text{softmax}$ 커널을 특징 맵의 내적으로 분해하는 것은 선형화의 첫 단추이다. 이는 $\text{softmax}$의 핵심 기능인 '유사도 측정'을 포기하는 것이 아니라, 그 기능을 계산적으로 훨씬 다루기 쉬운 '내적'의 형태로 재구성하는 정교한 과정이다. 이 재구성을 통해 다음 단계인 행렬 곱 결합법칙을 적용할 수 있는 길이 열리게 된다.


$\text{softmax}$ 커널을 $\kappa(q, k) \approx \phi(q)^T\phi(k)$로 근사하면, 전체 어텐션 계산식은 다음과 같이 변형된다. 여기서 $\Phi(\cdot)$는 행렬의 각 행 벡터에 특징 맵 $\phi$를 적용한 것을 의미한다.
$$
\text{Attention}(Q, K, V) \approx (\Phi(Q)\Phi(K)^T)V
$$
이 식은 여전히 $(\Phi(Q)\Phi(K)^T)$ 부분에서 $N \times N$ 크기의 행렬을 계산해야 하므로 $O(N^2)$의 복잡도를 가진다. 하지만 이 형태는 행렬 곱셈의 결합법칙, 즉 $(AB)C = A(BC)$를 적용할 수 있는 절호의 기회를 제공한다.21 계산 순서를 괄호의 뒤쪽부터 수행하도록 변경하면 다음과 같다.
$$
\text{Attention}(Q, K, V) \approx \Phi(Q)(\Phi(K)^TV)
$$
이 작은 순서 변경이 계산 복잡도에 미치는 영향은 극적이다. 두 계산 방식의 복잡도를 비교해 보자. $\Phi(Q)$와 $\Phi(K)$가 $(N, d')$ 크기를, $V$가 $(N, d_v)$ 크기를 가진다고 가정하자 ($d'$는 특징 맵 $\phi$의 출력 차원).

- **기존 순서 (괄호 앞부터):**
  1. $A' = \Phi(Q)\Phi(K)^T$: $(N, d')$ 행렬과 $(d', N)$ 행렬을 곱하여 $(N, N)$ 크기의 중간 행렬 $A'$ 생성. 복잡도: $O(N^2d')$.
  2. $\text{Output} = A'V$: $(N, N)$ 행렬과 $(N, d_v)$ 행렬을 곱하여 최종 출력 생성. 복잡도: $O(N^2d_v)$.
  3. **총 복잡도:** $O(N^2(d' + d_v))$.
- **변경된 순서 (괄호 뒤부터):**
  1. $\text{KV\_merged} = \Phi(K)^TV$: $(d', N)$ 행렬과 $(N, d_v)$ 행렬을 곱하여 $(d', d_v)$ 크기의 중간 행렬 $\text{KV\_merged}$ 생성. 복잡도: $O(Nd'd_v)`.
  2. $\text{Output} = \Phi(Q)\text{KV\_merged}$: $(N, d')$ 행렬과 $(d', d_v)$ 행렬을 곱하여 최종 출력 생성. 복잡도: $O(Nd'd_v)$.
  3. **총 복잡도:** $O(Nd'd_v)$.

일반적으로 시퀀스 길이 $N$은 임베딩 차원 $d$보다 훨씬 크다 ($N \gg d$). 따라서 $d'$, $d_v$를 $d$와 비슷한 수준으로 간주하면, 총 복잡도는 $O(N^2d)$에서 $O(Nd^2)$로 극적으로 감소한다.14

$d$는 고정된 하이퍼파라미터이므로, 이는 시퀀스 길이 $N$에 대해 선형적인 복잡도를 달성했음을 의미한다. 이처럼 행렬 곱의 결합법칙을 활용하여 $N \times N$ 행렬의 명시적 계산을 회피하는 것이 선형화 어텐션의 핵심 원리이다.


선형화 어텐션의 계산 구조는 순환 신경망(RNN)의 작동 방식과 놀라운 유사성을 보인다.21 이는 특히 자기회귀(auto-regressive) 모델에서 다음 토큰을 생성하는 추론(inference) 과정에서 중요한 함의를 가진다.

변경된 계산 순서 $\Phi(Q)(\Phi(K)^TV)$에서, $\Phi(K)^TV$는 $\sum_{j=1}^{N} \phi(k_j)v_j^T$ (행 벡터 형태의 `v`를 사용한 경우)와 같이 모든 키-값 쌍의 외적(outer product)을 합산한 형태로 볼 수 있다. $l$번째 타임스텝에서의 출력을 계산한다고 가정해 보자. 이 출력 $o_l$은 현재 쿼리 $\phi(q_l)$과 $l$번째까지의 모든 키-값 쌍의 정보를 결합하여 계산된다.
$$
o_l = \phi(q_l) (\sum_{j=1}^{l} \phi(k_j)v_j^T)
$$
여기서 $S_l = \sum_{j=1}^{l} \phi(k_j)v_j^T$를 $l$번째 타임스텝까지의 '상태(state)' 행렬로 정의할 수 있다. 그러면 이 상태는 다음과 같은 순환적 업데이트 규칙을 따른다.31
$$
S_l = S_{l-1} + \phi(k_l)v_l^T
$$
그리고 $l$번째 출력은 $o_l = \phi(q_l)S_l$로 계산된다. 이 형태는 RNN이 이전 타임스텝의 은닉 상태($S_{l-1}$)와 현재 입력($\phi(k_l)v_l^T$)을 받아 새로운 상태($S_l$)를 만들고, 이를 이용해 현재 출력($o_l$)을 계산하는 과정과 정확히 일치한다.

이러한 순환적 관점은 자기회귀 추론 시 엄청난 효율성 증대를 가져온다. 표준 트랜스포머는 다음 토큰을 생성할 때마다 이전까지 생성된 모든 토큰들의 키와 값(KV 캐시)을 참조하여 어텐션을 다시 계산해야 한다. $l$번째 토큰을 생성할 때의 복잡도는 $O(ld)$이다. 반면, 선형 어텐션은 고정된 크기($d' \times d_v$)의 상태 행렬 $S$만 유지하면 된다. 다음 토큰을 생성할 때, 새로운 키-값 쌍을 이용해 상태 행렬을 $O(d^2)$의 복잡도로 업데이트하고, 이를 현재 쿼리와 곱하기만 하면 된다.21 즉, 시퀀스 길이에 무관하게 거의 상수 시간($O(1)$) 복잡도로 다음 토큰 생성이 가능하다.31

이러한 발견은 흥미로운 시사점을 던진다. 트랜스포머는 RNN의 순차적 계산 병목을 극복하기 위해 등장했지만, 역설적으로 트랜스포머의 계산 병목을 해결하기 위한 선형화 어텐션은 다시 RNN의 핵심 원리인 '상태 기반 순차 처리'로 회귀하는 모습을 보인다. 이는 딥러닝 아키텍처의 발전이 단순히 과거를 부정하는 선형적 진보가 아니라, 서로 다른 패러다임의 장점을 재발견하고 융합하는 순환적 과정임을 보여주는 강력한 증거이다. Mamba와 같은 최신 상태 공간 모델(SSM)이 RNN의 효율성과 트랜스포머의 표현력을 결합하려는 시도 역시 이러한 흐름의 연장선상에 있다.36


선형화 어텐션의 이론적 기반 위에서, 다양한 철학과 접근법을 가진 구체적인 모델들이 제안되었다. 이들은 크게 어텐션 행렬의 구조적 특성을 이용하는 방식과, $\text{softmax}$ 함수 자체를 수학적으로 근사하는 방식으로 나눌 수 있다. 또한, 초기 모델들의 한계를 보완하여 안정성과 성능을 개선하려는 후속 연구들도 중요한 흐름을 형성하고 있다.


Linformer는 선형화 어텐션 연구 초기에 제안된 대표적인 모델로, 그 철학은 '구조적 제약'에 기반한다.17 Linformer의 핵심 가정은, 대규모 데이터로 사전 학습된 트랜스포머 모델의 셀프 어텐션 행렬을 특이값 분해(SVD) 등으로 분석해 보면, 그 본질적인 계급(rank)이 매우 낮다는 경험적 관찰에서 출발한다.17 이는 $N \times N$ 크기의 전체 어텐션 행렬에 담긴 정보가 실제로는 훨씬 더 낮은 차원의 부분 공간(subspace)에 집중되어 있음을 시사한다.

이 관찰에 기반하여, Linformer는 $N \times N$ 어텐션 행렬을 직접 계산하는 대신, 처음부터 저계급 행렬로 근사하는 방식을 제안한다. 이는 키($K$)와 값($V$) 행렬에 $N \times k$ 크기의 선형 투영(linear projection) 행렬을 곱하여, 이들을 $k \times d$ 차원으로 압축함으로써 이루어진다. 여기서 $k$는 $N$보다 훨씬 작은 값(예: 128, 256)으로, 근사할 계급의 크기를 결정하는 하이퍼파라미터이다.39

구체적인 메커니즘은 다음과 같다. $i$번째 어텐션 헤드에 대해, 학습 가능한 투영 행렬 $E_i$와 $F_i$ (모두 $N \times k$ 크기)를 도입한다. 기존의 키와 값 행렬 $K$와 $V$는 먼저 $E_i$와 $F_i$에 의해 각각 $k \times d$ 크기의 $K'$와 $V'$로 투영된다. 그 후, 쿼리 $Q$는 이 압축된 키 $K'$와 어텐션을 계산하여 $N \times k$ 크기의 근사된 컨텍스트 매핑 행렬 $\tilde{P}$를 생성한다. 최종 컨텍스트 벡터는 이 $\tilde{P}$와 압축된 값 $V'$를 곱하여 얻는다.17 수식으로 표현하면 다음과 같다.
$$
\tilde{P} = \text{softmax}\left(\frac{Q(E_i K)^T}{\sqrt{d_k}}\right)
$$

$$
\text{Context} = \tilde{P}(F_i V)
$$

이 과정에서 가장 계산량이 많은 부분은 $Q(E_iK)^T$인데, 이는 $(N, d)$와 $(d, k)$ 행렬의 곱이므로 $O(Nkd)$의 복잡도를 가진다. $k$를 시퀀스 길이 $N$과 무관한 상수로 고정하면, 전체 어텐션 복잡도는 $O(Nk)$에서 $O(N)$으로 감소한다.39

Linformer는 모델의 효율성을 더욱 높이기 위해 투영 행렬 $E_i$, $F_i$를 공유하는 다양한 전략을 제안한다.38 예를 들어, 한 레이어 내의 모든 헤드가 동일한 투영 행렬을 공유하는 '헤드 단위 공유(headwise sharing)', 키와 값의 투영 행렬을 동일하게 사용하는 '키-값 공유(key-value sharing)', 심지어 모델 전체에서 단 하나의 투영 행렬을 공유하는 '레이어 단위 공유(layerwise sharing)' 등이 있다. 놀랍게도, 가장 극단적인 형태인 레이어 단위 공유 전략이 종종 가장 좋은 성능을 보였는데, 이는 어텐션의 저계급 구조가 모델 전반에 걸쳐 보편적인 특성일 수 있음을 시사한다.39


Linformer가 어텐션 행렬의 '결과물'에 대한 경험적 가정에 기반했다면, Performer는 어텐션 '과정'의 핵심인 $\text{softmax}$ 함수를 직접 근사하는 '함수적 근사' 철학을 따른다.33 이는 특정 데이터 분포나 학습된 가중치에 대한 가정 없이, 보다 일반적인 수학적 원리에 기반하여 선형 복잡도를 달성하고자 하는 시도이다.

Performer의 핵심은 FAVOR+(Fast Attention Via Positive Orthogonal Random Features)라는 알고리즘이다.33 FAVOR+는 커널 이론에 기반하여 $\text{softmax}$ 커널 $\exp(x^Ty)$를 무작위 특징 맵(random feature maps) $\phi$의 내적 기댓값 $E$으로 근사한다. 여기서 특징 맵 $\phi$는 무작위로 샘플링된 벡터들과의 내적을 삼각함수나 지수 함수와 같은 비선형 함수에 통과시켜 구성된다.33 예를 들어, 가우시안 커널은 무작위 가우시안 투영과 $\sin$, $\cos$ 함수의 조합으로 근사될 수 있다. $\text{softmax}$ 커널은 여기에 추가적인 스케일링 함수 $h(x)$를 곱하여 근사한다.

FAVOR+의 핵심적인 혁신은 근사의 효율성과 안정성을 크게 향상시킨 두 가지 기법에 있다. 첫째, '양수(Positive)' 특징 맵을 사용하여 근사 과정에서 음수 값이 발생하지 않도록 보장한다. 이는 $\text{softmax}$의 출력이 항상 양수인 특성과 부합하며, 학습 안정성을 높이는 데 기여한다.33 둘째, '직교(Orthogonal)' 무작위 특징을 사용하여 근사의 분산(variance)을 줄인다. 무작위로 샘플링된 특징 벡터들이 서로 직교하도록 만들면, 더 적은 수의 무작위 특징($r$의 값)으로도 원본 $\text{softmax}$ 커널을 더 정확하게 근사할 수 있다.44

이러한 수학적 엄밀함 덕분에, Performer는 다른 휴리스틱한 접근법들과 달리 $\text{softmax}$ 어텐션 행렬에 대한 불편향(unbiased) 또는 거의 불편향된 추정을 제공하며, 균등 수렴(uniform convergence) 및 낮은 추정 분산에 대한 강력한 이론적 보장을 갖추고 있다.41 이는 Performer가 단순히 빠른 모델이 아니라, 이론적으로도 견고한 기반 위에서 작동함을 의미한다.

또한, Performer의 프레임워크는 매우 일반적이어서 $\text{softmax}$ 외의 다른 커널 함수로 쉽게 확장될 수 있다. 실제로 실험에서는 $\text{ReLU}$를 활성화 함수로 사용하는 'Performer-ReLU'가 특정 태스크에서 $\text{softmax}$ 기반 Performer보다 더 나은 성능을 보이기도 했다.41 이는 $\text{softmax}$가 어텐션을 위한 유일하거나 최적의 선택이 아닐 수 있다는 중요한 가능성을 제시하며, 새로운 어텐션 메커니즘을 탐구할 수 있는 길을 열어주었다.

이처럼 Linformer와 Performer는 선형 복잡도라는 동일한 목표를 향해 나아가지만, 그 경로와 철학은 근본적으로 다르다. Linformer는 학습된 어텐션의 '구조'에 대한 경험적 관찰을 바탕으로 모델 자체를 제약하는 귀납적 접근을 취하는 반면, Performer는 $\text{softmax}$라는 함수의 '성질'을 수학적으로 근사하는 연역적 접근을 취한다. 이러한 철학의 차이는 각 모델의 강점과 약점으로 이어진다. Linformer는 직관적이고 구현이 비교적 간단하지만, 어텐션이 본질적으로 고계급(high-rank) 정보를 요구하는 태스크에서는 성능 한계를 보일 수 있다. 반면 Performer는 이론적으로 견고하고 일반적이지만, 무작위성에 의존하기 때문에 근사 오차로 인한 성능 변동의 가능성을 내포한다.


초기 선형화 어텐션 모델들이 $O(N)$ 복잡도 달성에 집중하면서, 그 부작용으로 학습 불안정성과 성능 저하라는 새로운 문제들이 대두되었다. 후속 연구들은 이러한 문제들을 해결하고 선형 어텐션의 실용성을 높이는 데 초점을 맞추었다. 이는 연구의 흐름이 '효율성 달성'이라는 1차 목표에서 '부작용 해결'이라는 2차 목표로 진화했음을 보여준다.


TransNormer는 기존 커널 기반 선형 어텐션이 겪는 두 가지 근본적인 문제, 즉 **비제한적 경사도(unbounded gradients)**와 **어텐션 희석(attention dilution)**을 정면으로 다룬다.46

첫째, **비제한적 경사도** 문제는 $\text{softmax}$ 함수가 제거되면서 발생한다. 표준 어텐션에서 $\text{softmax}$는 스케일링과 함께 어텐션 스코어를 $$ 범위로 정규화하여 경사도를 안정시키는 역할을 한다. 그러나 많은 선형 어텐션은 이 기능이 없어, 특정 입력에 대해 경사도 값이 무한히 커질 수 있다. 이는 학습 과정을 불안정하게 만들고 수렴을 방해하는 주요 원인이 된다.47 TransNormer는 이 문제를 해결하기 위해 **NormAttention**이라는 새로운 모듈을 제안한다. NormAttention은 어텐션 행렬 자체에 대한 스케일링을 제거하는 대신, 어텐션 연산이 끝난 *이후*에 Layer Normalization이나 RMS Normalization과 같은 표준화 연산을 적용한다. 이 '사후 정규화' 방식은 경사도를 효과적으로 제어하여 학습 안정성을 크게 향상시킨다.47

둘째, **어텐션 희석** 문제는 $\text{softmax}$의 지수 함수가 가진 '집중' 효과가 사라지면서 발생한다. $\text{softmax}$는 중요한 소수의 토큰에 높은 가중치를 부여하고 나머지는 거의 무시하도록 만드는 경향이 있다. 반면, 선형 어텐션은 가중치가 시퀀스 전체에 비교적 균등하게 분산되는 경향을 보인다. 이로 인해 모델이 중요한 지역적(local) 정보에 집중하지 못하고 문맥 정보가 희석되어 성능이 저하된다.46 TransNormer는 이 문제를 완화하기 위해 **대각 어텐션(Diagonal Attention)**을 도입한다. 이는 모델의 초기 레이어에서 각 토큰이 자신의 주변 이웃 토큰에만 집중하도록 제한하는 일종의 윈도우 어텐션이다. 이를 통해 모델이 먼저 지역적 문맥을 확실히 학습하도록 유도하고, 이후 상위 레이어에서는 NormAttention을 통해 전역적 문맥을 학습하게 한다. 이 계층적 접근은 $\text{softmax}$ 어텐션의 지역적 집중 특성을 모방하여 어텐션 희석 문제를 효과적으로 해결한다.47


Based는 선형 어텐션의 한계를 다른 관점에서 접근한다. 선형 어텐션 자체를 수정하기보다는, 선형 어텐션이 가지는 약점을 보완할 수 있는 다른 메커니즘을 결합하는 하이브리드(hybrid) 아키텍처를 제안한다.48 Based는 **선형 어텐션**과 **슬라이딩 윈도우 어텐션**이라는 두 가지 잘 알려진 요소를 결합하여 시너지 효과를 창출한다.

- **선형 어텐션**은 모델이 시퀀스 전체에 걸친 **전역적(long-range) 의존성**을 포착하는 역할을 담당한다. Based는 특징 맵으로 $\exp(QK^T)$의 2차 테일러 근사(Taylor approximation)를 사용하는데, 이는 계산 효율성과 표현력 사이의 좋은 균형점을 제공한다.49
- **슬라이딩 윈도우 어텐션**은 **지역적(local) 상호작용**과 특정 정보를 정확하게 찾아내는 **연관 리콜(associative recall)** 능력을 담당한다.48 순수 선형 어텐션은 어텐션 희석 문제 등으로 인해 이러한 정밀한 지역 정보 처리에 약점을 보이는 경향이 있는데 48, 슬라이딩 윈도우 어텐션이 이 부분을 효과적으로 보완해 준다. Based는 특히 64 또는 128과 같이 매우 작은 윈도우 크기를 사용하여, 하드웨어 효율성을 극대화하면서도 필요한 지역 정보를 충분히 포착한다.49

이 두 메커니즘은 상호 보완적이다. 선형 어텐션이 전체적인 문맥의 숲을 본다면, 슬라이딩 윈도우 어텐션은 그 숲 속의 특정 나무들을 자세히 살피는 역할을 한다. 이 결합을 통해 Based는 Mamba와 같은 최신 상태 공간 모델과 경쟁할 수 있는 강력한 성능을 보이면서도, 특히 정보 리콜이 중요한 태스크에서 뛰어난 능력을 입증했다.48 이는 선형 어텐션의 미래가 단일 메커니즘의 완벽한 구현이 아니라, 여러 메커니즘의 장점을 지능적으로 조합하는 방향으로 나아갈 수 있음을 시사한다.


아래 표는 본문에서 논의된 주요 선형화 어텐션 모델들의 핵심 아이디어와 특성을 요약하여 비교한다. 각 모델이 어떤 철학을 바탕으로 어떤 문제를 해결하려 했는지, 그리고 그에 따른 장단점이 무엇인지를 한눈에 파악할 수 있도록 구성되었다.

| 모델 (Model)    | 핵심 아이디어 (Core Idea) | 복잡도 (Complexity)                 | 기반 기술 (Underlying Technology)    | 장점 (Pros)                                                  | 단점 (Cons)                                                  |
| --------------- | ------------------------- | ----------------------------------- | ------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Linformer**   | 저계급 행렬 근사          | $O(Nk)$                             | 선형 투영 (Linear Projection)        | 개념이 직관적이고 구현이 간단함.                             | 어텐션 행렬이 고계급(high-rank) 정보를 요구할 경우 성능 저하 발생. |
| **Performer**   | `softmax` 커널 근사       | $O(Nr)$ ($r$은 무작위 특징 수)      | FAVOR+ (무작위 특징 맵)              | 강력한 이론적 보장 (불편향 추정). 다른 커널로 확장이 용이한 일반성. | 무작위성에 기반하여 성능 변동 가능성이 있으며, 근사 오차가 발생할 수 있음. |
| **TransNormer** | 학습 안정성 및 성능 개선  | $O(Nd^2)$                           | NormAttention & Diagonal Attention   | 비제한적 경사도 및 어텐션 희석 문제 해결. 학습 안정성 및 수렴성 향상. | 아키텍처가 상대적으로 복잡해짐. 두 가지 새로운 모듈의 도입 필요. |
| **Based**       | 하이브리드 접근법         | $O(N(d^2+w^2))$ ($w$는 윈도우 크기) | 선형 어텐션 + 슬라이딩 윈도우 어텐션 | 전역 및 지역 정보 처리의 균형. 연관 리콜(associative recall) 성능 우수. | 두 메커니즘의 상호작용을 고려한 하이퍼파라미터 튜닝이 필요함. |


선형화 어텐션은 트랜스포머의 근본적인 확장성 문제를 해결하기 위한 강력한 대안으로 부상했지만, 그 과정에서 새로운 기술적 난제와 트레이드오프를 드러냈다. 이는 효율성과 성능 사이의 복잡한 관계를 보여주며, 선형화 어텐션이 '만병통치약'이 아님을 명확히 한다.


선형화 어텐션의 가장 명백하고 강력한 장점은 계산 효율성의 혁신이다.

첫째, **선형 복잡도**를 달성했다는 점이다. 시퀀스 길이 $N$에 대한 시간 및 공간 복잡도를 $O(N^2)$에서 $O(N)$으로 줄임으로써, 기존 트랜스포머 아키텍처로는 상상하기 어려웠던 초장문 시퀀스의 처리를 이론적으로 가능하게 했다.16 이는 수만 토큰 이상의 문서를 한 번에 처리하거나, 고해상도 이미지를 더 세밀한 패치로 분할하여 분석하는 등 새로운 응용의 문을 열었다.

둘째, **빠른 자기회귀 추론** 능력이다. 2.4절에서 설명했듯이, 선형 어텐션은 RNN과 유사한 순환적 형태로 표현될 수 있다. 이 특성은 특히 대규모 언어 모델(LLM)의 텍스트 생성과 같이 한 번에 한 토큰씩 순차적으로 결과를 만들어내는 자기회귀 추론 과정에서 빛을 발한다.31 표준 트랜스포머는 다음 토큰을 생성할 때마다 이전에 생성된 모든 토큰과의 관계를 다시 계산해야 하므로, 생성된 시퀀스가 길어질수록 추론 속도가 선형적으로 느려진다. 반면, 선형 어텐션은 고정된 크기의 상태(state) 행렬만 업데이트하면 되므로, 시퀀스 길이에 거의 영향을 받지 않고 $O(1)$에 가까운 복잡도로 다음 토큰을 예측할 수 있다.21 이는 실시간 대화형 AI나 긴 글을 생성하는 서비스에서 지연 시간(latency)을 획기적으로 줄일 수 있는 엄청난 잠재력을 의미한다.


이러한 혁신적인 장점에도 불구하고, 선형화 어텐션은 $\text{softmax}$ 어텐션이 제공하던 여러 유용한 특성을 포기한 대가로 여러 단점과 기술적 난제에 직면했다. 이는 효율성을 얻기 위해 치러야 하는 비용으로, 선형화 어텐션 연구의 주요 과제가 되었다.


가장 근본적인 문제는 대부분의 선형화 어텐션 모델이 표준 $\text{softmax}$ 어텐션에 비해 정확도, Perplexity 등 핵심 성능 지표에서 열세를 보인다는 점이다.47 이는 선형화 과정에서 사용되는 커널 함수가 $\text{softmax}$의 고유한 비선형적 특성을 완벽하게 모사하지 못하는 근사 오차에서 비롯된다.34

최근 연구는 이 성능 저하의 근본적인 원인 중 하나로 **비단사성(Non-Injectivity)**을 지목했다.52

$\text{softmax}$ 함수는 입력 벡터가 다르면 출력 벡터도 달라지는 단사 함수(injective function)의 특성을 가진다. 즉, 서로 다른 쿼리-키 쌍은 서로 다른 어텐션 분포를 생성한다. 하지만 `ReLU`나 `ELU` 기반의 많은 선형 커널 함수는 단사 함수가 아니다. 이로 인해 서로 다른 쿼리 벡터가 동일한 키 벡터 집합에 대해 동일한 어텐션 가중치를 부여하는 **의미론적 혼동(semantic confusion)** 현상이 발생할 수 있다. 이는 모델이 미묘한 문맥 차이를 구분하는 능력을 저해하고, 결과적으로 전체적인 표현력을 약화시키는 원인이 된다.52


$\text{softmax}$ 함수는 단순한 비선형 변환을 넘어, 학습 과정을 안정시키는 암묵적인 '안전장치' 역할을 수행한다. 선형화는 이 안전장치를 제거하는 것과 같아서, 학습 과정에서 여러 불안정성 문제를 야기한다.

- **비제한적 경사도 (Unbounded Gradients):** $\text{softmax}$는 출력을 확률 분포처럼 $$ 범위로 정규화하고, 입력값의 스케일을 조절하여 경사도의 크기를 안정적으로 유지한다. 선형 어텐션에서는 이러한 메커니즘이 부재하여, 특정 입력값에 대해 경사도가 비정상적으로 폭증하거나 소실될 수 있다.46 이는 모델 학습을 불안정하게 만들고, 최적의 해로 수렴하는 것을 방해한다. TransNormer가 이 문제를 해결하기 위해 사후 정규화(post-normalization)를 제안한 것도 이 때문이다.47
- **어텐션 희석 (Attention Dilution):** $\text{softmax}$ 내부의 지수 함수($\exp$)는 입력값의 차이를 증폭시키는 효과가 있다. 즉, 유사도 점수가 조금이라도 높은 토큰에 훨씬 더 큰 가중치를 부여하고, 나머지는 거의 0에 가까운 가중치를 할당하여 어텐션을 '집중'시킨다. 반면, 대부분의 선형 커널은 이러한 집중 효과가 약해, 어텐션 가중치가 시퀀스 전체에 얕고 넓게 퍼지는 경향이 있다.46 이를 '어텐션 희석'이라 하며, 모델이 핵심적인 정보에 집중하지 못하고 주변 노이즈에 의해 방해받게 만들어 성능 저하의 주요 원인이 된다.


선형 어텐션의 빠른 추론 능력은 RNN과 유사한 고정 크기의 상태 행렬에 과거 정보를 압축하는 데서 비롯된다. 그러나 이는 역설적으로 **정보 병목(information bottleneck)** 문제를 야기할 수 있다.34 시퀀스가 매우 길어질 경우, 고정된 크기의 상태 행렬이 과거의 모든 중요한 정보를 손실 없이 담는 것은 불가능하다.35 이는 표준 어텐션이 전체 KV 캐시를 통해 과거 정보에 손실 없이 접근할 수 있는 것과 대조되는 근본적인 한계이다.

또한, GPT와 같은 자기회귀 모델을 학습시킬 때 필수적인 **인과적 마스킹(causal masking)**은 선형화 어텐션의 병렬 학습을 방해하는 심각한 걸림돌이 된다.31 인과적 마스킹은 각 타임스텝이 미래의 정보를 참조하지 못하도록 어텐션 스코어 행렬의 상삼각 부분을 마스킹하는 작업이다. 이 마스킹 연산은 행렬 곱의 결합법칙과 호환되지 않는다. 즉, $\text{Mask}((AB)C) \neq A(\text{Mask}(B)C)$ 이다. 따라서 $\Phi(Q)(\Phi(K)^TV)$와 같은 계산 순서 변경을 직접 적용할 수 없게 된다.31 이를 해결하기 위해서는 시퀀스를 순차적으로 처리하며 누적 합(prefix-sum)을 계산해야 하는데 44, 이는 GPU의 대규모 병렬 처리 능력을 충분히 활용하지 못하게 만들어 학습 속도를 저해한다. 이처럼 추론 시의 효율성을 위해 도입된 순환적 구조가 학습 시에는 병렬성을 해치는 **'병렬성 역설(Parallelism Paradox)'**이 발생하는 것이다.


선형화 어텐션은 고립된 기술이 아니라, '효율적 트랜스포머'라는 더 큰 생태계 내에서 다른 접근법들과 경쟁하고 상호작용한다. 아래 표는 선형화 어텐션을 표준 어텐션, 그리고 다른 주요 효율화 기법들과 비교하여 그 트레이드오프 관계를 명확히 보여준다.

| 메커니즘 (Mechanism)           | 계산 복잡도 (Computational Complexity) | 메모리 복잡도 (Memory Complexity) | 대표 태스크 성능 (Performance)                  | 주요 한계점 (Key Limitations)                                |
| ------------------------------ | -------------------------------------- | --------------------------------- | ----------------------------------------------- | ------------------------------------------------------------ |
| **표준 어텐션 (Standard)**     | $O(N^2 d)$                             | $O(N^2)$                          | 기준 성능 (Baseline)                            | 긴 시퀀스 처리 사실상 불가.                                  |
| **FlashAttention**             | $O(N^2 d)$ (이론적)                    | $O(N)$ (실질적 I/O)               | 표준 어텐션과 수학적으로 동일 (근사 없음).      | 하드웨어(GPU SRAM)에 의존적이며, 여전히 $N^2$에 비례하는 연산을 수행함. |
| **선형화 어텐션 (Linearized)** | $O(Nd^2)$                              | $O(Nd^2)` (학습 시)               | 기준 대비 소폭 하락하는 경향.                   | 근사로 인한 성능 저하, 학습 불안정성, 표현력 한계(비단사성 등). |
| **상태 공간 모델 (SSMs)**      | $O(N d \log(d))$ 또는 $O(Nd^2)$        | $O(Nd)`                           | 선형 어텐션과 경쟁적이며, 일부 태스크에서 우위. | 연관 리콜 등 특정 태스크에서 약점을 보일 수 있으며, 수학적 배경이 복잡함. |

이 표는 선형화 어텐션이 왜 '만병통치약'이 될 수 없는지를 명확히 보여준다. 예를 들어, FlashAttention은 알고리즘의 이론적 복잡도를 바꾸지 않고 하드웨어 최적화를 통해 실제 벽시계 시간(wall-clock time)을 단축함으로써, 근사로 인한 성능 저하 없이 중간 길이의 시퀀스(예: 16K~32K) 처리를 실용적으로 만들었다.51 이는 선형화 어텐션이 제공하는 이론적 효율성이 항상 실제 속도 우위로 이어지지 않을 수 있음을 시사한다. 또한, Mamba와 같은 상태 공간 모델(SSM)은 선형 복잡도를 달성하면서도 다른 종류의 유도 편향을 제공하여 특정 태스크에서 선형 어텐션보다 우수한 성능을 보이기도 한다.36 따라서, 최적의 아키텍처 선택은 단순히 이론적 복잡도뿐만 아니라, 요구되는 성능 수준, 가용 하드웨어, 그리고 특정 태스크의 성격까지 종합적으로 고려해야 하는 다차원적인 문제이다.


선형화 어텐션의 효용성은 다루는 데이터의 특성과 태스크의 요구사항에 따라 크게 달라진다. 시퀀스 길이가 극단적으로 길고, 개별 토큰 간의 정밀한 상호작용보다 전역적인 패턴 인식이 더 중요한 분야에서 그 잠재력이 극대화된다. 반면, 미묘한 문맥 파악과 정확한 정보 추출이 중요한 태스크에서는 성능 저하라는 단점이 더 부각될 수 있다.


**문제점:** 표준 트랜스포머 기반의 요약 모델(예: BART, T5)은 일반적으로 512에서 4096 토큰 사이의 컨텍스트 윈도우 한계를 가진다.54 이로 인해 학술 논문, 정부 보고서, 법률 문서, 소설과 같이 수만 토큰에 달하는 장문서를 직접 처리하는 것이 불가능하다.18 기존의 해결책은 문서를 여러 개의 작은 덩어리(chunk)로 나누어 각각 요약한 후, 이를 다시 합치거나 재귀적으로 요약하는 방식이었으나, 이 과정에서 문맥이 단절되거나 중요한 정보가 손실될 위험이 컸다.56

**활용:** 선형화 어텐션은 이러한 길이 제약을 이론적으로 해결할 수 있는 강력한 도구를 제공한다. 모델이 문서 전체를 한 번에 읽고 전역적인 문맥을 파악할 수 있게 함으로써, 더 일관성 있고 정확한 요약을 생성할 잠재력을 가진다. 실제로, HEPOS와 같은 모델은 Linformer와 같은 저계급 근사 기법이나 희소 어텐션 패턴을 인코더-디코더 어텐션에 결합하여 10,000개 이상의 토큰을 처리하고, PubMed 및 GOVREPORT와 같은 장문서 요약 벤치마크에서 기존 모델을 능가하는 최첨단(SOTA) 성능을 달성했다.58 이는 선형화 어텐션이 장문서 요약의 실질적인 성능 향상에 기여할 수 있음을 보여주는 사례이다.

**한계 및 동향:** 그러나 단순히 표준 어텐션을 선형 어텐션으로 교체하는 것만으로는 최적의 결과를 얻기 어렵다. 장문서 요약은 문서의 특정 핵심 문장이나 구절을 정확하게 '찾아내는(pinpoint)' 능력이 매우 중요하다.58

$\text{softmax}$ 어텐션의 강력한 가중치 집중 능력은 이 작업에 매우 유리한 반면, 선형 어텐션의 '어텐션 희석' 경향은 핵심 정보를 놓칠 위험을 증가시킨다. 이 때문에 최근 연구들은 선형 어텐션을 다른 기법과 결합하는 하이브리드 방식에 주목하고 있다. 예를 들어, 문서를 문단이나 문장 단위로 나누어 계층적으로 처리하는 모델 18이나, 먼저 핵심 문장을 추출(extractive)한 후 이를 바탕으로 요약문을 생성(abstractive)하는 방식 55 등이 제안되었다. 이는 선형 어텐션이 장문서 처리의 '기반 기술'로서 기능하되, 태스크의 특수성을 고려한 추가적인 아키텍처 설계가 동반되어야 함을 시사한다.


**문제점:** Vision Transformer(ViT)는 이미지를 작은 패치(patch)들의 시퀀스로 간주하여 처리한다.60 예를 들어, 224x224 크기의 이미지를 16x16 패치로 나누면 `14x14=196`개의 패치 시퀀스가 생성된다. 이미지의 해상도가 높아질수록 패치의 수($N$)는 제곱에 비례하여 급증한다. 1024x1024 해상도의 이미지는 4096개의 패치를 생성하며, 이는 NLP에서의 장문서와 유사한 계산량 문제를 야기한다.61 따라서 표준 $\text{softmax}$ 어텐션은 고해상도 이미지 처리에 있어 심각한 계산 병목이 된다.62

**활용:** 컴퓨터 비전 태스크는 선형화 어텐션의 장점이 극대화되는 분야 중 하나이다. 일반적으로 이미지 패치의 수($N$, 수천 단위)가 임베딩 차원($d$, 수백 단위)보다 훨씬 크기 때문에($N \gg d$), $O(N^2d)$에서 $O(Nd^2)$로의 복잡도 감소 효과가 매우 크다.61 이를 바탕으로 FLatten Transformer, Sana와 같은 모델들은 선형 어텐션을 핵심 모듈로 채택하여 고해상도 이미지 분류, 객체 탐지, 의미론적 분할(semantic segmentation) 등의 태스크에서 높은 효율성과 성능을 동시에 달성했다.61 특히, Sana 모델은 선형 어텐션과 고압축 오토인코더를 결합하여 4096x4096 해상도의 이미지를 노트북 GPU에서도 빠르게 생성할 수 있음을 보여주며 선형 어텐션의 실용적 가치를 입증했다.63

**최신 동향:** 비전 분야의 연구는 선형 어텐션의 고질적인 문제인 표현력 부족을 해결하는 데 집중하고 있다. 선형 어텐션은 어텐션 맵이 전반적으로 밋밋해져 객체의 세밀한 경계나 질감을 포착하는 데 어려움을 겪을 수 있다. 이를 해결하기 위해 FLatten Transformer는 어텐션 가중치를 더 구별되게 만드는 '포커싱 함수(focusing function)'와, 어텐션 행렬의 랭크를 복원하여 특징의 다양성을 높이는 '랭크 복원 모듈(rank restoration module)'을 제안했다.61 또한, 최근 InLine Attn 연구에서는 선형 어텐션에 '단사성(injectivity)'과 '지역 모델링 능력(local modeling ability)'을 부여함으로써, $\text{softmax}$ 어텐션의 성능을 능가할 수 있음을 실험적으로 증명했다.52 이는 선형 어텐션이 단순히 $\text{softmax}$의 효율적인 근사체가 아니라, 적절한 보완을 통해 그 자체로 더 우수한 성능을 낼 수 있는 잠재력을 가졌음을 보여주는 중요한 발견이다.


**문제점:** 유전체(DNA/RNA)와 단백질 서열은 인공지능이 다루는 데이터 중 가장 긴 시퀀스에 속한다. 인간 유전체는 수십억 개의 염기쌍으로 이루어져 있으며, 하나의 유전자나 단백질 서열만 해도 수천에서 수백만 개의 단위(염기 또는 아미노산)로 구성된다. DNABERT나 Nucleotide Transformer와 같은 기존의 트랜스포머 기반 모델들은 강력한 성능에도 불구하고, 최대 입력 길이에 대한 제약 때문에 전체 유전체나 긴 단백질을 한 번에 분석하지 못하고, 서열을 작은 조각으로 나누어 처리해야 했다.64

**잠재력:** 이러한 극단적인 길이의 시퀀스 데이터는 선형화 어텐션이 가장 큰 잠재력을 발휘할 수 있는 분야이다. $N$이 압도적으로 크기 때문에 $O(N^2)$ 복잡도를 가진 아키텍처는 아예 고려 대상이 될 수 없으며, $O(N)$ 복잡도는 사실상 필수 조건이다.66 선형 어텐션을 통해 전체 서열의 장거리 상호작용(long-range interactions)을 모델링할 수 있게 되면, 단백질의 3차원 구조 예측(protein folding), 특정 기능 예측, 유전자 발현 조절 메커니즘 규명 등 생물학의 난제들을 해결하는 데 획기적인 돌파구를 마련할 수 있다.67 예를 들어, 단백질의 접힘 구조는 서열 상에서 멀리 떨어진 아미노산 잔기들 간의 상호작용에 의해 결정되는데, 선형 어텐션은 이러한 상호작용을 직접적으로 모델링하는 데 이상적인 도구이다.70

**현실:** 아직 생물정보학 분야에서 선형 어텐션이 표준으로 자리 잡지는 못했지만, 그 가능성을 탐색하는 연구들이 활발히 진행 중이다. 예를 들어, 단백질 서열 데이터셋을 이용한 실험에서 Performer 모델이 Linformer나 Reformer와 같은 다른 효율적 트랜스포머보다 우수한 성능을 보인 사례가 있다.45 이는 $\text{softmax}$를 더 정밀하게 근사하는 방식이 생물학적 서열의 복잡한 패턴을 포착하는 데 더 유리할 수 있음을 시사한다. 비록 현재는 AlphaFold2와 같이 $\text{softmax}$ 기반의 고도로 최적화된 모델이 단백질 구조 예측 분야를 지배하고 있지만 71, 처리 가능한 서열의 길이를 확장하고 새로운 종류의 상호작용을 발견하기 위한 미래 연구에서 선형 어텐션의 역할은 더욱 중요해질 것으로 전망된다.67

결론적으로, 선형화 어텐션은 만능 해결책이 아니라, 각 도메인의 데이터 특성과 문제 정의에 따라 그 가치가 달라지는 '특화된 도구'로 이해해야 한다. $N$이 $d$에 비해 압도적으로 크고, 전역적 패턴 인식이 중요한 비전 및 생물정보학 분야에서 가장 큰 파급력을 가질 것으로 기대되며, 정밀한 정보 추출이 중요한 NLP 분야에서는 다른 메커니즘과의 현명한 결합을 통해 그 효용성을 찾아가고 있다.


선형화 어텐션은 트랜스포머 아키텍처의 핵심적인 한계였던 이차 복잡도 문제를 해결하기 위한 중요한 이론적, 실용적 돌파구를 제시했다. 커널 이론과 행렬 곱 결합법칙을 통해 계산 복잡도를 $O(N)$으로 낮춤으로써, 이전에는 불가능했던 초장문 시퀀스 처리에 대한 가능성을 열었다. 그러나 이 과정에서 $\text{softmax}$ 어텐션이 제공하던 암묵적인 유도 편향들을 포기하게 되면서, 성능 저하, 학습 불안정성, 표현력 한계와 같은 새로운 문제들에 직면했다. 이로 인해 선형화 어텐션은 $\text{softmax}$ 어텐션을 완전히 대체하기보다는, 효율성과 효과성 사이의 복잡한 트레이드오프 관계 속에서 특정 목적에 맞는 하나의 선택지로 자리매김하게 되었다.


초기 선형화 어텐션 연구의 병목이 '계산 복잡도' 자체였다면, 현재와 미래의 연구는 '컨텍스트 활용 능력'과 '실제 하드웨어 처리량'이라는 새로운 병목으로 그 초점을 옮겨가고 있다. 선형화 어텐션이 첫 번째 병목을 해결했지만, 그 자체만으로는 후자의 문제들에 대한 완전한 해답이 되지 못하기 때문이다. 이러한 배경 속에서, 선형화 어텐션을 넘어서거나 보완하는 새로운 패러다임들이 부상하고 있다.

- **하이브리드 아키텍처 (Hybrid Architectures):** 단일 메커니즘의 한계를 극복하기 위해 여러 접근법의 장점을 결합하는 연구가 활발하다. Based 모델은 선형 어텐션의 전역적 문맥 포착 능력과 슬라이딩 윈도우 어텐션의 지역적 정밀성을 결합하여 성능과 효율의 균형을 맞추었다.48 Longformer와 같은 희소 어텐션 모델 역시 지역적 윈도우와 일부 전역적 토큰을 결합하는 하이브리드 방식이다.12 이는 미래의 아키텍처가 단일한 형태가 아닌, 여러 모듈을 조합하는 형태로 발전할 것임을 시사한다.
- **상태 공간 모델 (State Space Models, SSMs):** Mamba와 같은 SSM은 어텐션 메커니즘과는 다른 경로를 통해 선형 복잡도를 달성한다.36 RNN과 CNN의 원리를 현대 제어 이론과 결합하여, 입력에 따라 동적으로 변하는 상태(state)를 통해 장거리 의존성을 효율적으로 모델링한다. SSM은 특히 연속적인 데이터(raw audio, DNA) 처리와 연관 리콜(associative recall)과 같은 특정 태스크에서 강점을 보이며, 어텐션 기반 모델의 강력한 대안으로 부상하고 있다.37


최근 Gemini 1.5 Pro(1M 토큰)나 Llama 4(10M 토큰)와 같은 모델들이 등장하며 '긴 컨텍스트'의 정의가 급격하게 확장되고 있다.73 이는 선형화 어텐션과 같은 효율적인 아키텍처의 중요성을 더욱 부각시키는 동시에, 새로운 도전 과제를 제시한다.

현재 가장 큰 논쟁 중 하나는 이러한 초장문 컨텍스트 모델이 외부 지식 베이스를 실시간으로 참조하는 RAG(Retrieval-Augmented Generation)를 대체할 수 있을지에 대한 것이다.75 이론적으로는 모든 정보를 컨텍스트에 넣고 처리하는 것이 이상적이지만, 실제로는 모델이 수백만 토큰의 컨텍스트 중간에 있는 정보를 정확히 찾아내지 못하는 'lost in the middle' 현상이 관찰된다.73 이는 계산량 병목이 모델의 '인지 능력' 병목으로 이동하고 있음을 보여준다. 따라서 당분간은 RAG와 장문 컨텍스트 모델이 상호 보완적으로 발전할 가능성이 높으며, 선형 어텐션은 이 거대한 컨텍스트를 효율적으로 처리하는 기반 기술로서 그 역할을 계속 수행할 것이다.


AI 아키텍처의 발전은 순수한 알고리즘 혁신만으로 이루어지지 않는다. 알고리즘과 하드웨어의 상호작용은 효율성을 결정하는 또 다른 핵심 축이며, 이는 마치 서로를 추월하며 발전하는 '군비 경쟁'과 같다.

FlashAttention의 등장은 이 관계를 극적으로 보여준 사례이다.51 FlashAttention은 $O(N^2)$의 이론적 복잡도를 가진 표준 어텐션 알고리즘을 변경하지 않았다. 대신, GPU의 메모리 계층 구조(느린 HBM과 빠른 SRAM) 사이의 데이터 이동(I/O)을 최소화하는 방식으로 연산을 재구성했다. 그 결과, 이론적으로는 더 비효율적인 알고리즘이 실제 하드웨어에서는 이론적으로 더 효율적인 (하지만 최적화되지 않은) 선형 어텐션 구현보다 훨씬 빠른 속도를 보이는 역전 현상이 발생했다.48

이러한 현상은 미래의 효율적 어텐션 연구가 더 이상 알고리즘의 이론적 복잡도에만 머물러서는 안 되며, 하드웨어 아키텍처의 특성을 깊이 이해하고 이를 고려한 설계를 해야 함을 명확히 보여준다. Mamba의 병렬 스캔(parallel scan) 알고리즘이나 Based의 IO-aware CUDA 구현과 같은 최신 연구들은 이미 이러한 방향으로 나아가고 있다.37


결론적으로, 선형화 어텐션은 트랜스포머의 확장성 문제를 해결하는 데 중요한 기여를 했으며, 그 과정에서 드러난 한계들은 오히려 더 정교하고 다각적인 후속 연구를 촉발하는 계기가 되었다. 미래의 어텐션 메커니즘은 하나의 지배적인 아키텍처가 아니라, 주어진 태스크, 데이터의 특성, 그리고 가용 하드웨어에 따라 최적의 솔루션을 동적으로 선택하고 조합하는 '모듈식' 접근법으로 발전할 가능성이 높다.

선형 어텐션의 효율성, $\text{softmax}$ 어텐션의 표현력, SSM의 순환적 특성, 그리고 하드웨어 최적화 기법들이 결합된 하이브리드 모델이 차세대 표준 아키텍처의 유력한 후보가 될 것이다. 궁극적으로 이 분야의 연구는 계산 효율성, 모델 성능, 그리고 아직 충분히 탐구되지 않은 해석 가능성(interpretability)이라는 세 마리 토끼를 모두 잡기 위한 끊임없는 여정을 계속할 것이다.19 선형화 어텐션은 그 여정에서 중요한 이정표로 기록될 것이다.


1. (PDF) Attention is All you Need (2017) | Chat PDF - Nanonets, 8월 21, 2025에 액세스, https://nanonets.com/chat-pdf/attention-is-all-you-need
2. Attention is All you Need - NIPS Paper, 8월 21, 2025에 액세스, https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
3. Attention is All you Need - NIPS, 8월 21, 2025에 액세스, https://papers.nips.cc/paper/7181-attention-is-all-you-need
4. Attention Is All You Need | Request PDF - ResearchGate, 8월 21, 2025에 액세스, https://www.researchgate.net/publication/317558625_Attention_Is_All_You_Need
5. Attention Is All You Need | by Xupeng Wang - Medium, 8월 21, 2025에 액세스, https://medium.com/@marvelous_catawba_otter_200/attention-is-all-you-need-f9fe38d6e2fc
6. Attention (machine learning) - Wikipedia, 8월 21, 2025에 액세스, https://en.wikipedia.org/wiki/Attention_(machine_learning)
7. Transformer (deep learning architecture) - Wikipedia, 8월 21, 2025에 액세스, https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)
8. 5 Attention Mechanism Insights Every AI Developer Should Know - Shelf.io, 8월 21, 2025에 액세스, https://shelf.io/blog/attention-mechanism/
9. The Detailed Explanation of Self-Attention in Simple Words | by Maninder Singh | Medium, 8월 21, 2025에 액세스, https://medium.com/@manindersingh120996/the-detailed-explanation-of-self-attention-in-simple-words-dec917f83ef3
10. What is an attention mechanism? | IBM, 8월 21, 2025에 액세스, https://www.ibm.com/think/topics/attention-mechanism
11. Attention Is All You Need - Wikipedia, 8월 21, 2025에 액세스, https://en.wikipedia.org/wiki/Attention_Is_All_You_Need
12. 긴 글을 위한 트랜스포머 모델 - Longformer와 BigBird (1편) |, 8월 21, 2025에 액세스, https://ncsoft.github.io/ncresearch/6fc502ede05ca318787928fa22332a82b112805b
13. Self-attention does not need O(n^2) memory - JungSoo_AI_Study - 티스토리, 8월 21, 2025에 액세스, https://jungsoo-ai-study.tistory.com/30
14. Attention Mechanism Complexity Analysis | by Mridul Rao - Medium, 8월 21, 2025에 액세스, https://medium.com/@mridulrao674385/attention-mechanism-complexity-analysis-7314063459b1
15. Complexity of transformer attention network : r/LanguageTechnology - Reddit, 8월 21, 2025에 액세스, https://www.reddit.com/r/LanguageTechnology/comments/9gulm9/complexity_of_transformer_attention_network/
16. On The Computational Complexity of Self-Attention - Proceedings of Machine Learning Research, 8월 21, 2025에 액세스, https://proceedings.mlr.press/v201/duman-keles23a/duman-keles23a.pdf
17. Linformer: Self-Attention with Linear Complexity, 8월 21, 2025에 액세스, https://arxiv.org/pdf/2006.04768
18. Jishnu8/Hierarchical-Transformer-for-Long-Text-Summarization - GitHub, 8월 21, 2025에 액세스, https://github.com/Jishnu8/Hierarchical-Transformer-for-Long-Text-Summarization
19. (PDF) Future Directions in Attention Research: Beyond Transformers - ResearchGate, 8월 21, 2025에 액세스, https://www.researchgate.net/publication/392364353_Future_Directions_in_Attention_Research_Beyond_Transformers
20. Linformer: Self-Attention with Linear Complexity (Paper Explained) - YouTube, 8월 21, 2025에 액세스, https://www.youtube.com/watch?v=-_2AF9Lhweo
21. Linear Transformers, 8월 21, 2025에 액세스, https://linear-transformers.com/
22. Attention is Kernel Trick Reloaded - Gokhan Egri, 8월 21, 2025에 액세스, https://egrigokhan.github.io/data/cs_229_br_Project_Report_KernelAttention.pdf
23. Implicit Kernel Attention - AAAI, 8월 21, 2025에 액세스, https://cdn.aaai.org/ojs/17168/17168-13-20662-1-2-20210518.pdf
24. An Unified Understanding for Transformer's Attention via the Lens of Kernel - ACL Anthology, 8월 21, 2025에 액세스, https://aclanthology.org/D19-1443/
25. Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding, 8월 21, 2025에 액세스, https://papers.neurips.cc/paper_files/paper/2021/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf
26. Can someone explain Kernel Trick intuitively? : r/MachineLearning - Reddit, 8월 21, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/1joh9v/can_someone_explain_kernel_trick_intuitively/
27. Kernel/Kernel trick(커널과 커널트릭) - 휴블로그 - 티스토리, 8월 21, 2025에 액세스, https://sanghyu.tistory.com/14
28. Kernel Trick의 개념적 이해 - YouTube, 8월 21, 2025에 액세스, https://www.youtube.com/watch?v=tz9XwX_mfVE
29. 커널 방법 이론(kernel method, kernel trick) - yolo_study - 티스토리, 8월 21, 2025에 액세스, [https://yololifestudy.tistory.com/entry/%EC%BB%A4%EB%84%90-%EB%B0%A9%EB%B2%95-%EC%9D%B4%EB%A1%A0kernel-method-kernel-trick](https://yololifestudy.tistory.com/entry/커널-방법-이론kernel-method-kernel-trick)
30. Linear Attention and Mamba: New Power to Old Ideas - Synthesis AI, 8월 21, 2025에 액세스, https://synthesis.ai/2024/11/20/linear-attention-and-mamba-new-power-to-old-ideas/
31. Linear Attention Fundamentals | Hailey Schoelkopf, 8월 21, 2025에 액세스, https://haileyschoelkopf.github.io/blog/2024/linear-attn/
32. Efficient Transformer Attention for GenAI - DZone, 8월 21, 2025에 액세스, https://dzone.com/articles/efficient-transformer-attention-for-genai
33. Paper Explained- Rethinking Attention with Performers - Medium, 8월 21, 2025에 액세스, https://medium.com/analytics-vidhya/paper-explained-rethinking-attention-with-performers-b207f4bf4bc5
34. Linearizing Attention. Breaking the Quadratic Barrier: Modern… | by Shitanshu Bhushan | TDS Archive | Medium, 8월 21, 2025에 액세스, https://medium.com/data-science/linearizing-attention-204d3b86cc1e
35. Is Attention All You Need?. An intuitive approach to understand the… | by Manthandeshpande | Accredian | Medium, 8월 21, 2025에 액세스, https://medium.com/accredian/is-attention-really-all-you-need-78f74dd806fa
36. On the Tradeoffs of SSMs and Transformers - YouTube, 8월 21, 2025에 액세스, https://www.youtube.com/watch?v=yirZ_XucDg8
37. On the Tradeoffs of SSMs and Transformers | Goomba Lab, 8월 21, 2025에 액세스, https://goombalab.github.io/blog/2025/tradeoffs/
38. Paper Review: Linformer: Self-Attention with Linear Complexity - Andrey Lukyanenko, 8월 21, 2025에 액세스, https://andlukyane.com/blog/paper-review-linformer
39. Brief Review — Linformer: Self-Attention with Linear Complexity | by Sik-Ho Tsang | Medium, 8월 21, 2025에 액세스, https://sh-tsang.medium.com/brief-review-linformer-self-attention-with-linear-complexity-d87fce25fe8f
40. My take on a practical implementation of Linformer for Pytorch. - GitHub, 8월 21, 2025에 액세스, https://github.com/tatp22/linformer-pytorch
41. Rethinking Attention with Performers - OpenReview, 8월 21, 2025에 액세스, https://openreview.net/forum?id=Ua6zuk0WRH
42. Performer Explained | Papers With Code, 8월 21, 2025에 액세스, https://paperswithcode.com/method/performer
43. Google AI Introduces Performer: A Generalized Attention Framework based on the Transformer architecture - MarkTechPost, 8월 21, 2025에 액세스, https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/
44. Rethinking Attention with Performers (Paper Explained) - YouTube, 8월 21, 2025에 액세스, https://www.youtube.com/watch?v=xJrKIPwVwGM
45. Brief Review — Rethinking Attention with Performers | by Sik-Ho Tsang - Medium, 8월 21, 2025에 액세스, https://sh-tsang.medium.com/brief-review-rethinking-attention-with-performers-e9fba834ab95
46. [2210.10340] The Devil in Linear Transformer - arXiv, 8월 21, 2025에 액세스, https://arxiv.org/abs/2210.10340
47. The Devil in Linear Transformer - ACL Anthology, 8월 21, 2025에 액세스, https://aclanthology.org/2022.emnlp-main.473.pdf
48. Simple linear attention language models balance the recall-throughput tradeoff - arXiv, 8월 21, 2025에 액세스, https://arxiv.org/html/2402.18668v1
49. Based: Simple linear attention language models balance the recall ..., 8월 21, 2025에 액세스, https://hazyresearch.stanford.edu/blog/2024-03-03-based
50. LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems, 8월 21, 2025에 액세스, https://arxiv.org/html/2411.01537v1
51. A Visual Guide to Flash Attention, Linear Attention, and Efficient Transformers - GoPenAI, 8월 21, 2025에 액세스, https://blog.gopenai.com/a-visual-guide-to-flash-attention-linear-attention-and-efficient-transformers-7ba1456af70a
52. Bridging the Divide: Reconsidering Softmax and Linear Attention - arXiv, 8월 21, 2025에 액세스, https://arxiv.org/html/2412.06590v1
53. Bridging the Divide: Reconsidering Softmax and Linear Attention - OpenReview, 8월 21, 2025에 액세스, [https://openreview.net/forum?id=RSiGFzQapl&referrer=%5Bthe%20profile%20of%20Yizeng%20Han%5D(%2Fprofile%3Fid%3D~Yizeng_Han1)](https://openreview.net/forum?id=RSiGFzQapl&referrer=[the+profile+of+Yizeng+Han](/profile?id%3D~Yizeng_Han1))
54. Efficient Memory-Enhanced Transformer for Long-Document Summarization in Low-Resource Regimes - PubMed Central, 8월 21, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10098576/
55. Summarizing Long Regulatory Documents with a Multi-Step Pipeline - arXiv, 8월 21, 2025에 액세스, https://arxiv.org/html/2408.09777v1
56. Automatic Summarization of Long Documents - ACL Anthology, 8월 21, 2025에 액세스, https://aclanthology.org/2024.icon-1.72.pdf
57. 4 Powerful Long Text Summarization Methods With Real Examples | Width.ai, 8월 21, 2025에 액세스, https://www.width.ai/post/4-long-text-summarization-methods
58. Efficient Attentions for Long Document Summarization - ACL Anthology, 8월 21, 2025에 액세스, https://aclanthology.org/2021.naacl-main.112.pdf
59. Long Document Summarization with Top-down and Bottom-up Inference - ACL Anthology, 8월 21, 2025에 액세스, https://aclanthology.org/2023.findings-eacl.94.pdf
60. What is a Transformer Model? - IBM, 8월 21, 2025에 액세스, https://www.ibm.com/think/topics/transformer-model
61. FLatten Transformer: Vision Transformer using Focused Linear Attention - CVF Open Access, 8월 21, 2025에 액세스, https://openaccess.thecvf.com/content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf
62. The linear attention. | Download Scientific Diagram - ResearchGate, 8월 21, 2025에 액세스, https://www.researchgate.net/figure/The-linear-attention_fig3_353697125
63. SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers, 8월 21, 2025에 액세스, https://openreview.net/forum?id=N8Oj1XhtYZ
64. Interpreting Attention Mechanisms in Genomic Transformer Models: A Framework for Biological Insights | bioRxiv, 8월 21, 2025에 액세스, https://www.biorxiv.org/content/10.1101/2025.06.26.661544v1.full-text
65. Convolutions are competitive with transformers for protein sequence pretraining - bioRxiv, 8월 21, 2025에 액세스, https://www.biorxiv.org/content/10.1101/2022.05.19.492714v2.full-text
66. transformer-based genomic prediction method fused with knowledge-guided module | Briefings in Bioinformatics | Oxford Academic, 8월 21, 2025에 액세스, https://academic.oup.com/bib/article/25/1/bbad438/7459582
67. Transformer Architecture and Attention Mechanisms in Genome Data Analysis: A Comprehensive Review - PMC, 8월 21, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10376273/
68. Transformers in Protein: A Survey - arXiv, 8월 21, 2025에 액세스, https://arxiv.org/html/2505.20098v2
69. (PDF) Attention Mechanism and Transformer Learning Based Protein Structure Prediction, 8월 21, 2025에 액세스, https://www.researchgate.net/publication/391583107_Attention_Mechanism_and_Transformer_Learning_Based_Protein_Structure_Prediction
70. Transformer-based deep learning for predicting protein properties in the life sciences | eLife, 8월 21, 2025에 액세스, https://elifesciences.org/articles/82819
71. The transformative power of transformers in protein structure prediction - PNAS, 8월 21, 2025에 액세스, https://www.pnas.org/doi/10.1073/pnas.2303499120
72. Self- and cross-attention accurately predicts metabolite–protein interactions | NAR Genomics and Bioinformatics | Oxford Academic, 8월 21, 2025에 액세스, https://academic.oup.com/nargab/article/5/1/lqad008/7016432
73. Context Rot: How Increasing Input Tokens Impacts LLM Performance | Chroma Research, 8월 21, 2025에 액세스, https://research.trychroma.com/context-rot
74. Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL Experiments, Analysis and Benchmark Paper - arXiv, 8월 21, 2025에 액세스, https://arxiv.org/html/2501.12372v5
75. Will Long-Context LLMs Make RAG Obsolete? | Medium, 8월 21, 2025에 액세스, https://masteringllm.medium.com/will-long-context-llms-make-rag-obsolete-17ddbc6f6412
76. Long Context RAG Performance of LLMs | Databricks Blog, 8월 21, 2025에 액세스, https://www.databricks.com/blog/long-context-rag-performance-llms

