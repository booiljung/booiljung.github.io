[트랜스포머 (Transformer)](./index.md)
# 희소 어텐션(Sparse Attention)


2017년 "Attention Is All You Need" 논문을 통해 트랜스포머(Transformer) 아키텍처가 세상에 등장한 이래, 인공지능 분야는 근본적인 패러다임 전환을 맞이하였다.1 순환 신경망(RNN)의 순차적 처리 방식이 가지는 장기 의존성 포착의 어려움과 병렬 처리의 한계를 극복한 셀프 어텐션(self-attention) 메커니즘은 자연어 처리(NLP) 분야를 순식간에 평정하였고, 이내 컴퓨터 비전, 음성 처리 등 인접 분야로 그 영향력을 확장하며 명실상부한 표준 아키텍처로 자리매김하였다.3 모델이 시퀀스 내의 모든 요소 쌍 간의 관계를 직접적으로 모델링하여 전역적인 문맥을 효과적으로 파악할 수 있게 한 것이 성공의 핵심 동력이었다.3

그러나 이 혁신적인 아키텍처의 성공 이면에는 치명적인 계산적 제약이 존재한다. 바로 셀프 어텐션의 계산량과 메모리 요구량이 입력 시퀀스의 길이 $n$에 대해 이차적으로($O(n^2)$) 증가하는 문제이다.6 이 이차적 복잡도는 트랜스포머가 처리할 수 있는 시퀀스 길이에 근본적인 족쇄를 채웠다. BERT와 같은 초기 모델들이 최대 512 토큰이라는 제한된 길이만을 다룰 수 있었던 이유가 바로 여기에 있다.8 이로 인해 장문서 요약, 고해상도 이미지 생성, 장시간 오디오 처리와 같이 긴 문맥에 대한 이해가 필수적인 수많은 과제에서 트랜스포머의 잠재력을 온전히 발휘하기 어려웠다.10

이러한 계산적 장벽을 극복하고 트랜스포머를 진정한 '장문맥(long-context)' 시대로 이끌기 위한 필연적인 대안으로 희소 어텐션(Sparse Attention)이 등장하였다.12 희소 어텐션의 핵심 철학은 모든 토큰 쌍 간의 상호작용이 동등하게 중요하지 않다는 관찰에서 출발한다.13 실제로 대부분의 정보 교환은 일부 핵심적인 연결을 통해 이루어지므로, 전체 어텐션 행렬($n \times n$)에서 중요도가 낮은 연결을 생략하고 일부 중요한 연결에만 계산 자원을 집중함으로써 효율성을 극대화하고자 하는 것이다.9

초기 연구는 단순히 $O(n^2)$라는 계산 복잡도를 줄이는 데 집중하였으나, 연구가 심화됨에 따라 문제는 '어떻게 모든 계산을 피할 것인가'에서 '어떤 연결을 유지해야 모델의 표현력을 보존하면서 효율성을 얻을 수 있는가'로 재정의되었다. 즉, 단순한 계산량 감소의 문제를 넘어, 정보가 모델 내에서 어떻게 효율적으로 흐를 수 있는지에 대한 '연결성(connectivity)' 설계의 문제로 발전한 것이다. 이는 Longformer의 '로컬+글로벌' 어텐션이나 BigBird의 '로컬+글로벌+랜덤' 어텐션과 같은 설계 철학에서 명확히 드러나며, 데이터의 내재적 구조에 대한 귀납적 편향(inductive bias)을 아키텍처에 주입하는 정교한 접근법으로 이어졌다.7

본 보고서는 표준 셀프 어텐션의 근본적인 한계를 명확히 규명하고, 이를 해결하기 위해 제안된 다양한 희소 어텐션 기법들의 철학과 원리를 심층적으로 분석한다. OpenAI의 Sparse Transformer, Longformer, BigBird, Reformer 등 선구적인 아키텍처들의 수학적 원리, 성능 벤치마크, 장단점을 비교 평가하고, 나아가 하드웨어 최적화 동향과 미래 연구 방향을 조망함으로써 희소 어텐션 분야에 대한 포괄적이고 깊이 있는 통찰을 제공하고자 한다.


희소 어텐션의 필요성을 이해하기 위해서는 먼저 표준 셀프 어텐션, 특히 스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)의 작동 방식과 그로 인한 계산적 병목 현상을 정확히 파악해야 한다.


트랜스포머의 셀프 어텐션은 "쿼리(Query), 키(Key), 값(Value)이라는 세 가지 벡터를 사용하여 입력 시퀀스 내의 각 요소에 대한 가중치를 계산하는 메커니즘"으로 요약할 수 있다.1 입력 시퀀스의 각 토큰 임베딩 벡터는 세 개의 서로 다른 가중치 행렬($W^Q, W^K, W^V$)과 곱해져 각각 쿼리($Q$), 키($K$), 값($V$) 벡터로 변환된다. 이 과정을 통해 모델은 각 토큰이 다른 토큰들과의 관계를 파악(쿼리-키)하고, 그 관계의 중요도에 따라 정보를 취합(값)할 수 있는 능력을 갖추게 된다.1

스케일드 닷-프로덕트 어텐션 함수는 다음 수식으로 정의된다.1
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
이 수식은 다음과 같은 단계로 해석할 수 있다.

1. **유사도 계산 (Score):** 각 쿼리 벡터는 모든 키 벡터와 내적(dot-product)을 수행한다. 이 결과($QK^T$)는 각 쿼리가 다른 모든 위치의 키와 얼마나 유사한지를 나타내는 스코어 행렬이 된다.4
2. **스케일링 (Scaling):** 계산된 스코어 행렬을 키 벡터의 차원 $d_k$의 제곱근($\sqrt{d_k}$)으로 나누어준다. 이 단계는 $d_k$ 값이 클 경우 내적 값이 지나치게 커져 소프트맥스(softmax) 함수가 포화 상태에 빠지고 기울기가 0에 가까워지는 문제(vanishing gradient)를 방지하여 학습 안정성을 높이는 중요한 역할을 한다.4
3. **정규화 (Normalization):** 스케일링된 스코어 행렬에 소프트맥스 함수를 적용하여 각 행의 합이 1이 되는 확률 분포, 즉 어텐션 가중치(attention weights)를 얻는다. 이 가중치는 각 쿼리가 어떤 값 벡터에 얼마나 '집중'해야 하는지를 결정한다.5
4. **가중합 (Weighted Sum):** 마지막으로, 계산된 어텐션 가중치 행렬을 값($V$) 행렬과 곱한다. 이를 통해 각 위치의 최종 출력 벡터는 시퀀스 내 모든 값 벡터들의 가중합으로 표현되며, 이는 문맥 정보가 풍부하게 반영된 새로운 표현이 된다.4


표준 셀프 어텐션의 강력한 표현력은 모든 토큰 쌍 간의 상호작용을 계산하는 데서 비롯되지만, 바로 이 지점이 계산적 병목을 야기하는 아킬레스건이 된다. 시퀀스 길이를 $n$, 각 토큰의 임베딩 차원을 $d$라고 가정하자. 멀티-헤드 어텐션에서는 이 차원이 헤드 수 $h$로 나뉘어 각 헤드의 차원은 $d_k = d_v = d/h$가 된다.

핵심적인 병목 현상은 쿼리 행렬 $Q$와 전치된 키 행렬 $K^T$의 곱셈에서 발생한다.6

- $Q$ 행렬의 크기는 $(n, d_k)$이다. (시퀀스 내 $n$개의 토큰 각각에 대한 $d_k$ 차원의 쿼리 벡터)
- $K$ 행렬의 크기는 $(n, d_k)$이므로, $K^T$ 행렬의 크기는 $(d_k, n)$이 된다.
- 따라서, 어텐션 스코어 행렬 $QK^T$의 크기는 $(n, d_k) \times (d_k, n) = (n, n)`이 된다.

이 $(n, n)$ 크기의 어텐션 스코어 행렬을 계산하고 메모리에 저장하는 데 필요한 시간 및 공간 복잡도는 각각 $O(n^2 \cdot d_k)$와 $O(n^2)$이다. 일반적으로 헤드 차원 $d_k$는 모델 하이퍼파라미터로 고정된 상수 취급되므로, 전체 복잡도는 시퀀스 길이 $n$에 대해 $O(n^2)$로 표현된다.6


이 $O(n^2)$ 복잡도는 시퀀스 길이가 길어질수록 재앙에 가까운 계산 비용 증가를 초래한다. 예를 들어, 시퀀스 길이가 64,000($64K$)인 문서를 처리한다고 가정해보자. 이 경우 어텐션 스코어 행렬의 크기는 $(64K, 64K)$가 되며, 각 원소를 32비트 부동소수점(4 bytes)으로 저장할 경우 필요한 메모리는 $64000 \times 64000 \times 4$ bytes, 즉 약 16GB에 달한다.14 이는 단일 어텐션 헤드, 단일 레이어에서 발생하는 메모리 요구량이며, 현대의 대규모 모델이 수십 개의 레이어와 헤드를 사용하는 것을 감안하면 사실상 감당 불가능한 수준이다.

이러한 근본적인 제약으로 인해 표준 트랜스포머는 장문서 분류, 긴 대화형 QA, 고해상도 이미지 생성, 비디오 처리 등 방대한 문맥 정보가 필수적인 수많은 현실 세계의 문제에 적용되기 어려웠다. 이는 훈련 단계뿐만 아니라 추론 단계에서도 마찬가지로 심각한 병목으로 작용하며 6, 트랜스포머 아키텍처의 잠재력을 제한하는 가장 큰 걸림돌로 지적되어 왔다.


표준 어텐션의 $O(n^2)$ 장벽을 넘기 위해 등장한 희소 어텐션은 단순히 계산을 생략하는 기술적 트릭을 넘어, 어텐션 메커니즘의 본질과 데이터의 내재적 구조에 대한 깊은 통찰에 기반한다.


희소 어텐션이 계산 효율성을 높이면서도 모델 성능을 유지하거나 심지어 향상시킬 수 있는 이유는 여러 이론적, 경험적 근거에 의해 뒷받침된다.

- **어텐션 분포의 자연적 희소성:** 대규모 언어 모델을 분석한 결과, 소프트맥스 함수를 통과한 후의 어텐션 가중치 분포는 대부분의 경우 자연적으로 희소(sparse)한 특성을 보인다.12 즉, 각 토큰은 문맥을 이해하기 위해 시퀀스 내의 모든 다른 토큰에 균등하게 주의를 기울이는 것이 아니라, 의미적으로 가장 관련 있는 소수의 특정 토큰에만 강하게 집중하는 경향이 있다.15 이는 모든 $n^2$개의 연결을 계산하는 것이 정보적으로 낭비일 수 있으며, 중요한 연결만 선택적으로 계산해도 충분하다는 강력한 근거가 된다.
- **정보의 지역성(Locality)과 계층성:** 텍스트, 이미지, 오디오와 같은 대부분의 자연 신호는 강한 지역적 상관관계를 가진다. 예를 들어, 텍스트에서 한 단어의 의미는 주로 인접한 단어들에 의해 결정되며, 이미지에서 한 픽셀의 정보는 주변 픽셀들과 밀접하게 연관된다. 희소 어텐션의 '슬라이딩 윈도우'와 같은 기법은 이러한 데이터의 지역적 구조를 아키텍처에 직접 반영하여 효율성을 높인다.11 더 나아가, 정보는 지역적 개념(단어, 픽셀 블록)에서 점차 추상화되어 전역적인 개념(문단, 객체)으로 계층적으로 통합된다. 희소 어텐션은 이러한 계층적 정보 흐름을 모델링하는 효과적인 수단을 제공한다.
- **창발적 능력(Emergent Abilities)과의 연관성:** 최근 연구는 희소 어텐션이 단순한 효율성 개선 도구를 넘어 모델의 핵심적인 학습 메커니즘과 깊이 연관되어 있음을 시사한다. 특히, 대규모 언어 모델(LLM)이 특정 규모 이상에서 갑자기 발현하는 인-컨텍스트 학습(in-context learning)과 같은 창발적 능력은 훈련 과정에서 특정 희소 어텐션 패턴(예: Induction Head)이 형성되는 시점과 강한 상관관계를 보인다.15 이는 모델이 이전 문맥에서 패턴을 찾아 새로운 예시에 적용하는 능력이, 특정 토큰에 집중하고 그 다음 토큰으로 정보를 복사하는 희소한 정보 흐름을 통해 구현됨을 의미한다. 따라서 희소성은 모델의 고차원적인 추론 능력을 가능하게 하는 근본적인 원리일 수 있다.

이러한 배경을 바탕으로, 희소 어텐션 연구는 '정적 설계'에서 '동적 적응'으로, 그리고 최근에는 '하드웨어와의 공동 설계'로 진화하는 뚜렷한 궤적을 보여왔다. 초기에는 Longformer나 Sparse Transformer처럼 데이터에 대한 일반적인 가정(예: 지역성)을 바탕으로 '설계된' 고정 희소 패턴을 제안했다.11 이는 효율적이었지만, 가정이 데이터의 특성과 맞지 않을 경우 성능 저하의 한계를 보였다. 이후 Reformer와 같이 콘텐츠의 유사성에 기반해 동적으로 클러스터를 형성하거나 18, VSA처럼 중요한 토큰을 데이터로부터 직접 '학습'하는 적응형 방식으로 발전했다.19 가장 최근에는 NSA(Natively Trainable Sparse Attention)와 같이, 이론적 계산량 감소가 실제 하드웨어(GPU)에서의 속도 향상으로 직결되도록 메모리 접근 패턴과 산술 강도(Arithmetic Intensity)까지 고려하는 '하드웨어-정렬(Hardware-Aligned)' 설계 패러다임이 부상하고 있다.10 이 진화 과정은 딥러닝 연구가 순수한 알고리즘 설계를 넘어, 실제 배포 환경의 물리적 제약까지 고려하는 방향으로 성숙하고 있음을 보여준다.


다양한 희소 어텐션 기법들은 그 접근 방식에 따라 여러 범주로 체계적으로 분류할 수 있다. "A Survey of Efficient Transformers"와 같은 연구에서는 다음과 같은 분류 체계를 제시한다.20

- **고정 패턴(Fixed Patterns) 기반 접근법:** 데이터와 무관하게 미리 정의된 정적(static) 희소 패턴을 적용하는 방식이다. 구현이 비교적 간단하고 예측 가능한 계산 패턴으로 인해 하드웨어 최적화가 용이하다. 하지만 데이터의 동적인 특성을 반영하지 못하는 한계가 있다.
  - **주요 기법:** 블록 단위(Blockwise) 어텐션, 슬라이딩 윈도우(Sliding Window) 어텐션, 스트라이드/확장(Strided/Dilated) 어텐션.
  - **대표 모델:** Sparse Transformer 17, Longformer.11
- **학습 가능 패턴(Learnable Patterns) 및 적응형(Adaptive) 접근법:** 데이터로부터 어텐션 패턴을 직접 학습하거나, 입력 콘텐츠에 따라 동적으로 중요한 토큰을 선택하는 방식이다. 모델에 높은 유연성을 부여하지만, 패턴을 결정하기 위한 추가적인 계산 오버헤드가 발생할 수 있다.
  - **주요 기법:** 클러스터링 기반(Clustering-based) 어텐션, 라우팅(Routing) 메커니즘, Top-k 선택.
  - **대표 모델:** Reformer (LSH 기반 클러스터링) 18, Routing Transformer (k-means 클러스터링) 20, VSA (학습 가능한 Top-k 선택).19
- **저랭크(Low-Rank) 근사 및 커널(Kernel) 기반 접근법:** $n \times n$ 어텐션 행렬이 실제로는 낮은 내재적 랭크(low intrinsic rank)를 가질 것이라는 가정에 기반한다. 어텐션 행렬을 직접 계산하는 대신, 저랭크 행렬로 분해하거나 커널 함수를 통해 계산을 근사하여 복잡도를 줄인다.
  - **주요 기법:** 행렬 분해, 랜덤 피처(Random Features)를 이용한 커널 근사.
  - **대표 모델:** Linformer, Performer.20
- **메모리/압축(Memory/Compression) 기반 접근법:** 시퀀스의 일부 토큰을 '글로벌 메모리' 또는 '압축된 표현'으로 활용하여 정보의 병목(bottleneck)을 형성한다. 이를 통해 모든 토큰이 이 메모리 토큰을 매개로 간접적으로 상호작용하도록 하여 전역적인 정보 흐름을 유지하면서 계산량을 줄인다.
  - **주요 기법:** 글로벌 토큰(Global Tokens), 토큰 압축(Token Compression), 유도점(Inducing Points).
  - **대표 모델:** Longformer/BigBird (글로벌 토큰) 7, Compressed Attention 20, NSA (토큰 압축).10

이러한 분류 체계는 복잡하고 방대한 희소 어텐션 연구의 지형을 이해하는 데 유용한 나침반 역할을 한다. 각 기법의 핵심 철학과 장단점을 파악하고, 특정 문제에 가장 적합한 접근법을 선택하는 데 체계적인 기준을 제공한다.


희소 어텐션의 개념을 구체화하고 그 가능성을 입증한 여러 선구적인 아키텍처들이 있다. 이들은 각기 다른 철학과 방식으로 $O(n^2)$의 장벽에 도전했으며, 후속 연구에 지대한 영향을 미쳤다.


OpenAI에서 제안한 Sparse Transformer는 밀집된(dense) $n \times n$ 어텐션 계산을 여러 개의 더 작고 희소한 어텐션 계산으로 분해(factorize)하는 아이디어를 처음으로 제시했다.17 핵심은 정보가 희소한 연결망을 통해 여러 단계를 거치더라도 결국 시퀀스 내의 모든 위치 쌍 간에 도달할 수 있는 경로를 보장하는 것이다.21 이를 위해 두 가지 주요 2차원 분해 패턴을 제안했다.

- **스트라이드(Strided) 어텐션:** 이 패턴은 두 종류의 어텐션 헤드를 결합한다. 한 헤드는 각 토큰이 자신의 바로 이전 `l`개 토큰에만 주의를 기울이는 지역적(local) 어텐션을 수행한다. 다른 헤드는 일정한 보폭(stride) `l`만큼 떨어진 토큰들에만 주의를 기울인다. 예를 들어, 100번째 토큰은 99, 98,...번째 토큰에 주목하는 동시에, 88, 76, 64,...번째(보폭이 12인 경우) 토큰에도 주목한다. 이 방식은 2D 격자 구조를 가지는 이미지나 주기적인 패턴을 보이는 오디오 데이터에 특히 효과적이다.17
- **고정(Fixed) 어텐션:** 텍스트와 같이 명확한 주기성이 없는 데이터를 위해 설계되었다. 이 패턴에서는 특정 고정된 위치의 토큰들이 '정보 허브' 역할을 수행한다. 예를 들어, 시퀀스를 길이 `l`의 블록으로 나눌 때, 각 블록의 마지막 `c`개 토큰이 해당 블록의 정보를 요약하고, 이후의 모든 토큰들이 이 요약 토큰들에 주목할 수 있도록 한다. 이를 통해 정보가 블록 경계를 넘어 효율적으로 전파될 수 있다.21

Sparse Transformer는 이러한 분해된 어텐션을 통해 각 어텐션 헤드가 전체 $n$개가 아닌 약 $\sqrt{n}$개의 토큰에만 주목하도록 설계했다. 그 결과, 전체 계산 복잡도는 $O(n^2)$에서 $O(n\sqrt{n})$으로 크게 감소하였고, 수만 타임스텝 길이를 갖는 시퀀스 모델링의 가능성을 열었다.21


Longformer는 대부분의 NLP 과제에서 어텐션이 주로 지역적으로 발생한다는 강력한 직관에 기반하여 설계되었다.11 이는 Sparse Transformer보다 더 공격적으로 복잡도를 줄여 선형 복잡도($O(n)$)를 달성하는 것을 목표로 한다.

- **슬라이딩 윈도우(Sliding Window) 어텐션:** Longformer의 핵심은 각 토큰이 자신의 주변에 위치한 고정된 크기($w$)의 윈도우 내 토큰들에만 어텐션하도록 제한하는 것이다.9 예를 들어, 윈도우 크기가 512라면 각 토큰은 자신을 기준으로 좌우 255개의 토큰에만 주목한다. 이 방식은 CNN의 수용 필드(receptive field) 개념과 유사하며, 여러 어텐션 레이어를 쌓음으로써 상위 레이어는 더 넓은 범위의 정보를 통합하여 장거리 의존성을 간접적으로 포착할 수 있다.8
- **글로벌(Global) 어텐션:** 지역적 어텐션만으로는 분류를 위한 `` 토큰이나 질의응답(QA)에서의 질문 토큰처럼 전체 시퀀스의 정보를 종합해야 하는 특정 토큰의 역할을 수행하기 어렵다. Longformer는 이 문제를 해결하기 위해 일부 중요한 토큰에 '글로벌 어텐션'을 부여한다. 글로벌 어텐션을 가진 토큰은 시퀀스 내 모든 토큰에 주목할 수 있으며, 반대로 시퀀스 내 모든 토큰 역시 이 글로벌 토큰에 주목할 수 있다. 이는 지역적 정보 흐름에 전역적 정보 고속도로를 놓아주는 것과 같으며, 모델의 표현력을 크게 향상시킨다.8

윈도우 크기 $w$와 글로벌 토큰의 수 $g$는 시퀀스 길이 $n$에 비해 매우 작은 상수이므로, 슬라이딩 윈도우 어텐션($O(n \cdot w)$)과 글로벌 어텐션($O(n \cdot g)$)을 결합한 전체 계산 복잡도는 실질적으로 $O(n)$이 된다.11 이 선형 복잡도 덕분에 Longformer는 수천 개 이상의 토큰으로 구성된 긴 문서를 효율적으로 처리할 수 있게 되었다.


Longformer와 같은 초기 모델들이 경험적 성능과 효율성에 집중한 반면, BigBird는 희소 어텐션이 과연 표준(full) 어텐션의 강력한 이론적 속성을 유지할 수 있는가라는 근본적인 질문에 답하고자 했다.7 연구진은 BigBird가 완전 어텐션과 마찬가지로 튜링 완전(Turing Complete)하며, 모든 연속적인 시퀀스 함수에 대한 보편적 근사자(Universal Approximator)임을 수학적으로 증명했다.7

이를 위해 BigBird는 세 가지 어텐션 구성 요소를 정교하게 결합한다.7

1. **윈도우(Window) 어텐션:** Longformer와 마찬가지로 지역적 문맥을 포착하는 역할을 한다.
2. **랜덤(Random) 어텐션:** 각 토큰이 무작위로 선택된 소수의($r$개) 다른 토큰에 주목한다. 이는 그래프 이론에서 무작위 엣지가 노드 간 평균 경로 길이를 크게 줄이는 것과 같은 원리로, 멀리 떨어진 토큰들 사이에 정보가 빠르게 전파될 수 있는 '숏컷'을 제공한다.
3. **글로벌(Global) 어텐션:** Longformer와 유사하게 일부 토큰이 전체 시퀀스와 상호작용하는 정보 허브 역할을 한다. BigBird의 이론적 분석에서 이 글로벌 토큰의 존재가 완전 어텐션의 표현력을 보존하는 데 핵심적인 역할을 함이 밝혀졌다.

이 세 가지 메커니즘의 조합을 통해 BigBird는 계산 복잡도를 $O(n)$으로 줄이면서도, 정보가 시퀀스 내의 어떤 노드 쌍 사이에서도 제한된 단계를 거쳐 흐를 수 있는 경로를 보장한다. 이는 BigBird가 단순한 근사 기법을 넘어, 이론적으로도 견고한 기반을 갖춘 범용적인 장문맥 처리 아키텍처임을 의미한다.7


Reformer는 두 가지 혁신적인 기술을 통해 트랜스포머의 효율성을 극한으로 끌어올리고자 했다: 국소 민감 해싱(Locality-Sensitive Hashing, LSH) 어텐션과 가역 잔차 네트워크(Reversible Residual Networks)이다.18

- **LSH 어텐션:** 이 기법의 핵심 아이디어는 '유사한 벡터는 높은 확률로 같은 해시 버킷에 속한다'는 LSH의 원리를 이용하는 것이다.18 전체 

  $n$개의 키에 대해 유사도를 계산하는 대신, 각 쿼리는 자신과 동일한 해시 버킷에 속하는 키들에 대해서만 어텐션을 계산한다. 구현의 효율성을 위해 모든 쿼리-키 벡터를 해시값으로 정렬한 뒤, 인접한 벡터들끼리 묶어(chunking) 어텐션을 계산하는 방식을 사용한다. 이를 통해 모든 쌍을 비교하지 않고도 높은 확률로 가장 유사한 쌍을 찾을 수 있게 된다.18 이 근사적 접근법을 통해 시간 복잡도는 

  $O(n \log n)$으로 감소한다.18

- **가역 잔차 네트워크:** 어텐션 계산 외에 트랜스포머의 또 다른 메모리 병목은 역전파(backpropagation)를 위해 각 레이어의 활성화(activation) 값을 모두 저장해야 한다는 점이다. 모델의 레이어 수가 $N$개일 때 메모리 사용량은 $N$에 비례하여 증가한다. Reformer는 가역 잔차 네트워크를 도입하여 이 문제를 해결했다. 이 구조에서는 다음 레이어의 활성화 값으로부터 이전 레이어의 활성화 값을 수학적으로 복원(재계산)할 수 있다. 따라서 훈련 중에 활성화 값을 저장할 필요가 없어지며, 모델의 깊이와 무관하게 메모리 사용량을 $O(1)$ 수준으로 유지할 수 있다. 이 혁신 덕분에 이전에는 상상할 수 없었던 수백 개의 레이어를 가진 매우 깊은 트랜스포머 모델의 훈련이 가능해졌다.14


다양한 희소 어텐션 아키텍처들은 각기 다른 벤치마크에서 그 효과를 입증하며 장문맥 처리 능력의 새로운 기준을 제시했다. 이들의 성능을 종합적으로 비교하고 분석하는 것은 각 아키텍처의 강점과 적용 분야를 이해하는 데 필수적이다.


- **생성 모델링 (enwik8, CIFAR-10):** 생성 모델링은 모델이 데이터의 근본적인 분포를 학습하여 새로운 샘플을 만들어내는 과제로, 장기 의존성 포착 능력을 평가하는 좋은 척도이다. Sparse Transformer는 위키피디아 텍스트 데이터셋인 enwik8에서 0.99 bits/dim, 이미지 데이터셋인 CIFAR-10에서 2.80 bits/dim이라는 당시 최고 수준(SOTA)의 성능을 기록하며, 희소 패턴이 장문맥 생성에 매우 효과적임을 입증했다.21 Reformer 역시 enwik8과 같은 장문맥 텍스트 생성 과제에서 훨씬 적은 메모리를 사용하면서도 표준 트랜스포머와 비견되는 성능을 달성했다.18
- **장문서 이해 (WikiHop, TriviaQA):** 여러 문서에 흩어져 있는 정보를 종합하여 질문에 답해야 하는 장문서 QA 벤치마크는 희소 어텐션의 실용성을 평가하는 중요한 시험대이다. Longformer는 RoBERTa와 같은 강력한 사전 훈련 모델을 기반으로 하면서도, 더 긴 문맥을 처리할 수 있는 능력 덕분에 WikiHop, TriviaQA 등의 벤치마크에서 RoBERTa를 일관되게 능가하며 새로운 SOTA를 달성했다.8 BigBird 또한 HotpotQA, Natural Questions와 같은 까다로운 QA 데이터셋에서 강력한 성능을 보였으며, 특히 모델이 더 긴 문맥에 접근할 수 있을 때 성능 향상이 두드러지는 경향을 보였다.7
- **요약 (arXiv, PubMed):** 긴 학술 논문이나 특허 문서를 짧은 초록으로 요약하는 과제는 문서 전체의 핵심 정보를 파악하는 능력을 요구한다. Longformer의 인코더-디코더 버전인 LED(Longformer-Encoder-Decoder)와 BigBird는 arXiv, PubMed, BigPatent와 같은 장문서 요약 데이터셋에서 기존 모델들을 큰 폭으로 앞서는 성능을 보였다.7 이는 요약에 필요한 핵심 정보가 문서의 특정 부분(예: 서론)에만 국한되지 않고 전체에 흩어져 있는 경우가 많아, 전체 문맥을 보는 것이 유리하기 때문이다.


각 희소 어텐션 아키텍처의 핵심적인 특징과 장단점을 한눈에 비교하기 위해 다음 표를 제시한다. 이 표는 각 모델의 근본적인 메커니즘, 계산 복잡도, 이론적 기반, 그리고 주요 적용 분야를 종합적으로 요약하여 독자의 이해를 돕는다.

| 특징               | **Standard Transformer**       | **Sparse Transformer**                 | **Longformer**                   | **BigBird**                        | **Reformer**                      |
| ------------------ | ------------------------------ | -------------------------------------- | -------------------------------- | ---------------------------------- | --------------------------------- |
| **핵심 메커니즘**  | Full Attention                 | Factorized Attention (Strided, Fixed)  | Sliding Window + Global          | Window + Random + Global           | Locality-Sensitive Hashing (LSH)  |
| **시간 복잡도**    | $O(n^2)$                       | $O(n\sqrt{n})$                         | $O(n)$                           | $O(n)$                             | $O(n \log n)$                     |
| **공간 복잡도**    | $O(n^2)$                       | $O(n\sqrt{n})$                         | $O(n)$                           | $O(n)$                             | $O(n \log n)$                     |
| **이론적 속성**    | Turing Complete                | -                                      | -                                | Turing Complete, Universal Approx. | Approximation                     |
| **장점**           | 높은 표현력, 구현 용이         | 장문맥 생성에 강력, 이미지/오디오 적용 | 선형 복잡도, 긴 문서 이해에 탁월 | 이론적 보장, 범용성, 강력한 성능   | 극도의 메모리 효율성(가역 레이어) |
| **단점**           | $n^2$ 복잡도, 장문맥 처리 불가 | 패턴이 데이터와 안 맞을 수 있음        | 글로벌 토큰 수동 지정 필요       | 랜덤 어텐션 구현 복잡성            | 근사로 인한 정확도 손실 가능성    |
| **주요 적용 분야** | 단문 NLP 과제                  | 고해상도 이미지/오디오 생성            | 장문서 QA, 분류, 요약            | 장문서 처리 전반, 유전체학         | 수십만 토큰 길이의 초장문맥 처리  |

이 분석을 통해 각 희소 어텐션 모델이 특정 트레이드오프(trade-off) 관계 위에 있음을 알 수 있다. Longformer는 단순하고 효과적인 선형 복잡도를 제공하지만, 글로벌 토큰을 수동으로 지정해야 하는 등 태스크 의존적인 측면이 있다. BigBird는 이론적 견고함과 뛰어난 범용성을 자랑하지만, 랜덤 어텐션으로 인해 구현이 다소 복잡할 수 있다. Reformer는 LSH와 가역 레이어를 통해 압도적인 메모리 효율성을 달성하지만, 해싱 기반 근사로 인해 미세한 정확도 손실이 발생할 수 있다. 따라서 특정 응용 분야에 가장 적합한 모델을 선택하기 위해서는 이러한 장단점을 종합적으로 고려하는 것이 중요하다.


희소 어텐션의 연구가 심화되면서, 알고리즘의 이론적 계산 복잡도(FLOPs) 감소가 실제 하드웨어에서의 속도(throughput) 향상으로 반드시 이어지지는 않는다는 점이 명확해졌다. 이는 현대 GPU 아키텍처의 작동 방식과 밀접한 관련이 있으며, '하드웨어-인식(hardware-aware)' 최적화의 중요성을 부각시켰다.


GPU와 같은 현대 가속기는 계산(computation) 속도에 비해 메모리 접근(memory access) 속도가 상대적으로 느리다. 따라서 연산의 효율성은 순수한 계산량뿐만 아니라, 계산에 필요한 데이터를 얼마나 효율적으로 메모리에서 가져오고 쓰는가에 크게 좌우된다. 이 비율을 나타내는 척도가 산술 강도(Arithmetic Intensity), 즉 메모리 접근 단위 당 수행되는 계산의 양이다.10

희소 어텐션, 특히 랜덤 어텐션과 같이 불규칙하고 분산된 메모리 접근 패턴을 요구하는 알고리즘은 GPU의 메모리 대역폭에 큰 부담을 준다.10 각 어텐션 헤드가 시퀀스의 서로 다른 위치에서 데이터를 무작위로 가져와야 한다면(gather operation), 이는 메모리 접근의 지역성(locality)을 해치고 캐시 효율성을 떨어뜨린다. 결과적으로 GPU의 연산 유닛은 데이터를 기다리며 유휴 상태에 머무는 시간이 길어지고, 산술 강도가 낮아져 이론적으로 FLOPs를 줄였음에도 불구하고 실제 실행 시간은 기대만큼 단축되지 않거나 오히려 느려질 수 있다. 이 때문에 하드웨어의 특성을 고려하지 않은 희소 어텐션은 '공짜 점심이 아님'이 밝혀졌다.6


이러한 간극을 메우기 위해 초기 희소 어텐션 연구들은 특정 희소 패턴에 최적화된 저수준(low-level)의 커스텀 CUDA 커널을 개발하는 데 많은 노력을 기울였다. 예를 들어, OpenAI의 Sparse Transformer는 블록-희소(block-sparse) 행렬 곱셈을 효율적으로 수행하는 커널을 제공했으며 28, Longformer 역시 슬라이딩 윈도우 어텐션을 위한 커스텀 CUDA 커널을 구현했다.29 이러한 커널들은 메모리 접근을 최대한 병합하고 GPU의 병렬 처리 능력을 극대화하도록 설계되었다.

최근에는 Triton과 같은 프로그래밍 언어의 등장으로 이러한 최적화 과정이 더욱 용이해졌다. Triton은 연구자들이 파이썬과 유사한 구문으로 고성능 GPU 커널을 작성할 수 있게 해주어, 복잡한 CUDA C++ 코드 없이도 특정 희소 어텐션 패턴에 맞는 고도로 최적화된 커널을 신속하게 개발하고 실험할 수 있는 환경을 제공한다.10 이는 알고리즘 아이디어와 하드웨어 구현 사이의 거리를 좁히는 데 중요한 역할을 하고 있다.


희소 어텐션과는 다른 방향에서 어텐션 효율성 문제를 해결한 FlashAttention의 등장은 이 분야에 중요한 시사점을 던졌다.32 FlashAttention은 어텐션 행렬을 희소화하지 않고도 표준 어텐션을 획기적으로 가속화한다. 그 비결은 계산량(FLOPs)이 아닌 메모리 I/O를 최적화하는 데 있다.

FlashAttention의 핵심 아이디어는 거대한 $n \times n$ 어텐션 행렬을 GPU의 주 메모리인 HBM(High Bandwidth Memory)에 아예 생성하거나 저장하지 않는 것이다. 대신, 타일링(Tiling) 기법을 사용하여 입력 $Q, K, V$ 행렬을 작은 블록으로 나눈다. 각 블록은 GPU 코어에 훨씬 가깝고 빠른 SRAM(On-chip Static RAM)으로 로드되어 어텐션 계산의 일부(행렬 곱, 소프트맥스 등)가 수행된다. 이 과정에서 중간값들은 HBM에 다시 쓰이지 않고 SRAM 내에서 처리되거나, 역전파 시 재계산(recomputation)된다. 이러한 I/O-인식(I/O-aware) 설계는 HBM과의 데이터 교환 횟수를 극적으로 줄여 메모리 대역폭 병목을 완화하고 전체 어텐션 연산을 크게 가속화한다.30

FlashAttention의 성공은 희소 어텐션 연구에도 새로운 방향을 제시했다. 즉, 단순히 0이 아닌 원소의 수를 줄이는 것을 넘어, 그 0이 아닌 원소들이 어떻게 메모리에 배치되고 접근되는지가 실제 성능에 결정적이라는 것이다. 이에 따라, FlashAttention의 I/O 효율적인 계산 방식을 희소 패턴에 맞게 수정하려는 연구가 활발히 진행되고 있다. 예를 들어, Binary Block Masking과 같은 기술은 FlashAttention의 타일링 구조 내에서 희소 마스크를 효율적으로 적용하여, 희소성의 계산량 감소 이점과 FlashAttention의 I/O 효율성 이점을 모두 취하고자 한다.33 이는 차세대 희소 어텐션이 알고리즘적 희소성과 하드웨어적 I/O 효율성을 동시에 고려하는 방향으로 나아갈 것임을 예고한다.


희소 어텐션 분야는 지난 몇 년간 눈부신 발전을 거듭해왔으며, 현재도 활발한 연구가 진행 중이다. 미래의 희소 어텐션은 더욱 동적이고, 하드웨어에 최적화되며, 모델의 근본적인 학습 능력과 깊이 연관된 방향으로 진화할 것으로 전망된다.


고정된 패턴을 사용하는 초기 희소 어텐션의 한계를 넘어, 입력 데이터의 내용에 따라 실시간으로 어텐션 패턴을 결정하는 동적(dynamic) 및 데이터 의존적(data-dependent) 희소성 연구가 주목받고 있다.12 이러한 접근법은 모델이 각 입력에 가장 적합한 정보 흐름 경로를 스스로 찾게 함으로써 유연성과 성능을 극대화하는 것을 목표로 한다.

예를 들어, 비디오 처리를 위한 VSA(Video Sparse Attention)는 두 단계의 계층적 접근법을 제안한다.19 먼저, 비디오 토큰들을 거친(coarse) 수준에서 풀링(pooling)하여 경량의 어텐션을 수행한다. 이 단계의 목적은 전체 비디오에서 정보적으로 가장 중요한 '핵심' 영역(critical tokens)이 어디인지를 예측하는 것이다. 그런 다음, 두 번째 단계에서는 예측된 핵심 영역 내에서만 고해상도의 세밀한(fine-grained) 어텐션을 집중적으로 계산한다. 이처럼 중요한 부분과 그렇지 않은 부분을 동적으로 구분하여 계산 자원을 효율적으로 배분하는 방식은 미래 희소 어텐션의 중요한 패러다임이 될 것이다.


이론적 효율성이 실제 하드웨어에서의 성능 향상으로 이어지기 위해서는 알고리즘 설계 단계부터 하드웨어 아키텍처를 깊이 고려하는 하드웨어-소프트웨어 공동 설계(co-design)가 필수적이다. NSA(Natively Trainable Sparse Attention)와 같은 최신 연구는 이러한 철학을 적극적으로 반영한다.10 NSA는 토큰 압축, Top-k 선택, 슬라이딩 윈도우를 결합한 계층적 희소 전략을 사용하면서, 각 연산이 GPU의 메모리 계층 구조와 연산 특성에 잘 부합하도록 설계되었다. 예를 들어, 불규칙한 메모리 접근을 최소화하고, 행렬 곱셈과 같은 연산이 GPU의 텐서 코어를 효율적으로 활용할 수 있도록 블록 단위로 연산을 구성한다. 이를 통해 산술 강도를 균형 있게 유지하고, 이론적 FLOPs 감소를 실제 속도 향상으로 극대화한다.10 미래의 효율적인 아키텍처는 이처럼 알고리즘의 우아함과 하드웨어의 실용성을 동시에 추구하는 방향으로 발전할 것이다.


희소 어텐션 연구의 가장 흥미로운 미래 방향 중 하나는 희소성이 모델의 창발적 능력과 맺는 근본적인 관계를 탐구하는 것이다. 최근 연구에 따르면, 인-컨텍스트 학습과 같은 LLM의 고차원적인 추론 능력은 훈련 과정에서 특정 희소 어텐션 패턴(예: 이전 토큰에 주목하여 다음 토큰 정보를 복사하는 Induction Head)이 형성되면서 발현된다는 강력한 증거가 제시되었다.15

이는 희소성이 단순히 계산 효율성을 위한 사후 최적화 기법이 아니라, 모델이 복잡한 알고리즘을 학습하고 일반화하는 핵심 메커니즘일 수 있음을 시사한다. 데이터의 통계적 특성(예: 텍스트 내의 반복적인 패턴)이 이러한 유용한 희소 패턴의 형성을 촉진하고, 결과적으로 특정 능력의 발현 시점을 앞당길 수 있다는 가설도 제기되었다.15 앞으로 희소 어텐션의 학습 동역학을 더 깊이 이해하고, 유용한 희소 패턴의 형성을 유도하는 훈련 기법을 개발하는 것은 더 작고 효율적이면서도 강력한 추론 능력을 갖춘 모델을 만드는 데 중요한 열쇠가 될 것이다.


다양한 희소 어텐션 아이디어가 쏟아져 나오면서, 이를 체계적으로 구현하고 평가하는 것의 어려움 또한 커지고 있다. 이러한 문제를 해결하기 위해 AttentionEngine과 같은 통합 프레임워크가 등장하고 있다.36 이러한 프레임워크는 다양한 희소 어텐션 메커니즘(예: 마스킹, 스케일링, 집계)을 모듈화된 기본 연산으로 추상화한다. 사용자는 이 기본 연산들을 조합하여 새로운 희소 어텐션 아이디어를 손쉽게 프로토타이핑할 수 있다. 더 나아가, 이 프레임워크는 주어진 희소 패턴과 백엔드 하드웨어(GPU, TPU 등)의 특성을 고려하여 최적의 커널 코드를 자동으로 생성하고 스케줄링하는 기능까지 제공한다. 이러한 통합 프레임워크의 발전은 새로운 희소 어텐션 연구의 가속화와 산업 현장에서의 신속한 적용을 가능하게 할 것이다.


희소 어텐션은 트랜스포머 아키텍처의 고질적인 $O(n^2)$ 계산 복잡도 병목을 성공적으로 해결하며, 인공지능 모델이 처리할 수 있는 문맥의 길이를 획기적으로 확장시켰다. 이는 단순히 더 긴 시퀀스를 처리하는 기술적 진보를 넘어, 모델이 이전에는 접근할 수 없었던 방대한 정보 속에서 더 깊은 의미적 관계를 학습하고 복잡한 추론을 수행할 수 있는 기반을 마련했다는 데 근본적인 의의가 있다.

본 보고서에서 심층적으로 분석한 바와 같이, 희소 어텐션 분야는 고정된 패턴을 적용하는 초기 단계를 지나, 데이터로부터 동적으로 패턴을 학습하고 하드웨어의 물리적 특성까지 고려하는 정교한 단계로 성숙해왔다. Longformer의 선형 복잡도 달성, BigBird의 이론적 속성 보존, Reformer의 혁신적인 메모리 효율성, 그리고 FlashAttention이 제시한 I/O-인식 최적화 패러다임에 이르기까지, 각 연구는 효율적인 어텐션을 향한 여정에서 중요한 이정표를 세웠다.

장문맥 시대의 효율적인 트랜스포머를 향한 미래의 길은 하나의 정답이 아닌, 여러 요소의 정교한 조화를 요구한다. 이는 알고리즘의 혁신, 하드웨어 아키텍처에 대한 깊은 이해, 그리고 모델의 학습 동역학에 대한 근본적인 통찰이 결합된 형태가 될 것이다. 동적인 데이터 의존적 희소 패턴을 탐구하고, 하드웨어-소프트웨어 공동 설계를 통해 이론과 실제의 간극을 좁히며, 희소성이 모델의 창발적 능력과 맺는 관계를 규명하는 것은 앞으로의 연구를 이끌어갈 핵심 과제이다.

결론적으로, 희소성은 더 이상 선택적인 최적화 기법이 아니라, 차세대 인공지능 모델의 성능과 효율성을 동시에 달성하기 위한 필수불가결한 요소로 자리 잡았다. 희소성의 원리를 더 깊이 탐구하고 정교하게 활용하는 노력은 계속해서 인공지능 기술의 경계를 넓혀나가는 핵심 동력이 될 것이다.


1. Chapter 8 Attention and Self-Attention for NLP | Modern Approaches ..., 8월 16, 2025에 액세스, https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html
2. (Open Access) Longformer: The Long-Document Transformer (2020) | Iz Beltagy - SciSpace, 8월 16, 2025에 액세스, https://scispace.com/papers/longformer-the-long-document-transformer-18yjwxjc7v
3. Attention (machine learning) - Wikipedia, 8월 16, 2025에 액세스, https://en.wikipedia.org/wiki/Attention_(machine_learning)
4. The Detailed Explanation of Self-Attention in Simple Words | by Maninder Singh | Medium, 8월 16, 2025에 액세스, https://medium.com/@manindersingh120996/the-detailed-explanation-of-self-attention-in-simple-words-dec917f83ef3
5. Deep Dive into Self-Attention by Hand✍︎ | Towards Data Science, 8월 16, 2025에 액세스, [https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-%EF%B8%8E-f02876e49857/](https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-︎-f02876e49857/)
6. On The Computational Complexity of Self-Attention - Proceedings of Machine Learning Research, 8월 16, 2025에 액세스, https://proceedings.mlr.press/v201/duman-keles23a/duman-keles23a.pdf
7. Big Bird: Transformers for Longer Sequences, 8월 16, 2025에 액세스, https://arxiv.org/pdf/2007.14062
8. [2004.05150] Longformer: The Long-Document Transformer, 8월 16, 2025에 액세스, https://ar5iv.labs.arxiv.org/html/2004.05150
9. Sparse Transformers. From naive sparse attention to Kimi's... | by Mengliu Zhao | AI Advances, 8월 16, 2025에 액세스, https://ai.gopubby.com/sparse-transformers-d46463e0a9a6
10. Hardware-Aligned and Natively Trainable Sparse Attention - ACL Anthology, 8월 16, 2025에 액세스, https://aclanthology.org/2025.acl-long.1126.pdf
11. Longformer: The Long-Document Transformer - ResearchGate, 8월 16, 2025에 액세스, https://www.researchgate.net/publication/340598399_Longformer_The_Long-Document_Transformer
12. Sparse Attention Models - Emergent Mind, 8월 16, 2025에 액세스, https://www.emergentmind.com/topics/sparse-attention-models
13. PyTorch Implementation of Sparse Attention | by Amit Yadav | Biased-Algorithms | Medium, 8월 16, 2025에 액세스, https://medium.com/biased-algorithms/pytorch-implementation-of-sparse-attention-6c14514f3dd9
14. [2001.04451] Reformer: The Efficient Transformer - ar5iv, 8월 16, 2025에 액세스, https://ar5iv.labs.arxiv.org/html/2001.04451
15. The emergence of sparse attention: impact of data distribution and benefits of repetition, 8월 16, 2025에 액세스, https://arxiv.org/html/2505.17863v1
16. Review - Big Bird: Transformers for Longer Sequences | by Sik-Ho Tsang | Medium, 8월 16, 2025에 액세스, https://sh-tsang.medium.com/brief-review-big-bird-transformers-for-longer-sequences-12ccd3430e3b
17. Sparse Transformer: Stride and Fixed Factorized Attention - GeeksforGeeks, 8월 16, 2025에 액세스, https://www.geeksforgeeks.org/machine-learning/sparse-transformer-stride-and-fixed-factorized-attention/
18. Reformer: The Efficient Transformer, 8월 16, 2025에 액세스, https://arxiv.org/pdf/2001.04451
19. Faster Video Diffusion with Trainable Sparse Attention - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2505.13389v3
20. arXiv:2009.06732v3 [cs.LG] 14 Mar 2022, 8월 16, 2025에 액세스, https://arxiv.org/abs/2009.06732
21. Generating Long Sequences with Sparse Transformers, 8월 16, 2025에 액세스, https://arxiv.org/abs/1904.10509
22. Generative modeling with sparse transformers - OpenAI, 8월 16, 2025에 액세스, https://openai.com/index/sparse-transformer/
23. Paper page - Longformer: The Long-Document Transformer - Hugging Face, 8월 16, 2025에 액세스, https://huggingface.co/papers/2004.05150
24. Big Bird: Transformers for Longer Sequences | Request PDF - ResearchGate, 8월 16, 2025에 액세스, https://www.researchgate.net/publication/343279169_Big_Bird_Transformers_for_Longer_Sequences
25. Big Bird: Transformers for Longer Sequences, 8월 16, 2025에 액세스, https://papers.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf
26. Reformer: The Efficient Transformer - ResearchGate, 8월 16, 2025에 액세스, https://www.researchgate.net/publication/338569863_Reformer_The_Efficient_Transformer
27. Reformer: The Efficient Transformer - BibSonomy, 8월 16, 2025에 액세스, https://www.bibsonomy.org/bibtex/2afcb7d1d8971f1dc55e2816c8b3235e6/stdiff
28. openai/sparse_attention: Examples of using sparse attention, as in "Generating Long Sequences with Sparse Transformers" - GitHub, 8월 16, 2025에 액세스, https://github.com/openai/sparse_attention
29. Longformer: The Long-Document Transformer - GitHub, 8월 16, 2025에 액세스, https://github.com/allenai/longformer
30. Understanding Flash Attention: Writing the Algorithm from Scratch in Triton - Alex Dremov, 8월 16, 2025에 액세스, https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/
31. Efficient triton implementation of Native Sparse Attention. - GitHub, 8월 16, 2025에 액세스, https://github.com/XunhaoLai/native-sparse-attention-triton
32. FlashAttention: Implementing High-Performance Attention with ..., 8월 16, 2025에 액세스, https://medium.com/@kimdoil1211/flashattention-implementing-high-performance-attention-with-cuda-and-triton-9ee635ab1200
33. Efficiently Dispatching Flash Attention For Partially Filled Attention Masks - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2409.15097v2
34. Native Sparse Attention Surpasses Full Attention in Performance at Lightning Speed | by Yu Yang, Ph.D., PE, PMP | Medium, 8월 16, 2025에 액세스, https://medium.com/@ligtleyang/native-sparse-attention-surpasses-full-attention-in-performance-at-lightning-speed-0dc9af7c3f95
35. Towards efficient generative AI and beyond-AI computing: New trends on ISSCC 2024 machine learning accelerators - Journal of Semiconductors, 8월 16, 2025에 액세스, https://www.jos.ac.cn/article/doi/10.1088/1674-4926/45/4/040204
36. AttentionEngine: A Versatile Framework for Efficient Attention Mechanisms on Diverse Hardware Platforms - arXiv, 8월 16, 2025에 액세스, https://arxiv.org/html/2502.15349v1

