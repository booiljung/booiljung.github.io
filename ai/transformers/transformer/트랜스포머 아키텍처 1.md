# 트랜스포머 아키텍처 1

## 1. 시퀀스 모델링의 패러다임 전환

### 1.1 순환 신경망(RNN)의 시대와 그 한계

2017년 트랜스포머가 등장하기 이전, 순차적 데이터(sequential data) 처리의 지배적인 패러다임은 순환 신경망(Recurrent Neural Network, RNN)이었다. RNN은 시퀀스의 각 요소를 순서대로 처리하며, 이전 타임스텝의 정보를 '은닉 상태(hidden state)'라는 벡터에 압축하여 다음 타임스텝으로 전달하는 구조를 가진다.1 이러한 순환적 메커니즘은 가변 길이의 시퀀스를 유연하게 모델링할 수 있다는 장점을 제공했으나, 동시에 근본적인 한계를 내포하고 있었다.

가장 치명적인 문제는 '장기 의존성 문제(long-term dependency problem)'였다.1 시퀀스의 길이가 길어질수록, 초반부에 위치한 중요한 정보가 여러 타임스텝을 거치면서 희석되거나 소실되는 현상이 발생했다. 이는 역전파(backpropagation) 과정에서 기울기가 점차 0에 수렴하는 '기울기 소실(vanishing gradient)' 또는 무한대로 발산하는 '기울기 폭발(exploding gradient)' 문제에서 비롯되었다.3

이러한 문제를 완화하기 위해 Long Short-Term Memory (LSTM) 2 및 Gated Recurrent Unit (GRU)과 같은 정교한 모델이 제안되었다.1 LSTM과 GRU는 '셀 상태(cell state)'와 '게이트(gate)'라는 메커니즘을 도입하여 정보의 흐름을 선택적으로 제어함으로써 장기 의존성을 더 효과적으로 포착하고자 했다.1 실제로 LSTM은 오랜 기간 RNN 아키텍처의 표준으로 자리 잡으며 다양한 분야에서 성공을 거두었다.2

하지만 이러한 개선 노력은 기존 순환 패러다임 내에서의 증분적 발전에 머물렀다. LSTM과 GRU는 기울기 소실이라는 *증상*을 완화했을 뿐, 문제의 근본적 *원인*인 순차적 정보 전달 경로 자체를 해결하지는 못했다. 더 심각한 문제는 순차적 처리 방식이 야기하는 '병렬 처리의 부재'였다. 각 타임스텝의 계산은 이전 타임스텝의 계산이 완료되어야만 시작될 수 있었기 때문에, 하나의 훈련 샘플 내에서는 병렬화가 원천적으로 불가능했다.6 이는 그래픽 처리 장치(GPU)와 같은 병렬 연산 하드웨어의 발전에도 불구하고 모델 훈련이 시퀀스 길이에 의해 제약을 받는 심각한 병목 현상을 초래했다. 즉, RNN의 한계는 단순히 이론적인 문제를 넘어, 하드웨어의 잠재력을 최대한 활용하여 모델을 대규모로 확장하는 것을 가로막는 실질적인 장벽이었다.6

### 1.2 트랜스포머의 등장: 순환을 버리다

2017년, 구글 연구팀이 발표한 논문 "Attention Is All You Need"는 이러한 교착 상태를 타개하는 혁명적인 대안을 제시했다.6 이 논문은 순환(recurrence)이라는 개념 자체를 완전히 폐기하고, 오직 '어텐션(attention)' 메커니즘에만 의존하는 새로운 아키텍처, '트랜스포머(Transformer)'를 제안했다. 이는 기존의 연구 흐름이 '순환 구조를 어떻게 개선할 것인가'에 초점을 맞추었던 것과 달리, '순환 구조 없이 시퀀스를 모델링할 수 있는가'라는 근본적인 질문을 던진 개념적 도약이었다.

트랜스포머의 핵심 아이디어는 시퀀스 내의 모든 토큰 쌍 간의 관계를 직접적으로, 그리고 동시에 계산하는 것이다. 어텐션 메커니즘을 통해 특정 토큰을 처리할 때 시퀀스 내의 다른 모든 토큰을 동시에 참조함으로써, 정보가 순차적인 경로를 따라 전달될 필요가 없어졌다. 이로 인해 임의의 두 토큰 간의 정보 전달 경로 길이는 시퀀스 길이에 상관없이 상수($O(1)$)가 되었고, 이는 장기 의존성 문제를 구조적으로 해결하는 결과를 낳았다.6

더욱 중요한 것은 모든 토큰에 대한 계산이 독립적으로 이루어질 수 있다는 점이었다. 이는 GPU의 대규모 병렬 연산 능력을 극대화할 수 있음을 의미했고, 이전에는 불가능했던 규모의 모델과 데이터셋을 활용한 훈련을 가능하게 했다.10 이처럼 트랜스포머는 알고리즘 혁신과 하드웨어 트렌드의 완벽한 결합을 통해 시퀀스 모델링의 새로운 시대를 열었다.

| 특성                    | RNN / LSTM                        | 트랜스포머                          |
| ----------------------- | --------------------------------- | ----------------------------------- |
| **기본 처리 방식**      | 순차적 (Sequential)               | 병렬적 (Parallel)                   |
| **의존성 모델링**       | 은닉 상태를 통한 순차적 정보 전달 | 셀프 어텐션을 통한 직접적 관계 계산 |
| **병렬 처리**           | 시퀀스 내 병렬 처리 불가          | 완벽한 병렬 처리 가능               |
| **정보 전달 경로 길이** | 시퀀스 길이에 비례 ($O(n)$)       | 상수 ($O(1)$)                       |
| **주요 한계**           | 장기 의존성 문제, 기울기 소실     | 연산량 및 메모리 복잡도             |

## 2. 트랜스포머 아키텍처 해부

트랜스포머 아키텍처는 그 자체로 하나의 완성된 작품이라기보다는, 재사용 가능하고 조합 가능한 여러 모듈의 집합체로 이해하는 것이 더 정확하다. 이러한 모듈식 설계는 이후 BERT, GPT 등 다양한 파생 모델이 탄생할 수 있었던 핵심적인 이유 중 하나이다. 각 구성 요소는 특정 기능을 수행하며, 이들이 유기적으로 결합하여 강력한 시퀀스 처리 능력을 발휘한다.

### 2.1 전체 구조: 인코더-디코더 스택

원본 트랜스포머 모델은 기계 번역과 같은 시퀀스-투-시퀀스(sequence-to-sequence) 태스크를 위해 설계되었으며, 크게 '인코더(Encoder)' 스택과 '디코더(Decoder)' 스택으로 구성된다.6

- **인코더 스택 (Encoder Stack):** 인코더의 역할은 입력 시퀀스(예: 원문)를 받아 각 토큰에 대한 풍부한 문맥적 표현(contextual representation)을 생성하는 것이다. 원본 논문에서는 6개의 동일한 인코더 레이어를 쌓아 스택을 구성했다. 각 인코더 레이어는 두 개의 주요 하위 레이어(sub-layer)를 포함한다: '멀티-헤드 셀프 어텐션(Multi-Head Self-Attention)'과 '위치별 완전 연결 피드-포워드 신경망(Position-wise Fully Connected Feed-Forward Network)'이다.6
- **디코더 스택 (Decoder Stack):** 디코더의 역할은 인코더가 생성한 문맥적 표현과 이전에 생성된 출력 토큰들을 바탕으로 다음 출력 토큰을 생성하는 것이다. 디코더 역시 6개의 동일한 레이어로 구성되지만, 인코더의 두 하위 레이어에 더해 세 번째 하위 레이어인 '인코더-디코더 어텐션(Encoder-Decoder Attention)'이 추가된다.6 이 레이어는 디코더가 출력 시퀀스를 생성할 때 입력 시퀀스의 어느 부분에 집중해야 할지를 결정하는 역할을 한다. 또한, 디코더의 셀프 어텐션은 미래의 정보를 참조하지 못하도록 '마스킹(masking)' 처리가 되어, 자기회귀적(autoregressive) 생성을 가능하게 한다.15

### 2.2 핵심 메커니즘 1: 어텐션

트랜스포머의 심장부에는 어텐션 메커니즘이 자리 잡고 있다. 어텐션은 본래 기계 번역에서 인코더-디코더 모델의 성능을 향상시키기 위해 도입되었으나, 트랜스포머에서는 모델의 유일한 계산 단위로 격상되었다.

#### 2.2.1 개념 정의: 쿼리, 키, 값

어텐션의 연산은 '쿼리(Query)', '키(Key)', '값(Value)'이라는 세 가지 벡터의 상호작용으로 설명할 수 있다.6 이는 정보 검색 시스템에 비유할 수 있다. 사용자가 검색어(쿼리)를 입력하면, 시스템은 데이터베이스의 각 항목(키)과 검색어의 관련도를 계산한다. 그리고 이 관련도를 가중치로 삼아 각 항목의 실제 내용(값)들을 종합하여 최종 검색 결과를 반환한다.

마찬가지로, 어텐션 메커니즘에서 출력은 값(Value) 벡터들의 가중 합으로 계산된다. 각 값에 대한 가중치는 해당 키(Key)와 현재 처리 중인 토큰의 쿼리(Query) 벡터 간의 유사도(compatibility)를 측정하여 결정된다.6

#### 2.2.2 Scaled Dot-Product Attention

트랜스포머는 '스케일드 닷-프로덕트 어텐션(Scaled Dot-Product Attention)'이라는 특정 형태의 어텐션을 사용한다. 쿼리 행렬을 $Q$, 키 행렬을 $K$, 값 행렬을 $V$라고 할 때, 어텐션의 계산식은 다음과 같다.6
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
이 수식의 각 부분은 다음과 같은 역할을 수행한다.

1. **$QK^T$**: $Q$와 $K$의 전치 행렬($K^T$) 간의 행렬 곱셈을 통해 모든 쿼리와 키 벡터 쌍의 내적(dot product)을 계산한다. 이는 각 쿼리가 모든 키와 얼마나 유사한지를 나타내는 '어텐션 점수(attention score)' 행렬을 생성한다.16
2. **$\frac{1}{\sqrt{d_k}}$**: 스케일링 팩터(scaling factor)이다. 키 벡터의 차원 $d_k$가 클수록 내적 값의 분산이 커져 소프트맥스 함수가 포화 상태에 빠지고 기울기가 매우 작아지는 문제가 발생할 수 있다. 이를 방지하기 위해 $d_k$의 제곱근으로 나누어 점수의 스케일을 조정한다.6
3. **$\text{softmax}(\cdot)$**: 어텐션 점수 행렬의 각 행에 소프트맥스 함수를 적용하여, 합이 1이 되는 확률 분포 형태의 가중치로 변환한다. 이 가중치는 각 쿼리가 어떤 키(즉, 어떤 입력 토큰)에 얼마나 '집중'해야 하는지를 나타낸다.18
4. **$(\cdot)V$**: 마지막으로, 이렇게 계산된 가중치 행렬을 값(Value) 행렬 $V$에 곱한다. 이는 각 값 벡터를 해당 가중치만큼 가중하여 합산하는 과정으로, 최종적으로 문맥 정보가 반영된 출력 벡터를 생성한다.18

#### 2.2.3 Multi-Head Attention

트랜스포머는 단일 어텐션을 한 번만 수행하는 대신, '멀티-헤드 어텐션(Multi-Head Attention)'을 통해 여러 관점에서 정보를 동시에 처리한다. 이는 $Q, K, V$를 서로 다른 학습 가능한 선형 변환(linear projection)을 통해 $h$개의 '헤드(head)'로 분할하고, 각 헤드에서 독립적으로 스케일드 닷-프로덕트 어텐션을 병렬 수행하는 방식이다. 이후 각 헤드의 출력은 다시 하나로 연결(concatenate)되고, 최종 선형 변환을 거쳐 최종 결과물을 산출한다.6

멀티-헤드 어텐션은 두 가지 주요 이점을 제공한다.

- **다양한 표현 부분 공간 학습 (Learning Diverse Representation Subspaces):** 각 헤드는 서로 다른 '표현 부분 공간(representation subspace)'에서 정보를 포착하도록 학습된다. 예를 들어, 한 헤드는 "The animal didn't cross the street because **it** was too tired"라는 문장에서 대명사 "it"과 명사 "animal" 간의 문법적 관계에 집중하는 반면, 다른 헤드는 "it"과 형용사 "tired" 간의 의미적 관계에 집중할 수 있다. 이를 통해 모델은 단일 어텐션으로는 포착하기 어려운 다각적인 관계를 학습할 수 있다.14
- **다양한 위치에 대한 집중 능력 확장 (Expanding Ability to Focus on Different Positions):** 여러 헤드를 사용함으로써 모델은 특정 위치에 대한 집중을 분산시키고, 더 넓은 범위의 문맥 정보를 종합할 수 있다. 이는 단일 어텐션이 특정 토큰 자체에만 과도하게 집중하는 것을 방지하고, 더 풍부하고 안정적인 표현을 생성하는 데 도움을 준다.14

### 2.3 핵심 메커니즘 2: 위치 정보 표현

어텐션 메커니즘은 집합 연산(set operation)의 일종으로, 입력 시퀀스의 순서가 바뀌어도 결과가 동일한 순서 불변성(permutation-invariance)을 특징으로 한다. 이는 "나는 너를 사랑해"와 "사랑해 나는 너를"을 동일하게 처리한다는 의미이며, 언어와 같이 순서가 중요한 데이터를 다룰 때 치명적인 약점이 된다. 따라서 모델에 토큰의 위치 정보를 명시적으로 주입해줄 방법이 필요하다.10

#### 2.3.1 Sinusoidal Positional Encoding

트랜스포머는 학습 가능한 파라미터 대신, 고정된 삼각함수를 이용한 '위치 인코딩(Positional Encoding)' 방식을 제안했다. 시퀀스 내 `pos` 번째 위치에 있는 토큰의 $d_{\text{model}}$ 차원 임베딩 벡터에 더해지는 위치 인코딩 벡터 $PE$의 각 차원 값은 다음과 같이 계산된다.6
$$
PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}})
$$

$$
PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}})
$$

여기서 $i$는 인코딩 벡터의 차원 인덱스를 의미한다. 즉, 짝수 차원에는 사인 함수를, 홀수 차원에는 코사인 함수를 적용한다. 각 차원마다 다른 주파수(파장)를 가진 사인파가 사용되며, 이 파장은 $2\pi$에서 $10000 \cdot 2\pi$까지 기하급수적으로 변한다.19

이러한 설계는 단순한 위치 표기를 넘어 깊은 수학적 속성을 내포한다. 논문 저자들은 학습 가능한 위치 임베딩과 삼각함수 기반의 고정 인코딩을 모두 실험했고, 두 방식이 거의 동일한 성능을 보임을 발견했다.6 그럼에도 불구하고 삼각함수 방식을 채택한 것은, 그것이 가진 일반화 능력에 대한 깊은 통찰을 보여준다. 고정된 함수는 훈련 중에 보지 못했던 더 긴 시퀀스에 대해서도 위치 값을 생성할 수 있어, 모델의 외삽(extrapolation) 능력을 향상시킬 수 있다.6

더 중요한 속성은 임의의 고정된 오프셋 $k$에 대해, $PE_{pos+k}$가 $PE_{pos}$의 선형 변환으로 표현될 수 있다는 점이다. 이는 삼각함수의 덧셈 정리에 의해 $\sin(A+B)$와 $\cos(A+B)$가 $\sin(A), \cos(A), \sin(B), \cos(B)$의 선형 결합으로 나타나는 것과 동일한 원리이다. 이 속성 덕분에 모델은 각 토큰의 절대적인 위치가 아닌, 토큰들 간의 '상대적인 위치'에 기반하여 어텐션을 학습하는 것이 매우 용이해진다.21

### 2.4 안정적인 훈련을 위한 보조 장치

깊은 신경망의 안정적인 훈련을 위해 트랜스포머는 두 가지 중요한 보조 장치를 각 하위 레이어에 적용한다.

#### 2.4.1 잔차 연결 (Residual Connections)

각 하위 레이어(멀티-헤드 어텐션, 피드-포워드 신경망)는 잔차 연결(residual connection) 구조를 가진다. 이는 하위 레이어의 입력 $x$를 하위 레이어의 출력 $\text{Sublayer}(x)$에 더하는 것을 의미한다. 즉, 최종 출력은 $x + \text{Sublayer}(x)$ 형태가 된다.15 이 구조는 깊은 네트워크에서 역전파 시 기울기가 하위 레이어로 직접 전달될 수 있는 '지름길'을 제공함으로써, 기울기 소실 문제를 완화하고 훨씬 더 깊은 모델의 훈련을 가능하게 한다.15

#### 2.4.2 층 정규화 (Layer Normalization)

잔차 연결 후에는 '층 정규화(Layer Normalization)'가 적용된다. 최종적인 하위 레이어의 출력은 $\text{LayerNorm}(x + \text{Sublayer}(x))$가 된다.26 층 정규화는 각 훈련 샘플(예: 하나의 문장) 내의 모든 특성(feature)에 대해 독립적으로 평균을 0, 분산을 1로 만드는 정규화 기법이다.15 이는 훈련 과정을 안정화시키고 수렴 속도를 높이는 효과를 가져온다.

트랜스포머에서 배치 정규화(Batch Normalization) 대신 층 정규화가 사용된 것은 필연적인 선택이었다. 컴퓨터 비전 분야에서 표준으로 사용되던 배치 정규화는 배치 내의 여러 샘플에 걸쳐 동일한 특성의 통계를 계산한다.28 하지만 자연어 처리 데이터는 본질적으로 가변 길이라는 특성을 가진다. 모델에 입력하기 위해 짧은 문장들은 '패딩(padding)' 토큰으로 길이를 맞추게 되는데, 이 무의미한 패딩 토큰들이 배치 정규화의 통계 계산을 심각하게 왜곡시켜 훈련을 불안정하게 만든다.29 반면, 층 정규화는 각 문장 샘플에 대해 독립적으로 정규화를 수행하므로 패딩의 영향을 받지 않으며, 배치 크기가 작아도 안정적으로 작동한다.31 따라서 층 정규화의 채택은 단순히 기술적 미세 조정이 아니라, 자연어 데이터의 본질적인 특성에 딥러닝 모델을 적응시키기 위한 핵심적인 결정이었다.

## 3. 트랜스포머의 강점과 약점 분석

트랜스포머는 시퀀스 모델링 분야에 혁명을 일으켰지만, 모든 문제를 해결하는 만능 해결책은 아니다. 그 구조적 특성은 명확한 강점과 동시에 뚜렷한 약점을 낳았으며, 이 약점은 이후 AI 아키텍처 연구의 방향을 결정하는 중요한 계기가 되었다.

### 3.1 장점

트랜스포머의 핵심적인 강점은 순환 구조를 제거함으로써 얻어지는 두 가지 특성에서 기인한다.

- **탁월한 병렬 처리 능력 (Superior Parallel Processing):** 모델의 가장 근본적인 장점은 계산의 병렬화이다. RNN과 달리, 트랜스포머는 시퀀스 내 모든 토큰에 대한 표현을 동시에 계산할 수 있다. 이는 GPU와 같은 현대 하드웨어의 병렬 연산 아키텍처를 최대한 활용할 수 있게 하여, 이전 모델들과는 비교할 수 없을 정도로 훈련 시간을 단축시켰다.6 이 효율성은 모델의 규모와 훈련 데이터의 양을 폭발적으로 증가시킬 수 있는 기반이 되었다.
- **효과적인 장거리 의존성 포착 (Effective Capture of Long-Range Dependencies):** 셀프 어텐션 메커니즘은 시퀀스 내 임의의 두 토큰 간의 상호작용을 직접 모델링한다. 정보 전달 경로의 길이가 항상 1이기 때문에, 문장의 시작 부분에 있는 단어와 끝 부분에 있는 단어 간의 관계도 순차적 정보 손실 없이 효과적으로 포착할 수 있다.6 이는 RNN이 고질적으로 어려움을 겪었던 장거리 의존성 문제를 근본적으로 해결한 것이다.

### 3.2 단점: 이차 복잡도의 저주

트랜스포머의 강력한 성능에는 값비싼 대가가 따른다. 그 아킬레스건은 바로 셀프 어텐션의 계산 복잡도이다.

- **$O(N^2)$ 복잡도 (O(N^2) Complexity):** 셀프 어텐션은 시퀀스 내의 모든 토큰 쌍에 대해 어텐션 점수를 계산해야 한다. 따라서 시퀀스 길이가 $N$일 때, 필요한 연산량과 메모리는 $N$에 제곱하여 비례하는, 즉 $O(N^2)$의 복잡도를 가진다.11 시퀀스 길이가 두 배로 늘어나면, 계산 비용은 네 배로 증가하는 것이다.
- **실용적 한계 (Practical Limitations):** 이 이차 복잡도는 트랜스포머의 적용 범위를 심각하게 제한하는 실질적인 병목이다. 예를 들어, 고해상도 이미지는 수만 개의 픽셀 패치로 구성될 수 있고, 유전체 서열은 수백만 개의 염기로 이루어질 수 있다. 이러한 초장문 시퀀스에 표준 트랜스포머를 적용하는 것은 현실적으로 불가능에 가깝다.7 이 문제는 트랜스포머의 성공이 역설적으로 더 긴 컨텍스트를 처리하려는 수요를 창출했기 때문에 더욱 부각되었다. 결국, $O(N^2)$ 복잡도는 트랜스포머 이후의 거의 모든 주요 아키텍처 혁신이 해결하고자 하는 중심 과제가 되었다.

### 3.3 귀납적 편향의 관점

모델이 학습하는 방식은 그 아키텍처에 내재된 '귀납적 편향(inductive bias)'에 의해 크게 영향을 받는다. 귀납적 편향이란 모델이 처음 보는 데이터에 대해 일반화하기 위해 사용하는 사전 가정의 집합이다.39

- **CNN과 RNN의 강력한 편향 (Strong Biases of CNNs and RNNs):** CNN과 RNN은 데이터의 구조에 대한 강력한 가정을 내장하고 있다. CNN은 이미지 데이터에 대해 '지역성(locality, 인접 픽셀은 서로 관련이 깊다)'과 '이동 불변성(translation invariance, 객체의 위치가 바뀌어도 동일한 객체다)'이라는 편향을 가진다.40 RNN은 시계열이나 텍스트 데이터에 대해 '순차성(sequentiality, 데이터는 순서대로 처리되어야 한다)'이라는 편향을 가진다.40 이러한 편향은 인간이 세상을 인식하는 방식과 유사한 가정을 모델에 주입하는 '지능적 설계' 철학을 반영한다.
- **트랜스포머의 약한 편향 (Weak Bias of Transformers):** 반면, 트랜스포머는 이러한 구조적 가정이 거의 없다. 셀프 어텐션은 기본적으로 모든 토큰이 다른 모든 토큰과 동등하게 상호작용할 수 있다고 가정하므로, 매우 약한 귀납적 편향을 가진다.39 이는 모델이 특정 구조에 얽매이지 않고 데이터로부터 직접 패턴을 학습할 수 있는 유연성을 부여한다.
- **데이터와 편향의 상호작용 (The Interplay of Data and Bias):** 이 상호작용은 AI 연구의 근본적인 철학적 질문을 제기한다: 지능은 인간의 가정을 통해 '설계'되어야 하는가, 아니면 대규모 데이터로부터 '학습'되어야 하는가? 트랜스포머의 약한 편향은 적은 양의 데이터로 훈련할 경우, 강력한 편향을 가진 CNN이나 RNN보다 일반화 성능이 떨어지는 경향을 보인다.41 하지만 전례 없는 규모의 데이터셋으로 훈련될 때, 트랜스포머는 데이터에 내재된 복잡하고 미묘한 패턴을 스스로 학습하여 특화된 편향을 가진 모델의 성능을 압도한다.41 이는 "충분한 데이터와 컴퓨팅 파워가 있다면, 일반적인 아키텍처가 특화된 귀납적 편향을 이길 수 있다"는 '스케일링 가설(scaling hypothesis)'을 강력하게 뒷받침하며, AI 연구의 패러다임을 정교한 아키텍처 설계에서 대규모 데이터 및 컴퓨팅 활용으로 전환시키는 계기가 되었다.

## 4. 트랜스포머 패밀리: 아키텍처의 분화와 확장

트랜스포머의 모듈식 구조는 그 자체로 강력한 기반이 되어, 특정 목적에 맞게 일부를 변형하거나 선택적으로 활용하는 다양한 파생 모델들의 탄생을 촉진했다. 이 모델들은 트랜스포머의 인코더와 디코더 중 어느 부분을, 그리고 어떻게 활용하는지에 따라 고유한 특성과 전문 분야를 가지게 되었다. 이러한 분화는 단순히 아키텍처의 변형을 넘어, 자연어 처리의 근본적인 두 축인 '이해(understanding)'와 '생성(generation)'이라는 과제의 본질을 반영한다.

### 4.1 인코더 중심 모델 (NLU): BERT

BERT(Bidirectional Encoder Representations from Transformers)는 트랜스포머의 '인코더' 스택만을 활용하여 탄생한 모델이다.45 BERT의 핵심 철학은 문장의 의미를 깊이 이해하기 위해서는 단어의 양쪽 문맥을 모두 고려해야 한다는 것이다. 이를 구현하기 위해 BERT는 '마스크된 언어 모델(Masked Language Model, MLM)'이라는 독창적인 사전 훈련(pre-training) 방식을 도입했다.46 MLM은 입력 문장의 일부 토큰을 무작위로 `` 토큰으로 바꾼 뒤, 주변의 양방향 문맥을 모두 이용하여 원래 토큰이 무엇이었는지를 예측하도록 모델을 훈련시킨다.

이러한 양방향 학습 방식 덕분에 BERT는 특정 단어에 대한 매우 풍부하고 깊이 있는 문맥적 표현을 학습할 수 있다. 이는 자연어 이해(Natural Language Understanding, NLU)가 핵심인 과제, 예를 들어 문장의 긍정/부정을 판단하는 감성 분석, 텍스트에서 인명이나 지명 등을 추출하는 개체명 인식, 그리고 사용자의 질문에 가장 적합한 답변을 찾는 질의응답 등에서 압도적인 성능을 발휘했다.46 BERT는 NLU 분야의 새로운 기준을 제시하며, 수많은 후속 연구의 기반이 되었다.

### 4.2 디코더 중심 모델 (NLG): GPT

GPT(Generative Pre-trained Transformer) 계열의 모델들은 트랜스포머의 '디코더' 스택만을 기반으로 한다.45 GPT의 설계 철학은 텍스트 생성(generation)이라는 과제의 본질에 맞춰져 있다. 텍스트 생성은 본질적으로 순차적인 과정으로, 이전에 생성된 단어들을 바탕으로 다음 단어를 예측하는 방식으로 이루어진다.

GPT는 이러한 자기회귀(autoregressive)적 특성을 모델링하기 위해 디코더의 '마스크된 셀프 어텐션'을 핵심 메커니즘으로 사용한다.45 이 마스킹은 각 타임스텝에서 어텐션이 미래의 토큰, 즉 아직 생성되지 않은 단어들을 참조하지 못하도록 막는다. 오직 현재 위치와 그 이전의 토큰들만을 바탕으로 다음 토큰을 예측하도록 강제함으로써, 모델은 왼쪽에서 오른쪽으로(left-to-right) 일관성 있는 문장을 생성하는 능력을 학습하게 된다.

이러한 구조 덕분에 GPT는 챗봇, 기사 작성, 코드 생성, 창의적인 이야기 집필 등 자연어 생성(Natural Language Generation, NLG)이 요구되는 모든 분야에서 뛰어난 성능을 보인다.46 특히 모델의 크기를 극적으로 키운 GPT-3와 그 후속 모델들은 인간과 구별하기 어려울 정도의 유창한 텍스트를 생성하며 생성형 AI 시대를 열었다.

### 4.3 인코더-디코더 모델 (Seq2Seq): T5, BART

T5(Text-to-Text Transfer Transformer)와 BART와 같은 모델들은 트랜스포머의 인코더와 디코더 구조를 모두 활용하여, '이해'와 '생성' 능력을 결합한다.45 이 모델들은 특히 기계 번역이나 문서 요약과 같이 입력 시퀀스를 완전히 이해한 후, 이를 바탕으로 새로운 출력 시퀀스를 생성해야 하는 시퀀스-투-시퀀스(Seq2Seq) 과제에 강점을 보인다.

T5는 모든 NLP 과제를 "텍스트-투-텍스트"라는 통일된 프레임워크로 해결하려는 야심찬 시도를 했다. 예를 들어, "translate English to German: That is good."이라는 텍스트를 입력하면 "Das ist gut."이라는 텍스트를 출력하고, "summarize: [long article]..."을 입력하면 요약된 텍스트를 출력하는 식이다.46 이 접근법은 인코더가 입력 텍스트(과제 지시 포함)의 의미를 완전히 파악하고, 디코더가 그 의미를 바탕으로 적절한 출력 텍스트를 생성하는 구조를 통해 구현된다. 이러한 유연성 덕분에 T5와 같은 인코더-디코더 모델은 단일 모델로 매우 다양한 NLP 태스크를 효과적으로 수행할 수 있다.46

| 모델 패밀리   | 사용된 트랜스포머 파트 | 어텐션 메커니즘                                       | 주요 훈련 목표                         | 주요 용도                  |
| ------------- | ---------------------- | ----------------------------------------------------- | -------------------------------------- | -------------------------- |
| **BERT**      | 인코더 전용            | 양방향 셀프 어텐션                                    | Masked Language Model (MLM)            | 자연어 이해 (NLU)          |
| **GPT**       | 디코더 전용            | 마스크된 (단방향) 셀프 어텐션                         | Next Token Prediction (Autoregressive) | 자연어 생성 (NLG)          |
| **T5 / BART** | 인코더-디코더          | 인코더: 양방향, 디코더: 단방향 + 인코더-디코더 어텐션 | Denoising Objective (Text-to-Text)     | 시퀀스-투-시퀀스 (Seq2Seq) |

### 4.4 자연어를 넘어: 비전 트랜스포머 (ViT)

트랜스포머의 영향력은 자연어 처리에만 국한되지 않았다. Vision Transformer(ViT)의 등장은 트랜스포머가 범용적인 아키텍처임을 입증하는 결정적인 계기가 되었다.50 ViT 이전의 컴퓨터 비전 분야는 CNN이 확고한 지배력을 가지고 있었다.

ViT의 혁신적인 아이디어는 이미지를 '단어들의 시퀀스'처럼 취급하는 것이었다. 이를 위해 이미지를 겹치지 않는 작은 사각형 '패치(patch)'(예: 16x16 픽셀)들로 분할한다. 각 패치는 평탄화(flatten)되어 하나의 긴 벡터가 되고, 선형 투영(linear projection)을 거쳐 트랜스포머 인코더가 처리할 수 있는 차원의 벡터로 변환된다. 이렇게 생성된 패치 벡터들의 시퀀스에 위치 정보를 담은 '위치 임베딩'을 더하여 최종적으로 트랜스포머 인코더에 입력한다.50

이러한 접근은 '시퀀스'라는 개념을 시간이나 텍스트의 순서에서 벗어나, 구조화된 데이터를 구성하는 요소들의 집합으로 확장시켰다. 셀프 어텐션 메커니즘은 데이터의 종류(modality)에 구애받지 않고, 단지 집합 내 요소들 간의 관계를 학습하는 데에만 집중하기 때문에 이러한 확장이 가능했다. 대규모 이미지 데이터셋으로 훈련되었을 때, ViT는 CNN의 지역적 편향을 극복하고 이미지 전체에 걸친 전역적인 관계를 학습함으로써 기존의 최첨단 CNN 모델들의 성능을 뛰어넘었다.37 이는 트랜스포머가 언어를 넘어 오디오, 비디오, 단백질 구조 예측 등 다양한 분야로 확장될 수 있는 문을 활짝 열어젖힌 사건이었다.

## 5. 한계 극복과 미래 전망

트랜스포머의 성공은 그 자체로 완결된 이야기가 아니라, 그 내재된 한계를 극복하려는 끊임없는 연구와 혁신의 출발점이었다. 특히 $O(N^2)$ 복잡도라는 근본적인 제약은 새로운 아키텍처의 등장을 촉발하는 '창조적 파괴'의 원동력이 되었다. 이러한 연구의 흐름은 트랜스포머라는 '정립(Thesis)'에 대한 '반정립(Antithesis, 이차 복잡도)'이 새로운 '종합(Synthesis, 차세대 아키텍처)'을 낳는 변증법적 발전 과정을 보여준다.

### 5.1 효율적인 트랜스포머를 향하여

$O(N^2)$ 복잡도를 해결하기 위한 초기 연구들은 완전한 어텐션(full attention)의 계산을 근사(approximate)하거나 희소(sparse)하게 만들어 효율성을 높이는 데 집중했다.7

- **저계급 근사 (Low-Rank Approximation):** **Linformer**와 같은 모델은 어텐션 행렬이 본질적으로 저계급(low-rank)이라는 가정에서 출발한다. 즉, 정보의 대부분이 몇 개의 주요한 패턴에 집중되어 있다는 것이다. 이를 바탕으로 키($K$)와 값($V$) 행렬을 더 낮은 차원으로 선형 투영(projection)하여 어텐션을 계산함으로써, 복잡도를 $O(N \cdot k)$ (여기서 $k$는 투영된 차원)로 낮춰 사실상 선형 복잡도($O(N)$)를 달성한다.54

  **Performer**는 커널(kernel) 함수를 이용해 소프트맥스 어텐션을 근사하고 행렬 곱셈의 순서를 바꿔 선형 복잡도를 구현했다.54

- **희소 어텐션 (Sparse Attention):** **Longformer**나 **BigBird**와 같은 모델들은 모든 토큰이 서로 상호작용할 필요는 없다는 아이디어에 기반한다. 대신, 각 토큰이 일부 제한된 패턴의 토큰들과만 어텐션을 계산하도록 만든다. 예를 들어, 인접한 토큰들(sliding window attention), 사전에 지정된 전역 토큰(global attention), 또는 무작위로 선택된 토큰(random attention)과만 상호작용하게 하여 전체적인 계산량을 줄인다.35

- **해싱 기반 (Hashing-based):** **Reformer**는 지역성 민감 해싱(Locality-Sensitive Hashing, LSH) 기법을 사용한다. 유사한 쿼리와 키 벡터들이 높은 확률로 동일한 해시 버킷(hash bucket)에 할당된다는 점을 이용하여, 같은 버킷 내의 토큰들 사이에서만 어텐션을 계산한다. 이를 통해 복잡도를 $O(N \log N)$ 수준으로 낮춘다.54

| 접근 방식 분류  | 대표 모델            | 핵심 아이디어                  | 결과 복잡도   |
| --------------- | -------------------- | ------------------------------ | ------------- |
| **저계급 근사** | Linformer, Performer | 키/값 행렬 투영, 커널화        | $O(N)$        |
| **희소 어텐션** | Longformer, BigBird  | 고정된 희소 패턴 적용          | $O(N)$        |
| **해싱 기반**   | Reformer             | LSH를 통한 유사 쿼리/키 그룹화 | $O(N \log N)$ |
| **어텐션-프리** | Mamba                | 선택적 상태 공간 모델          | $O(N)$        |

### 5.2 어텐션을 넘어서: 상태 공간 모델 (Mamba)

효율적인 트랜스포머들이 어텐션을 '개선'하려는 시도였다면, Mamba는 어텐션을 완전히 '대체'하려는 급진적인 접근법을 제시한다.58 Mamba는 고전적인 제어 이론에서 영감을 받은 상태 공간 모델(State Space Model, SSM)을 기반으로 한다.

SSM은 연속적인 신호나 시퀀스를 은닉 상태(hidden state)를 통해 모델링하는 방식으로, 본질적으로 RNN과 유사한 순환 구조를 가진다. Mamba의 핵심 혁신은 '선택적 SSM(Selective SSM)'에 있다. 기존 SSM의 파라미터($A, B, C$)가 고정되어 있는 것과 달리, Mamba는 이 파라미터들을 입력 데이터에 따라 동적으로 변화시킨다. 이 '선택성' 덕분에 모델은 문맥에 따라 어떤 정보를 유지하고 어떤 정보를 버릴지 결정할 수 있어, 장기 의존성을 효과적으로 포착할 수 있다.60

Mamba는 시퀀스 길이에 대해 선형 복잡도($O(N)$)를 가지며, 훈련 시에는 병렬 스캔 알고리즘을 통해 효율적인 병렬 처리가 가능하다. 추론 시에는 RNN처럼 이전 타임스텝의 상태 벡터만 유지하면 되므로 매우 빠르고 메모리 효율적이다.60 이는 어텐션의 이차 복잡도 문제를 근본적으로 해결하는 강력한 대안으로 부상하고 있다.

### 5.3 스케일링의 새로운 지평: 전문가 혼합 (MoE)

Mixture of Experts(MoE)는 계산 복잡도가 아닌, 모델의 파라미터 수를 효율적으로 확장하는 문제에 대한 해답을 제시한다. MoE의 기본 아이디어는 하나의 거대한 신경망 대신, 여러 개의 작고 전문화된 '전문가(expert)' 네트워크와 이들을 적재적소에 호출하는 '게이팅 네트워크(gating network)' 또는 '라우터(router)'를 두는 것이다.64

트랜스포머 아키텍처에서는 일반적으로 피드-포워드 신경망(FFN) 레이어가 MoE 레이어로 대체된다. 입력으로 들어온 각 토큰에 대해, 라우터는 어떤 전문가가 이 토큰을 가장 잘 처리할지 판단하여 소수의 전문가(예: Mixtral 8x7B의 경우 8개 중 상위 2개)만을 활성화시킨다.65

이러한 '조건부 연산(conditional computation)' 덕분에 모델의 총 파라미터 수는 수천억 개에 달하더라도, 각 토큰을 처리하는 데 실제로 사용되는 활성 파라미터의 수는 훨씬 적다. 이는 추론 비용을 크게 줄이면서도 대규모 모델이 갖는 표현력과 용량(capacity)의 이점을 누릴 수 있게 한다.64 MoE는 모델 스케일링의 패러다임을 '더 빽빽하게(denser)'에서 '더 넓고 희소하게(wider and sparser)'로 전환시키고 있다.

### 5.4 신뢰성 강화: 검색 증강 생성 (RAG)

대규모 언어 모델(LLM)의 발전은 '환각(hallucination)' 현상, 즉 사실과 다르거나 존재하지 않는 정보를 그럴듯하게 생성하는 문제와 훈련 데이터의 시점 이후 정보를 반영하지 못하는 '최신성 부족' 문제를 동반했다.67 검색 증강 생성(Retrieval-Augmented Generation, RAG)은 이러한 문제를 해결하기 위한 강력한 프레임워크이다.

RAG는 모델이 지식을 내부 파라미터에만 저장하는 '폐쇄형 시험(closed-book exam)' 방식에서 벗어나, 외부의 신뢰할 수 있는 지식 베이스를 참조하는 '개방형 시험(open-book exam)' 방식으로 전환하는 것을 목표로 한다.69 이 패러다임의 전환은 모델이 '무엇을 아는가'에 대한 정의를 근본적으로 바꾼다.

RAG의 작동 과정은 세 단계로 이루어진다.68

1. **검색 (Retrieve):** 사용자 쿼리가 입력되면, 먼저 벡터 데이터베이스와 같은 외부 지식 소스에서 쿼리와 의미적으로 관련된 문서나 정보 조각을 검색한다.
2. **증강 (Augment):** 검색된 정보를 원래의 쿼리와 결합하여 LLM에 전달할 새로운 프롬프트를 구성한다.
3. **생성 (Generate):** LLM은 이렇게 증강된 프롬프트를 바탕으로, 외부에서 제공된 사실에 근거하여 답변을 생성한다.

이 접근법은 LLM의 추론 엔진과 지식 베이스를 분리하는 효과를 가져온다. 이를 통해 값비싼 재훈련 없이도 지식 베이스를 쉽게 업데이트하여 최신성을 유지할 수 있으며 70, 생성된 답변의 출처를 명시하여 사용자가 사실을 검증할 수 있게 함으로써 모델의 신뢰성과 투명성을 획기적으로 향상시킨다.69

## 6. 결론: 지속적인 혁신의 중심, 트랜스포머

트랜스포머 아키텍처는 단순히 하나의 성공적인 모델을 넘어, 현대 인공지능 연구의 방향을 근본적으로 바꾼 하나의 현상으로 평가받아야 한다. 순환이라는 오랜 패러다임을 과감히 폐기하고 어텐션 메커니즘을 전면에 내세움으로써, 트랜스포머는 시퀀스 모델링의 고질적인 한계였던 장기 의존성 문제를 해결하고, 전례 없는 수준의 병렬 처리를 통해 AI 모델의 스케일링을 가능하게 했다.

트랜스포머가 남긴 가장 중요한 유산은 그 범용성에 있다. BERT, GPT와 같은 파생 모델들은 각각 자연어 이해와 생성 분야의 표준을 정립했으며, ViT의 등장은 트랜스포머가 언어의 경계를 넘어 컴퓨터 비전을 비롯한 AI 전반을 아우르는 핵심 아키텍처로 자리매김했음을 증명했다. 이는 트랜스포머가 특정 문제에 대한 해결책이 아니라, 다양한 데이터 내의 관계를 학습하는 일반적인 원리를 제공했기 때문이다.

그러나 트랜스포머의 여정은 여기서 멈추지 않는다. 그 자체의 한계, 특히 이차 복잡도의 문제는 오히려 새로운 혁신을 촉발하는 기폭제가 되었다.

- **효율성과의 끊임없는 싸움:** $O(N^2)$ 복잡도를 극복하려는 노력은 Mamba와 같은 선형 시간 아키텍처의 등장으로 이어졌다. 미래의 모델들은 성능과 계산 효율성 사이의 최적점을 찾는 것이 핵심적인 설계 원칙이 될 것이다.
- **스케일링과 모듈화:** MoE와 같은 기술은 거대한 단일 모델이 아닌, 여러 전문화된 모듈이 협력하는 새로운 형태의 아키텍처 가능성을 제시한다. 이는 모델의 규모를 더욱 확장하면서도 효율성을 유지하는 중요한 방향이다.
- **신뢰성과 외부 세계와의 연결:** RAG는 모델의 지능을 내부 파라미터에서 외부의 검증 가능한 데이터 소스로 확장함으로써, AI 시스템이 현실 세계와 상호작용하는 방식에 대한 근본적인 변화를 예고한다. 이는 AI의 신뢰성, 최신성, 투명성을 확보하는 데 필수적인 단계이다.

결론적으로, 트랜스포머는 그 자체로 완성된 최종 아키텍처가 아니다. 오히려 그 성공과 한계가 맞물리며 후속 연구자들에게 끊임없이 새로운 질문을 던지고, 그 해답을 찾는 과정에서 Mamba, MoE, RAG와 같은 차세대 기술들을 탄생시키는 '지속적인 혁신의 플랫폼'으로서 기능하고 있다. 트랜스포머의 진정한 의의는 그것이 제공한 답이 아니라, 그것이 제기한 문제들에 있다.

#### **참고 자료**

1. (PDF) A Critical Review of RNN and LSTM Variants in Hydrological Time Series Predictions, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/383985786_A_Critical_Review_of_RNN_and_LSTM_Variants_in_Hydrological_Time_Series_Predictions
2. Recurrent neural network - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/Recurrent_neural_network
3. Recurrent Neural Networks: A Comprehensive Review of Architectures, Variants, and Applications - MDPI, 8월 15, 2025에 액세스, https://www.mdpi.com/2078-2489/15/9/517
4. LSTM Inefficiency in Long-Term Dependencies Regression Problems | Journal of Advanced Research in Applied Sciences and Engineering Technology - Semarak Ilmu Publishing, 8월 15, 2025에 액세스, https://semarakilmu.com.my/journals/index.php/applied_sciences_eng_tech/article/view/1477
5. A Review on the Long Short-Term Memory Model - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/340493274_A_Review_on_the_Long_Short-Term_Memory_Model
6. Attention is All you Need - NIPS, 8월 15, 2025에 액세스, https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
7. A Practical Survey on Faster and Lighter Transformers - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2103.14636
8. [1706.03762] Attention Is All You Need - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/1706.03762
9. Attention Is All You Need - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/Attention_Is_All_You_Need
10. Transformer (deep learning architecture) - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)
11. What is the Transformer architecture? | PromptLayer, 8월 15, 2025에 액세스, https://www.promptlayer.com/glossary/transformer-architecture
12. Korean Grammatical Error Correction Based on Transformer with Copying Mechanisms and Grammatical Noise Implantation Methods - MDPI, 8월 15, 2025에 액세스, https://www.mdpi.com/1424-8220/21/8/2658
13. Transformer Encoder-Decoder architecture, taken from Vaswani et al. [9]... - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/figure/Transformer-Encoder-Decoder-architecture-taken-from-Vaswani-et-al-9-for-illustration_fig2_338223294
14. The Illustrated Transformer – Jay Alammar – Visualizing machine ..., 8월 15, 2025에 액세스, https://jalammar.github.io/illustrated-transformer/
15. Transformer Building Blocks - Packt, 8월 15, 2025에 액세스, https://www.packtpub.com/en-us/learning/how-to-tutorials/transformer-building-blocks
16. Tutorial 6: Transformers and Multi-Head Attention - UvA DL Notebooks v1.2 documentation, 8월 15, 2025에 액세스, https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html
17. What is the intuition behind the dot product attention? - AI Stack Exchange, 8월 15, 2025에 액세스, https://ai.stackexchange.com/questions/20176/what-is-the-intuition-behind-the-dot-product-attention
18. In Depth Understanding of Attention Mechanism (Part II) - Scaled Dot-Product Attention and Example | by FunCry | Medium, 8월 15, 2025에 액세스, https://medium.com/@funcry/in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e
19. A Gentle Introduction to Positional Encoding in Transformer Models, Part 1 - MachineLearningMastery.com, 8월 15, 2025에 액세스, https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/
20. Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2506.06398
21. Understanding Sinusoidal Positional Encoding in Transformers | by Pranay Janupalli, 8월 15, 2025에 액세스, https://medium.com/@pranay.janupalli/understanding-sinusoidal-positional-encoding-in-transformers-26c4c161b7cc
22. Inside Sinusoidal Position Embeddings: A Sense of Order - LearnOpenCV, 8월 15, 2025에 액세스, https://learnopencv.com/sinusoidal-position-embeddings/
23. Positional Encoding - Notes on AI, 8월 15, 2025에 액세스, https://notesonai.com/positional+encoding
24. A Predictive Model Based on Transformer with Statistical Feature Embedding in Manufacturing Sensor Dataset - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2407.06682v1
25. Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2505.11881v1
26. Transformers. Statutory warning: You will need lot of... | by Tapan Mittal - Medium, 8월 15, 2025에 액세스, https://tapanmittal.medium.com/transformers-9b38bb212fa0?source=rss-5e4e20bc9506------2
27. Layer Normalization in Transformers | Layer Norm Vs Batch Norm - YouTube, 8월 15, 2025에 액세스, https://www.youtube.com/watch?v=qti0QPdaelg
28. Transformers without Normalization - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2503.10622v2
29. What is layer normalization? What's it trying to achieve? High-level idea of its mathematical underpinnings? Its use-cases? : r/computervision - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/computervision/comments/ypshs9/what_is_layer_normalization_whats_it_trying_to/
30. Layer Normalization in Transformer | by Sachinsoni - Medium, 8월 15, 2025에 액세스, https://medium.com/@sachinsoni600517/layer-normalization-in-transformer-1a2efbff8b85
31. Build Better Deep Learning Models with Batch and Layer ... - Pinecone, 8월 15, 2025에 액세스, https://www.pinecone.io/learn/batch-layer-normalization/
32. Attention is all you need: utilizing attention in AI-enabled drug discovery - Oxford Academic, 8월 15, 2025에 액세스, https://academic.oup.com/bib/article/25/1/bbad467/7512647
33. Chapter 6: Advanced Architectural Variants and Analysis - ApX Machine Learning, 8월 15, 2025에 액세스, https://apxml.com/courses/foundations-transformers-architecture/chapter-6-advanced-architectural-variants-analysis
34. End of Transformers - follow the idea - Obsidian Publish, 8월 15, 2025에 액세스, https://publish.obsidian.md/followtheidea/Content/AI/End+of+Transformers
35. Why does attention need to be fully quadratic? : r/LocalLLaMA - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/LocalLLaMA/comments/150owmj/why_does_attention_need_to_be_fully_quadratic/
36. Breaking the attention bottleneck - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2406.10906v1
37. A survey of Transformer applications for histopathological image analysis: New developments and future directions - PubMed Central, 8월 15, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10518923/
38. Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2404.16112v1
39. CNNs vs Vision Transformers - Biological Computer Vision (3/3) | by Niranjan Rajesh | Bits and Neurons | Medium, 8월 15, 2025에 액세스, https://medium.com/bits-and-neurons/cnns-vs-vision-transformers-biological-computer-vision-3-3-56ff955ba463
40. A fAIry tale of the Inductive Bias | Towards Data Science, 8월 15, 2025에 액세스, https://towardsdatascience.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c/
41. ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | Learning-Deep-Learning, 8월 15, 2025에 액세스, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/vit.html
42. Inductive Bias In Deep Learning - 1 | by Sanjithkumar - Medium, 8월 15, 2025에 액세스, https://medium.com/@sanjithkumar986/inductive-bias-in-deep-learning-1-17a7c3f35381
43. [D] A look at Attention and Transformers from the lens of Inductive Bias in ML - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/1bgghee/d_a_look_at_attention_and_transformers_from_the/
44. Vision Transformers vs CNNs at the Edge, 8월 15, 2025에 액세스, https://www.edge-ai-vision.com/2024/03/vision-transformers-vs-cnns-at-the-edge/
45. Transformer-based Korean Pretrained Language Models: A Survey on Three Years of Progress - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2112.03014
46. Comparing Large Language Models : GPT vs. BERT vs. T5 ..., 8월 15, 2025에 액세스, https://automotivevisions.wordpress.com/2025/03/21/comparing-large-language-models-gpt-vs-bert-vs-t5/
47. Transformer, GPT-3,GPT-J, T5 and BERT. | by Ali Issa - Medium, 8월 15, 2025에 액세스, https://aliissa99.medium.com/transformer-gpt-3-gpt-j-t5-and-bert-4cf8915dd86f
48. How does OpenAI compare to other models like BERT and T5? - Milvus, 8월 15, 2025에 액세스, https://milvus.io/ai-quick-reference/how-does-openai-compare-to-other-models-like-bert-and-t5
49. (PDF) Revolutionising Translation Technology: A Comparative Study of Variant Transformer Models - BERT, GPT, and T5 - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/381553531_REVOLUTIONISING_TRANSLATION_TECHNOLOGY_A_COMPARATIVE_STUDY_OF_VARIANT_TRANSFORMER_MODELS_-BERT_GPT_AND_T5
50. Three things everyone should know about Vision Transformers - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2203.09795
51. Vision transformer architecture and applications in digital health: a tutorial and survey - PMC, 8월 15, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10333157/
52. Visual Explanations of Vision Transformer Guided by Self-Attention - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2402.04563
53. big.LITTLE Vision Transformer for Efficient Visual Recognition - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2410.10267v1
54. Paper Summary #7 - Efficient Transformers: A Survey | Shreyansh ..., 8월 15, 2025에 액세스, https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/
55. [2009.06732] Efficient Transformers: A Survey - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2009.06732
56. [R] Linformer: Self-Attention with Linear Complexity : r/MachineLearning - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/h0eup6/r_linformer_selfattention_with_linear_complexity/
57. [2001.04451] Reformer: The Efficient Transformer - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2001.04451
58. state-spaces/mamba: Mamba SSM architecture - GitHub, 8월 15, 2025에 액세스, https://github.com/state-spaces/mamba
59. From S4 to Mamba: A Comprehensive Survey on Structured State Space Models - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2503.18970
60. Mamba: Linear-Time Sequence Modeling with Selective State ..., 8월 15, 2025에 액세스, https://www.oxen.ai/blog/mamba-linear-time-sequence-modeling-with-selective-state-spaces-arxiv-dives
61. Here's how we can remain competitive with closed source cloud models which have access to MASSIVE amounts of compute for inference particularly on LONG context tasks. Quadratic Transformers vs Subquadratic or Linear models (e.g. Mamba) + RAG. In context learning is all you need: : r/LocalLLaMA - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/LocalLLaMA/comments/1azhcjd/heres_how_we_can_remain_competitive_with_closed/
62. [2406.07592] MambaLRP: Explaining Selective State Space Sequence Models - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2406.07592
63. MambaLRP: Explaining Selective State Space Sequence Models - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2406.07592v1
64. What Is Mixture of Experts (MoE)? How It Works, Use Cases & More | DataCamp, 8월 15, 2025에 액세스, https://www.datacamp.com/blog/mixture-of-experts-moe
65. Mixture of Experts LLMs: Key Concepts Explained - neptune.ai, 8월 15, 2025에 액세스, https://neptune.ai/blog/mixture-of-experts-llms
66. Applying Mixture of Experts in LLM Architectures | NVIDIA Technical Blog, 8월 15, 2025에 액세스, https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/
67. What is Retrieval-Augmented Generation (RAG)? | Google Cloud, 8월 15, 2025에 액세스, https://cloud.google.com/use-cases/retrieval-augmented-generation
68. Retrieval-Augmented Generation for Large Language Models: A Survey - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2312.10997
69. What is retrieval-augmented generation (RAG)? - IBM Research, 8월 15, 2025에 액세스, https://research.ibm.com/blog/retrieval-augmented-generation-RAG
70. What is RAG? - Retrieval-Augmented Generation AI Explained - AWS, 8월 15, 2025에 액세스, https://aws.amazon.com/what-is/retrieval-augmented-generation/
71. Retrieval-augmented generation - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/Retrieval-augmented_generation
72. Understanding RAG: 6 Steps of Retrieval Augmented Generation (RAG) - Acorn Labs, 8월 15, 2025에 액세스, https://www.acorn.io/resources/learning-center/retrieval-augmented-generation/

