# 트랜스포머 아키텍처 2

## 서론: 순환 신경망의 한계와 트랜스포머의 등장

자연어 처리(NLP)를 비롯한 순차 데이터 모델링 분야는 오랫동안 순환 신경망(Recurrent Neural Network, RNN)과 그 변형인 Long Short-Term Memory(LSTM)에 의해 지배되어 왔다. 이 모델들은 이전 시간 단계(time step)의 정보를 은닉 상태(hidden state)라는 벡터에 압축하여 현재 시간 단계의 입력과 함께 처리하는 순환적 구조를 통해 시퀀스의 동적 특성을 포착하도록 설계되었다.1 그러나 이러한 구조는 본질적인 한계를 내포하고 있었다.

### 0.1 순차적 처리의 본질적 한계 분석

RNN 계열 모델의 가장 근본적인 한계는 순차적(sequential) 데이터 처리 방식에 있다. 각 시간 단계의 계산은 이전 단계의 은닉 상태에 의존하기 때문에, 전체 시퀀스에 대한 연산을 병렬화하는 것이 원천적으로 불가능하다.1 이는 대규모 병렬 연산에 최적화된 그래픽 처리 장치(GPU)와 같은 현대 컴퓨팅 하드웨어의 잠재력을 충분히 활용하지 못하게 만들어, 모델의 훈련 속도를 저하시키는 심각한 병목 현상을 야기했다.4 모델의 크기가 커지고 데이터셋이 방대해짐에 따라 이러한 계산 비효율성은 더욱 두드러진 문제로 부상했다.

### 0.2 장기 의존성 문제 (Long-Term Dependency Problem) 심층 탐구

순차 처리 방식은 또 다른 고질적인 문제인 '장기 의존성 문제'를 야기했다.2 시퀀스의 길이가 길어질수록, 초반부에 위치한 중요한 정보가 여러 시간 단계를 거치면서 희석되거나 소실되어 시퀀스 후반부까지 제대로 전달되지 못하는 현상이다.3 예를 들어, "철수가 교실에서 공부하고 있었다.... 영희가 그에게 인사를 했다."라는 문장에서 '그'가 '철수'를 지칭한다는 것을 파악하려면, 모델은 '철수'라는 정보를 먼 거리까지 기억하고 있어야 한다. 하지만 RNN은 이 과정에서 정보가 희미해지는 경향이 있다.3

이 문제의 수학적 근원은 역전파(backpropagation) 과정에서 발생하는 기울기 소실(vanishing gradient) 또는 기울기 폭발(exploding gradient) 현상에 있다. 시퀀스를 따라 역전파되는 기울기는 동일한 가중치 행렬을 반복적으로 곱하게 되는데, 이 가중치의 특이값(singular value)이 1보다 작으면 기울기는 지수적으로 0에 수렴하고, 1보다 크면 무한대로 발산한다.3 LSTM은 망각 게이트(forget gate), 입력 게이트(input gate), 출력 게이트(output gate)와 같은 정교한 게이트 메커니즘을 도입하여 정보의 흐름을 제어함으로써 이 문제를 상당 부분 완화했다.2 그러나 게이트 구조가 복잡해지면서 파라미터 수가 크게 증가했고, 순차 처리라는 근본적인 틀은 벗어나지 못했다.6

### 0.3 'Attention Is All You Need'의 패러다임 전환

2017년, Vaswani et al.은 "Attention Is All You Need"라는 논문을 통해 이러한 문제들에 대한 혁신적인 해결책을 제시하며 NLP 분야의 새로운 패러다임을 열었다.7 이들이 제안한 트랜스포머(Transformer) 아키텍처는 순환 구조를 완전히 배제하고 '어텐션(Attention)'이라는 메커니즘에만 전적으로 의존한다.1 어텐션 메커니즘은 시퀀스 내의 모든 토큰 쌍 간의 관계를 직접적으로, 그리고 동시에 계산할 수 있게 해준다.

이러한 설계는 두 가지 혁신적인 결과를 낳았다.

첫째, 모든 토큰에 대한 계산이 병렬적으로 수행될 수 있어 GPU의 연산 능력을 극대화하고 훈련 시간을 획기적으로 단축시켰다.1

둘째, 시퀀스 내 임의의 두 토큰 사이의 경로 길이가 `O(1)`로 고정되어, 거리에 상관없이 정보가 직접 전달될 수 있게 되었다. 이는 RNN의 `O(n)` 경로 길이와 대조적으로, 장기 의존성 문제를 구조적으로 해결하는 결정적인 계기가 되었다.4

트랜스포머의 등장은 단순히 성능 개선을 넘어, 순차 데이터 모델링에 대한 근본적인 접근 방식을 바꾸어 놓은 사건이었다. 이는 알고리즘 설계가 하드웨어의 특성과 어떻게 시너지를 이룰 수 있는지를 보여주는 대표적인 사례로 평가된다.

## 1.  트랜스포머 아키텍처의 구조적 해부

트랜스포머 아키텍처는 기계 번역과 같은 시퀀스-투-시퀀스(sequence-to-sequence) 작업을 효과적으로 수행하기 위해 설계된 인코더-디코더(Encoder-Decoder) 구조를 기반으로 한다.8 이 구조는 입력 시퀀스를 이해하는 부분과 출력 시퀀스를 생성하는 부분을 명확히 분리하여 각자의 역할에 집중하도록 한다.

### 1.1 전체 구조 개요

표준 트랜스포머 모델은 동일한 구조를 가진 `N`개의 인코더 레이어를 쌓아 올린 인코더 스택과, 역시 동일한 구조의 `N`개의 디코더 레이어로 구성된 디코더 스택으로 이루어진다.8

- **인코더 (Encoder):** 인코더의 역할은 입력 시퀀스(예: 원문 언어의 문장)를 받아 각 토큰의 문맥적 의미를 풍부하게 담은 연속적인 벡터 표현(contextual representation)의 시퀀스로 변환하는 것이다. 각 인코더 레이어는 셀프 어텐션(Self-Attention) 메커니즘과 위치별 피드포워드 신경망(Position-wise Feed-Forward Network)이라는 두 개의 하위 레이어(sub-layer)로 구성된다.10
- **디코더 (Decoder):** 디코더는 인코더가 생성한 문맥 벡터 시퀀스를 입력받아, 목표 시퀀스(예: 번역된 문장)를 토큰 단위로 순차적으로 생성한다. 각 디코더 레이어는 인코더의 두 하위 레이어에 더해, 인코더의 출력과 디코더의 입력 사이의 관계를 모델링하는 인코더-디코더 어텐션(Encoder-Decoder Attention) 하위 레이어를 추가로 가진다.11

### 1.2 데이터 흐름의 시각화

트랜스포머 내부에서 데이터는 다음과 같은 정교한 과정을 거쳐 처리된다.

1. **입력 처리:** 입력 문장은 먼저 토크나이저(Tokenizer)에 의해 개별적인 토큰(단어 또는 하위 단어)의 시퀀스로 분할된다.11
2. **임베딩 및 위치 인코딩:** 각 토큰은 임베딩 레이어(Embedding Layer)를 통해 고차원의 벡터로 변환된다. 이 임베딩 벡터는 토큰의 의미론적 정보를 담고 있다. 여기에, 시퀀스 내 토큰의 순서 정보를 주입하기 위해 위치 인코딩(Positional Encoding) 벡터가 더해진다.8
3. **인코더 스택:** 위치 정보가 추가된 임베딩 벡터 시퀀스는 인코더 스택의 첫 번째 레이어로 입력된다. 각 레이어에서 데이터는 셀프 어텐션을 통해 시퀀스 전체의 문맥 정보를 통합하고, 피드포워드 신경망을 통해 비선형적으로 변환된다. 한 레이어의 출력은 다음 레이어의 입력으로 전달된다.
4. **인코더-디코더 연결:** 최종 인코더 레이어의 출력은 키(Key)와 값(Value) 벡터의 형태로 디코더 스택의 모든 레이어에 전달된다. 이 벡터들은 디코더의 인코더-디코더 어텐션 하위 레이어에서 사용되어, 출력 토큰을 생성할 때 입력 시퀀스의 어떤 부분에 집중해야 할지를 결정하는 데 중요한 역할을 한다.13
5. **디코더 스택 및 출력 생성:** 디코더는 자기회귀적(auto-regressive) 방식으로 작동한다. 즉, 각 시간 단계에서 이전에 생성된 모든 토큰들을 입력으로 받아 다음 토큰을 예측한다.14 이 과정은 디코더 내부의 마스크드 셀프 어텐션(Masked Self-Attention)과 인코더-디코더 어텐션을 거쳐 수행된다.
6. **최종 출력:** 최종 디코더 레이어의 출력 벡터는 선형 변환(Linear Transformation)을 거쳐 어휘(vocabulary) 크기의 로짓(logit) 벡터로 변환된다. 이 로짓 벡터에 소프트맥스(Softmax) 함수를 적용하면 각 단어에 대한 확률 분포가 계산되고, 가장 확률이 높은 단어가 다음 토큰으로 선택된다.15

### 1.3 핵심 철학: 재귀의 완전한 배제

이 모든 과정에서 가장 중요한 철학은 순환(recurrence) 구조를 완전히 배제했다는 점이다.1 어텐션과 피드포워드 네트워크는 모두 행렬 곱셈과 같은 병렬 처리가 용이한 연산으로 구성되어 있다. 이는 트랜스포머가 이전 시퀀스 모델들의 고질적인 문제였던 계산 효율성을 극복하고 대규모 데이터와 모델에 대한 학습을 가능하게 한 핵심적인 설계 원리다.16

## 2.  핵심 메커니즘 - 셀프 어텐션

트랜스포머 아키텍처의 심장부에는 셀프 어텐션(Self-Attention) 메커니즘이 자리 잡고 있다. 이는 시퀀스 내의 각 요소가 다른 모든 요소와의 상호작용을 통해 자신의 표현을 정제하고 문맥화하는 과정이다. 이 메커니즘은 스케일드 닷-프로덕트 어텐션이라는 구체적인 형태로 구현되며, 멀티-헤드 구조를 통해 그 표현력을 극대화한다.

### 2.1  스케일드 닷-프로덕트 어텐션 (Scaled Dot-Product Attention)

#### 2.1.1 개념 정의

셀프 어텐션은 "쿼리(Query)가 키(Key)와의 유사도를 바탕으로 값(Value)의 가중합을 구하는 과정"으로 개념화할 수 있다.8 이는 정보 검색 시스템에서 검색어(쿼리)를 입력하면, 각 문서의 색인(키)과 비교하여 관련성이 높은 문서의 내용(값)을 찾아 보여주는 원리와 유사하다. 셀프 어텐션에서는 시퀀스 내의 모든 토큰이 동시에 쿼리, 키, 값의 역할을 수행한다.18

- **쿼리 (Query, Q):** 현재 처리 중인 토큰을 나타내는 벡터. 다른 토큰들과의 관계를 파악하기 위한 '질문'의 역할을 한다.12
- **키 (Key, K):** 시퀀스 내의 모든 토큰을 나타내는 벡터. 쿼리와의 유사도를 계산하기 위한 '색인' 역할을 한다.12
- **값 (Value, V):** 시퀀스 내의 모든 토큰에 대한 정보를 담고 있는 벡터. 어텐션 가중치에 따라 가중합되어 최종 출력에 기여하는 '실제 내용'이다.12

#### 2.1.2 수학적 공식화

스케일드 닷-프로덕트 어텐션의 계산 과정은 다음과 같은 수식으로 요약된다.11
$$
Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
이 과정은 몇 가지 단계로 나눌 수 있다.

1. **Q, K, V 행렬 생성:** 입력 임베딩 시퀀스 행렬 `X`에 대해, 학습 가능한 세 개의 가중치 행렬 $W^Q$, $W^K$, $W^V$를 각각 곱하여 쿼리, 키, 값 행렬 $Q = XW^Q$, $K = XW^K$, $V = XW^V$를 생성한다.11
2. **어텐션 스코어 계산:** 쿼리 행렬 `Q`와 키 행렬의 전치 `K^T`를 곱하여 어텐션 스코어 행렬을 계산한다. 이 행렬의 `(i, j)` 원소는 `i`번째 토큰의 쿼리 벡터와 `j`번째 토큰의 키 벡터 간의 내적(dot product) 값으로, 두 토큰 간의 유사도 또는 연관성을 나타낸다.19
3. **스케일링:** 계산된 어텐션 스코어를 키 벡터의 차원 $d_k$의 제곱근($\sqrt{d_k}$)으로 나누어준다.
4. **소프트맥스 정규화:** 스케일링된 스코어 행렬의 각 행에 소프트맥스 함수를 적용한다. 이를 통해 각 행의 합이 1이 되는 확률 분포, 즉 어텐션 가중치(attention weights)를 얻는다. 이는 각 쿼리 토큰이 다른 모든 키 토큰에 얼마나 '주의'를 기울여야 하는지를 나타내는 가중치 분포다.8
5. **가중합 계산:** 마지막으로, 이 어텐션 가중치 행렬을 값 행렬 `V`와 곱한다. 이 결과로 생성된 행렬의 각 행은 해당 토큰에 대한 문맥화된 새로운 표현 벡터가 된다. 이는 시퀀스 내 모든 토큰의 값(Value) 벡터들을 어텐션 가중치에 따라 가중 평균한 결과다.11

#### 2.1.3 스케일링의 중요성

어텐션 스코어를 $\sqrt{d_k}$로 나누는 스케일링 단계는 사소해 보이지만 모델의 학습 안정성에 결정적인 역할을 한다.9 키 벡터의 차원 $d_k$가 커질수록, 내적 값의 분산도 커지는 경향이 있다. 만약 스케일링 없이 큰 값의 스코어가 소프트맥스 함수에 입력되면, 함수는 기울기가 거의 0에 가까운 포화(saturation) 영역으로 들어갈 수 있다.21 이 경우 역전파 시 기울기가 소실되어 학습이 제대로 이루어지지 않는다. 스케일링은 내적 값의 분산을 일정하게 유지시켜 이러한 문제를 방지하고 안정적인 학습을 가능하게 한다.8

### 2.2  멀티-헤드 어텐션 (Multi-Head Attention)

#### 2.2.1 필요성 및 직관

단일 어텐션 메커니즘은 한 가지 측면에서만 토큰 간의 관계를 학습한다. 그러나 문장 속 단어들은 다양한 관계를 맺는다. 예를 들어 "그가 만든 로봇은 매우 빨랐다"라는 문장에서 '로봇'은 '만든'과 주체-행위 관계를, '빨랐다'와는 주체-상태 관계를 맺는다. 멀티-헤드 어텐션은 이러한 다양한 관계를 동시에 포착하기 위해 도입되었다.15

이 아이디어는 단일 어텐션을 여러 개의 '헤드(head)'로 나누어 병렬적으로 수행하는 것이다.15 각 헤드는 입력 벡터를 서로 다른 저차원 부분 공간(subspace)으로 투영하여 독립적으로 어텐션을 계산한다. 이를 통해 어떤 헤드는 구문적 관계에, 다른 헤드는 의미론적 관계에, 또 다른 헤드는 장거리 의존성에 집중하는 등 각기 다른 측면의 정보를 학습하도록 전문화될 수 있다.15

#### 2.2.2 작동 원리 및 공식

멀티-헤드 어텐션은 다음과 같은 과정으로 작동한다.15

1. **선형 투영:** $d_{\text{model}}$ 차원의 쿼리, 키, 값 벡터를 $h$개의 헤드로 나눈다. 각 헤드 $i$에 대해, 서로 다른 학습 가능한 가중치 행렬 $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$를 사용하여 입력 $Q, K, V$를 저차원 공간으로 투영한다. 여기서 일반적으로 $d_k = d_v = d_{\text{model}} / h$로 설정된다.

2. 병렬 어텐션: 각 헤드는 투영된 $Q_i, K_i, V_i$를 사용하여 스케일드 닷-프로덕트 어텐션을 독립적으로, 그리고 병렬적으로 계산한다.
   $$
   \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
   $$
   **결과 연결:** $h$개의 모든 헤드에서 나온 출력 벡터 $\text{head}_1, \dots, \text{head}_h$를 다시 하나로 연결(concatenate)한다.

3. **최종 투영:** 연결된 벡터에 또 다른 학습 가능한 가중치 행렬 $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$를 곱하여 최종적으로 $d_{\text{model}}$ 차원의 출력 벡터를 생성한다.

이 전체 과정은 다음 수식으로 표현된다.
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
$$

#### 2.2.3 효과

멀티-헤드 어텐션은 모델이 다양한 표현 부분 공간에서 정보를 종합적으로 고려할 수 있게 함으로써 표현력을 크게 향상시킨다. 이는 마치 여러 전문가가 각자의 관점에서 의견을 제시하고 이를 종합하여 최종 결정을 내리는 것과 같은 앙상블 효과를 낳는다.15 이 구조는 트랜스포머가 복잡하고 미묘한 언어적 관계를 학습하는 데 핵심적인 역할을 수행한다.

셀프 어텐션 메커니즘의 수학적 특성을 깊이 살펴보면, 이는 본질적으로 순열 등변성(permutation equivariance)을 가진다.25 즉, 입력 시퀀스에서 토큰들의 순서를 뒤섞어도 출력되는 벡터들의 집합은 동일하며, 단지 순서만 입력과 동일하게 바뀔 뿐이다. 

`softmax(QK^T)` 계산의 핵심인 내적 연산은 순서에 무관하기 때문이다. 예를 들어, "개는 사람을 문다"와 "사람은 개를 문다"라는 두 문장은 완전히 다른 의미를 가지지만, 셀프 어텐션은 이 두 문장에 대해 동일한 벡터 집합을 출력하게 된다. 이는 모델이 단어의 순서 정보를 내재적으로 이해할 수 없음을 의미하며, 언어 이해에 치명적인 약점이다. 따라서, 순서 정보를 외부에서 명시적으로 주입해주는 메커니즘이 구조적으로 반드시 필요하게 되는데, 이것이 바로 다음 장에서 다룰 위치 인코딩의 존재 이유다.

## 3.  순서 정보의 주입 - 위치 인코딩

앞서 분석한 바와 같이, 트랜스포머의 셀프 어텐션 메커니즘은 입력 시퀀스를 순서가 없는 집합(set)으로 처리하는 순열 등변성(permutation equivariance) 특성을 가진다. 이로 인해 모델은 "어떤 단어가 먼저 나왔는가" 또는 "두 단어가 얼마나 떨어져 있는가"와 같은 순서 정보를 자체적으로 파악할 수 없다.8 이 문제를 해결하기 위해, 트랜스포머는 위치 인코딩(Positional Encoding)이라는 명시적인 장치를 통해 각 토큰의 위치 정보를 주입한다.28

### 3.1 필요성

위치 인코딩의 목표는 각 토큰의 임베딩 벡터에 해당 토큰의 절대적 또는 상대적 위치에 대한 정보를 추가하는 것이다.27 이렇게 위치 정보가 추가된 벡터를 모델의 입력으로 사용함으로써, 트랜스포머는 순서가 중요한 언어의 구조적 특성을 학습할 수 있게 된다. 이상적인 위치 인코딩은 다음과 같은 조건을 만족해야 한다.30

1. **고유성:** 각 위치(time step)마다 고유한 인코딩 값을 출력해야 한다.
2. **일관성:** 문장의 길이에 상관없이 두 위치 사이의 거리가 일관되게 표현되어야 한다.
3. **일반화:** 훈련 시 보지 못했던 더 긴 시퀀스에 대해서도 일반화(외삽, extrapolation)할 수 있어야 한다.
4. **결정성 및 유계성:** 값은 결정적(deterministic)으로 생성되어야 하며, 특정 범위 내에 있어야 한다(bounded).

### 3.2 사인/코사인 함수 기반 인코딩

"Attention Is All You Need" 논문에서는 이러한 조건들을 만족시키는 우아한 해결책으로 주기 함수인 사인(sine)과 코사인(cosine) 함수를 활용한 위치 인코딩 방식을 제안했다.7 이 방식은 별도의 파라미터를 학습하지 않는 고정된(fixed) 방법이다.

#### 3.2.1 공식

시퀀스 내의 위치를 나타내는 인덱스 $pos$와, $d_{\text{model}}$ 차원의 임베딩 벡터 내의 차원 인덱스 $i$에 대해, 위치 인코딩 벡터 `PE`의 각 요소는 다음과 같이 정의된다.30
$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

여기서 $2i$는 짝수 차원 인덱스를, $2i+1$은 홀수 차원 인덱스를 의미한다. 즉, 각 위치 $pos$에 대한 $d_{\text{model}}$ 차원의 위치 인코딩 벡터는 서로 다른 주기를 가진 사인 및 코사인 함수의 값들로 구성된다. 분모의 $10000^{2i/d_{\text{model}}}$ 항은 파장(wavelength)을 결정하며, 차원 $i$가 증가함에 따라 파장은 $2\pi$에서 $10000 \cdot 2\pi$까지 기하급수적으로 길어진다.28

#### 3.2.2 특성 및 장점

이러한 삼각함수 기반 인코딩은 여러 중요한 장점을 가진다.

1. **고유한 위치 표현:** 서로 다른 주파수의 사인과 코사인 함수를 조합함으로써, 각 위치 $pos$는 고유한 벡터 표현을 갖게 된다.30

2. **긴 시퀀스로의 외삽:** 사인과 코사인 함수의 값은 항상 `[-1, 1]` 범위 내에 있으므로, 훈련 시 경험한 길이보다 더 긴 시퀀스가 입력되더라도 안정적인 인코딩 값을 생성할 수 있다.30

3. **상대 위치 학습 용이성:** 이 방식의 가장 강력한 특징은 상대적 위치 정보를 효과적으로 인코딩한다는 점이다. 임의의 고정된 오프셋 $k$에 대해, $PE_{pos+k}$는 $PE_{pos}$의 선형 변환으로 표현될 수 있다. 이는 삼각함수의 덧셈 정리 $\sin(A+B) = \sin(A)\cos(B) + \cos(A)\sin(B)$와 $\cos(A+B) = \cos(A)\cos(B) - \sin(A)\sin(B)$에 의해 수학적으로 증명된다.30
   $$
   \begin{bmatrix} \sin(\omega_i \cdot (t+\phi)) \\ \cos(\omega_i \cdot (t+\phi)) \end{bmatrix} = \begin{bmatrix} \cos(\omega_i \cdot \phi) & \sin(\omega_i \cdot \phi) \\ -\sin(\omega_i \cdot \phi) & \cos(\omega_i \cdot \phi) \end{bmatrix} \begin{bmatrix} \sin(\omega_i \cdot t) \\ \cos(\omega_i \cdot t) \end{bmatrix}
   $$
   위 식에서 $t=pos$, $\phi=k$로 볼 수 있다. 즉, 위치가 $k$만큼 이동하는 것은 각 주파수 쌍에 해당하는 2차원 부분 공간에서 특정 각도만큼 회전하는 선형 변환과 동일하다. 모델은 이 간단한 선형 관계를 쉽게 학습할 수 있으므로, "3칸 앞에 있는 단어"와 같은 상대적 위치 관계를 파악하는 데 매우 효율적이다.

이러한 기하학적 해석은 사인/코사인 위치 인코딩이 단순한 인덱싱을 넘어, 시퀀스의 구조적 관계를 내재적으로 표현하는 정교한 방법임을 보여준다. 각 위치 $pos$는 $d_{\text{model}}/2$개의 서로 다른 속도로 회전하는 2차원 벡터들의 집합으로 표현되며, 이들의 조합을 통해 모델은 복잡한 순서 정보를 학습하게 된다. 이 위치 인코딩 벡터는 단어 임베딩 벡터에 더해져, 최종적으로 의미 정보와 위치 정보를 모두 담은 입력 표현을 완성한다.28

## 4.  아키텍처의 보조 구성 요소

트랜스포머 아키텍처는 셀프 어텐션과 위치 인코딩이라는 핵심 메커니즘 외에도, 모델의 표현력을 증대시키고 깊은 네트워크의 학습을 안정시키는 몇 가지 중요한 보조 구성 요소들을 포함한다. 이들은 바로 위치별 피드포워드 신경망, 그리고 잔차 연결과 계층 정규화다.

### 4.1  위치별 피드포워드 신경망 (Position-wise Feed-Forward Networks, FFN)

#### 4.1.1 구조와 역할

인코더와 디코더의 각 레이어는 멀티-헤드 어텐션 하위 레이어를 통과한 후, 동일한 구조의 위치별 피드포워드 신경망(FFN)을 거친다.13 FFN은 두 개의 선형 변환(fully connected layer)과 그 사이에 위치한 비선형 활성화 함수로 구성된다.11

이 네트워크의 중요한 특징은 '위치별(position-wise)'이라는 이름에서 알 수 있듯이, 시퀀스 내의 각 토큰 위치에 독립적으로, 그리고 동일한 파라미터(가중치 행렬)를 사용하여 적용된다는 점이다.36 즉, 시퀀스의 모든 토큰들은 같은 FFN을 통과하지만, 서로의 계산에 영향을 주지 않는다.

어텐션 하위 레이어가 시퀀스 전체에 걸쳐 토큰 간의 정보를 혼합하고 재분배하는 역할을 담당한다면, FFN은 각 위치에서 어텐션을 통해 얻어진 문맥적 표현을 비선형 공간으로 변환하여 모델의 표현력을 한층 더 높이는 역할을 수행한다. 어텐션 연산 자체는 값(Value) 벡터에 대한 선형 결합이므로, FFN의 비선형 변환은 각 트랜스포머 블록이 더 복잡한 함수를 학습할 수 있도록 하는 필수적인 요소다.

#### 4.1.2 수학적 표현

어텐션 하위 레이어의 출력 벡터 $x$에 대한 FFN의 연산은 다음과 같이 표현된다.20
$$
\text{FFN}(x) = \text{Linear}_2(\text{Activation}(\text{Linear}_1(x)))
$$
이를 가중치 행렬 $W$와 편향 벡터 $b$를 사용하여 더 구체적으로 나타내면 다음과 같다.
$$
\text{FFN}(x) = (xW_1 + b_1)W_2 + b_2
$$
원본 "Attention Is All You Need" 논문에서는 활성화 함수로 ReLU(Rectified Linear Unit)를 사용했다.11
$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$
일반적으로 첫 번째 선형 변환($W_1, b_1$)은 입력 차원 $d_{\text{model}}$을 더 큰 중간 차원 $d_{ff}$으로 확장하고 (보통 $d_{ff} = 4 \times d_{\text{model}}$), 두 번째 선형 변환($W_2, b_2$)은 다시 원래의 $d_{\text{model}}$ 차원으로 축소하는 역할을 한다.20

#### 4.1.3 활성화 함수의 진화

초기 트랜스포머는 ReLU를 사용했지만, 이후 연구를 통해 더 정교한 활성화 함수들이 제안되었다. BERT와 GPT-1에서는 GELU(Gaussian Error Linear Unit)가 사용되었고 16, 최근의 Llama와 같은 대규모 언어 모델(LLM)에서는 SwiGLU(Swish Gated Linear Unit)와 같은 게이트(gating) 메커니즘을 포함한 활성화 함수가 더 나은 성능을 보이는 것으로 알려져 널리 채택되고 있다.40 이러한 게이트 기반 활성화 함수는 입력에 따라 동적으로 정보 흐름을 조절하여 모델의 학습 능력을 향상시킨다.

### 4.2  잔차 연결과 계층 정규화 (Residual Connections and Layer Normalization)

#### 4.2.1 목적

트랜스포머는 수십, 때로는 수백 개의 레이어를 깊게 쌓아 매우 강력한 표현력을 확보한다. 그러나 신경망이 깊어질수록 학습이 어려워지는 문제가 발생하는데, 이를 해결하기 위해 잔차 연결(Residual Connection)과 계층 정규화(Layer Normalization)라는 두 가지 핵심적인 기법이 모든 하위 레이어에 적용된다.10

#### 4.2.2 잔차 연결 (Add)

잔차 연결은 ResNet에서 처음 제안된 기법으로, 각 하위 레이어(어텐션 또는 FFN)의 입력 $x$를 해당 하위 레이어의 출력 $Sublayer(x)$에 그대로 더해주는 방식이다.20
$$
\text{Output} = x + \text{Sublayer}(x)
$$
이 구조는 네트워크 내에 정보가 흐를 수 있는 '지름길(shortcut)'을 만들어준다. 이를 통해 역전파 과정에서 기울기가 하위 레이어를 건너뛰고 직접 상위 레이어로 전달될 수 있어, 깊은 네트워크에서 흔히 발생하는 기울기 소실 문제를 효과적으로 완화한다.10 결과적으로 모델은 전체 변환을 처음부터 학습하는 대신, 입력과의 차이, 즉 '잔차(residual)'만을 학습하면 되므로 훈련이 훨씬 쉬워지고 안정화된다.

#### 4.2.3 계층 정규화 (Norm)

계층 정규화는 각 훈련 샘플에 대해 독립적으로, 해당 샘플의 모든 특성(feature)에 걸쳐 평균을 0, 분산을 1로 정규화하는 기법이다.45 이는 미니배치 내의 샘플들 간에 정규화를 수행하는 배치 정규화(Batch Normalization)와 대조된다. 배치 정규화는 배치 크기에 민감하고 순환적인 모델에 적용하기 어렵기 때문에, 가변 길이 시퀀스를 다루는 NLP 태스크에서는 계층 정규화가 더 선호된다.45

계층 정규화는 각 레이어의 입력 분포를 안정시켜 학습 과정을 원활하게 하고, 모델의 수렴 속도를 높이는 역할을 한다.10

#### 4.2.4 Pre-LN vs. Post-LN

잔차 연결과 계층 정규화를 결합하는 순서에 따라 두 가지 방식이 존재한다.

- **Post-LN (원본 방식):** 원본 트랜스포머 논문에서 제안된 방식으로, 하위 레이어의 연산과 잔차 연결을 먼저 수행한 후 계층 정규화를 적용한다 ($\text{LayerNorm}(x + \text{Sublayer}(x))$).11 이 방식은 학습 초기에 기울기가 불안정해지는 경향이 있어, 학습률을 서서히 증가시키는 '학습률 예열(learning rate warm-up)'과 같은 세심한 하이퍼파라미터 튜닝이 필수적이었다.44
- **Pre-LN (개선된 방식):** 이후 연구에서 제안된 방식으로, 계층 정규화를 먼저 적용한 후 하위 레이어 연산과 잔차 연결을 수행한다 ($x + \text{Sublayer}(\text{LayerNorm}(x))$).11 이 방식은 각 하위 레이어의 입력이 항상 정규화된 상태를 유지하도록 하여 학습 과정을 훨씬 안정시킨다. 그 결과, 학습률 예열 없이도 빠르고 안정적인 수렴이 가능해져 현대의 깊은 트랜스포머 모델에서 표준적인 방식으로 자리 잡았다.16 이처럼 사소해 보이는 구조적 변경이 대규모 모델의 안정적인 학습을 가능하게 한 핵심적인 개선점 중 하나였다.

## 5.  트랜스포머의 주요 변형 모델

원본 트랜스포머 아키텍처는 인코더와 디코더를 모두 사용하는 시퀀스-투-시퀀스(sequence-to-sequence) 작업, 특히 기계 번역에 최적화되어 있었다. 그러나 이후 연구자들은 특정 과제의 요구에 맞춰 아키텍처의 일부만을 활용하거나 변형하는 것이 더 효과적이라는 사실을 발견했다. 이 과정에서 인코더-온리, 디코더-온리, 그리고 기존의 인코더-디코더라는 세 가지 주요 패러다임이 확립되었다.

### 5.1  인코더-온리 (Encoder-Only): BERT (Bidirectional Encoder Representations from Transformers)

BERT는 "Bidirectional Encoder Representations from Transformers"의 약자로, 이름에서 알 수 있듯이 트랜스포머의 인코더 스택만을 활용하는 모델이다.47

- **구조:** BERT의 핵심은 마스킹되지 않은 양방향 셀프 어텐션(bidirectional self-attention)이다. 각 토큰은 어텐션 계산 시 자신의 왼쪽과 오른쪽에 있는 모든 토큰을 제한 없이 참조할 수 있다.49 이를 통해 문장 전체의 문맥을 깊이 있게 양방향으로 이해하는 능력을 갖추게 된다.
- **사전학습 목표:** BERT는 양방향 문맥을 효과적으로 학습하기 위해 두 가지 독창적인 비지도 사전학습 목표를 사용한다.
  1. **Masked Language Model (MLM):** 입력 시퀀스에서 일부 토큰(통상 15%)을 무작위로 ``라는 특수 토큰으로 치환한 후, 모델이 주변 문맥만을 이용하여 원래 토큰이 무엇이었는지를 예측하도록 훈련한다.12 이는 마치 빈칸 채우기 문제를 푸는 것과 같으며, 모델이 단어의 양방향 문맥적 의미를 깊이 학습하도록 강제한다.
  2. **Next Sentence Prediction (NSP):** 두 개의 문장을 입력으로 받아, 두 번째 문장이 첫 번째 문장 바로 다음에 실제로 이어지는 문장인지, 아니면 단순히 무작위로 추출된 문장인지를 예측하는 이진 분류 과제다.50 이를 통해 문장 간의 논리적 관계를 이해하는 능력을 함양한다.
- **용도:** BERT는 문맥을 깊이 이해하는 능력이 뛰어나기 때문에, 문장 분류(감성 분석 등), 개체명 인식(NER), 질의응답(QA)과 같은 자연어 이해(Natural Language Understanding, NLU) 과제에서 압도적인 성능을 보였다.48 BERT의 등장은 사전학습된 거대 모델을 특정 다운스트림 태스크에 미세 조정(fine-tuning)하는 패러다임을 정착시키는 계기가 되었다.

### 5.2  디코더-온리 (Decoder-Only): GPT (Generative Pre-trained Transformer)

GPT는 "Generative Pre-trained Transformer"의 약자로, 트랜스포머의 디코더 스택만을 기반으로 하는 모델 계열이다.8

- **구조:** 디코더-온리 아키텍처의 가장 중요한 특징은 '인과적 마스킹(causal masking)' 또는 '룩어헤드 마스킹(look-ahead masking)'을 사용한다는 점이다.11 셀프 어텐션 계산 시, 각 토큰은 오직 자신과 자신보다 앞에 위치한 토큰들만 참조할 수 있다. 미래의 토큰 정보는 마스킹되어 참조가 불가능하다.55 이는 정보의 흐름을 단방향(unidirectional)으로 제한한다.
- **사전학습 목표:** 이 구조는 자기회귀 언어 모델링(Autoregressive Language Modeling)에 최적화되어 있다. 즉, 이전까지의 토큰 시퀀스가 주어졌을 때, 바로 다음 토큰이 무엇일지를 예측하는 방식으로 학습이 진행된다.48 이는 인간이 문장을 생성하는 방식과 유사하다.
- **용도:** GPT 계열 모델은 텍스트 생성(text generation)에 매우 강력한 성능을 보인다. 대화형 AI(챗봇), 문장 요약, 창의적인 글쓰기, 코드 생성 등 다양한 자연어 생성(Natural Language Generation, NLG) 과제에 특화되어 있다.57 특히 GPT-3 이후 대규모 언어 모델(LLM)의 폭발적인 성장을 이끌며, 현재 AI 분야의 주류 아키텍처로 자리매김했다.

### 5.3  인코더-디코더 (Encoder-Decoder): T5 (Text-to-Text Transfer Transformer)

T5는 원본 트랜스포머의 인코더-디코더 구조를 계승하면서, 모든 NLP 과제를 단일한 프레임워크로 해결하려는 혁신적인 접근법을 제시했다.58

- **구조:** T5는 인코더와 디코더를 모두 사용한다. 인코더는 입력 텍스트를 이해하고, 디코더는 그 이해를 바탕으로 출력 텍스트를 생성한다.
- **통일된 프레임워크:** T5의 핵심 아이디어는 모든 NLP 과제를 "텍스트-투-텍스트(Text-to-Text)" 문제로 재정의하는 것이다.59 예를 들어,
  - **기계 번역:** 입력 텍스트에 "translate English to German: That is good."과 같이 과제를 명시하는 접두사(prefix)를 붙이면, 모델은 "Das ist gut."이라는 텍스트를 출력하도록 학습된다.
  - **감성 분석:** "sentiment: This movie is great!"이라는 입력에 대해 "positive"라는 텍스트를 출력하도록 한다.
  - **요약:** "summarize: [긴 기사 내용]"이라는 입력에 대해 요약된 텍스트를 출력하도록 한다.59
- **용도:** 이 접근법을 통해 단 하나의 모델과 동일한 학습 목표(손실 함수), 동일한 디코딩 과정을 사용하여 매우 다양한 종류의 과제를 수행할 수 있다. T5는 전이 학습(transfer learning)의 잠재력을 극대화하고, 다양한 NLP 과제에 대한 모델의 일반화 성능을 체계적으로 연구하는 데 중요한 이정표가 되었다.60

이 세 가지 패러다임의 등장은 트랜스포머 아키텍처의 유연성과 확장성을 보여준다. BERT의 성공으로 NLU 분야에서 사전학습-미세조정 패러다임이 확고히 자리 잡았으나, LLM 시대의 개막은 GPT와 같은 디코더-온리 모델의 생성 능력에 의해 주도되었다. 이는 학계와 산업계의 관심이 언어의 '이해'에서 '생성'으로 이동했음을 시사한다. 디코더-온리 아키텍처는 다음 토큰 예측이라는 학습 목표가 실제 추론(생성) 작업과 완벽하게 일치하여 학습 신호가 매우 직접적이고 효율적이다.63 또한, 인코더-디코더 구조에 비해 구조가 단순하고, 이전 토큰들의 키-값(KV) 쌍을 캐싱하여 재사용할 수 있어 자기회귀적 생성 과정에서 계산 효율성이 높다는 장점이 있다.65 이러한 확장성과 효율성이 결합되어, 디코더-온리 모델은 수천억 개 이상의 파라미터를 가진 초거대 모델로 스케일업하는 데 가장 적합한 아키텍처로 부상하며 현재 LLM 시장을 지배하게 되었다.

| 특성 (Feature)         | 인코더-온리 (BERT)                              | 디코더-온리 (GPT)       | 인코더-디코더 (T5)                 |
| ---------------------- | ----------------------------------------------- | ----------------------- | ---------------------------------- |
| **주요 구성요소**      | Transformer Encoder                             | Transformer Decoder     | Transformer Encoder & Decoder      |
| **어텐션 마스킹**      | 양방향 (마스킹 없음)                            | 인과적 (Causal Masking) | 인코더: 양방향, 디코더: 인과적     |
| **정보 흐름**          | $P(x_i \rvert x_{<i}, x_{>i})$                  | $P(x_i \rvert x_{<i})$  | $P(y_i \rvert x, y_{<i})$          |
| **주요 사전학습 목표** | Masked LM (MLM), Next Sentence Prediction (NSP) | Autoregressive LM       | Denoising Objective (Text-to-Text) |
| **강점**               | 문맥 이해 (NLU)                                 | 텍스트 생성 (NLG)       | 유연한 시퀀스 변환                 |
| **대표 모델**          | BERT, RoBERTa                                   | GPT series, Llama, PaLM | T5, BART                           |

## 6.  비평적 분석 및 아키텍처의 한계

트랜스포머는 혁신적인 성능으로 AI 분야를 재편했지만, 그 아키텍처에는 몇 가지 근본적인 한계와 비판점이 존재한다. 이러한 한계를 이해하는 것은 현재 모델의 제약을 파악하고 차세대 아키텍처의 발전 방향을 모색하는 데 매우 중요하다.

| 계층 유형 (Layer Type)  | 계층 당 복잡도 (Complexity per Layer) | 순차 연산 수 (Sequential Operations) | 최대 경로 길이 (Maximum Path Length) |
| ----------------------- | ------------------------------------- | ------------------------------------ | ------------------------------------ |
| **Self-Attention**      | $O(n^2 \cdot d)$                      | $O(1)$                               | $O(1)$                               |
| **Recurrent (RNN)**     | $O(n \cdot d^2)$                      | $O(n)$                               | $O(n)$                               |
| **Convolutional (CNN)** | $O(k \cdot n \cdot d^2)$              | $O(1)$                               | $O(n/k)$                             |

*n: 시퀀스 길이, d: 표현 차원, k: 커널 크기*

### 6.1 계산 및 메모리 복잡도

트랜스포머의 가장 잘 알려진, 그리고 가장 심각한 약점은 셀프 어텐션 메커니즘의 계산 복잡도다. 위 표에서 볼 수 있듯이, 셀프 어텐션은 시퀀스 길이 $n$에 대해 계산량과 메모리 요구량이 제곱에 비례하여($O(n^2)$) 증가한다.4 이는 시퀀스 내 모든 토큰 쌍 간의 상호작용을 계산해야 하기 때문이다. 시퀀스 길이가 두 배로 늘어나면 필요한 계산량과 메모리는 네 배로 증가한다.

이러한 제곱 복잡도는 트랜스포머의 적용 범위를 심각하게 제한한다. 수천 토큰을 넘는 긴 문서의 요약, 고해상도 이미지 처리(픽셀 수가 많아져 시퀀스 길이가 급증), 유전체 서열 분석과 같이 장거리 시퀀스를 다루어야 하는 태스크에서는 계산 비용이 기하급수적으로 증가하여 현실적으로 적용하기 어렵게 만든다.66 이 문제는 트랜스포머의 컨텍스트 창(context window) 길이를 늘리는 데 가장 큰 걸림돌로 작용하며, 이를 해결하기 위한 수많은 '효율적인 어텐션' 연구를 촉발시켰다.

### 6.2 귀납적 편향의 부재 (Lack of Inductive Bias)

귀납적 편향은 모델이 보지 못한 데이터에 대해 합리적인 가정을 하도록 유도하는 내재된 구조적 특성이다. 예를 들어, CNN은 이미지의 일부 패턴이 다른 위치에서도 동일하게 나타날 것이라는 '지역성(locality)'과 '평행이동 불변성(translation invariance)'이라는 강력한 귀납적 편향을 가지고 있다. RNN은 데이터가 순차적으로 처리되어야 한다는 '순차성(sequentiality)' 편향을 가진다.

반면, 트랜스포머의 셀프 어텐션은 입력을 순서가 없는 토큰들의 집합으로 간주하므로 이러한 구조적 편향이 매우 약하다.69 모든 정보는 데이터로부터 학습되어야 한다. 이는 모델의 유연성을 높여 다양한 데이터에 적용될 수 있다는 장점이 있지만, 동시에 몇 가지 단점을 야기한다. 첫째, 강력한 편향이 없기 때문에 데이터에서 패턴을 학습하기 위해 훨씬 더 많은 양의 데이터가 필요하다.68 둘째, 특정 종류의 구조적 규칙을 학습하는 데 비효율적일 수 있다. 예를 들어, 입력 시퀀스에서 특정 토큰의 개수가 짝수인지 홀수인지 판단하는 간단한 PARITY 문제와 같은 형식 언어를 학습하는 데 어려움을 겪는 것으로 알려져 있다.69

### 6.3 어텐션 가중치의 해석 가능성에 대한 비판

트랜스포머가 처음 등장했을 때, 어텐션 가중치 맵은 모델이 예측을 내릴 때 입력의 어떤 부분에 '집중'하는지를 보여주는 직관적인 해석 도구로 기대를 모았다.55 예를 들어, 번역 모델에서 특정 단어에 대한 높은 어텐션 가중치는 해당 단어가 번역의 중요한 근거가 되었음을 시사하는 것으로 보였다.

그러나 후속 연구들은 이러한 해석에 대해 신중해야 함을 경고했다. 여러 연구에서 어텐션 가중치와 모델의 최종 예측 사이에는 강한 인과 관계가 없을 수 있음이 밝혀졌다.72 즉, 높은 어텐션 가중치를 받은 토큰이 반드시 예측에 결정적인 영향을 미치는 것은 아니라는 것이다. 또한, 멀티-헤드 어텐션 구조에서는 수십, 수백 개의 어텐션 맵이 생성되는데, 이를 종합적으로 해석하는 것은 거의 불가능에 가깝다.72 따라서 어텐션 맵을 모델의 '추론 과정'에 대한 직접적인 설명으로 간주하는 것은 오해의 소지가 있으며, 보조적인 해석 도구로 제한적으로 사용해야 한다는 것이 현재의 중론이다.

### 6.4 기타 구조적 한계

- **합성성(Compositionality) 문제:** 트랜스포머는 여러 단계의 논리적 추론을 요구하는 과제에 약점을 보일 수 있다. 예를 들어, "앨리스의 어머니는 베티이고, 베티의 남편은 찰리다. 앨리스의 아버지는 누구인가?"와 같은 질문에 답하려면, '어머니' 관계와 '남편' 관계를 합성하여 '아버지' 관계를 추론해야 한다. 연구에 따르면, 단일 트랜스포머 레이어는 이러한 함수 합성을 수행하는 데 이론적인 한계를 가지며, 문제의 복잡도가 커질수록 성능이 급격히 저하될 수 있다.73
- **장기적 제약 조건 생성의 어려움:** GPT와 같은 자기회귀 모델은 한 번에 한 토큰씩, 국소적으로 가장 확률이 높은 다음 토큰을 선택하는 탐욕적(greedy) 방식으로 텍스트를 생성한다. 이 방식은 긴 글을 생성할 때 초반에 설정한 주제나 인물의 성격, 또는 특정 결말과 같은 장기적인 제약 조건을 일관되게 유지하는 데 어려움을 겪는다.70 생성 과정에서 실수를 하더라도 되돌아가서 수정할 수 없기 때문에, 이야기가 산만해지거나 논리적 모순이 발생할 수 있다.

## 7.  트랜스포머의 진화와 미래

트랜스포머 아키텍처의 근본적인 한계, 특히 제곱 복잡도 문제는 이를 극복하기 위한 수많은 후속 연구를 촉발시켰다. 이러한 노력은 기존 어텐션 메커니즘을 더 효율적으로 만드는 방향, 아예 새로운 아키텍처를 모색하는 방향, 그리고 모델을 더 효과적으로 확장하는 방법론을 연구하는 방향으로 전개되었다.

### 7.1  효율적인 어텐션 메커니즘 (Efficient Attention Mechanisms)

제곱 복잡도 문제를 해결하기 위한 연구는 크게 네 가지 흐름으로 분류할 수 있다.

- **희소 어텐션 (Sparse Attention):** 모든 토큰 쌍 간의 상호작용을 계산하는 대신, 일부 중요한 쌍에 대해서만 어텐션을 계산하는 접근법이다. **Longformer**는 이 아이디어를 구체화한 대표적인 모델로, 각 토큰이 자신 주변의 고정된 크기의 '슬라이딩 윈도우(sliding window)' 내의 토큰들과, 그리고 `` 토큰과 같이 미리 지정된 몇몇 '전역(global)' 토큰들과 어텐션을 계산한다.75 이를 통해 계산 복잡도를 

  $O(n \sqrt{n})$ 또는 $O(n \log n)$ 수준으로 낮추면서도, 지역적 문맥과 전역적 문맥을 모두 포착할 수 있다.66

- **저계급 근사 (Low-Rank Approximation):** 어텐션 행렬의 정보 대부분이 소수의 특이값에 집중되어 있어, 본질적으로 저계급(low-rank) 행렬로 근사할 수 있다는 관찰에서 출발한다. **Linformer**는 $n \times n$ 크기의 어텐션 행렬을 직접 계산하는 대신, 입력 시퀀스를 더 낮은 차원 $k (k \ll n)$로 투영한 후 어텐션을 계산한다.78 이는 결과적으로 

  $n \times k$와 $k \times n$ 행렬의 곱으로 어텐션 행렬을 근사하는 것과 같으며, 계산 복잡도를 시퀀스 길이에 선형적인 $O(n)$으로 줄인다.80

- **커널화 및 선형 어텐션 (Kernelization and Linear Attention):** 어텐션 수식의 소프트맥스 함수를 커널 함수(kernel function)로 근사하여 계산 순서를 변경하는 기법이다. 기존 어텐션이 $(QK^T)V$ 순서로 계산되어 $n \times n$ 행렬이 생성되는 반면, 선형 어텐션은 행렬 곱의 결합 법칙을 이용하여 $Q(K^T V)$ 순서로 계산한다.81

  $K^T V$는 $d \times d$ 크기의 행렬이므로, $n \times n$ 행렬을 생성하지 않고도 동일한 결과를 얻을 수 있어 복잡도가 $O(n)$으로 감소한다. 이 방식은 RNN 형태로도 표현될 수 있어, 추론 시 상태를 순차적으로 업데이트하는 데 효율적이다.82

- **하드웨어 인식 최적화 (Hardware-Aware Optimization):** 알고리즘의 이론적 복잡도가 아닌, 실제 하드웨어에서의 실행 속도(wall-clock time)에 초점을 맞춘 접근법이다. **FlashAttention**은 GPU 메모리 계층 구조의 특성을 활용한다. GPU에는 상대적으로 느리지만 용량이 큰 HBM(High-Bandwidth Memory)과, 매우 빠르지만 용량이 작은 SRAM(Static RAM)이 있다. 표준 어텐션은 거대한 $n \times n$ 어텐션 행렬을 HBM에 쓰고 다시 읽어오는 과정에서 I/O 병목이 발생한다.84 FlashAttention은 '타일링(tiling)' 기법을 사용하여 입력 행렬을 작은 블록으로 나누어 SRAM에 올리고, SRAM 내에서 모든 계산을 마친 후 최종 결과만 HBM에 쓰는 방식으로 I/O를 최소화한다. 또한, 역전파에 필요한 중간값인 어텐션 행렬을 저장하는 대신, 필요할 때마다 '재계산(recomputation)'하여 메모리 사용량을 획기적으로 줄인다.86 이를 통해 정확한 어텐션 계산을 유지하면서도 기존 방식보다 훨씬 빠른 속도를 달성했다.

### 7.2  아키텍처의 대안과 확장

- **상태 공간 모델 (State Space Models, SSMs):** 트랜스포머의 대안으로 급부상한 아키텍처다. SSM은 고전 제어 이론에서 영감을 받아, 연속적인 동적 시스템을 이산적인 상태 업데이트로 모델링한다. **Mamba**는 SSM을 기반으로 하되, 입력에 따라 동적으로 파라미터를 조절하는 '선택적(selective)' 메커니즘과 GPU에 최적화된 병렬 스캔 알고리즘을 도입했다.88 그 결과 시퀀스 길이에 대해 선형적인 복잡도(

  $O(n)$)와 고정된 크기의 은닉 상태를 가지면서도, 장기 의존성을 효과적으로 포착하는 데 성공했다.90 Mamba는 긴 시퀀스 처리에서 트랜스포머보다 월등한 효율성을 보이지만, 문맥에서 특정 정보를 정확히 복사하거나 검색하는 능력은 트랜스포머에 비해 떨어진다는 비판도 제기된다.91

- **하이브리드 아키텍처 (Hybrid Architectures):** 서로 다른 아키텍처의 장점을 결합하려는 시도도 활발하다. **Jamba**는 트랜스포머의 어텐션 블록과 Mamba의 SSM 블록을 교차로 쌓은 하이브리드 구조를 제안했다.93 이를 통해 트랜스포머의 강력한 인-컨텍스트 학습 능력과 Mamba의 긴 컨텍스트 처리 효율성을 동시에 확보하고자 한다.95

- **전문가 혼합 (Mixture-of-Experts, MoE):** 단일 아키텍처의 성능을 극대화하기 위한 스케일링 기법이다. MoE는 모델의 피드포워드 네트워크(FFN) 부분을 여러 개의 독립적인 '전문가(expert)' 네트워크로 대체한다. 그리고 각 토큰이 입력될 때마다, 가벼운 '라우팅(routing)' 네트워크가 해당 토큰을 처리할 소수의 전문가(예: 상위 2개)를 동적으로 선택한다.98 이 방식을 통해 모델의 전체 파라미터 수는 수조 개까지 늘리면서도, 각 토큰을 처리하는 데 사용되는 활성 파라미터 수와 계산 비용은 일정하게 유지할 수 있다.100

  **Mixtral**과 같은 모델은 MoE를 성공적으로 적용하여, 상대적으로 적은 계산량으로 거대 밀집(dense) 모델에 필적하는 성능을 달성했다.102

### 7.3  스케일링 법칙과 하드웨어 로터리

- **스케일링 법칙 (Scaling Laws):** 대규모 언어 모델의 성능은 모델의 크기(파라미터 수, N), 훈련 데이터셋의 크기(토큰 수, D), 그리고 사용된 총 계산량(C)이라는 세 가지 요소에 따라 예측 가능한 멱법칙(power-law)을 따른다는 경험적 발견이다.104 초기에 OpenAI 연구진은 모델 크기(N)를 늘리는 것이 가장 중요하다고 주장했으나 107, 2022년 DeepMind는 

  **Chinchilla**라는 모델을 통해 새로운 스케일링 법칙을 제시했다.108 Chinchilla 연구는 주어진 계산 예산 내에서 최적의 성능을 얻기 위해서는 모델 크기와 데이터셋 크기를 비례하여 함께 확장해야 함을 보였다.110 이는 기존의 거대 모델들이 모델 크기에 비해 데이터가 부족하여 '과소 훈련(undertrained)' 상태였음을 시사하며, 이후 LLM 훈련 전략의 표준을 바꾸어 놓았다.112

- **하드웨어 로터리 (The Hardware Lottery):** 특정 연구 아이디어나 아키텍처가 그 자체의 본질적인 우수성 때문이 아니라, 단지 당대의 지배적인 하드웨어(예: GPU)에 더 잘 부합하기 때문에 성공하게 되는 현상을 일컫는 용어다.114 트랜스포머의 성공은 이러한 하드웨어 로터리의 대표적인 사례로 분석된다. 트랜스포머의 핵심 연산인 밀집 행렬 곱셈(dense matrix multiplication)은 GPU의 대규모 병렬 처리 아키텍처에 완벽하게 부합한다.116 이론적으로 트랜스포머는 모든 노드가 연결된 완전 그래프(fully connected graph)에 적용되는 그래프 신경망(Graph Neural Network, GNN)의 특수한 형태로 볼 수 있지만, GNN의 일반적인 희소(sparse) 메시지 패싱 방식은 GPU에서 비효율적이다. 반면, 트랜스포머는 GNN과 수학적으로 유사한 연산을 하드웨어에 최적화된 방식으로 구현함으로써 '하드웨어 로터리'에서 승리했다고 볼 수 있다.117

이러한 진화의 과정은 '만능' 아키텍처는 없다는 것을 보여준다. 효율적인 어텐션, Mamba, 하이브리드 모델 등은 성능, 계산 효율, 메모리 사용량, 특정 과제에 대한 적합성 등 다양한 축으로 구성된 복잡한 트레이드오프 공간을 탐색하는 과정이다. 미래의 아키텍처는 단일한 형태가 아니라, 특정 목적과 하드웨어 환경에 최적화된 다양한 형태의 모델들이 공존하는 생태계가 될 가능성이 높다.

## 8. 결론: 트랜스포머가 남긴 유산과 향후 전망

트랜스포머 아키텍처는 2017년 등장 이후 지난 몇 년간 인공지능 분야에 지대한 영향을 미쳤으며, 그 유산은 기술적 성취를 넘어 연구와 산업의 패러다임 자체를 바꾸어 놓았다.

### 8.1 패러다임의 확립

트랜스포머는 자연어 처리를 넘어 인공지능 전반의 사실상 표준(de facto) 아키텍처로 자리매김했다. 그 영향력은 다양한 분야로 빠르게 확산되었다.

- **컴퓨터 비전:** **Vision Transformer (ViT)**는 이미지를 고정된 크기의 패치(patch) 시퀀스로 간주하고, 이를 트랜스포머 인코더에 직접 입력하는 혁신적인 접근법을 제시했다.12 이는 오랫동안 컴퓨터 비전 분야를 지배해 온 합성곱 신경망(CNN)의 아성에 도전하며, 대규모 데이터셋에서 더 뛰어난 성능을 보일 수 있음을 입증했다.120 ViT의 성공 이후, 객체 탐지, 이미지 분할, 의료 영상 분석 등 거의 모든 비전 태스크에 트랜스포머 기반 모델이 활발히 적용되고 있다.123
- **음성, 멀티모달 등:** 음성 인식 및 합성, 시계열 데이터 분석, 강화학습 등 순차적 데이터가 존재하는 모든 영역에서 트랜스포머는 강력한 성능을 입증하며 그 적용 범위를 넓혀가고 있다.

### 8.2 과학적 발견의 가속화

트랜스포머의 능력은 공학적 응용을 넘어 기초 과학 연구의 도구로서도 그 가치를 증명하고 있다. 복잡한 데이터 내에서 장거리 패턴과 고차원적 관계를 포착하는 능력은 과학적 발견의 속도를 가속화하고 있다.

- **생물학 및 신약 개발:** 단백질 아미노산 서열, 분자 구조(SMILES), 유전체 데이터 등은 모두 언어와 유사한 순차적 특성을 가진다. 트랜스포머는 이러한 복잡한 생물학적 시퀀스를 모델링하여 단백질 기능 예측, 신약 후보 물질 발굴, 약물-표적 상호작용 예측 등에서 획기적인 성과를 내고 있다.126 특히, 딥마인드의 

  **AlphaFold2**는 트랜스포머 아키텍처를 기반으로 단백질 구조를 매우 높은 정확도로 예측하는 데 성공했으며, 이는 구조생물학 분야의 수십 년 된 난제를 해결한 것으로 평가받는다.130 이러한 발전은 신약 개발 파이프라인의 초기 단계를 극적으로 단축시킬 잠재력을 가지고 있다.133

- **재료 과학 및 화학:** 트랜스포머는 원자 배열과 화학 결합의 규칙을 학습하여 안정적인 신소재 구조를 생성하거나, 주어진 반응물로부터 생성물을 예측하는 등 재료 과학 및 화학 분야에서도 새로운 가능성을 열고 있다.135

### 8.3 향후 전망

트랜스포머는 여전히 진화하고 있으며, 그 한계를 극복하려는 노력은 AI 아키텍처 연구의 최전선을 형성하고 있다.

- **효율성과의 싸움:** 제곱 복잡도 문제는 여전히 트랜스포머의 근본적인 아킬레스건이다. FlashAttention과 같은 하드웨어 최적화는 실용적인 해결책을 제시했지만, 근본적인 복잡도를 낮추려는 희소 어텐션, 선형 어텐션 등의 연구는 계속될 것이다.
- **새로운 아키텍처와의 경쟁:** Mamba와 같은 상태 공간 모델은 트랜스포머의 대안으로서 그 가능성을 입증했다. 이 두 아키텍처의 장점을 결합하려는 Jamba와 같은 하이브리드 모델의 등장은, 미래가 단일 아키텍처의 독주가 아닌, 특정 과제와 하드웨어에 최적화된 다양한 모델들의 공존이 될 것임을 시사한다.
- **스케일링의 미래:** MoE와 같은 기법은 모델의 파라미터 수를 확장하는 새로운 길을 열었지만, 이 역시 라우팅의 효율성, 전문가 간의 부하 분산 등 새로운 과제를 안고 있다. Chinchilla가 제시한 스케일링 법칙을 넘어, 데이터의 질과 다양성이 모델 성능에 미치는 영향에 대한 더 깊은 이해가 요구될 것이다.

결론적으로, 트랜스포머는 단순히 하나의 성공적인 모델을 넘어, 병렬 처리, 어텐션 메커니즘, 대규모 사전학습이라는 핵심 원칙을 AI 커뮤니티에 각인시켰다. 그 한계를 극복하려는 과정에서 파생된 수많은 아이디어들은 앞으로도 오랫동안 인공지능 아키텍처의 발전을 이끌어갈 원동력이 될 것이다. 트랜스포머의 진정한 유산은 그 자체의 성능이 아니라, 그것이 촉발한 끝없는 혁신과 탐구의 과정에 있다.

#### **참고 자료**

1. Attention is All You Need 리뷰, 8월 15, 2025에 액세스, https://rauleun.github.io/attention-is-all-you-need
2. 장단기 메모리(Long Short-Term Memory, LSTM) - tgwon - 티스토리, 8월 15, 2025에 액세스, https://tgwon.tistory.com/49
3. RNN의 한계를 해결한 순환구조 네트워크, LSTM과 GRU, 8월 15, 2025에 액세스, https://aiheroes.ai/community/77
4. Attention Is All You Need - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/1706.03762v7
5. LSTM (Long-Short Term Memory)이란? - 데이터 분석 공부하는 블로그 - 티스토리, 8월 15, 2025에 액세스, https://hul980.tistory.com/37
6. LSTM(Long short term memory) - 곰곰의 일지 - 티스토리, 8월 15, 2025에 액세스, https://secundo.tistory.com/46
7. [1706.03762] Attention Is All You Need - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/1706.03762
8. 트랜스포머 모델이란? | 용어 해설 | HPE 대한민국, 8월 15, 2025에 액세스, https://www.hpe.com/kr/ko/what-is/transformer-model.html
9. [NLP 스터디] Scaled Dot-Product Attention - 아는 것의 미학 - 티스토리, 8월 15, 2025에 액세스, https://applepy.tistory.com/123
10. Normalization and Residual Connections in Generative AI, 8월 15, 2025에 액세스, https://www.tutorialspoint.com/gen-ai/normalization-and-residual-connections.htm
11. 트랜스포머 (기계 학습) - 위키백과, 우리 모두의 백과사전, 8월 15, 2025에 액세스, [https://ko.wikipedia.org/wiki/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)](https://ko.wikipedia.org/wiki/트랜스포머_(기계_학습))
12. 트랜스포머 모델이란 무엇인가요? - IBM, 8월 15, 2025에 액세스, https://www.ibm.com/kr-ko/think/topics/transformer-model
13. 트랜스포머(인공신경망) - 나무위키:대문, 8월 15, 2025에 액세스, [https://namu.wiki/w/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8(%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D)](https://namu.wiki/w/트랜스포머(인공신경망))
14. 인공 지능에서 트랜스포머란 무엇인가요? - AWS, 8월 15, 2025에 액세스, https://aws.amazon.com/ko/what-is/transformers-in-artificial-intelligence/
15. Transformer의 Multi-Head Attention과 Transformer에서 쓰인 다양한 ..., 8월 15, 2025에 액세스, [https://glanceyes.com/entry/Transformer%EC%9D%98-Multi-Head-Attention%EA%B3%BC-Transformer%EC%97%90%EC%84%9C-%EC%93%B0%EC%9D%B8-%EB%8B%A4%EC%96%91%ED%95%9C-%EA%B8%B0%EB%B2%95](https://glanceyes.com/entry/Transformer의-Multi-Head-Attention과-Transformer에서-쓰인-다양한-기법)
16. Transformer (deep learning architecture) - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)
17. Attention 개념 | Inhwan's Digital Blog, 8월 15, 2025에 액세스, https://nextjs-blog-v2-amber.vercel.app/blog/NLP/2023-01-05-attention
18. Transformer(1) - Scaled Dot-Product Attention - simpling - 티스토리, 8월 15, 2025에 액세스, https://simpling.tistory.com/3
19. 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention) - velog, 8월 15, 2025에 액세스, [https://velog.io/@cha-suyeon/%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%93%9C-%EB%8B%B7-%ED%94%84%EB%A1%9C%EB%8D%95%ED%8A%B8-%EC%96%B4%ED%85%90%EC%85%98Scaled-dot-product-Attention](https://velog.io/@cha-suyeon/스케일드-닷-프로덕트-어텐션Scaled-dot-product-Attention)
20. 트랜스포머(Transformer) 모델 / 인코더, 포지션-와이즈 피드 포워드 신경망, 잔차 연결, 정규화, 8월 15, 2025에 액세스, https://zorba-blog.tistory.com/27
21. The Detailed Explanation of Self-Attention in Simple Words | by Maninder Singh | Medium, 8월 15, 2025에 액세스, https://medium.com/@manindersingh120996/the-detailed-explanation-of-self-attention-in-simple-words-dec917f83ef3
22. [NLP] Transformer Multi-Head Attention 파이썬으로 정리 - Tiabet 공부일지 - 티스토리, 8월 15, 2025에 액세스, https://tiabet0929.tistory.com/83
23. 4-2. Transformer(Multi-head Attention) [초등학생도 이해하는 자연어처리] - 코딩 오페라, 8월 15, 2025에 액세스, https://codingopera.tistory.com/44
24. Mastering LLama — Multihead Attention — (1/3) : SHA 에서 MHA로 확장 - Medium, 8월 15, 2025에 액세스, https://medium.com/@hugmanskj/mastering-llama-multihead-attention-1-3-c6227082b36f
25. A Mathematical View of Attention Models in Deep Learning - Texas A&M University, 8월 15, 2025에 액세스, https://people.tamu.edu/~sji/classes/attn-slides.pdf
26. A Mathematical View of Attention Models in Deep Learning - Texas A&M University, 8월 15, 2025에 액세스, https://people.tamu.edu/~sji/classes/attn.pdf
27. 포지셔널 인코딩(Positional Encoding) - 아는 것의 미학 🌼 - 티스토리, 8월 15, 2025에 액세스, https://applepy.tistory.com/145
28. A Gentle Introduction to Positional Encoding in Transformer Models, Part 1 - MachineLearningMastery.com, 8월 15, 2025에 액세스, https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/
29. A Gentle Introduction to Positional Encoding in Transformer Models, Part 1, 8월 15, 2025에 액세스, https://www.cs.bu.edu/fac/snyder/cs505/PositionalEncodings.pdf
30. Positional Encoding은 무엇일까? | Standing-O, 8월 15, 2025에 액세스, https://standing-o.github.io/posts/positional-encoding/
31. 차근차근 이해하는 Transformer(5): Positional Encoding - Data Science - 티스토리, 8월 15, 2025에 액세스, [https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer5-Positional-Encoding](https://tigris-data-science.tistory.com/entry/차근차근-이해하는-Transformer5-Positional-Encoding)
32. Attention is all you need paper 뽀개기 - 포자랩스의 기술 블로그, 8월 15, 2025에 액세스, https://pozalabs.github.io/transformer/
33. [Transformer] Positional Encoding 이해하기 - Growth Scientist - 티스토리, 8월 15, 2025에 액세스, https://zzz0101.tistory.com/36
34. Positional Encodings in Transformer Models - MachineLearningMastery.com, 8월 15, 2025에 액세스, https://machinelearningmastery.com/positional-encodings-in-transformer-models/
35. Transformer Architecture: The Positional Encoding - Amirhossein Kazemnejad's Blog, 8월 15, 2025에 액세스, https://kazemnejad.com/blog/transformer_architecture_positional_encoding/
36. Transformer 설명 - taeeyeong - 티스토리, 8월 15, 2025에 액세스, https://taeeyeong.tistory.com/5
37. Transformer(3) - Positional Encoding, Position-Wise Feedforward, Residual connection, 8월 15, 2025에 액세스, https://simpling.tistory.com/5
38. Transformer (2) - YJJo - 티스토리, 8월 15, 2025에 액세스, https://yjjo.tistory.com/49
39. Position-Wise Feed-Forward Network (FFN) | by Hunter Phillips | Medium, 8월 15, 2025에 액세스, https://medium.com/@hunter-j-phillips/position-wise-feed-forward-network-ffn-d4cc9e997b4c
40. Unlocking Deeper Understanding: Gated Linear Units (GLU) and, 8월 15, 2025에 액세스, https://www.metriccoders.com/post/unlocking-deeper-understanding-gated-linear-units-glu-and-their-variants-in-llms
41. Linear Layers and Activation Functions in Transformer Models - Machine Learning Mastery, 8월 15, 2025에 액세스, https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/
42. Transformer Activation Functions and their Details - JoeLogs, 8월 15, 2025에 액세스, https://sathvikjoel.github.io/posts/tech/05032024_activationfunctions/
43. residual connection - learnius, 8월 15, 2025에 액세스, https://learnius.com/llms/2+LLMs+and+Transformers/residual+connection
44. On Layer Normalization in the Transformer Architecture - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2002.04745
45. Transformers: Attention is all you need — Layer Normalization | by Shravan Kumar - Medium, 8월 15, 2025에 액세스, https://medium.com/@shravankoninti/transformers-attention-is-all-you-need-layer-normalization-1435248866d6
46. 트랜스포머 (Transformer) - 3. Feed Forward Neural Network - Attention - 티스토리, 8월 15, 2025에 액세스, [https://deeeepdive.tistory.com/entry/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8-Transformer-3-Feed-Forward-Neural-Network](https://deeeepdive.tistory.com/entry/트랜스포머-Transformer-3-Feed-Forward-Neural-Network)
47. BERT (language model) - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/BERT_(language_model)
48. Paper summary — BERT: Bidirectional Transformers for Language Understanding | by Sanna Persson | Analytics Vidhya | Medium, 8월 15, 2025에 액세스, https://medium.com/analytics-vidhya/paper-summary-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-861456fed1f9
49. arXiv:1810.04805v2 [cs.CL] 24 May 2019, 8월 15, 2025에 액세스, https://arxiv.org/pdf/1810.04805
50. Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processin, 8월 15, 2025에 액세스, https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/
51. arXiv:1810.04805v2 [cs.CL] 24 May 2019, 8월 15, 2025에 액세스, https://arxiv.org/abs/1810.04805
52. [CS224n] T5 and Large Language Models - velog, 8월 15, 2025에 액세스, https://velog.io/@tobigs1617text/CS224n-T5-and-Large-Language-Models
53. GPT-1 - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/GPT-1
54. Paper summary: GPT 1 — Improving Language Understanding by Generative Pre-Training, 8월 15, 2025에 액세스, https://sannaperzon.medium.com/paper-summary-gpt-1-improving-language-understanding-by-generative-pre-training-c43bd7ff242a
55. Self - Attention in NLP - GeeksforGeeks, 8월 15, 2025에 액세스, https://www.geeksforgeeks.org/nlp/self-attention-in-nlp/
56. The GPT-3 Architecture, on a Napkin, 8월 15, 2025에 액세스, https://dugas.ch/artificial_curiosity/GPT_architecture.html
57. What is GPT (generative pre-trained transformer)? - IBM, 8월 15, 2025에 액세스, https://www.ibm.com/think/topics/gpt
58. T5 (language model) - Wikipedia, 8월 15, 2025에 액세스, https://en.wikipedia.org/wiki/T5_(language_model)
59. Exploring the Limits of Transfer Learning with a Unified Text-to-Text ..., 8월 15, 2025에 액세스, https://arxiv.org/pdf/1910.10683
60. Understand T5 — Text-to-Text Transfer Transformer | by Yu Yang | Analytics Vidhya, 8월 15, 2025에 액세스, https://medium.com/analytics-vidhya/understand-t5-text-to-text-transfer-transformer-9bc1757989ab
61. T5 - Hugging Face, 8월 15, 2025에 액세스, https://huggingface.co/docs/transformers/model_doc/t5
62. Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer, 8월 15, 2025에 액세스, https://research.google/blog/exploring-transfer-learning-with-t5-the-text-to-text-transfer-transformer/
63. why all the large language models are decoder-only based model? : r/LanguageTechnology, 8월 15, 2025에 액세스, https://www.reddit.com/r/LanguageTechnology/comments/11ajhat/why_all_the_large_language_models_are_decoderonly/
64. [D] ELI5: Why is the GPT family of models based on the decoder-only architecture? - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/14onn56/d_eli5_why_is_the_gpt_family_of_models_based_on/
65. Why are most LLMs decoder-only?. Dive into the rabbit hole of recent… | by Yumo Bai, 8월 15, 2025에 액세스, https://medium.com/@yumo-bai/why-are-most-llms-decoder-only-590c903e4789
66. [Transformer Survey] #2 Sparse Attention - YouTube, 8월 15, 2025에 액세스, https://www.youtube.com/watch?v=m8rWN2-VkcU
67. The Problem with Quadratic Attention in Transformer Architectures | tips - Wandb, 8월 15, 2025에 액세스, https://wandb.ai/wandb_fc/tips/reports/The-Problem-with-Quadratic-Attention-in-Transformer-Architectures--Vmlldzo3MDE0Mzcz
68. Limitations of Transformer Architecture | by Thirupathi Thangavel - Medium, 8월 15, 2025에 액세스, https://medium.com/@thirupathi.thangavel/limitations-of-transformer-architecture-4e6118cbf5a4
69. [2402.09963] Why are Sensitive Functions Hard for Transformers? - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2402.09963
70. [Discussion] In this age of LLMs, What are the limitations of Transformer architecture and downside to it? : r/MachineLearning - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/18qh1hp/discussion_in_this_age_of_llms_what_are_the/
71. Transformers Learn Low Sensitivity Functions: Investigations and Implications - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2403.06925v2
72. Rethinking Self-Attention: Towards Interpretability in Neural Parsing - ACL Anthology, 8월 15, 2025에 액세스, https://aclanthology.org/2020.findings-emnlp.65.pdf
73. On Limitations of the Transformer Architecture - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2402.08164v1
74. On Limitations of the Transformer Architecture - OpenReview, 8월 15, 2025에 액세스, https://openreview.net/forum?id=KidynPuLNW
75. Day 31: Longformer - Efficient Attention Mechanism for Long Documents - DEV Community, 8월 15, 2025에 액세스, https://dev.to/nareshnishad/day-31-longformer-efficient-attention-mechanism-for-long-documents-475j
76. Exploring Longformer - Scaler Topics, 8월 15, 2025에 액세스, https://www.scaler.com/topics/nlp/longformer/
77. kyegomez/SparseAttention: Pytorch Implementation of the sparse attention from the paper: "Generating Long Sequences with Sparse Transformers" - GitHub, 8월 15, 2025에 액세스, https://github.com/kyegomez/SparseAttention
78. Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation - Microsoft, 8월 15, 2025에 액세스, https://www.microsoft.com/en-us/research/wp-content/uploads/2021/05/LighterandBetter_Low-RankDecomposedSelf-AttentionNetworksforNext-ItemRecommendation.pdf
79. Self-Attention is Low Rank: Explained from Linformer | by Banashrii | Jul, 2025 | Medium, 8월 15, 2025에 액세스, https://medium.com/@sbanashri25/self-attention-is-low-rank-explained-from-linformer-6fa3e8629e94
80. Linformer: Self-Attention with Linear Complexity (Paper Explained) - YouTube, 8월 15, 2025에 액세스, https://www.youtube.com/watch?v=-_2AF9Lhweo
81. Efficient Attention Mechanisms for Large Language Models: A Survey - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2507.19595v1
82. Gated Linear Attention Transformers with Hardware-Efficient Training - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2312.06635
83. Breaking the attention bottleneck - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2406.10906v1
84. Basic idea behind flash attention (V1) | Damek Davis' Website, 8월 15, 2025에 액세스, https://damek.github.io/random/basic-idea-behind-flash-attention/
85. Paper Explained 2: FlashAttention | by Shirley Li - Medium, 8월 15, 2025에 액세스, https://medium.com/@lixue421/paper-explained-2-flashattention-172bc2e90440
86. Understanding Flash Attention: Writing the Algorithm from Scratch in Triton - Alex Dremov, 8월 15, 2025에 액세스, https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/
87. Flash Attention massively accelerate gpt-oss-120b inference speed on Apple silicon - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/LocalLLaMA/comments/1mp92nc/flash_attention_massively_accelerate_gptoss120b/
88. MambaByte: Token-free Selective State Space Model - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2401.13660v1
89. A Survey of Mamba - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2408.01129v1
90. Mamba-ST: State Space Model for Efficient Style Transfer - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/384075792_Mamba-ST_State_Space_Model_for_Efficient_Style_Transfer
91. Repeat After Me: Transformers are Better than State Space Models at Copying, 8월 15, 2025에 액세스, https://kempnerinstitute.harvard.edu/research/deeper-learning/repeat-after-me-transformers-are-better-than-state-space-models-at-copying/
92. [D] What Are the Fundamental Drawbacks of Mamba Compared to Transformers? - Reddit, 8월 15, 2025에 액세스, https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/
93. Jamba - Hugging Face, 8월 15, 2025에 액세스, https://huggingface.co/docs/transformers/v4.47.1/model_doc/jamba
94. Jamba: A Hybrid Transformer-Mamba Language Model - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2403.19887v1
95. Jamba: Hybrid Transformer-Mamba Language Models - OpenReview, 8월 15, 2025에 액세스, https://openreview.net/forum?id=JFPaD7lpBD
96. Open-source LLM Jamba focuses on performance and efficiency with unique hybrid architecture - The Decoder, 8월 15, 2025에 액세스, https://the-decoder.com/open-source-llm-jamba-focuses-on-performance-and-efficiency-with-a-new-architecture/
97. Jamba Transformer - Kaggle, 8월 15, 2025에 액세스, https://www.kaggle.com/code/mpwolke/jamba-transformer
98. Transformer vs. Mixture of Experts in LLMs - Daily Dose of Data Science, 8월 15, 2025에 액세스, https://www.dailydoseofds.com/p/transformer-vs-mixture-of-experts-in-llms/
99. Applying Mixture of Experts in LLM Architectures | NVIDIA Technical Blog, 8월 15, 2025에 액세스, https://developer.nvidia.com/blog/applying-mixture-of-experts-in-llm-architectures/
100. A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2503.07137v1
101. What is mixture of experts? | IBM, 8월 15, 2025에 액세스, https://www.ibm.com/think/topics/mixture-of-experts
102. Toward Inference-optimal Mixture-of-Expert Large Language Models - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2404.02852v1
103. Unraveling the Complexity of Mixture of Experts (MoE) in Machine Learning - Medium, 8월 15, 2025에 액세스, https://medium.com/@juanc.olamendy/unraveling-the-complexity-of-mixture-of-experts-moe-in-machine-learning-7db682d76b00
104. Scaling Laws for Neural Language Models - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/338789955_Scaling_Laws_for_Neural_Language_Models
105. [2001.08361] Scaling Laws for Neural Language Models - ar5iv - arXiv, 8월 15, 2025에 액세스, https://ar5iv.labs.arxiv.org/html/2001.08361
106. Scaling Laws for Neural Language Models - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2001.08361
107. [2001.08361] Scaling Laws for Neural Language Models - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2001.08361
108. Chinchilla: Training Compute-Optimal Large Language Models | by Isaac Kargar - Medium, 8월 15, 2025에 액세스, https://medium.com/aiguys/chinchilla-training-compute-optimal-large-language-models-a922a0d9eebb
109. Training Compute-Optimal Large Language Models, 8월 15, 2025에 액세스, https://proceedings.neurips.cc/paper_files/paper/2022/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf
110. Paper page - Training Compute-Optimal Large Language Models - Hugging Face, 8월 15, 2025에 액세스, https://huggingface.co/papers/2203.15556
111. Chinchilla : A Guide To Training Compute Optimal Large Language Models - Medium, 8월 15, 2025에 액세스, https://medium.com/@pranjalkhadka/chinchilla-a-guide-to-training-compute-optimal-large-language-models-3d3ad3787f1d
112. [Link] Training Compute-Optimal Large Language Models - LessWrong, 8월 15, 2025에 액세스, https://www.lesswrong.com/posts/4dbK5dPiqHCgNdKnq/link-training-compute-optimal-large-language-models
113. Training Compute-Optimal Large Language Models - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/pdf/2203.15556
114. The Hardware Lottery: the cost of straying off the beaten path - LightOn.ai, 8월 15, 2025에 액세스, https://www.lighton.ai/fr-blog-posts/the-hardware-lottery-the-cost-of-straying-off-the-beaten-path
115. The Hardware Lottery (Paper Explained) - YouTube, 8월 15, 2025에 액세스, https://www.youtube.com/watch?v=MQ89be_685o
116. The hardware lottery | Request PDF - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/356687388_The_hardware_lottery
117. Transformers are Graph Neural Networks - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2506.22084v1
118. [2506.22084] Transformers are Graph Neural Networks - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/abs/2506.22084
119. ViT 살펴보기 | Vision Transformer - Standing-O, 8월 15, 2025에 액세스, https://standing-o.github.io/posts/vision-transformer/
120. [스노피 AI] Vision Transformer 쉽게 이해하기 - 1. Introduction, 8월 15, 2025에 액세스, [https://wafour.tistory.com/entry/%EC%8A%A4%EB%85%B8%ED%94%BC-AI-Vision-Transformer-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-1-Introduction](https://wafour.tistory.com/entry/스노피-AI-Vision-Transformer-쉽게-이해하기-1-Introduction)
121. [Hands-On] ViT를 활용한 헤드 기반 이미지 분류 - Medium, 8월 15, 2025에 액세스, [https://medium.com/@hugmanskj/hands-on-vit%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%ED%97%A4%EB%93%9C-%EA%B8%B0%EB%B0%98-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B6%84%EB%A5%98-d845b2665676](https://medium.com/@hugmanskj/hands-on-vit를-활용한-헤드-기반-이미지-분류-d845b2665676)
122. Vision Transformer에 대한 시각적 설명 (A Visual Guide to Vision Transformers) - 파이토치 한국 사용자 모임, 8월 15, 2025에 액세스, https://discuss.pytorch.kr/t/vision-transformer-a-visual-guide-to-vision-transformers/4158
123. (PDF) Vision Transformers in Medical Imaging: a Comprehensive Review of Advancements and Applications Across Multiple Diseases - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/390372350_Vision_Transformers_in_Medical_Imaging_a_Comprehensive_Review_of_Advancements_and_Applications_Across_Multiple_Diseases
124. Comparative Analysis of Vision Transformers and Convolutional Neural Networks for Medical Image Classification - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2507.21156v1
125. Vision Transformers in Medical Imaging: A Review - ResearchGate, 8월 15, 2025에 액세스, https://www.researchgate.net/publication/365614966_Vision_Transformers_in_Medical_Imaging_A_Review
126. A review of transformer models in drug discovery and beyond - PMC - PubMed Central, 8월 15, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC12240084/
127. Application of artificial intelligence large language models in drug target discovery - PMC, 8월 15, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC12279696/
128. Full article: Natural language processing in drug discovery: bridging the gap between text and therapeutics with artificial intelligence - Taylor & Francis Online, 8월 15, 2025에 액세스, https://www.tandfonline.com/doi/full/10.1080/17460441.2025.2490835
129. Integrating artificial intelligence in drug discovery and early drug development: a transformative approach - PMC, 8월 15, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC11909971/
130. Assessing Fairness of AlphaFold2 Prediction of Protein 3D Structures - PMC, 8월 15, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10245900/
131. How is AlphaFold 2 used by scientists?, 8월 15, 2025에 액세스, https://www.ebi.ac.uk/training/online/courses/alphafold/validation-and-impact/how-is-alphafold-used-by-scientists/
132. Progress and trends on machine learning in proteomics during 1997-2024: a bibliometric analysis - Frontiers, 8월 15, 2025에 액세스, https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2025.1594442/pdf
133. Insilico Medicine: Main, 8월 15, 2025에 액세스, https://insilico.com/
134. Transformer-Based Generative Model Accelerating the Development of Novel BRAF Inhibitors | ACS Omega - ACS Publications, 8월 15, 2025에 액세스, https://pubs.acs.org/doi/10.1021/acsomega.1c05145
135. Molecular Quantum Transformer - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2503.21686v2
136. GraphXForm: Graph transformer for computer-aided molecular design - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2411.01667v2
137. A generative material transformer using Wyckoff representation - arXiv, 8월 15, 2025에 액세스, https://arxiv.org/html/2501.16051v2
138. A Transformer Model for Predicting Generic Chemical Reaction Products from Templates, 8월 15, 2025에 액세스, https://arxiv.org/html/2503.05810v1

