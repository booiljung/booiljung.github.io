# 트랜스포머 기반 아키텍처의 시각적 토큰화


자연어 처리(NLP) 분야에서 트랜스포머 아키텍처가 거둔 혁신적인 성공은 컴퓨터 비전 연구의 지형을 근본적으로 바꾸어 놓았습니다.1 "An Image is Worth 16x16 Words"라는 도발적인 제목의 논문에서 제안된 Vision Transformer(ViT)는 컨볼루션 신경망(CNN)에 대한 의존 없이, 순수 트랜스포머 아키텍처를 이미지 분류 문제에 직접 적용하여 탁월한 성능을 달성할 수 있음을 입증했습니다.2 ViT의 성공은 이미지라는 시각적 데이터를 NLP 모델이 처리할 수 있는 '토큰(token)'의 시퀀스로 변환하는 독창적인 전처리 과정, 즉 '시각적 토큰화'에 기반합니다. 이 과정은 후속 연구와 멀티모달 AI 발전의 초석을 다졌다는 점에서 그 중요성이 매우 큽니다. 본 파트에서는 ViT의 핵심을 이루는 이 시각적 토큰화의 기본 패러다임을 단계별로 상세히 분석하고, 그 기저에 깔린 원리를 해부합니다.


ViT의 가장 혁신적인 기여는 컴퓨터 비전 문제를 NLP의 핵심 패러다임인 시퀀스-투-시퀀스(sequence-to-sequence) 문제로 재정의한 것입니다.2 이는 기존의 CNN 기반 접근법과는 완전히 다른 관점을 제시했으며, 최소한의 수정만으로 NLP 트랜스포머를 비전 태스크에 적용할 수 있는 길을 열었습니다. 이 변환 과정은 크게 두 단계로 구성됩니다: 이미지 패치화(Image Patching)와 선형 투영(Linear Projection).


시각적 토큰화의 첫 단계는 2차원의 이미지 그리드를 1차원의 토큰 시퀀스로 변환하는 것입니다. 예를 들어, 224×224 픽셀 크기의 입력 이미지가 주어지면, ViT는 이를 일정한 크기(예: 16×16 픽셀)의 겹치지 않는 정사각형 패치(patch)들로 분할합니다.2 이 과정은 마치 문장을 단어 단위로 나누는 것과 유사하며, 이미지를 '단어'에 해당하는 패치들의 '문장'으로 간주하는 개념적 전환을 의미합니다.6

224×224 크기의 이미지를 16×16 크기의 패치로 분할할 경우, 총 (224/16)×(224/16)=14×14=196개의 패치가 생성됩니다.7 이렇게 생성된 2차원 패치 그리드는 순차적으로 배열되어 196개 길이를 갖는 1차원 시퀀스를 형성합니다. 이 간단한 분할 과정이 바로 이미지를 트랜스포머가 처리할 수 있는 형식으로 만드는 첫걸음입니다.


다음으로, 각 2차원 패치는 1차원 벡터로 평탄화(flattening)됩니다. 예를 들어, RGB 이미지의 16×16 패치는 16×16×3=768 차원의 벡터로 변환됩니다. 이렇게 생성된 고차원 벡터는 학습 가능한 선형 투영 레이어(trainable linear projection layer)를 통해 모델의 고유한 임베딩 공간(예: D 차원)으로 투영됩니다.5

이 선형 투영의 목적은 두 가지입니다. 첫째, 각 패치를 트랜스포머 인코더가 처리할 수 있는 고정된 차원의 벡터, 즉 '패치 임베딩(patch embedding)'으로 변환하는 것입니다. 둘째, 이 투영 과정을 통해 시각적으로 유사한 내용을 담고 있는 패치들이 벡터 공간상에서 서로 가까운 위치에 매핑되도록 학습하는 것입니다.6 이렇게 생성된 패치 임베딩들의 시퀀스가 트랜스포머 인코더의 최종 입력이 됩니다.


트랜스포머의 핵심 메커니즘인 자기-어텐션(self-attention)은 본질적으로 순서에 무관(permutation-invariant)합니다. 즉, 입력 토큰들을 순서가 없는 집합(set)으로 간주합니다.7 만약 위치 정보가 없다면, 모델은 정상적인 이미지와 패치들이 무작위로 섞인 이미지를 구분할 수 없게 됩니다. 따라서 이미지의 공간적 구조를 보존하기 위해서는 각 패치의 위치 정보를 명시적으로 주입해주어야 합니다.5 이를 위해 다양한 위치 임베딩(Positional Embedding) 기법이 사용됩니다.


ViT 원본 논문에서 채택한 방식입니다. APE는 시퀀스 내 각 패치 위치에 해당하는 고유한 학습 가능 벡터입니다. 이 위치 벡터들은 첫 번째 트랜스포머 레이어에 입력되기 전에 패치 임베딩에 직접 더해집니다.5 모델은 훈련 과정에서 이 위치 벡터들을 학습하여 각 위치의 공간적 의미를 파악하게 됩니다. APE는 NLP 트랜스포머처럼 고정된 사인/코사인 함수를 사용할 수도 있지만, 현대의 ViT에서는 학습 가능한 파라미터를 사용하는 것이 더 일반적입니다.11


Swin Transformer와 같은 모델에서 대중화된 대안적 접근 방식입니다. 각 토큰에 절대적인 위치 벡터를 더하는 대신, RPB는 두 토큰(쿼리와 키) 간의 *상대적인 거리*에 기반하여 학습 가능한 편향(bias) 값을 어텐션 행렬에 직접 더합니다.13 이 방식은 다양한 스케일에서 수많은 토큰을 처리해야 하는 계층적 ViT에서 특히 선호됩니다.13


대규모 언어 모델(LLM)에서 큰 성공을 거둔 회전 위치 임베딩(Rotary Position Embedding, RoPE)도 비전 분야에서 탐색되고 있습니다. RoPE는 쿼리(query)와 키(key) 벡터를 회전시켜 위치 정보를 적용하며, 더 나은 외삽(extrapolation) 특성을 제공하는 것으로 알려져 있지만, 비전 태스크에서의 효과는 아직 활발히 연구 중입니다.13


ViT는 패치 임베딩 시퀀스의 맨 앞에 특별하고 학습 가능한 `` (classification) 토큰을 추가합니다.5 이 토큰은 이미지의 특정 패치와 직접적으로 대응되지 않으며, 일종의 "백지 상태(blank slate)"에서 시작합니다.6

`토큰이 여러 트랜스포머 레이어를 통과하면서, 자기-어텐션 메커니즘은 이 토큰이 다른 모든 패치 토큰들로부터 정보를 집계하도록 만듭니다.[14] 최종적으로 이미지 분류 결정이 오직 이` 토큰의 출력 임베딩에만 기반하여 내려지기 때문에, 모델은 전체 이미지의 내용을 요약하는 전역적인 표현(global representation)을 이 단일 벡터에 인코딩하는 방법을 학습하도록 강제됩니다.6

ViT 논문에서는 `토큰의 대안으로 모든 패치 토큰에 대해 전역 평균 풀링(Global Average Pooling, GAP)을 수행하는 방법도 언급합니다. 이는 ResNet과 같은 CNN에서 흔히 사용되는 기법이지만 [14],` 토큰 방식이 많은 ViT 변형 모델에서 표준으로 자리 잡았습니다.

이처럼 ViT의 시각적 토큰화는 이미지를 패치 시퀀스로 변환하고, 위치 정보를 주입하며, 전역 정보 집계를 위한 `` 토큰을 활용하는 일련의 체계적인 과정입니다. 이 패러다임의 가장 심오한 결과는 시각적 정보를 '토큰'이라는 일반적인 형태로 추상화했다는 점입니다. 이미지 패치를 벡터 시퀀스로 변환함으로써, ViT는 모달리티에 구애받지 않는(modality-agnostic) 입력 파이프라인을 창조했습니다. NLP 트랜스포머가 단어 토큰 시퀀스를 처리하는 원리를 그대로 차용하여, 이미지를 '문장'으로, 패치를 '단어'로 간주하는 가설을 성공적으로 입증한 것입니다.2

이러한 추상화는 단순히 비전 문제를 해결하는 것을 넘어, 훨씬 더 큰 파급 효과를 낳았습니다. 바로 이 통일된 토큰 표현 방식이 멀티모달 학습(multimodal learning) 분야 전체를 가능하게 한 핵심 동력이 되었기 때문입니다. ViT에서 나온 시각 토큰과 언어 모델에서 나온 텍스트 토큰을 단순히 이어 붙여 *동일한* 어텐션 메커니즘으로 처리할 수 있게 되면서, 서로 다른 모달리티 간의 깊은 융합이 가능해졌습니다.15 결국, ViT의 간단해 보이는 토큰화 단계는 여러 모달리티를 넘나들며 추론할 수 있는 범용 인공지능을 향한 중요한 기초 공사였던 셈입니다.


1부에서 설명한 ViT의 기본 토큰화 방식은 혁신적이었지만, 동시에 여러 내재적 한계를 안고 있었습니다. 이러한 한계점들은 이후 3부에서 논의될 다양한 아키텍처 발전의 핵심적인 동기가 되었습니다. 본 파트에서는 단순한 그리드 기반 패치 분할 방식의 약점을 비판적으로 분석하고, 그로 인해 발생하는 문제들을 심층적으로 탐구합니다.


토큰화의 근본적인 목표는 정보를 의미 있는 단위로 분할하는 것입니다. 그러나 ViT의 토큰화 방식은 이 목표를 달성하는 데 있어 NLP와 본질적인 차이를 보입니다.


NLP에서 사용되는 WordPiece나 Byte Pair Encoding(BPE)과 같은 토큰화 기법은 의미론적 무결성(semantic integrity)을 보존하도록 설계되었습니다. 문장은 의미를 가진 단위인 단어나 하위 단어(sub-word)로 분리되며, 이를 통해 서로 다른 개념이 하나의 토큰으로 임의로 융합되는 것을 방지합니다.1


반면, ViT의 그리드 기반 패치 분할은 이미지의 내용과는 전혀 무관하게 기계적으로 이루어지는 "강제 분할(hard split)"입니다.18 이로 인해 하나의 패치가 여러 객체의 일부(예: 고양이 귀의 절반과 배경의 일부)를 임의로 섞거나, 작은 객체 하나를 여러 패치에 걸쳐 조각내는 현상이 발생합니다.1

그 결과, 생성된 토큰들은 명확한 의미론적 내용을 결여하게 되어 해석을 어렵게 만들고, 모델이 정보를 효과적으로 캡슐화하는 능력을 저해합니다.19 이는 마치 NLP 모델이 단어 단위가 아닌, 고정된 글자 수마다 문장을 잘라버리는 것과 같아서 원래의 의미를 파악하기 어렵게 만드는 것과 유사합니다.19


귀납적 편향(inductive bias)은 모델이 훈련 데이터로부터 보지 못한 데이터로 일반화하기 위해 사용하는 일련의 가정을 의미합니다.20 CNN은 시각적 태스크에 매우 효과적인 강력한 귀납적 편향을 내장하고 있습니다.


- **지역성 (Locality):** CNN은 공간적으로 인접한 픽셀들이 멀리 떨어진 픽셀들보다 더 강하게 연관되어 있다는 가정을 바탕으로, 작은 크기의 지역 필터(local filter)를 사용하여 이미지를 처리합니다. 이는 엣지나 질감과 같은 지역적 구조를 학습하는 데 매우 효율적입니다.21
- **이동 등변성 (Translation Equivariance):** 컨볼루션 필터의 가중치 공유(weight sharing) 특성으로 인해, 이미지 내 객체의 위치가 이동하면 그 특징 표현(feature representation)도 동일하게 이동하지만 형태는 유지됩니다. 이는 객체 인식에 강력한 사전 지식으로 작용합니다.9
- **계층적 특징 추출 (Hierarchical Feature Extraction):** CNN은 초기 레이어의 단순한 엣지부터 시작하여 더 깊은 레이어에서 복잡한 객체 부분으로 이어지는 특징의 계층 구조를 구축합니다.21


표준 ViT 아키텍처는 이러한 시각적 귀납적 편향을 대부분 버립니다. 자기-어텐션 메커니즘은 이론적으로 어떤 두 픽셀(또는 패치)이든 거리에 상관없이 관계를 맺을 수 있게 하여, 첫 번째 레이어부터 전역적인 수용장(global receptive field)을 갖습니다.9 이러한 설계는 몇 가지 중요한 결과를 초래합니다.

- 결과 1: 극심한 데이터 의존성 (Data Hunger)

  내장된 가정 없이는, ViT가 이미지의 근본적인 속성(예: 지역적 구조의 중요성)을 처음부터 데이터로부터 학습해야 합니다. 이 때문에 엄청난 양의 훈련 데이터가 필요합니다. ImageNet-1K와 같은 중간 크기의 데이터셋에서 훈련될 때, 바닐라 ViT는 비슷한 크기의 CNN에 비해 성능이 저조합니다.3 JFT-300M(3억 개 이상의 이미지)과 같은 대규모 데이터셋으로 사전 훈련을 거쳐야만, 이른바 "대규모 훈련이 귀납적 편향을 이기는(large-scale training trumps inductive bias)" 현상이 나타나며 ViT가 CNN을 능가하는 성능을 보입니다.3

- 결과 2: 형태 대 질감 편향 (Shape vs. Texture Bias)

  연구에 따르면 CNN은 인간의 시각과 달리 형태(shape)보다는 질감(texture)에 기반하여 이미지를 분류하는 강한 편향을 보입니다.20 더 느슨한 귀납적 편향을 가진 ViT는 지역적 질감에 훨씬 더 편향되어 있지만, 유연한 어텐션 메커니즘 덕분에 데이터가 충분하다면 형태 정보도 효과적으로 학습할 수 있는 잠재력을 가집니다.20

- 결과 3: 낮은 스케일 불변성 (Poor Scale Invariance)

  고정된 패치 크기와 절대 위치 임베딩은 바닐라 ViT가 다양한 스케일의 객체나 다양한 해상도의 이미지를 처리하는 데 어려움을 겪게 만듭니다. 이는 풀링 레이어와 계층적 구조를 통해 스케일 변화에 더 자연스럽게 대처하는 CNN과 대조되는 지점입니다.19

이러한 차이점들은 모델 설계에서의 근본적인 트레이드오프를 드러냅니다. CNN은 '견고한' 설계를 가집니다. 컨볼루션 필터는 지역성과 이동 등변성을 강제하며 9, 이러한 견고함은 학습을 더 쉽고 데이터 효율적으로 만드는 강력한 가정으로 작용합니다. 반면, ViT는 '유연한' 설계를 가집니다. 입력 데이터에 대해 거의 가정을 하지 않음으로써 비지역적인 패턴을 포함한 어떤 패턴이든 학습할 수 있는 잠재력을 지닙니다.23 하지만 이 유연성 때문에 모델이 탐색해야 할 가설 공간이 훨씬 더 커지고 복잡해집니다. 귀납적 편향의 안내 없이는, 올바른 패턴을 찾고 우연한 상관관계에 과적합되는 것을 피하기 위해 방대한 양의 데이터가 필요하게 됩니다.3 이는 모델 설계における 편향(bias)과 분산(variance) 간의 근본적인 트레이드오프를 보여줍니다. CNN은 높은 편향(강한 가정)과 낮은 분산(적은 데이터 필요)을 가지는 반면, ViT는 낮은 편향(약한 가정)과 높은 분산(많은 데이터 필요)을 가집니다. 이후 ViT 아키텍처의 발전 전체는 이 스펙트럼 위에서 '최적점'을 찾는 과정, 즉 트랜스포머의 강력하고 유연한 전역 모델링 능력을 희생하지 않으면서 데이터 효율성을 향상시키기 위해 '좋은' 시각적 편향을 얼마나, 어떻게 다시 도입할 것인가에 대한 탐구로 볼 수 있습니다.

| 속성                 | 컨볼루션 신경망 (CNN)             | Vision Transformer (ViT)                            |
| -------------------- | --------------------------------- | --------------------------------------------------- |
| **기본 단위**        | 픽셀 (지역적 필터 내)             | 이미지 패치                                         |
| **수용장**           | 계층적으로 증가 (초기에는 지역적) | 처음부터 전역적                                     |
| **핵심 귀납적 편향** | 지역성, 이동 등변성, 계층 구조 9  | 최소한의 편향; 순서 무관성 (위치 임베딩으로 해결) 9 |
| **데이터 효율성**    | 높음 (내장된 편향 덕분)           | 낮음 (대규모 사전 훈련 필요) 3                      |
| **특징 계층**        | 명시적 (단순 -->> 복잡) 21           | 덜 명시적 (전역적 관계 모델링)                      |
| **스케일 불변성**    | 상대적으로 높음 (풀링 레이어)     | 낮음 (고정 패치 크기) 27                            |
| **핵심 연산**        | 컨볼루션 (가중치 공유)            | 자기-어텐션 (모든 토큰 간 상호작용)                 |

**표 1: 기본 토큰화 패러다임 비교 (ViT 대 CNN)**


ViT의 설계는 계산 비용 측면에서도 중요한 문제를 안고 있습니다.

- 제곱 복잡도 병목 현상 (Quadratic Complexity Bottleneck):

  자기-어텐션 메커니즘은 토큰의 수 N에 대해 $O(N^2)$의 계산 및 메모리 복잡도를 가집니다.3

- 고해상도 이미지의 영향:

  이미지의 경우, 토큰의 수(N)는 입력 이미지 해상도에 따라 제곱으로 증가합니다. 예를 들어, 이미지의 가로와 세로를 두 배로 늘리면 패치의 수는 네 배가 됩니다. 이로 인해 의미 분할(semantic segmentation)과 같이 고해상도 이미지를 처리해야 하는 태스크에 바닐라 ViT를 적용하는 것은 계산적으로 거의 불가능합니다.3

- 효율성 개선의 동기:

  이러한 제곱 복잡도 병목 현상은 더 효율적인 트랜스포머 아키텍처를 개발하는 주된 동기가 되었습니다. 연구는 토큰 수를 줄이거나, 희소(sparse) 또는 지역적 어텐션 패턴을 구현하거나, 계층적 모델을 만들어 이 문제를 완화하는 데 집중되었습니다.32

결론적으로, 바닐라 ViT의 토큰화 방식은 의미론적 정보의 손실, 시각적 귀납적 편향의 부재로 인한 데이터 비효율성, 그리고 제곱 복잡도로 인한 확장성 문제라는 세 가지 주요 한계를 가지고 있습니다. 이러한 문제점들은 ViT가 더 넓은 범위의 비전 태스크에 효과적으로 적용되기 위해 반드시 해결해야 할 과제였으며, 이는 곧 3부에서 다룰 다양한 아키텍처 혁신의 자양분이 되었습니다.


바닐라 ViT가 제시한 한계점들을 극복하기 위해, 연구 커뮤니티는 토큰화 방식과 아키텍처 자체를 개선하는 다양한 혁신적인 방법론을 제안했습니다. 이 파트에서는 비전 트랜스포머를 진정으로 효과적이고 효율적으로 만들기 위한 주요 연구 방향들을 심층적으로 탐구합니다. 이러한 노력들은 '순수 트랜스포머'라는 초기 이념을 넘어, 실용적인 고성능을 추구하는 방향으로 수렴하고 있습니다.


T2T-ViT는 바닐라 ViT의 두 가지 핵심 약점, 즉 1) 단순한 토큰화 방식이 엣지나 선과 같은 중요한 지역적 구조를 모델링하지 못하는 문제와 2) 표준 백본의 어텐션 계산이 중복적이라는 문제를 직접적으로 해결하고자 제안되었습니다.36


단일 강제 분할 대신, T2T-ViT는 점진적인 토큰화 과정(progressive tokenization)을 사용합니다.18

- **소프트 분할 (Soft Split):** 이 과정은 이미지를 겹치는(overlapping) 패치로 분할하는 것으로 시작합니다. 이러한 겹침은 인접한 토큰들이 정보를 공유하게 하여 지역적 연속성을 더 잘 보존하도록 돕습니다.38 이는 PyTorch의 

  `Unfold` 연산을 통해 구현될 수 있습니다.39

- **재귀적 집계 (Recursive Aggregation):** T2T 모듈은 여러 단계로 구성됩니다. 각 단계에서 토큰 시퀀스는 관계를 모델링하기 위해 소규모 트랜스포머 레이어를 통과합니다. 그 후, 토큰들은 다시 2차원 공간 레이아웃으로 "재구조화(restructured)"되고, 또 다른 소프트 분할을 통해 인접 토큰들이 하나의 새로운 토큰으로 재귀적으로 결합됩니다. 이 과정은 토큰의 수를 점진적으로 줄이면서 각 토큰을 지역적 이웃 정보로 풍부하게 만듭니다.37


이러한 점진적 구조화는 모델이 전역 어텐션을 적용하기 *전에* 지역적 특징을 명시적으로 학습할 수 있게 합니다. 이는 샘플 효율성을 크게 향상시켜, T2T-ViT가 ImageNet에서 처음부터 훈련될 때 바닐라 ViT나 일부 ResNet보다 더 적은 파라미터를 사용하면서도 더 높은 성능을 달성하게 만듭니다.36


Swin Transformer는 ViT의 제곱 복잡도 문제와 다중 스케일 특징 표현의 부재를 해결하여, 객체 탐지나 의미 분할과 같은 조밀한 예측(dense prediction) 태스크를 위한 범용 백본(general-purpose backbone)으로 사용될 수 있도록 설계되었습니다.29

3.2.1 CNN을 닮은 계층 구조

Swin Transformer는 CNN과 유사한 계층적 설계를 다시 도입합니다. 작은 크기의 패치로 시작하여 후속 단계에서는 "패치 병합(patch merging)" 레이어를 사용하여 특징 맵을 다운샘플링(토큰 수 감소)하고 채널 차원을 늘립니다. 이는 특징 피라미드(feature pyramid)와 같이 다중 스케일의 특징 맵을 생성하며, 이는 많은 비전 태스크에 필수적입니다.41


제곱 복잡도 문제를 해결하기 위해, 자기-어텐션은 전역적으로 계산되지 않습니다. 대신, 이미지는 겹치지 않는 윈도우(예: 7×7 패치)로 분할되고, 자기-어텐션은 *각 윈도우 내부에서만* 계산됩니다.41 이로 인해 계산 복잡도는 이미지 픽셀 수에 대해 선형(linear)이 됩니다.


W-MSA의 한계는 윈도우 간 정보 흐름이 없다는 것입니다. 이를 해결하기 위해, Swin은 한 레이어에서는 일반 윈도우 방식(W-MSA)을, 다음 레이어에서는 "이동된 윈도우(shifted window)" 방식을 번갈아 사용합니다. 이동된 윈도우는 이전 레이어 윈도우의 경계를 가로지르기 때문에, 윈도우 간 연결을 가능하게 하고 연속적인 레이어를 통해 효과적으로 전역적인 수용장을 구축합니다.41


이 접근 방식은 내용과 무관한 그리드 방식에서 벗어나 2.1절에서 논의된 "의미론적 불일치" 문제를 직접적으로 해결하려는 시도입니다.

- **슈퍼픽셀 기반 토큰화 (Superpixel-based Tokenization):** 이 방법은 그리드 기반 패치를 슈퍼픽셀(superpixel)로 대체할 것을 제안합니다. 슈퍼픽셀은 색상이나 질감과 같은 유사한 속성을 공유하는 연결된 픽셀들의 클러스터로, 단일 의미 개체에 더 잘 대응할 가능성이 높습니다.1
- **구현 과제 및 해결책:** 슈퍼픽셀은 모양과 크기가 다양하여 ViT의 표준적인 평탄화 및 투영 방식과 호환되지 않습니다. 제안된 해결책은 두 단계로 이루어집니다: 1) 전체 이미지에 대한 픽셀 수준의 임베딩을 생성하고, 2) 각 슈퍼픽셀 클러스터 내에서 이러한 픽셀 임베딩을 (예: 풀링을 사용하여) 집계하여 단일의 고정 크기 슈퍼픽셀 토큰을 형성합니다.1 이는 ViT 아키텍처에서 패치 토큰을 원활하게 대체할 수 있게 하여, 훨씬 높은 의미론적 무결성을 가진 토큰을 생성합니다.1


이 접근 방식의 핵심 아이디어는 두 세계의 장점을 결합하는 것입니다: CNN의 강력하고 데이터 효율적인 지역 특징 추출 및 귀납적 편향을 트랜스포머의 강력한 전역 컨텍스트 모델링과 결합하는 것입니다.22 이 방법은 특히 작은 데이터셋에서의 성능 향상이나 자원이 제한된 애플리케이션에 효과적입니다.22


한 서베이 논문25에 따르면, 하이브리드 모델은 컨볼루션 연산이 자기-어텐션 메커니즘과 통합되는 방식에 따라 여러 유형으로 분류될 수 있습니다.

| 통합 전략                 | 핵심 원리                                                    | 대표 모델                   |
| ------------------------- | ------------------------------------------------------------ | --------------------------- |
| **초기 레이어 통합**      | CNN을 특징 추출 '줄기(stem)'로 사용하여 초기 특징 맵을 생성한 후, 이를 트랜스포머 본체에 입력 | LeViT, Hybrid ViT 25        |
| **측면 레이어 통합**      | ViT를 인코더로, CNN을 디코더로 사용. 조밀한 예측 태스크에서 주로 사용 | DPT, LocalViT 25            |
| **순차적 통합**           | 네트워크 아키텍처 내에서 CNN 블록과 트랜스포머 블록을 순차적으로 번갈아 쌓음 | CoAtNet, BoTNet 25          |
| **병렬 통합**             | 별도의 CNN과 트랜스포머 브랜치가 입력을 병렬로 처리하고, 다양한 지점에서 특징을 융합 | Conformer, Mobile-Former 25 |
| **계층적 통합**           | 컨볼루션과 어텐션을 모두 포함하는 통합된 하이브리드 블록을 설계하고, 이를 반복적으로 쌓아 계층적 네트워크 구성 | CvT, MaxViT, Visformer 25   |
| **어텐션 기반 통합**      | 어텐션 메커니즘 *내부*에 컨볼루션 연산을 통합하여 지역적 편향을 추가 | ResT, CeiT 25               |
| **채널 부스팅 기반 통합** | 전이 학습 기반 보조 학습기(ViT)를 사용하여 부스팅된 채널을 생성하고, 이를 CNN 채널과 결합 | CB-HVT 25                   |

**표 2: 하이브리드 CNN-Transformer 통합 전략 분류**

EHCTNet (원격 탐사) 44 및 BEFUnet (의료 영상) 45과 같은 특정 도메인에 맞춰진 모델들은 이러한 하이브리드 원칙을 적용하여, 지역적(CNN) 및 전역적(Transformer) 특징을 모두 캡처하기 위해 이중 브랜치 인코더와 같은 맞춤형 설계를 사용합니다.

이러한 다양한 혁신들은 무작위적인 시도가 아니라, 명확하고 수렴적인 진화 경향을 나타냅니다. 초기 ViT가 제시한 '순수 트랜스포머'라는 이념적 접근 방식은 시각적 사전 지식의 부재라는 한계에 부딪혔습니다. 이에 대한 해결책으로 T2T-ViT 37나 슈퍼픽셀 ViT 1는 모델의 '입력단'을 수정하여 더 나은 토큰을 만드는 방식으로 지역적/의미론적 편향을 주입하려 했습니다. Swin Transformer 42는 핵심 어텐션 메커니즘 자체를 수정하여 백본이 CNN과 유사한 방식으로 작동하도록 만들었습니다. 그리고 하이브리드 모델들 25은 가장 직접적인 경로를 택해, CNN 구성 요소를 트랜스포머 아키텍처에 명시적으로 삽입하여 부족한 귀납적 편향을 제공했습니다. 이 세 가지 경로는 서로 다른 메커니즘을 사용하지만, 모두 자기-어텐션의 전역적 추론 능력과 컨볼루션의 효율적이고 계층적인 지역 특징 추출 능력을 결합하려는 동일한 목표를 향해 나아가고 있습니다. 이는 해당 분야가 이념적 순수성을 넘어 실용적인 고성능 엔지니어링으로 성숙하고 있음을 보여줍니다.

| 모델            | 토큰화 패러다임      | 핵심 혁신                 | 계산 복잡도 | 파라미터(M) | MACs(G) | ImageNet-1K Top-1 Acc(%) |
| --------------- | -------------------- | ------------------------- | ----------- | ----------- | ------- | ------------------------ |
| **ViT-Base/16** | 그리드 기반 패치     | 전역 자기-어텐션          | O(N2)       | 86          | 17.6    | 81.8 2                   |
| **T2T-ViT-14**  | 점진적 집계          | 소프트 분할 & 재귀적 집계 | O(N2)       | 21.5        | 5.2     | 81.5 36                  |
| **Swin-T**      | 계층적 윈도우        | 이동된 윈도우 어텐션      | O(N)        | 29          | 4.5     | 81.3 42                  |
| **sViT**        | 의미론적 (경계 상자) | 의미론적 무결성 토큰      | O(N2)       | -           | -       | - 1                      |
| **DHVT-Small**  | 하이브리드 (CNN+ViT) | 동적 하이브리드 구조      | -           | 22.8        | -       | 85.7 (CIFAR-100) 22      |

**표 3: 주요 고급 ViT 아키텍처의 분류 및 성능 비교**


ViT의 핵심 개념인 시각적 토큰화는 2D 이미지 분류를 넘어, 그 유연성과 강력한 표현력을 바탕으로 3D 비전, 비디오 분석, 멀티모달 학습 등 인공지능의 더 넓은 영역으로 확장되고 있습니다. 이 파트에서는 토큰화 개념이 어떻게 다른 데이터 모달리티에 맞게 변형되고 적용되는지, 그리고 이것이 AI 분야 전반에 미치는 영향을 탐구합니다.


포인트 클라우드나 복셀과 같은 3D 데이터는 2D 이미지와는 다른 고유한 특성을 가집니다. 포인트 클라우드는 순서가 없는 가변 크기의 집합이며, 복셀 그리드는 매우 희소(sparse)한 특성을 보입니다.46 이러한 특성은 새로운 토큰화 전략을 요구합니다.

- 포인트 클라우드 토큰화:

  가장 일반적인 접근 방식 중 하나는 포인트들을 지역적인 "포인트 패치"로 그룹화하고, mini-PointNet과 같은 소규모 네트워크를 사용하여 각 패치에 대한 임베딩을 생성하는 것입니다. 이 임베딩들이 바로 트랜스포머의 토큰 역할을 합니다.46 포인트 클라우드의 순서 없는 특성은 순서에 무관한 자기-어텐션 메커니즘과 자연스럽게 맞아떨어지지만, 트랜스포머에 고정된 크기의 입력 시퀀스를 제공하기 위해서는 최원점 샘플링(Farthest Point Sampling)과 같은 효율적인 샘플링 기법이 필수적입니다.46

- 복셀 토큰화:

  복셀 그리드는 3D 이미지처럼 취급될 수 있지만, 그 희소성 때문에 비효율적입니다. 따라서 많은 방법들이 비어있지 않은 복셀에만 집중하거나, "패치 어텐션 모듈(Patch Attention Modules)"과 같은 기술을 사용하여 입력 크기에 대한 선형 복잡도를 달성합니다.46 3D 비전 트랜스포머는 의미 분할, 객체 탐지, 형상 완성 등 다양한 태스크에 적용되고 있으며, 관련 연구 목록은 46 등에서 포괄적으로 다루어지고 있습니다.


비디오는 시간에 따른 차원을 추가하여 데이터의 차원성을 극적으로 높이고 계산 비용을 증가시킵니다.31 따라서 비디오 토큰화 전략은 공간과 시간을 모두 효율적으로 처리해야 합니다.

- 주요 토큰화 전략 31:

  - **패치 방식 (Patch-wise):** 비디오를 3D "큐브"(시공간 패치)로 분할하고, 이를 하나의 긴 시퀀스로 처리합니다.
  - **프레임 방식 (Frame-wise):** 2D CNN을 사용하여 각 프레임을 단일 토큰으로 임베딩한 후, 트랜스포머를 사용하여 이 프레임 토큰들 간의 시간적 관계를 모델링합니다.
  - **클립 방식 (Clip-wise):** 매우 긴 시간적 의존성을 모델링하기 위해 전체 비디오 클립을 단일 토큰으로 압축합니다.

- 비디오를 위한 효율적인 어텐션:

  긴 시퀀스를 처리하기 위해, 완전한 시공간 어텐션은 종종 분해됩니다. **분해된 어텐션(Factorized Attention)**은 일반적인 전략으로, 각 프레임 내에서 공간적 어텐션을 계산하고, 프레임 간에 시간적 어텐션을 계산하여 복잡도를 $O((S \cdot T)^2)$에서 $O(S^2 \cdot T + S \cdot T^2)$로 크게 줄입니다.31 TimeSformer와 ViViT와 같은 모델이 이 접근 방식의 대표적인 예입니다.48


1부에서 언급했듯이, '토큰'은 서로 다른 모달리티를 연결하는 공통 화폐 역할을 합니다. 이 섹션에서는 시각 토큰과 텍스트 토큰이 어떻게 융합되는지 더 자세히 살펴봅니다.

- 주요 융합 전략 17:
  - **초기 연결 (Early Concatenation / Co-attention):** 가장 간단한 접근 방식입니다. ViT에서 추출된 시각 토큰과 언어 모델에서 추출된 텍스트 토큰을 하나의 긴 시퀀스로 단순히 연결(concatenate)한 후, 하나의 큰 트랜스포머에 입력합니다. 이를 통해 모든 토큰이 모달리티에 상관없이 다른 모든 토큰에 어텐션을 수행할 수 있습니다.17
  - **교차 어텐션 (Cross-Attention):** ViLBERT와 같은 모델에서 사용되는 더 구조화된 접근 방식입니다. 비전과 언어를 위한 두 개의 개별 트랜스포머 스트림을 유지하면서, 한 스트림의 쿼리(Q) 벡터가 다른 스트림의 키(K) 및 값(V) 벡터에 어텐션을 수행하도록 하여 정보를 교환합니다. 이는 스트림을 완전히 병합하지 않고도 조건부 어텐션을 가능하게 합니다.17
  - **계층적 및 하이브리드 융합:** 더 복잡한 모델들은 풍부한 교차 모달 표현을 구축하기 위해 초기 및 후기 상호작용을 결합하는 다단계 융합 방식을 사용합니다.17 최근의 MLIF-Net은 교차 어텐션을 사용하여 ViT와 LLM의 특징을 융합하는 예시입니다.49

이러한 확장은 ViT의 토큰화 성공이 트랜스포머를 언어 특정 도구에서, 이산적이거나 연속적인 표현의 집합으로 구조화될 수 있는 모든 데이터에 대한 범용 계산 엔진으로 일반화했음을 보여줍니다. ViT는 공간 데이터(이미지)가 토큰화될 수 있음을 증명했고 2, 비디오 트랜스포머는 시공간 데이터의 토큰화를 31, 3D 트랜스포머는 순서 없는 포인트 집합과 희소 그리드의 토큰화를 가능하게 했습니다.46 더 나아가 멀티모달 트랜스포머는 이종 토큰 집합(예: 이미지 패치 + 단어)이 단일 아키텍처에서 원활하게 처리될 수 있음을 보여주었습니다.16 로보틱스 분야에서 연속적인 운동 동작을 토큰으로 이산화하려는 "행동 토큰화(action tokenization)" 연구 50는 이러한 경향의 정점을 보여줍니다.

이러한 흐름은 미래에 단일 범용 "파운데이션 모델" 아키텍처(아마도 트랜스포머)가 각 모달리티에 대한 전문화된 "토크나이저"를 통해 이미지, 텍스트, 비디오, 소리, 3D 스캔, 로봇 행동 등 방대한 종류의 데이터를 수용하고 추론할 수 있음을 시사합니다. 핵심 처리 엔진은 동일하게 유지되면서 입력단만 바뀌는 것입니다. 이는 보다 일반적인 인공지능을 향한 중요한 발걸음입니다.


본 보고서는 Vision Transformer의 시각적 토큰화가 단순한 전처리 기술을 넘어 컴퓨터 비전과 인공지능의 패러다임을 어떻게 바꾸었는지를 비판적으로 고찰했습니다. 이 마지막 파트에서는 지금까지의 논의를 종합하고, 시각적 토큰화 및 표현 학습의 미래 궤적을 전망합니다.


보고서의 서사를 요약하면, ViT의 등장은 '순수 트랜스포머'라는 급진적인 접근으로 시작되었으나, 곧 시각적 사전 지식의 부재라는 한계에 직면했습니다. 이후 T2T-ViT, Swin Transformer, 하이브리드 모델과 같은 혁신들은 이러한 사전 지식을 실용적으로 재통합하려는 노력의 산물입니다.

이 과정에서 "하나의 정답은 없다"는 결론에 도달합니다. 최적의 토큰화 전략은 특정 태스크, 데이터셋 크기, 그리고 계산 제약 조건에 따라 달라집니다.

- **바닐라 ViT:** 방대한 양의 데이터로 사전 훈련할 경우 여전히 강력한 성능을 발휘합니다.3
- **Swin Transformer:** 효율성과 계층적 특징 덕분에 범용 백본으로서 사실상의 표준(de-facto standard)으로 자리 잡았습니다.42
- **하이브리드 모델:** 데이터가 부족한 시나리오나 특정 전문 분야에서 탁월한 선택지입니다.25
- **의미론적 토큰화:** 개념적으로 유망하지만 아직 주류는 아니며, 세밀한 인식(fine-grained recognition) 태스크에서 잠재력을 가질 수 있습니다.1


미래는 정적이고 미리 정의된 토큰화를 넘어서는 방향으로 나아가고 있습니다.

- **적응형 토큰화 (Adaptive Tokenization):** A-ViT와 같은 모델은 네트워크를 통과하면서 정보량이 적은 토큰을 동적으로 버리도록 학습하여, 중요한 이미지 영역에 계산을 집중시킵니다. 이는 추론 비용을 입력의 복잡성에 따라 가변적으로 만듭니다.51 다른 방법들은 토큰 병합이나 가지치기(pruning)에 초점을 맞춥니다.52
- **모델의 일부로서의 토큰화:** Tokenformer 아키텍처는 모델의 파라미터(선형 레이어의 가중치) 자체를 학습 가능한 토큰으로 취급하는 급진적인 아이디어를 제안합니다. 이는 토큰-토큰 상호작용과 토큰-파라미터 상호작용을 단일 어텐션 메커니즘으로 통합하여 더 큰 유연성과 확장성을 제공할 수 있습니다.53
- **행동 토큰화 (Action Tokenization):** 로보틱스와 체화된 AI(embodied AI) 분야에서 "행동 토큰화"는 연속적인 행동 스트림을 이산적인 토큰으로 변환하여, 모델이 시퀀스 모델링 프레임워크 내에서 복잡한 행동을 학습, 예측, 생성할 수 있도록 하는 것을 목표로 합니다.50

이러한 발전에도 불구하고, 여전히 해결해야 할 중요한 연구 과제들이 남아 있습니다.

- **해석 가능성 (Interpretability):** 복잡한 계층적 또는 하이브리드 모델에서 개별 토큰과 어텐션 맵이 어떤 시각적 개념을 포착하고 있는지 어떻게 더 잘 이해할 수 있을까요? 12
- **진정한 스케일 불변성 (True Scale Invariance):** 위치 임베딩의 보간이나 고정된 패치 크기에 의존하지 않고, 이미지 해상도와 객체 스케일에 진정으로 불변하는 토크나이저를 어떻게 설계할 수 있을까요? 27
- **비지도/자기지도 토큰화 (Unsupervised/Self-Supervised Tokenization):** 미리 정의된 슈퍼픽셀이나 그리드를 넘어, 인간의 감독 없이 데이터로부터 직접 최적의 내용 인식 토큰화 전략을 학습할 수 있을까요? 55
- **궁극적인 통합:** 컨볼루션과 어텐션 기반 원리의 궁극적인 융합은 어떤 모습일까요? 컨볼루션과 자기-어텐션을 모두 대체하는 새로운 통합된 기본 연산(primitive)이 등장할 수 있을까요?

결론적으로, 시각적 토큰화는 ViT의 등장을 가능하게 한 핵심 기술에서 출발하여, 이제는 다양한 데이터 모달리티를 통합하고 인공지능의 범용성을 확장하는 근본적인 메커니즘으로 진화하고 있습니다. 고정된 그리드에서 의미론적, 계층적, 적응형 토큰화로 나아가는 여정은 모델이 세상을 더 효율적이고, 유연하며, 인간과 유사한 방식으로 인식하도록 만드는 과정 그 자체입니다. 앞으로의 연구는 이러한 미해결 과제들을 해결하며, 더욱 강력하고 일반화된 시각 표현 학습의 새로운 지평을 열어갈 것입니다.


1. Superpixel Tokenization for Vision Transformers: Preserving Semantic Integrity in Visual Tokens - arXiv, accessed July 16, 2025, https://arxiv.org/html/2412.04680v2
2. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, accessed July 16, 2025, https://research.google/pubs/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/
3. (PDF) An Image is Worth 16x16 Words: Transformers for Image ..., accessed July 16, 2025, https://scispace.com/papers/an-image-is-worth-16x16-words-transformers-for-image-v85s5ahlww
4. Paper page - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, accessed July 16, 2025, https://huggingface.co/papers/2010.11929
5. Tutorial 15: Vision Transformers - UvA DL Notebooks v1.2 documentation, accessed July 16, 2025, https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html
6. Vision Transformers (ViT) Explained - Pinecone, accessed July 16, 2025, https://www.pinecone.io/learn/series/image-search/vision-transformers/
7. Does the position of the tokens in Vision Transformer matter? - AI Stack Exchange, accessed July 16, 2025, https://ai.stackexchange.com/questions/38658/does-the-position-of-the-tokens-in-vision-transformer-matter
8. PATCH EMBEDDING | Vision Transformers explained - YouTube, accessed July 16, 2025, https://www.youtube.com/watch?v=lBicvB4iyYU
9. ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | Learning-Deep-Learning, accessed July 16, 2025, https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/vit.html
10. The Role of Position Embeddings in Transformers for Automatic Speech Recognition, accessed July 16, 2025, https://project-archive.inf.ed.ac.uk/msc/20226068/msc_proj.pdf
11. Maximizing the Position Embedding for Vision Transformers with Global Average Pooling, accessed July 16, 2025, https://arxiv.org/html/2502.02919v1
12. Vision Transformers Explained: The Future of Computer Vision? - Roboflow Blog, accessed July 16, 2025, https://blog.roboflow.com/vision-transformers/
13. Rotary Position Embedding for Vision Transformer, accessed July 16, 2025, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf
14. Class token in ViT and BERT - Data Science Stack Exchange, accessed July 16, 2025, https://datascience.stackexchange.com/questions/90649/class-token-in-vit-and-bert
15. Transformers in Vision: A Survey - arXiv, accessed July 16, 2025, http://arxiv.org/pdf/2101.01169
16. Multimodal Learning With Transformers: A Survey | by Eleventh Hour Enthusiast | Medium, accessed July 16, 2025, https://medium.com/@EleventhHourEnthusiast/multimodal-learning-with-transformers-a-survey-3b28b1dcaf03
17. (PDF) Multimodal Learning With Transformers: A Survey, accessed July 16, 2025, https://www.researchgate.net/publication/370688791_Multimodal_Learning_With_Transformers_A_Survey
18. A Layer-Wise Tokens-to-Token Transformer Network for Improved Historical Document Image Enhancement - arXiv, accessed July 16, 2025, https://arxiv.org/html/2312.03946v1
19. Vision Transformers with Natural Language Semantics - arXiv, accessed July 16, 2025, https://arxiv.org/html/2402.17863v1
20. CNNs vs Vision Transformers - Biological Computer Vision (3/3) | by Niranjan Rajesh | Bits and Neurons | Medium, accessed July 16, 2025, https://medium.com/bits-and-neurons/cnns-vs-vision-transformers-biological-computer-vision-3-3-56ff955ba463
21. Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review - MDPI, accessed July 16, 2025, https://www.mdpi.com/2076-3417/13/9/5521
22. Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets - NIPS, accessed July 16, 2025, https://proceedings.nips.cc/paper_files/paper/2022/file/5e0b46975d1bfe6030b1687b0ada1b85-Paper-Conference.pdf
23. Efficient Training of Visual Transformers with Small Datasets, accessed July 16, 2025, https://proceedings.neurips.cc/paper/2021/file/c81e155d85dae5430a8cee6f2242e82c-Paper.pdf
24. Visualization Comparison of Vision Transformers and ... - Rui SHI, accessed July 16, 2025, https://shirui-homepage.com/files/pdf/research/202310visViT-TMM.pdf
25. A survey of the Vision Transformers and their CNN-Transformer based Variants - arXiv, accessed July 16, 2025, https://arxiv.org/pdf/2305.09880
26. A Review of Transformer-Based Models for Computer Vision Tasks: Capturing Global Context and Spatial Relationships - arXiv, accessed July 16, 2025, https://arxiv.org/html/2408.15178v1
27. ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias - OpenReview, accessed July 16, 2025, https://openreview.net/pdf?id=_WnAQKse_uK
28. Are vision transformers scale invariant like CNNs? - AI Stack Exchange, accessed July 16, 2025, https://ai.stackexchange.com/questions/48381/are-vision-transformers-scale-invariant-like-cnns
29. Swin Transformers: The most powerful tool in Computer Vision | by Sieun Park | Medium, accessed July 16, 2025, https://sieunpark77.medium.com/swin-transformers-the-most-powerful-tool-in-computer-vision-659f78744871
30. arXiv:2308.09372v4 [cs.CV] 24 Feb 2025, accessed July 16, 2025, https://arxiv.org/pdf/2308.09372
31. Video Transformers: A Survey - arXiv, accessed July 16, 2025, http://arxiv.org/pdf/2201.05991
32. Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers - arXiv, accessed July 16, 2025, https://arxiv.org/html/2308.09372v2
33. ViT: Transformers began to conquer the computer vision field | by Dong-Keon Kim | Medium, accessed July 16, 2025, https://medium.com/@kdk199604/vit-transformers-began-to-conquer-the-computer-vision-field-777f3602136e
34. MicroViT: A Vision Transformer with Low Complexity Self Attention for Edge Device - arXiv, accessed July 16, 2025, https://arxiv.org/html/2502.05800v1
35. ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages - arXiv, accessed July 16, 2025, https://arxiv.org/html/2504.14825v1
36. [2101.11986] Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, accessed July 16, 2025, https://arxiv.org/abs/2101.11986
37. Tokens-to-token vit: Training vision transformers from scratch on imagenet - Zihang Jiang, accessed July 16, 2025, https://zihangjiang.github.io/publication/2021-01-28-t2t-vit
38. How T2T-ViT Enhances ViTs: An In-Depth Guide with PyTorch | by Övül Arslan | Medium, accessed July 16, 2025, https://medium.com/@ovularslan/how-t2t-vit-enhances-vits-an-in-depth-guide-with-pytorch-d127dd523c0a
39. Tokens-to-Token Vision Transformers, Explained - Towards Data Science, accessed July 16, 2025, https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa/
40. Review - Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, accessed July 16, 2025, https://sh-tsang.medium.com/review-tokens-to-token-vit-training-vision-transformers-from-scratch-on-imagenet-8b318325ad0d
41. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - Medium, accessed July 16, 2025, https://medium.com/@crlc112358/swin-transformer-hierarchical-vision-transformer-using-shifted-windows-ca1ccc8760b8
42. Swin Transformer: Hierarchical Vision Transformer using Shifted ..., accessed July 16, 2025, https://arxiv.org/pdf/2103.14030
43. Cumulative Spatial Knowledge Distillation for Vision Transformers - ICCV 2023 Open Access Repository, accessed July 16, 2025, https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Cumulative_Spatial_Knowledge_Distillation_for_Vision_Transformers_ICCV_2023_paper.html
44. EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote Sensing Image Change Detection - arXiv, accessed July 16, 2025, https://arxiv.org/html/2501.01238v1
45. BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation, accessed July 16, 2025, https://arxiv.org/html/2402.08793v1
46. 3D Vision with Transformers: A Survey - arXiv, accessed July 16, 2025, https://arxiv.org/pdf/2208.04309
47. lahoud/3d-vision-transformers: A list of 3D computer vision papers with Transformers - GitHub, accessed July 16, 2025, https://github.com/lahoud/3d-vision-transformers
48. Video Understanding with Large Language Models: A Survey - arXiv, accessed July 16, 2025, https://arxiv.org/html/2312.17432v5
49. MLIF-Net: Multimodal Fusion of Vision Transformers and Large Language Models for AI Image Detection - Preprints.org, accessed July 16, 2025, https://www.preprints.org/manuscript/202505.2370/v1
50. A Survey on Vision-Language-Action Models: An Action ..., accessed July 16, 2025, https://www.aimodels.fyi/papers/arxiv/survey-vision-language-action-models-action-tokenization
51. A-ViT: Adaptive Tokens for Efficient Vision Transformer - CVF Open Access, accessed July 16, 2025, https://openaccess.thecvf.com/content/CVPR2022/papers/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.pdf
52. A Unified and Training-Free Token Compression Framework for Vision Transformer Acceleration - arXiv, accessed July 16, 2025, https://arxiv.org/html/2506.05709v1
53. tokenformer: rethinking transformer scal - arXiv, accessed July 16, 2025, [https://arxiv.org/pdf/2410.23168?](https://arxiv.org/pdf/2410.23168)
54. Paper page - ViTAR: Vision Transformer with Any Resolution - Hugging Face, accessed July 16, 2025, https://huggingface.co/papers/2403.18361
55. A Survey of the Self Supervised Learning Mechanisms for Vision Transformers - arXiv, accessed July 16, 2025, https://arxiv.org/html/2408.17059v5

