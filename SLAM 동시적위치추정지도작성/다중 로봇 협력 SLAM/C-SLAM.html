<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:다중 로봇 협력 SLAM(C-SLAM)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>다중 로봇 협력 SLAM(C-SLAM)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">SLAM (Simultaneous Localization and Mapping)</a> / <a href="index.html">다중 로봇 협력 SLAM(C-SLAM)</a> / <span>다중 로봇 협력 SLAM(C-SLAM)</span></nav>
                </div>
            </header>
            <article>
                <h1>다중 로봇 협력 SLAM(C-SLAM)</h1>
<h2>1.  단일 로봇에서 다중 로봇 SLAM으로의 전환</h2>
<h3>1.1  SLAM의 근본적인 문제 정의</h3>
<p>동시적 위치 추정 및 지도 작성(Simultaneous Localization and Mapping, SLAM)은 로봇 공학의 가장 근본적인 문제 중 하나로, 미지의 환경에 대한 지도를 구축하거나 갱신하는 동시에, 그 환경 내에서 로봇 자신의 위치를 지속적으로 추적하는 계산적 과제를 의미한다.1 이 문제는 본질적으로 순환적인 의존성을 내포하고 있다. 즉, 정확한 지도가 있어야 로봇의 위치를 정확하게 추정할 수 있으며, 반대로 로봇의 위치를 정확하게 알아야 일관성 있는 지도를 작성할 수 있다. 이 때문에 SLAM은 종종 “닭이 먼저냐, 달걀이 먼저냐“의 문제에 비유되곤 한다. 로봇은 아무런 사전 정보 없이 미지의 공간에 놓였을 때, 자신의 움직임과 센서 관측만을 바탕으로 이 두 가지 과업을 동시에 해결해야 한다.</p>
<p>SLAM 시스템의 작동 원리는 크게 두 가지 핵심 구성 요소로 나눌 수 있다. 첫째는 ‘환경 관측(Range Measurement)’ 단계로, 로봇에 탑재된 다양한 센서를 통해 주변 환경에 대한 원시 데이터를 수집한다. 여기에 사용되는 센서는 매우 다양하며, 저렴하고 풍부한 정보를 제공하는 카메라(단안, 스테레오, RGB-D), 정밀한 3차원 거리 정보를 제공하는 LiDAR(Light Detection and Ranging), 수중이나 특정 환경에서 유용한 소나(Sonar), 그리고 로봇의 움직임을 추정하는 데 도움을 주는 관성 측정 장치(Inertial Measurement Unit, IMU) 등이 대표적이다.3 둘째는 ‘데이터 해석 및 지도 생성(Data Extraction)’ 단계로, 수집된 원시 데이터를 처리하여 환경 내의 구별 가능한 특징, 즉 랜드마크(Landmark)를 식별하고, 이 랜드마크들의 상대적인 위치 관계를 파악하여 지도를 생성한다.2 이 지도는 기하학적 정확성을 중시하는 메트릭 맵(Metric Map, 예: 특징점 맵, 점유 격자 맵)이 될 수도 있고, 장소 간의 연결 관계에 초점을 맞춘 위상 맵(Topological Map)이 될 수도 있다.1 이 두 구성 요소는 끊임없이 상호작용하며, 로봇이 움직이고 새로운 관측을 할 때마다 지도와 로봇의 위치 추정치를 함께 갱신해 나간다.</p>
<h3>1.2  단일 로봇 SLAM의 한계</h3>
<p>단일 로봇 SLAM 기술은 지난 수십 년간 괄목할 만한 발전을 이루었지만, 실제 세계의 복잡하고 넓은 환경에 적용될 때 몇 가지 근본적인 한계에 직면한다.</p>
<p>첫째, <strong>누적 오차(Error Accumulation)</strong> 문제가 가장 심각하다. 로봇의 움직임 추정(Odometry)과 센서 측정에는 필연적으로 작은 오차가 포함되어 있다. 단일 로봇은 이동 거리가 길어질수록 이 작은 오차들이 계속해서 쌓여 전체 궤적과 지도의 정확도를 심각하게 저하시키는 드리프트(drift) 현상을 겪게 된다.6 이러한 누적 오차를 보정하는 가장 효과적인 방법은 ’루프 폐쇄(Loop Closure)’이다. 이는 로봇이 이전에 방문했던 장소를 다시 인식하고, 현재 위치와 과거 위치 사이의 제약 조건을 생성하여 그동안 쌓인 오차를 전체 궤적에 분배하여 수정하는 과정이다.2 그러나 대규모 환경에서는 로봇이 의미 있는 루프를 형성하기까지 매우 오랜 시간이 걸리거나, 아예 루프를 형성하지 못할 수도 있다. 이 경우, 누적 오차는 보정되지 못한 채 계속 증가하여 지도의 일관성을 파괴할 수 있다.</p>
<p>둘째, **탐사 효율성 및 시간(Exploration Efficiency and Time)**의 한계다. 단 하나의 로봇이 광활한 지역을 탐사하고 지도를 작성하는 데는 막대한 시간이 소요된다.8 이는 재난 현장 수색, 대규모 물류 창고 관리, 행성 탐사와 같이 신속한 상황 파악과 지도 생성이 필수적인 응용 분야에서는 치명적인 단점으로 작용한다.9</p>
<p>셋째, <strong>강건성 및 실패 위험(Robustness and Risk of Failure)</strong> 문제다. 단일 로봇 시스템은 그 로봇 자체의 하드웨어 고장, 특정 지역에서의 센서 데이터 품질 저하로 인한 측위 실패(Localization Failure), 또는 움직이는 물체가 많은 동적 환경이나 특징이 부족한 복도와 같은 도전적인 환경에서의 성능 저하에 매우 취약하다.10 하나의 로봇에 문제가 생기면 전체 임무가 중단되거나 실패로 돌아갈 위험이 크다.</p>
<h3>1.3  C-SLAM의 등장과 핵심 이점</h3>
<p>이러한 단일 로봇 SLAM의 한계를 극복하기 위한 자연스러운 해법으로 협력적 동시적 위치 추정 및 지도 작성(Collaborative SLAM, C-SLAM)이 등장했다. C-SLAM은 여러 대의 로봇이 서로 협력하여 공동의 목표, 즉 하나의 통합된 전역 지도를 생성하고 그 안에서 각자의 위치를 추정하는 기술이다.13 C-SLAM은 단일 로봇 시스템에 비해 다음과 같은 명확하고 강력한 이점을 제공한다.</p>
<p>첫째, **효율성과 확장성(Efficiency and Scalability)**이 비약적으로 향상된다. 여러 로봇이 동시에 환경의 서로 다른 구역을 탐사함으로써 전체 지도 작성 시간을 획기적으로 단축할 수 있다.8 한 연구에서는 C-SLAM 시스템이 단일 로봇 시스템에 비해 평균적으로 약 40%의 성능 향상을 보였다고 보고했다.9 이는 단순히 작업을 병렬로 처리하는 것 이상의 의미를 갖는다.</p>
<p>둘째, **강건성과 신뢰성(Robustness and Reliability)**이 크게 증대된다. C-SLAM의 가장 강력한 장점 중 하나는 로봇 간 상호 관측을 통한 오차 보정 능력에 있다. 한 로봇이 다른 로봇을 직접 관측하거나, 두 로봇이 같은 장소를 서로 다른 시간에 방문하여 공통의 랜드마크를 인식하는 ’로봇 간 루프 폐쇄(Inter-robot loop closure)’는 개별 로봇의 누적 오차를 훨씬 더 빠르고 효과적으로 보정하는 강력한 제약 조건으로 작용한다.15 또한, 일부 로봇이 임무 수행 중 고장 나거나 손실되더라도, 나머지 로봇들이 수집한 정보를 바탕으로 임무를 계속 수행할 수 있어 전체 시스템의 내결함성(fault tolerance)이 매우 높아진다.9</p>
<p>셋째, **향상된 정확도(Enhanced Accuracy)**를 달성할 수 있다. 여러 로봇이 다양한 시점에서 수집한 풍부한 데이터를 융합함으로써, 단일 로봇이 생성하는 지도보다 훨씬 더 정확하고, 완전하며, 중복성이 제거된 고품질의 전역 지도를 생성할 수 있다.</p>
<p>이러한 이점들은 C-SLAM이 단순한 병렬 처리 시스템을 넘어서는 가치를 지니게 한다. 단일 로봇 SLAM의 핵심 문제가 누적 오차와 탐사 시간이라면, 가장 단순한 해결책은 여러 로봇을 투입하여 작업을 나누어 수행하는 것이다. 그러나 C-SLAM의 진정한 힘은 ‘협력’ 그 자체에서 나온다. 로봇 간의 상호 관측은 각 로봇의 독립적인 궤적 그래프를 하나의 거대한 네트워크로 연결하는 강력한 제약 조건으로 작용한다. 이러한 상호 제약 조건은 단일 로봇이 자신의 과거로 돌아가야만 얻을 수 있는 루프 폐쇄보다 훨씬 더 자주, 그리고 훨씬 더 넓은 공간적 범위에 걸쳐 발생할 수 있다. 결과적으로 로봇의 수가 증가함에 따라, 이러한 상호 제약 조건의 수는 잠재적으로 기하급수적으로 증가할 수 있다. 이는 단순히 개별 로봇의 오차를 보정하는 수준을 넘어, 시스템에 참여한 모든 로봇의 궤적과 전체 지도를 하나의 일관된 최적화 프레임워크 내에서 동시에 조정하여 전역적인 정확도와 강건성을 비선형적으로 향상시키는 ’네트워크 효과’를 창출한다. 따라서 C-SLAM은 N개의 로봇이 1/N의 시간으로 작업을 완료하는 것을 넘어, 로봇 간의 상호작용을 통해 시스템 전체의 성능을 질적으로 향상시키는 새로운 패러다임이라 할 수 있다.</p>
<h2>2.  C-SLAM 아키텍처: 중앙집중형 대 분산형</h2>
<p>C-SLAM 시스템을 구현하는 방식은 크게 중앙집중형(Centralized)과 분산형(Decentralized) 아키텍처로 나뉜다. 이 두 접근 방식은 데이터 처리, 통신, 시스템 관리 방식에서 근본적인 차이를 보이며, 각각 뚜렷한 장단점을 가진다. 아키텍처의 선택은 시스템의 확장성, 강건성, 그리고 적용 환경에 지대한 영향을 미친다.</p>
<h3>2.1  중앙집중형 아키텍처</h3>
<p>중앙집중형 아키텍처는 전통적인 클라이언트-서버 모델을 따른다.17 이 구조에서 다수의 로봇(에이전트 또는 클라이언트)들은 각자 자신의 프론트엔드(front-end)를 실행하여 센서 데이터를 처리하고, 이로부터 얻은 추상화된 정보를 중앙 서버(base station)로 전송한다. 중앙 서버는 시스템의 ‘두뇌’ 역할을 하며, 모든 로봇으로부터 수신한 데이터를 취합하여 전역 지도 최적화, 로봇 간 루프 폐쇄 검출, 여러 지도의 병합 등 계산적으로 매우 복잡하고 집약적인 백엔드(back-end) 작업을 전담한다.13</p>
<p>장점:</p>
<p>중앙집중형 구조의 가장 큰 장점은 자원이 제한적인 로봇에 유리하다는 점이다. 로봇은 상대적으로 가벼운 실시간 작업, 예를 들어 시각 주행 거리계(Visual Odometry) 계산과 같은 프론트엔드 처리만 수행하고, 무거운 최적화 계산은 고성능 중앙 서버에 오프로드(offload)할 수 있다.18 이는 컴퓨팅 성능, 메모리, 배터리 용량이 제한적인 소형 드론(UAV)이나 저전력 로봇을 C-SLAM 시스템에 통합하는 것을 가능하게 한다.20 또한, 중앙 서버가 모든 로봇의 정보(전역 뷰)를 가지고 있기 때문에, <strong>전역적으로 일관된 최적의 해를 구하기가 비교적 용이</strong>하다. 데이터 관리와 작업 스케줄링이 중앙에서 통합적으로 이루어지므로 시스템의 조정이 단순화된다.17</p>
<p>단점:</p>
<p>반면, 중앙집중형 구조는 몇 가지 치명적인 단점을 내포한다. 가장 큰 문제는 **단일 실패 지점(Single Point of Failure)**의 존재다. 만약 중앙 서버가 네트워크 문제나 하드웨어 고장으로 다운되면, 전체 협력 시스템이 마비된다.13 비록 개별 로봇이 독립적으로 자신의 위치를 추적하는 자율성은 유지될 수 있으나18, 로봇 간의 협력을 통한 지도 병합 및 오차 보정은 불가능해진다. 두 번째로, <strong>통신 병목 현상과 확장성 한계</strong>가 심각하다. 시스템에 참여하는 로봇의 수가 증가함에 따라, 중앙 서버로 향하는 통신 트래픽이 선형적으로 증가하여 결국 통신 대역폭의 한계에 부딪히게 된다.13 이는 수십, 수백 대의 로봇으로 구성된 대규모 로봇 팀(swarm)으로 시스템을 확장하는 것을 매우 어렵게 만든다.15 마지막으로, 이 아키텍처는 <strong>안정적인 통신 링크를 요구</strong>한다. 지하, 수중, 재난 현장, 또는 넓은 실외와 같이 통신 인프라가 불안정하거나 존재하지 않는 환경에서는 로봇들이 중앙 서버와 지속적으로 안정적인 연결을 유지하기가 현실적으로 어렵다.15</p>
<h3>2.2  분산형 아키텍처</h3>
<p>분산형 아키텍처는 중앙 서버라는 개념 없이, 시스템에 참여하는 모든 로봇이 동등한 역할을 수행하며 P2P(Peer-to-Peer) 방식으로 직접 통신하는 구조다.17 각 로봇은 자신의 지역 지도(local map)와 궤적을 독립적으로 유지 및 관리한다. 그러다 통신 범위 내에 들어온 다른 로봇과 정보를 교환하여 서로의 상대 위치를 파악하고, 각자의 지역 지도를 병합하며, 분산된 합의(consensus) 알고리즘을 통해 전역적으로 일관된 지도와 궤적을 점진적으로 구축해 나간다.13</p>
<p>장점:</p>
<p>분산형 구조의 가장 큰 미덕은 **뛰어난 강건성과 내결함성(Robustness and Fault Tolerance)**이다. 중앙 서버라는 단일 실패 지점이 없기 때문에, 일부 로봇이 고장나거나 특정 로봇 간의 통신 링크가 끊어지더라도 시스템 전체는 큰 영향 없이 계속해서 작동할 수 있다.9 이는 예측 불가능하고 위험한 환경에서 임무를 수행해야 하는 로봇 스웜(robot swarm)의 핵심 철학과 완벽하게 부합한다.22 또한, 통신이 중앙 서버가 아닌 이웃 로봇과 지역적으로(locally) 이루어지므로, 로봇의 수가 증가해도 전체 시스템의 통신 부하가 폭발적으로 증가하지 않는다. 이는 **탁월한 확장성(High Scalability)**을 보장하며, 대규모 로봇 스웜을 운용하는 데 매우 적합하다.13 마지막으로, 외부 통신 인프라에 대한 의존도가 낮아 <strong>인프라 독립성</strong>을 가지며, 다양한 환경에 매우 유연하게 적용될 수 있다.16</p>
<p>단점:</p>
<p>분산형 구조는 그 유연성만큼이나 높은 복잡성을 수반한다. 전역적인 정보를 가진 중앙 조정자 없이, 각 로봇이 지역적인 정보만을 바탕으로 전역적인 일관성을 유지하기 위한 분산 합의 및 최적화 알고리즘이 매우 복잡하다. 각 로봇은 정교한 데이터 관리 및 북키핑(bookkeeping) 전략을 필요로 한다.15 또한, 중앙 서버가 수행하던 계산의 일부를 각 로봇이 분담해야 하므로, 중앙집중형 시스템에 비해 <strong>더 많은 온보드 계산 자원을 요구</strong>한다.15 이로 인해 저사양 로봇에는 부담이 될 수 있다. 마지막으로, 전역 최적화 과정에서 모든 정보를 한 번에 고려하는 것이 아니기 때문에, 최적의 해로 수렴하는 속도가 상대적으로 느리거나, 경우에 따라 전역 최적해(globally optimal solution)가 아닌 차선책(sub-optimal solution)에 머무를 가능성도 존재한다.</p>
<p>결론적으로, 중앙집중형과 분산형 아키텍처의 선택은 단순히 기술적인 문제를 넘어, 시스템 설계의 근본적인 철학, 즉 ’통제’와 ‘자율성’ 사이의 트레이드오프를 반영한다.21 중앙집중형은 전역 최적화의 ’효율성’과 ’통제’를 우선시하는 반면, 분산형은 시스템의 ’강건성’과 ’자율성’을 최우선 가치로 둔다.</p>
<p>그러나 실제 시스템에서 이 두 가지 아키텍처가 순수한 형태로 존재하는 경우는 드물다. 많은 시스템들이 두 패러다임의 장점을 결합한 하이브리드 형태로 구현된다.24 예를 들어, 대표적인 중앙집중형 시스템인 CCM-SLAM에서도, 에이전트 로봇은 중앙 서버와의 통신이 끊기더라도 독립적으로 자신의 항법을 계속 수행할 수 있는 ’자율성’을 보장받는다.18 이는 협력 기능만 중앙에 의존할 뿐, 핵심 기능은 분산되어 있는 하이브리드적 특성을 보여준다. 반대로, 분산형 시스템에서도 완전한 무질서 상태로 작동하는 것이 아니라, 분산된 시스템 내에 ’구조’와 ’질서’를 부여하려는 노력이 이루어진다. 예를 들어, 분산 포즈 그래프 최적화(PGO)를 수행하기 위해 임시로 리더 로봇을 선출하는 방식이 제안되기도 하고 16, 통신 부하를 줄이기 위해 로봇 간 루프 폐쇄 정보 교환의 우선순위를 정하는 정교한 기법이 도입되기도 한다.13</p>
<p>따라서 C-SLAM 아키텍처 논의의 핵심은 이분법적인 선택이 아니라, 주어진 임무의 특성(예: 소규모 실내 탐사 vs. 광역 재난 현장 수색), 로봇의 하드웨어 제약(예: 저사양 드론 vs. 고성능 지상 로봇), 그리고 통신 환경의 가용성(예: 안정적인 Wi-Fi vs. 불안정한 ad-hoc 네트워크)에 맞춰 두 패러다임의 요소를 어떻게 전략적으로 융합하고 최적의 균형점을 찾을 것인가에 대한 ’설계 철학’의 문제로 귀결된다.</p>
<p>다음 표는 중앙집중형과 분산형 C-SLAM 아키텍처의 주요 특징을 요약하여 비교한다.</p>
<table><thead><tr><th>특성 (Feature)</th><th>중앙집중형 (Centralized)</th><th>분산형 (Decentralized)</th></tr></thead><tbody>
<tr><td><strong>기본 구조</strong></td><td>클라이언트-서버 모델: 다수의 에이전트와 하나의 중앙 서버 17</td><td>P2P 네트워크: 중앙 서버 없이 에이전트 간 직접 통신 21</td></tr>
<tr><td><strong>주요 장점</strong></td><td>전역 최적화 용이, 에이전트의 계산 부하 경감 17</td><td>높은 강건성, 뛰어난 확장성, 인프라 독립성 16</td></tr>
<tr><td><strong>주요 단점</strong></td><td>단일 실패 지점, 통신 병목, 확장성 제한 13</td><td>분산 조정의 복잡성, 높은 온보드 계산 부하 15</td></tr>
<tr><td><strong>확장성</strong></td><td>제한적: 서버의 성능과 네트워크 대역폭에 크게 의존 15</td><td>높음: 통신이 지역적으로 이루어져 로봇 수 증가에 강함 13</td></tr>
<tr><td><strong>내결함성</strong></td><td>낮음: 중앙 서버 실패 시 협력 기능 전체가 마비됨 16</td><td>높음: 개별 노드(로봇)의 실패가 전체 시스템에 미치는 영향이 적음 9</td></tr>
<tr><td><strong>통신 요구사항</strong></td><td>모든 에이전트와 서버 간의 안정적이고 지속적인 연결 필요 15</td><td>에이전트 간의 간헐적이고 지역적인 P2P 연결만으로도 작동 가능 13</td></tr>
<tr><td><strong>적합한 애플리케이션</strong></td><td>소규모 로봇 팀, 통신이 보장된 실내 환경, 자원이 제한된 로봇 운용 18</td><td>대규모 로봇 스웜, GPS 거부 환경(지하, 수중), 재난 현장 수색 15</td></tr>
</tbody></table>
<h2>3.  C-SLAM 시스템의 구성 요소: 프론트엔드와 백엔드</h2>
<p>SLAM 시스템, 특히 C-SLAM 시스템의 복잡한 구조는 일반적으로 두 개의 주요 논리적 블록, 즉 프론트엔드(Front-end)와 백엔드(Back-end)로 나뉘어 설명된다.25 이 두 구성 요소는 각각 뚜렷하면서도 상호 보완적인 역할을 수행하며, 이들의 명확한 분리는 C-SLAM 시스템의 모듈성과 유연성을 보장하는 핵심 설계 원칙이다.</p>
<h3>3.1  프론트엔드: 실시간 처리와 데이터 추상화</h3>
<p>프론트엔드는 SLAM 시스템의 ’감각 기관’이자 ‘실시간 반응’ 부분을 담당한다. 주요 역할은 로봇의 센서로부터 끊임없이 들어오는 원시 데이터(raw data)를 실시간으로 처리하여, 백엔드가 이해하고 처리할 수 있는 추상화된 데이터 형태로 변환하는 것이다.25 프론트엔드의 모든 연산은 속도가 매우 중요하며, 로봇의 움직임에 맞춰 지연 없이 수행되어야 한다.</p>
<p>프론트엔드의 주요 기능은 다음과 같다:</p>
<ul>
<li><strong>특징점 추출 및 추적 (Feature Extraction and Tracking):</strong> 시각 SLAM(Visual SLAM)의 경우, 카메라 이미지에서 코너나 블롭(blob)과 같이 반복적으로 검출 가능한 독특한 지점, 즉 특징점(feature point)을 추출한다. ORB, SIFT, SURF와 같은 알고리즘이 이를 위해 널리 사용된다.28 LiDAR SLAM의 경우, 포인트 클라우드에서 평면이나 모서리와 같은 기하학적 특징을 추출한다. 일단 특징이 추출되면, 연속된 센서 데이터 프레임 간에 동일한 특징을 추적하여 그 움직임을 파악한다.</li>
<li><strong>시각 주행 거리계 (Visual Odometry, VO):</strong> 연속된 프레임 간의 특징점 움직임을 분석하여 로봇의 상대적인 이동, 즉 포즈(위치 및 방향) 변화를 추정하는 과정이다.25 이는 외부 도움 없이 로봇의 이동 경로를 추정하는 ‘주행 거리계’ 역할을 하며, 드리프트가 누적되는 지역적인 궤적(local trajectory)을 생성한다. 이 과정은 백엔드의 전역 최적화가 이루어지기 전까지 로봇의 실시간 위치를 제공하는 데 매우 중요하다.18</li>
<li><strong>데이터 연관 (Data Association):</strong> 현재 관측된 특징점이 이전에 관측된 랜드마크 중 어느 것과 동일한지를 판단하는, SLAM에서 가장 어렵고 중요한 문제 중 하나다.30 데이터 연관은 두 가지 수준에서 이루어진다. ’단기적(short-term) 데이터 연관’은 연속된 프레임 간의 특징을 연결하여 VO를 계산하는 데 사용되며, ’장기적(long-term) 데이터 연관’은 현재의 관측이 아주 오래 전의 관측과 일치하는지를 판단하여 루프 폐쇄를 찾아내는 데 사용된다.27</li>
<li><strong>키프레임 생성 (Keyframe Generation):</strong> 센서에서 들어오는 모든 데이터 프레임을 처리하는 것은 계산적으로 비효율적이다. 따라서 프론트엔드는 이전 키프레임에 비해 충분한 움직임 변화나 새로운 정보가 관측되었을 때만 현재 프레임을 ’키프레임(keyframe)’으로 선별한다.13 이렇게 선별된 키프레임들과 그에 연관된 정보(포즈, 특징점 등)만이 백엔드로 전달되어, 전체 계산량을 크게 줄이면서도 중요한 정보는 유지할 수 있게 한다.</li>
</ul>
<h3>3.2  백엔드: 전역 최적화와 일관성 유지</h3>
<p>백엔드는 SLAM 시스템의 ’기억’과 ‘추론’ 부분을 담당한다. 프론트엔드에서 전달받은 추상화된 데이터(키프레임, 랜드마크 관측, 상대 포즈 제약 등)를 기반으로, 시간에 걸쳐 누적된 모든 정보를 통합하여 전역적으로 가장 일관성 있는 지도와 궤적을 추정하는 역할을 한다.6 백엔드 처리는 일반적으로 실시간으로 이루어질 필요는 없으며, 별도의 스레드에서 주기적으로 또는 특정 이벤트(예: 루프 폐쇄 검출)가 발생했을 때 수행된다.</p>
<p>백엔드의 주요 기능은 다음과 같다:</p>
<ul>
<li><strong>포즈 그래프 최적화 (Pose Graph Optimization, PGO):</strong> 백엔드의 핵심 작업으로, 로봇의 궤적과 지도 전체의 일관성을 맞추는 과정이다. 로봇의 각 키프레임 포즈를 그래프의 노드(node)로, 포즈 간의 상대적인 움직임(VO 측정값)이나 루프 폐쇄로 인한 제약을 엣지(edge)로 표현한 ’포즈 그래프’를 구축한다.6 이 그래프에서 엣지(제약)들은 측정 오차로 인해 서로 모순되는 경우가 많다. PGO는 이 모든 모순을 최소화하는, 즉 모든 제약 조건을 전역적으로 가장 잘 만족시키는 노드(포즈)들의 배치를 찾는 최적화 문제를 푼다.31</li>
<li><strong>루프 폐쇄 검출 (Loop Closure Detection):</strong> 로봇이 이전에 방문했던 장소를 다시 방문했는지를 인식하는 과정이다. 이는 누적된 드리프트를 극적으로 줄일 수 있는 가장 중요한 기회다.2 이를 위해 Bag-of-Words(BoW)와 같은 기법을 사용하여 현재 키프레임의 시각적 특징을 간결한 벡터로 표현하고, 이를 데이터베이스에 저장된 과거 키프레임들의 벡터와 비교하여 유사도가 높은 후보를 찾는다.13 후보가 발견되면 기하학적 검증을 통해 최종적으로 루프 폐쇄를 확정한다.</li>
<li><strong>전역 번들 조정 (Global Bundle Adjustment, BA):</strong> 시각 SLAM에서 가장 정확하지만 계산 비용이 가장 높은 최적화 기법이다. PGO가 포즈 간의 관계만을 최적화하는 반면, BA는 시스템의 모든 변수, 즉 모든 로봇의 포즈와 모든 3D 랜드마크의 위치를 동시에 최적화하여, 3D 랜드마크를 각 포즈의 카메라 평면에 재투영했을 때의 오차(reprojection error)를 최소화한다.6 C-SLAM에서는 이 작업이 중앙 서버에서 수행되는 경우가 많다.18</li>
<li><strong>맵 관리 및 병합 (Map Management and Merging):</strong> C-SLAM의 백엔드에는 단일 로봇 SLAM에는 없는 추가적인 기능이 요구된다. 바로 여러 로봇이 각자 생성한 지역 지도(local map)를 하나의 일관된 전역 지도(global map)로 병합하고, 중복되는 랜드마크나 키프레임을 제거하며 전체 맵 데이터를 효율적으로 관리하는 기능이다.18</li>
</ul>
<p>이처럼 프론트엔드와 백엔드를 명확히 분리하는 설계는 C-SLAM 시스템의 아키텍처 유연성을 가능하게 하는 핵심적인 역할을 한다. 프론트엔드는 본질적으로 센서에 종속적이다. 시각 SLAM의 프론트엔드는 카메라 데이터 처리에, LiDAR SLAM의 프론트엔드는 포인트 클라우드 처리에 특화되어 있다.6 반면, 백엔드는 센서에 독립적(sensor-agnostic)이다.6 백엔드가 다루는 것은 센서의 종류와 무관하게 ’포즈’와 ’제약’이라는 추상화된 그래프 구조이기 때문이다.</p>
<p>이러한 분리 덕분에 다양한 C-SLAM 아키텍처가 가능해진다. 중앙집중형 시스템에서는 각 로봇이 자신의 센서에 맞는 프론트엔드를 ’온보드’에서 실시간으로 실행하고, 그 결과로 나온 추상화된 데이터(키프레임, 지역 포즈 그래프 등)만을 중앙 서버로 전송한다.18 중앙 서버는 이렇게 수집된 데이터를 ’포즈 그래프’라는 공통된 형태로 받아 통합하고, 강력한 계산 자원을 이용해 전역 최적화라는 백엔드 작업을 수행한다. 이 구조는 서로 다른 센서를 장착한 이기종(heterogeneous) 로봇 팀을 지원하는 데 매우 효과적이다.15 분산형 시스템에서도 이 분리 원칙은 동일하게 적용된다. 각 로봇은 자신의 프론트엔드를 독립적으로 실행하고, 다른 로봇과는 백엔드 수준의 정보, 즉 포즈 그래프의 일부나 루프 폐쇄 후보 정보 등을 교환하여 분산된 방식으로 최적화를 수행한다.13</p>
<p>결론적으로, 프론트엔드(실시간, 지역적, 센서 종속적)와 백엔드(비실시간, 전역적, 센서 독립적)의 명확한 역할 분리는 C-SLAM 시스템이 중앙집중형, 분산형, 또는 이 둘을 혼합한 하이브리드형 등 다양한 아키텍처를 유연하게 채택하고, 이기종 로봇으로 구성된 팀을 효과적으로 운용할 수 있게 만드는 근본적인 설계 원리라고 할 수 있다.</p>
<h2>4.  그래프 기반 SLAM의 수학적 원리</h2>
<p>그래프 기반 SLAM은 현대 SLAM 기술의 주류를 이루는 접근법으로, 복잡한 SLAM 문제를 직관적인 그래프 구조를 통해 표현하고 최적화 이론을 바탕으로 해결한다. 이 접근법의 핵심은 SLAM 문제를 모든 센서 측정값과 가장 일관성이 높은 로봇의 궤적 및 지도 상태를 찾는 비선형 최소제곱(Non-linear Least Squares) 최적화 문제로 공식화하는 것이다.31</p>
<h3>4.1  문제 정의: 비선형 최소제곱 최적화</h3>
<p>그래프 기반 SLAM에서, 시스템의 상태(로봇의 포즈, 랜드마크의 위치 등)는 그래프의 노드(node)로 표현되고, 이 상태들 사이의 관계를 나타내는 센서 측정값은 노드들을 연결하는 엣지(edge)로 표현된다.34 예를 들어, 로봇이 시간 <code>$t-1$</code>에서 <code>$t$</code>로 이동했을 때의 움직임은 두 포즈 노드 <code>$x_{t-1}$</code>과 <code>$x_t$</code>를 연결하는 엣지가 되며, 이 엣지는 주행기록계(odometry) 측정값이라는 제약 조건을 나타낸다. 마찬가지로, 로봇이 포즈 <code>$x_t$</code>에서 랜드마크 <code>$m_j$</code>를 관측했다면, 노드 <code>$x_t$</code>와 <code>$m_j$</code> 사이에 관측 제약 조건을 나타내는 엣지가 생성된다.</p>
<p>모든 센서 측정에는 노이즈가 포함되어 있기 때문에, 이 엣지들이 나타내는 제약 조건들은 서로 완벽하게 일치하지 않고 모순을 포함하게 된다. 그래프 기반 SLAM의 목표는 이러한 모든 모순, 즉 오차를 전역적으로 최소화하는 노드들의 최적 배치(configuration)를 찾는 것이다.33 이 문제는 결국 모든 측정 오차의 가중 제곱 합을 최소화하는 비선형 최소제곱 문제로 귀결된다.31</p>
<h3>4.2  핵심 변수 정의</h3>
<p>이 최적화 문제를 수학적으로 명확히 정의하기 위해 몇 가지 핵심 변수들을 이해해야 한다.</p>
<ul>
<li>
<p><strong>상태 벡터 (State Vector) <code>$x$</code>:</strong> 최적화의 대상이 되는 모든 변수들을 모아놓은 벡터다. 일반적으로 시스템이 추정하고자 하는 모든 시간 단계에서의 로봇 포즈 <code>$x_i$</code>들과 모든 랜드마크의 위치 <code>$m_j$</code>를 포함한다.33 예를 들어, <code>$n$</code>개의 로봇 포즈와 <code>$k$</code>개의 랜드마크가 있는 시스템의 상태 벡터는 다음과 같이 표현될 수 있다:<br />
<span class="math math-display">
x = (x_1^T, x_2^T,..., x_n^T, m_1^T, m_2^T,..., m_k^T)^T
</span><br />
여기서 각 포즈 <code>$x_i$</code>는 2D 환경에서는 위치와 방향을 나타내는 <code>$(x, y, \theta)$</code>로, 3D 환경에서는 <code>$(x, y, z, q_x, q_y, q_z, q_w)$</code> (위치 + 쿼터니언) 등으로 표현된다. 랜드마크 <code>$m_j$</code>는 일반적으로 3D 공간상의 좌표 <code>$(x, y, z)$</code>로 표현된다.37</p>
</li>
<li>
<p><strong>측정 모델 (Measurement Model) <code>$z_{ij}$</code>:</strong> 센서를 통해 직접 얻은 측정값으로, 그래프에서 엣지를 형성하는 정보의 원천이다. 이는 두 노드 <code>$i$</code>와 <code>$j$</code> 사이의 상대적인 관계를 나타낸다. 예를 들어, <code>$z_{ij}$</code>는 두 포즈 노드 <code>$x_i$</code>와 <code>$x_j$</code> 사이의 상대적인 변위(relative displacement)를 나타내는 주행기록계 측정값일 수도 있고, 포즈 노드 <code>$x_i$</code>에서 랜드마크 노드 <code>$m_j$</code>를 바라본 상대적인 위치 및 방향을 나타내는 관측값일 수도 있다.33</p>
</li>
<li>
<p><strong>오차 함수 (Error Function) <code>$e_{ij}(x)$</code>:</strong> 최적화의 핵심이 되는 함수로, 현재 추정된 상태 벡터 <code>$x$</code>를 기반으로 예측한 측정값과 실제 센서 측정값 <code>$z_{ij}$</code> 사이의 차이를 계산한다. 예측된 측정값은 예측 함수 <code>$h(x_i, x_j)$</code>를 통해 계산되며, 이는 현재 추정된 포즈 <code>$x_i$</code>에서 <code>$x_j$</code>를 바라보면 어떤 측정값이 나와야 하는지를 나타낸다. 따라서 오차 함수는 다음과 같이 정의된다 31:<br />
<span class="math math-display">
e_{ij}(x) = z_{ij} - h(x_i, x_j)
</span><br />
이 오차 벡터의 차원은 측정의 종류에 따라 달라진다. 예를 들어 2D 포즈 간의 오차는 <code>$\Delta x, \Delta y, \Delta \theta$</code>의 3차원 벡터가 될 수 있다. SLAM의 목표는 이 오차 벡터의 크기를 가능한 한 작게 만드는 것이다.</p>
</li>
<li>
<p><strong>정보 행렬 (Information Matrix) <code>$\Omega_{ij}$</code>:</strong> 모든 측정값이 동일한 신뢰도를 갖지 않는다는 현실을 반영하는 매우 중요한 요소다. 정보 행렬 <code>$\Omega_{ij}$</code>는 측정 <code>$z_{ij}$</code>의 불확실성을 나타내는 공분산 행렬(covariance matrix) <code>$\Sigma_{ij}$</code>의 역행렬(<code>$\Omega_{ij} = \Sigma_{ij}^{-1}$</code>)이다.39 이 행렬은 각 측정 오차에 대한 가중치 역할을 한다. 만약 어떤 측정이 매우 정밀하여 불확실성이 작다면(공분산이 작다면), 그 정보 행렬의 값은 커지게 된다. 반대로 노이즈가 많은 부정확한 측정은 정보 행렬의 값이 작아진다.37 최적화 과정에서 정보 행렬 값이 큰, 즉 신뢰도 높은 측정의 오차는 더 큰 페널티를 받아 우선적으로 줄어들게 된다.</p>
</li>
</ul>
<h3>4.3  최적화 목표 함수</h3>
<p>위의 변수들을 바탕으로, 그래프 기반 SLAM의 전체 목표는 그래프에 존재하는 모든 제약 조건(엣지)에 대한 마할라노비스 거리(Mahalanobis distance)의 제곱 합, 즉 가중 제곱 오차의 총합을 최소화하는 최적의 상태 벡터 <code>$x^*$</code>를 찾는 것으로 공식화된다. 이 목표 함수 <code>$F(x)$</code>는 다음과 같다 31:<br />
<span class="math math-display">
x^* = \underset{x}{\operatorname{argmin}} F(x) = \underset{x}{\operatorname{argmin}} \sum_{(i,j) \in \mathcal{C}} e_{ij}(x)^T \Omega_{ij} e_{ij}(x)
</span><br />
여기서 <code>$\mathcal{C}$</code>는 그래프의 모든 엣지(제약 조건)의 집합을 의미하며, 합산 기호 <code>$\sum_{(i,j) \in \mathcal{C}}$</code>는 모든 제약 조건에 대한 오차를 더하는 것을 나타낸다.</p>
<h3>4.4  해법: 반복적 최적화</h3>
<p>예측 함수 <code>$h(x_i, x_j)$</code>는 로봇의 기구학 모델이나 카메라 투영 모델 등 비선형 함수를 포함하는 경우가 대부분이므로, 오차 함수 <code>$e_{ij}(x)$</code> 역시 비선형 함수가 된다. 따라서 위의 목표 함수 <code>$F(x)$</code>는 해석적으로(analytically) 한 번에 풀 수 없다. 대신, 가우스-뉴턴(Gauss-Newton) 또는 레벤버그-마쿼트(Levenberg-Marquardt)와 같은 반복적인 수치 최적화 기법을 사용하여 해를 구한다.31</p>
<p>이 과정은 다음과 같은 단계로 이루어진다:</p>
<ol>
<li>초기 상태 벡터 <code>$x_0$</code>를 추정한다.</li>
<li>현재 추정치 <code>$x_k$</code> 주변에서 오차 함수 <code>$e_{ij}(x)$</code>를 테일러 급수 전개를 통해 선형화한다.</li>
<li>선형화된 문제를 풀어, 현재 추정치를 최적해 방향으로 이동시킬 업데이트 단계 <code>$ \Delta x $</code>를 계산한다.</li>
<li>현재 추정치를 업데이트한다: <code>$x_{k+1} = x_k + \Delta x$</code>.</li>
<li>업데이트 양이 매우 작아지거나 최대 반복 횟수에 도달할 때까지 2-4단계를 반복한다.</li>
</ol>
<p>이 과정에서 정보 행렬 <code>$\Omega$</code>의 역할은 매우 중요하다. 만약 모든 측정이 완벽하여 오차가 없다면, SLAM은 단순한 기하학적 연립방정식 문제가 될 것이다.35 그러나 현실의 모든 센서 측정에는 불확실성이 존재한다.8 오차 함수 <code>$e_{ij}$</code>가 예측과 실제 측정 간의 ‘기하학적’ 불일치를 나타낸다면, 정보 행렬 <code>$\Omega_{ij}$</code>는 이 기하학적 오차에 ‘확률적’ 신뢰도를 부여하는 역할을 한다. 예를 들어, 고가의 정밀 LiDAR로 측정한 거리 제약은 높은 정보 값을 가지게 되어 최적화에 큰 영향을 미치지만, 저가 웹캠의 흔들리는 이미지로 추정한 부정확한 제약은 낮은 정보 값을 가져 그 영향력이 줄어든다.37</p>
<p>결과적으로, 정보 행렬 <code>$\Omega$</code>는 SLAM을 ‘모든 제약을 동등하게 만족시키려는’ 순수한 기하학적 문제에서, ‘각 제약의 신뢰도를 고려하여 통계적으로 가장 가능성이 높은 상태를 추론하려는’ 확률적 추정(probabilistic estimation) 문제로 격상시키는 핵심 요소다. 이는 사전 믿음(이전 상태)과 새로운 증거(측정값)를 각각의 신뢰도에 따라 가중하여 사후 믿음(최적화된 현재 상태)을 추론하는 베이즈 정리의 원리와도 맥을 같이 한다. 바로 이 확률적 접근법 덕분에 SLAM은 불확실성으로 가득한 실제 환경 속에서도 강건한 추론을 수행할 수 있게 되는 것이다.</p>
<h2>5.  C-SLAM의 주요 기술적 과제와 해결 방안</h2>
<p>C-SLAM은 단일 로봇 SLAM의 많은 한계를 극복하지만, 여러 로봇이 협력하는 과정에서 새롭고 더 복잡한 기술적 과제들을 마주하게 된다. 이러한 과제들은 데이터의 일관성, 시스템의 강건성, 확장성, 그리고 통신 효율성과 밀접하게 연관되어 있다.</p>
<h3>5.1  데이터 연관 문제</h3>
<p><strong>과제:</strong> 데이터 연관(Data Association) 또는 대응 문제(Correspondence Problem)는 SLAM의 고전적인 난제 중 하나로, C-SLAM에서는 그 복잡성이 더욱 증대된다. 이 문제는 센서로 관측한 현재의 측정값이 이전에 지도에 등록된 어떤 랜드마크에 해당하는지, 또는 더 나아가 다른 로봇이 관측했던 어떤 특징점과 동일한지를 정확히 판단하는 것이다.11 잘못된 데이터 연관은 치명적인 결과를 초래한다. 예를 들어, 서로 다른 두 랜드마크를 동일한 것으로 착각하면 지도가 심각하게 왜곡되며, 새로운 랜드마크를 이전에 본 것으로 오인하면 궤적 추정에 큰 오류가 발생한다. 특히, 움직이는 사람이나 차량이 많은 동적 환경에서는 정적인 배경과 동적인 객체를 구별하는 것이 어려워 데이터 연관 문제가 더욱 심각해진다.10</p>
<p><strong>해결 방안:</strong></p>
<ul>
<li><strong>강건한 특징 기술자(Robust Feature Descriptors):</strong> 조명, 시점, 크기 변화에 강인한 특징 기술자를 사용하여 데이터 연관의 정확도를 높인다. SIFT, SURF, ORB와 같은 전통적인 기술자들이 널리 사용되어 왔다.29</li>
<li><strong>딥러닝 기반 기술자:</strong> 최근에는 딥러닝을 통해 데이터로부터 직접 강건한 특징을 학습하는 방법이 각광받고 있다. SuperPoint와 같은 특징점 검출기나 NetVLAD와 같은 전역 기술자는 전통적인 방법에 비해 훨씬 더 높은 변별력과 강건성을 보여준다.8</li>
<li><strong>기하학적 검증(Geometric Verification):</strong> 특징 기술자를 통해 1차적으로 매칭된 후보들 중에서, RANSAC(Random Sample Consensus)과 같은 통계적 방법을 사용하여 기하학적으로 일관된 관계를 가지는 매칭 쌍(inliers)만을 최종적으로 채택하고, 나머지(outliers)는 폐기한다.</li>
</ul>
<h3>5.2  강건한 루프 폐쇄</h3>
<p><strong>과제:</strong> 루프 폐쇄는 누적 오차를 보정하는 핵심 과정이지만, 이 과정에서 발생하는 오류는 시스템 전체를 붕괴시킬 수 있다. 특히 ‘지각적 중복(Perceptual Aliasing)’ 문제는 심각한 도전 과제다. 이는 사무실의 복도나 주차장의 기둥처럼, 서로 다른 장소임에도 불구하고 시각적으로 매우 유사하게 보이는 경우를 말한다. 시스템이 이러한 장소들을 동일한 곳으로 잘못 인식하여 ‘가짜 양성(false positive)’ 루프 폐쇄를 생성하면, 백엔드 최적화 과정에서 이 잘못된 제약 조건이 전체 지도를 치명적으로 왜곡시키게 된다.6 C-SLAM에서는 서로 다른 로봇이 유사한 환경을 탐사할 때 로봇 간 루프 폐쇄(inter-robot loop closure)에서 이 문제가 더욱 빈번하고 심각하게 발생할 수 있다.</p>
<p><strong>해결 방안:</strong></p>
<ul>
<li><strong>순차적 일관성 검증(Sequential Consistency Check):</strong> 단 한 번의 유사도 매칭으로 루프 폐쇄를 확정하지 않고, 시간적, 공간적으로 연속된 여러 키프레임에 걸쳐 일관되게 높은 유사도가 나타나는지를 확인하여 일시적인 오인식을 걸러낸다.7</li>
<li><strong>강건한 백엔드 최적화(Robust Back-end Optimization):</strong> 스위처블 제약(Switchable Constraints)이나 GNC(Graduated Non-Convexity)와 같은 강건한 최적화 기법을 백엔드에 도입한다. 이러한 기법들은 최적화 과정에서 각 제약 조건이 옳은지(inlier) 그른지(outlier)를 판단하여, 잘못된 루프 폐쇄(outlier)가 전체 최적화에 미치는 영향을 동적으로 줄이거나 제거한다.13</li>
<li><strong>딥러닝 기반 장소 인식(Deep Learning-based Place Recognition):</strong> NetVLAD, CosPlace와 같은 딥러닝 기반의 전역 기술자를 사용하여 이미지 전체의 맥락과 의미를 포착한 표현(representation)을 생성한다. 이는 단순한 시각적 유사성이 아닌, 장소의 본질적인 특징을 비교하게 하여 지각적 중복 문제에 대한 강건성을 크게 향상시킨다.13</li>
</ul>
<h3>5.3  확장성 및 계산 복잡도</h3>
<p><strong>과제:</strong> C-SLAM 시스템의 규모, 즉 로봇의 수와 탐사 환경의 크기가 증가함에 따라, 시스템이 관리해야 할 상태 변수(모든 로봇의 모든 포즈, 모든 랜드마크)와 제약 조건의 수가 기하급수적으로 증가한다. 이는 SLAM 문제의 계산 복잡도를 폭발적으로 증가시키는 원인이 된다.7 중앙집중형 시스템에서는 중앙 서버의 계산 부하가 한계에 도달하게 되고, 분산형 시스템에서는 개별 로봇의 제한된 온보드 컴퓨팅 자원이 고갈될 위험에 처한다.</p>
<p><strong>해결 방안:</strong></p>
<ul>
<li><strong>희소성(Sparsity) 활용:</strong> 다행히도, 그래프 기반 SLAM의 최적화 문제에서 사용되는 정보 행렬(또는 Hessian 행렬)은 대부분의 요소가 0인 희소 행렬(sparse matrix)의 구조를 가진다. 이는 각 포즈가 오직 시간적으로 인접한 포즈나, 루프 폐쇄를 통해 연결된 소수의 포즈와만 직접적인 관계를 갖기 때문이다. g2o, Ceres Solver와 같은 현대 SLAM 라이브러리들은 이러한 희소 구조를 활용하는 효율적인 선형대수 기법을 사용하여, 상태 변수가 수만 개에 달하는 대규모 문제도 효과적으로 해결한다.34</li>
<li><strong>효율적인 맵 데이터 관리:</strong> 맵의 크기가 무한정 커지는 것을 막기 위해, 정보 가치가 낮거나 중복되는 키프레임 및 랜드마크를 시스템에서 제거하는 ‘한계화(marginalization)’ 또는 ‘가지치기(pruning)’ 기법을 사용한다. 이를 통해 계산 복잡도를 관리 가능한 수준으로 유지한다.20</li>
<li><strong>계층적/증분적 접근(Hierarchical/Incremental Approach):</strong> 전체 지도를 한 번에 최적화하는 대신, 최근의 데이터만을 포함하는 지역 지도(local map)를 자주 최적화하고, 루프 폐쇄와 같은 중요한 이벤트가 발생했을 때만 전역 최적화를 수행하는 증분적(incremental) 방식을 채택하여 실시간성을 확보한다.</li>
</ul>
<h3>5.4  동적 환경 대응</h3>
<p><strong>과제:</strong> 대부분의 전통적인 SLAM 알고리즘은 환경이 근본적으로 정적(static)이라는 강한 가정을 기반으로 한다. 그러나 실제 환경은 움직이는 사람들, 차량, 그리고 기타 동적 객체들로 가득 차 있다. 이러한 동적 객체들은 SLAM 시스템에 의해 정적인 랜드마크로 오인될 수 있으며, 이는 지도 오염과 위치 추정 실패의 주된 원인이 된다.8</p>
<p><strong>해결 방안:</strong></p>
<ul>
<li><strong>동적 객체 필터링:</strong> 딥러닝 기반의 시맨틱 분할(Semantic Segmentation)을 SLAM 파이프라인의 프론트엔드에 통합하는 것이 가장 유망한 해결책이다. YOLO, U-Net과 같은 네트워크를 사용하여 이미지 내에서 사람, 차와 같이 일반적으로 움직이는 객체들을 픽셀 단위로 식별하고, 해당 영역에서 추출된 특징점들을 SLAM의 위치 추정 및 지도 작성 과정에서 아예 제외시킨다.45</li>
<li><strong>다중 시점 기하학(Multi-view Geometry):</strong> 여러 시점에서 관측했을 때, 주변의 정적인 특징점들과의 기하학적 관계를 일관되게 유지하지 못하는 특징점들을 동적인 것으로 간주하고 제거하는 방법도 사용된다. 이는 딥러닝 모델이 학습하지 않은 예기치 않은 동적 객체에 대응하는 데 도움을 줄 수 있다.47</li>
</ul>
<h3>5.5  통신 제약</h3>
<p><strong>과제:</strong> C-SLAM은 본질적으로 로봇 간의 정보 교환에 의존하기 때문에, 통신 네트워크의 제약에 매우 민감하다. 제한된 통신 대역폭, 높은 지연 시간(latency), 그리고 간헐적인 통신 단절은 협력적 지도 작성을 방해하는 주요 요인이다.15 특히 고해상도 이미지나 LiDAR 스캔과 같은 대용량 원시 데이터를 로봇 간에 직접 교환하는 것은 대부분의 실제 무선 네트워크 환경에서 비현실적이다.7</p>
<p><strong>해결 방안:</strong></p>
<ul>
<li><strong>경량화된 데이터 교환:</strong> 원시 데이터를 직접 보내는 대신, 정보의 핵심만을 담고 있는 경량화된 데이터를 교환한다. 시각 SLAM에서는 특징 기술자(descriptor)나 Bag-of-Words 벡터가 이에 해당하며, LiDAR SLAM에서는 Scan Context, LiDAR-Iris와 같이 전체 스캔을 간결한 벡터로 압축하여 표현하는 기술자들이 개발되었다.13</li>
<li><strong>선별적 통신(Selective Communication):</strong> 모든 정보를 무차별적으로 교환하는 대신, 협력에 가장 큰 도움이 될 정보를 선별하여 통신하는 전략을 사용한다. 예를 들어, 로봇 간 루프 폐쇄를 형성할 가능성이 높은 키프레임에 대한 정보만을 우선적으로 교환하는 기법은 전체 통신량을 크게 줄이면서도 시스템 성능을 유지하는 데 효과적이다.13</li>
<li><strong>비동기 통신 설계:</strong> 실제 로봇 환경에서는 통신이 항상 가능하지 않다는 점을 인정하고, 간헐적이고 비동기적인 통신을 가정하여 시스템을 설계한다. 로봇들은 통신이 단절된 동안에는 독립적으로 작동하다가, 통신이 다시 가능해졌을 때 그동안 쌓인 정보를 교환하고 처리할 수 있도록 설계되어야 한다.</li>
</ul>
<p>이러한 C-SLAM의 기술적 과제들은 서로 독립적이지 않고 복잡하게 얽혀 있다. 예를 들어, 동적 환경 문제를 해결하기 위해 고성능 시맨틱 분할 네트워크를 도입하면, 이는 필연적으로 개별 로봇의 계산 복잡도 문제를 악화시킨다. 이 계산 문제를 해결하기 위해 중앙집중형 아키텍처를 채택하여 연산을 서버로 오프로드하면, 이번에는 통신 병목 문제가 심화된다. 이 통신 문제를 해결하기 위해 교환되는 데이터를 경량화하면, 정보 손실로 인해 데이터 연관이나 루프 폐쇄의 정확도가 저하될 위험이 생긴다. 이처럼 특정 문제에 대한 해결책이 다른 차원에서 새로운 트레이드오프를 발생시키는 ’풍선 효과’가 나타난다. 따라서 진정으로 강건하고 효율적인 C-SLAM 시스템을 설계하는 것은, 이러한 상호 의존적인 문제들 사이에서 주어진 애플리케이션의 특정 요구사항에 맞춰 각 요소 기술들을 신중하게 조합하고 절충하여 최적의 균형점을 찾는, 고차원적인 시스템 엔지니어링의 과정이라 할 수 있다.</p>
<h2>6.  주요 C-SLAM 시스템 비교 분석: 사례 연구</h2>
<p>C-SLAM의 이론적 개념과 아키텍처는 여러 연구 그룹에 의해 실제 시스템으로 구현되었다. 이들 중 대표적인 시스템인 CCM-SLAM, Kimera-Multi, 그리고 Swarm-SLAM을 비교 분석함으로써, 각 시스템이 어떤 설계 철학을 바탕으로 어떤 기술적 문제에 집중했는지, 그리고 어떤 장단점을 갖는지 심층적으로 이해할 수 있다.</p>
<h3>6.1  중앙집중형 시스템: CCM-SLAM</h3>
<p>CCM-SLAM(Centralized Collaborative Monocular SLAM)은 이름에서 알 수 있듯이 중앙집중형 아키텍처를 기반으로 하는 대표적인 C-SLAM 프레임워크다.18 이 시스템은 특히 컴퓨팅 자원이 극도로 제한된 소형 로봇(예: 드론)들의 협력을 염두에 두고 설계되었다.</p>
<ul>
<li>
<p><strong>아키텍처 및 작동 방식:</strong> CCM-SLAM은 전형적인 중앙집중형 구조를 따른다. 각 에이전트 로봇은 온보드에서 상대적으로 계산 비용이 저렴한 시각 주행 거리계(Visual Odometry)만을 실행하여 자신의 지역적인 움직임을 추적하고, 제한된 수의 키프레임과 맵포인트(MapPoints)를 유지한다.18 이렇게 생성된 데이터(키프레임, 3D 맵포인트, 특징 기술자 등)는 통신 모듈을 통해 중앙 서버(지상국)로 지속적으로 전송된다. 중앙 서버는 강력한 계산 자원을 바탕으로, 모든 에이전트로부터 수집된 데이터를 통합하여 맵 관리, 장소 인식(루프 폐쇄), 맵 병합, 그리고 전역 번들 조정(Global BA)과 같은 무거운 백엔드 작업을 전담하여 수행한다.18</p>
</li>
<li>
<p><strong>센서:</strong> 시스템의 기본 센서는 단안 카메라(Monocular Camera)이다. 단안 카메라는 저렴하고 가벼우며 전력 소모가 적어 소형 UAV에 탑재하기에 이상적이다.20 CCM-SLAM은 서로 다른 종류의 카메라를 탑재한 이기종(heterogeneous) 에이전트들도 지원하는 유연성을 갖추고 있다.18</p>
</li>
<li>
<p><strong>주요 특징 및 장단점:</strong> CCM-SLAM의 가장 큰 장점은 <strong>자원 효율적인 아키텍처</strong>에 있다. 계산 부하를 서버로 오프로드함으로써, 각 로봇은 항법에 필수적인 최소한의 작업만 수행하면 되므로 온보드 자원을 크게 절약할 수 있다.20 또한,</p>
</li>
</ul>
<p><strong>강건한 통신 전략</strong>을 갖추고 있어 제한된 대역폭이나 통신 지연 및 손실이 발생하는 실제 환경에서도 작동하도록 설계되었다. 만약 서버와의 통신이 완전히 두절되더라도, 협력 기능은 중단되지만 개별 에이전트는 독립적으로 VO를 계속 수행하며 자율성을 잃지 않는다.18 서버 측에서는 수신된 데이터의 중복성을 효율적으로 감지하고 제거하여 전체 맵의 크기를 관리함으로써 시스템의 <strong>확장성</strong>을 확보하려 노력한다.20</p>
<p>반면, 중앙집중형의 고질적인 문제인 서버 의존성과 통신 병목이라는 단점을 그대로 가지고 있다. 서버가 고장나면 협력이 불가능해지고, 로봇 수가 많아지면 서버로 향하는 통신량이 시스템의 한계로 작용한다.</p>
<h3>6.2  분산형 시스템: Kimera-Multi</h3>
<p>Kimera-Multi는 MIT에서 개발한 완전 분산형 C-SLAM 시스템으로, 강건성과 고수준의 환경 이해 능력을 목표로 한다.32</p>
<ul>
<li>
<p><strong>아키텍처 및 작동 방식:</strong> Kimera-Multi는 중앙 서버 없이 오직 로봇 간 P2P(Peer-to-Peer) 통신에만 의존하는 완전 분산형 아키텍처를 채택했다.32 각 로봇은 자신의 온보드 센서(카메라+IMU) 데이터를 처리하여 지역 궤적과 3D 메시 지도를 독립적으로 생성한다. 통신이 가능한 다른 로봇을 만나면, 분산된 장소 인식 프로토콜을 시작하여 로봇 간 루프 폐쇄를 찾는다. 이때, 잘못된 루프 폐쇄(outlier)를 강건하게 식별하고 제거하기 위해 분산 GNC(Graduated Non-Convexity) 알고리즘 기반의 포즈 그래프 최적화(PGO)를 수행한다.13 최적화된 궤적 정보는 다시 각 로봇의 지역 3D 메시 지도를 수정하여 전역적인 일관성을 높이는 데 사용된다.</p>
</li>
<li>
<p><strong>센서:</strong> 기본적으로 시각-관성 센서(Visual-Inertial Sensors), 즉 카메라와 IMU의 조합을 사용한다. VIO(Visual-Inertial Odometry)는 단일 카메라만 사용할 때 발생하는 스케일 모호성 문제를 해결하고, 빠른 움직임이나 텍스처가 부족한 환경에서도 더 강건한 궤적 추정을 가능하게 한다. 최근에는 다중 카메라와 외부 주행기록계(예: 바퀴 엔코더) 센서까지 지원하도록 확장되어 자율주행차와 같은 복잡한 시스템에도 적용되고 있다.50</p>
</li>
<li>
<p><strong>주요 특징 및 장단점:</strong> Kimera-Multi의 가장 두드러진 특징은 <strong>강건한 분산 최적화</strong>와 <strong>메트릭-시맨틱 매핑(Metric-Semantic Mapping)</strong> 능력이다. 분산 GNC 알고리즘은 지각적 중복으로 인해 발생하는 잘못된 루프 폐쇄에 대해 매우 강건한 성능을 보여주며, 이는 분산 시스템의 신뢰도를 크게 높인다.42 더 나아가, Kimera-Multi는 단순히 기하학적 정보(metric)만을 담은 지도를 넘어, ‘건물’, ‘도로’, ’차량’과 같은 의미론적(semantic) 레이블이 포함된 3D 메시(mesh) 지도를 실시간으로 생성한다.14 이는 로봇이 환경을 더 높은 수준에서 이해하고 지능적인 의사결정을 내리는, 소위 ’공간 AI(Spatial AI)’를 위한 기반을 제공한다.51</p>
</li>
</ul>
<p>분산형 구조 덕분에 확장성과 강건성이 뛰어나며, 중앙집중형 시스템에 필적하는 높은 정확도를 달성하는 것으로 보고되었다.42 단점으로는, 각 로봇이 PGO와 메시 생성 등 상대적으로 무거운 작업을 온보드에서 수행해야 하므로 CCM-SLAM에 비해 높은 계산 자원을 요구한다는 점과, 분산 합의 알고리즘의 내재적 복잡성을 들 수 있다.</p>
<h3>6.3  스웜 로보틱스 기반 시스템: Swarm-SLAM</h3>
<p>Swarm-SLAM은 스웜 로보틱스(Swarm Robotics)의 핵심 원칙을 C-SLAM에 접목하려는 시도로, 대규모 로봇 군집 운용을 목표로 한다.15</p>
<ul>
<li>
<p><strong>아키텍처 및 작동 방식:</strong> Swarm-SLAM은 분산형 아키텍처를 기반으로 하되, 스웜 로보틱스의 철학인 **확장성(scalability), 유연성(flexibility), 분산성(decentralization), 희소성(sparsity)**을 극대화하는 데 초점을 맞춘다.15 시스템은 중앙 권한이나 전역 지식(global knowledge)에 의존하지 않고, 각 로봇이 자신의 이웃 로봇과의 지역적인 감지 및 통신에만 의존하여 작동하도록 설계되었다.15</p>
</li>
<li>
<p><strong>센서:</strong> Swarm-SLAM의 주요 강점 중 하나는 <strong>센서 유연성</strong>이다. LiDAR, 스테레오 카메라, RGB-D 카메라 등 다양한 종류의 센서를 지원하는 통합 프레임워크를 제공하여, 이기종 센서로 구성된 로봇 스웜을 운용할 수 있다.15 장소 인식을 위해 LiDAR 데이터에는 Scan Context를, 이미지 데이터에는 CosPlace와 같은 최신 기술자를 적용하는 등 센서 종류에 맞는 다양한 기술을 지원한다.13</p>
</li>
<li>
<p><strong>주요 특징 및 장단점:</strong> Swarm-SLAM의 핵심 혁신은 <strong>통신 부하를 줄이기 위한 로봇 간 루프 폐쇄 우선순위 기법</strong>에 있다. 모든 로봇이 모든 루프 폐쇄 후보를 교환하는 대신, 대수적 연결성 최대화(algebraic connectivity maximization)라는 그래프 이론 기반의 기준을 사용하여 전체 포즈 그래프의 수렴을 가장 가속화할 수 있는 중요한 루프 폐쇄 정보만을 선별적으로 교환한다.13 이는 제한된 통신 환경에서 대규모 스웜의 성능을 유지하는 데 결정적인 역할을 한다. 또한, 로봇 운영체제인 ROS 2를 기반으로 하고 ad-hoc 네트워크를 지원하여, 연구자들이 쉽게 접근하고 실제 로봇에 적용할 수 있도록 <strong>접근성과 실용성</strong>을 높였다.15</p>
</li>
</ul>
<p>Swarm-SLAM은 스웜 로보틱스의 이상을 추구하지만, 아직 연구 초기 단계에 있으며, 복잡한 3D 메시 지도보다는 위상 맵이나 단순한 의미론적 맵과 같은 추상적인 지도를 생성하는 데 더 적합할 수 있다는 한계도 지적된다.22</p>
<p>이 세 시스템은 C-SLAM의 스펙트럼을 잘 보여준다. CCM-SLAM이 자원 제약과 중앙 통제의 실용성에 초점을 맞췄다면, Kimera-Multi는 강건성과 고수준의 시맨틱 이해라는 성능의 극한을 추구한다. 그리고 Swarm-SLAM은 대규모 군집 운용이라는 미래 비전을 향해 확장성과 유연성의 원칙을 탐구한다. 어떤 시스템이 ’더 좋은가’는 절대적인 질문이 아니며, 애플리케이션의 구체적인 요구사항-로봇의 사양, 팀의 규모, 임무의 목표, 통신 환경-에 따라 최적의 선택이 달라질 것이다.</p>
<p>다음 표는 세 가지 대표적인 C-SLAM 시스템의 특징을 요약하여 비교한다.</p>
<table><thead><tr><th>구분 (Category)</th><th>CCM-SLAM</th><th>Kimera-Multi</th><th>Swarm-SLAM</th></tr></thead><tbody>
<tr><td><strong>아키텍처</strong></td><td>중앙집중형 18</td><td>완전 분산형 32</td><td>분산형 (스웜 지향) 15</td></tr>
<tr><td><strong>주요 센서</strong></td><td>단안 카메라 20</td><td>시각-관성 (카메라+IMU) 32</td><td>다중 센서 지원 (LiDAR, Stereo, RGB-D) 15</td></tr>
<tr><td><strong>핵심 특징</strong></td><td>계산 오프로딩, 통신 손실에 대한 강건성 20</td><td>강건한 분산 PGO (GNC), 메트릭-시맨틱 매핑 42</td><td>루프 폐쇄 우선순위 기법, 스웜 원칙 준수, 센서 유연성 13</td></tr>
<tr><td><strong>장점</strong></td><td>저사양 로봇에 적합, 단순화된 전역 최적화 20</td><td>Outlier에 대한 높은 강건성, 고수준의 의미론적 지도 생성 42</td><td>뛰어난 확장성, 이기종 센서 지원, 통신 효율성 15</td></tr>
<tr><td><strong>단점</strong></td><td>중앙 서버 의존성, 통신 병목, 확장성 한계 15</td><td>높은 온보드 계산 요구량, 분산 조정 알고리즘의 복잡성 15</td><td>아직 상대적으로 초기 연구 단계, 복잡한 맵 표현보다는 추상적 맵에 더 적합할 수 있음 22</td></tr>
<tr><td><strong>지도 형태</strong></td><td>희소 특징점 기반 맵 (Sparse Feature Map) 29</td><td>3D 시맨틱 메시 (3D Semantic Mesh) 42</td><td>포즈 그래프 및 포인트 클라우드 53</td></tr>
</tbody></table>
<h2>7.  미래 동향: 딥러닝과 시맨틱 SLAM</h2>
<p>C-SLAM 기술은 지속적으로 발전하고 있으며, 특히 딥러닝(Deep Learning) 기술의 통합은 이 분야에 혁신적인 변화를 가져오고 있다. 딥러닝은 전통적인 SLAM 파이프라인의 여러 구성 요소, 특히 장소 인식과 환경 이해 능력을 근본적으로 향상시키고 있으며, 이는 ’시맨틱 SLAM’이라는 새로운 패러다임으로 이어지고 있다.</p>
<h3>7.1  딥러닝 기반 장소 인식</h3>
<p><strong>배경:</strong> 전통적인 루프 폐쇄 기법은 주로 Bag-of-Words(BoW) 모델에 의존해왔다. 이 방법은 이미지에서 추출한 지역 특징점(local features)들을 미리 정의된 ‘시각적 단어(visual words)’ 사전에 매핑하여 이미지를 히스토그램 벡터로 표현하고, 이 벡터들의 유사도를 비교하여 같은 장소를 찾아낸다.40 그러나 이 접근법은 조명 변화, 계절 변화, 시점의 급격한 변화 등 외형(appearance)이 크게 달라지는 상황에 매우 취약하다. 또한, 시각적으로 유사하지만 실제로는 다른 장소(perceptual aliasing)를 잘못 인식하는 문제가 빈번하게 발생한다.43</p>
<p><strong>딥러닝의 역할:</strong> 딥러닝, 특히 CNN(Convolutional Neural Networks)은 이러한 문제에 대한 강력한 해결책을 제시한다. CNN은 대규모 이미지 데이터셋을 통해 학습하면서, 이미지의 저수준 픽셀 정보로부터 점차 추상적이고 의미론적인 고수준 특징을 추출하는 능력을 갖게 된다.40 SLAM의 장소 인식에 이 능력을 활용하면, 이미지의 표면적인 외형 변화에는 덜 민감하면서도 장소의 본질적인 구조나 ’의미’를 포착하는 강건한 표현(representation)을 얻을 수 있다.43</p>
<p><strong>주요 기법:</strong> NetVLAD, CosPlace, Region-VLAD와 같은 딥러닝 기반 장소 인식 기법들은 사전 훈련된 CNN을 기반으로, 전체 이미지를 고차원의 간결한 벡터, 즉 전역 기술자(global descriptor)로 변환한다.13 이 벡터들은 장소의 고유한 ’지문’처럼 작용하여, 벡터 공간에서의 거리 계산만으로 매우 효율적이고 정확한 장소 간 유사도 검색을 가능하게 한다.43 이러한 딥러닝 기반 기술자들은 전통적인 BoW 방법에 비해 월등히 높은 정확도와 강건성을 보여주며, C-SLAM 시스템에서 로봇 간 루프 폐쇄의 성공률을 극적으로 향상시키고 있다.</p>
<p><strong>효과:</strong> 딥러닝 모델을 장소 인식에 사용하면, 애초에 백엔드로 전달되는 잘못된 루프 폐쇄 후보의 수를 크게 줄일 수 있다. 이는 백엔드의 강건한 최적화 알고리즘(예: GNC)이 처리해야 할 outlier의 수를 줄여주므로, 전체 시스템의 계산 부담을 덜고 신뢰성을 높이는 선순환 구조를 만든다.41</p>
<h3>7.2  시맨틱 SLAM: 기하학을 넘어 의미의 세계로</h3>
<p><strong>개념:</strong> 전통적인 SLAM의 결과물은 점, 선, 면으로 구성된 순수한 기하학적 지도였다. 이 지도는 로봇에게 ’어디에 장애물이 있는지’를 알려줄 수는 있지만, ’그 장애물이 무엇인지’는 알려주지 못한다. 시맨틱 SLAM(Semantic SLAM)은 이러한 한계를 넘어, 지도에 ’의미(semantics)’를 부여하는 것을 목표로 한다.55 즉, 지도상의 객체가 단순한 3D 점들의 집합이 아니라 ‘의자’, ‘문’, ‘도로’, ’사람’과 같은 의미론적 레이블을 갖게 하는 것이다.</p>
<p><strong>필요성:</strong> 로봇이 인간과 자연스럽게 상호작용하고 복잡한 작업을 수행하기 위해서는 환경에 대한 의미론적 이해가 필수적이다. 예를 들어, “부엌으로 가서 테이블 위에 있는 사과를 가져와“와 같은 고수준의 인간 명령을 로봇이 이해하고 수행하려면, ‘부엌’, ‘테이블’, ’사과’라는 객체의 의미와 그들 간의 공간적 관계를 파악할 수 있어야 한다.56 이는 기하학적 정보만으로는 불가능한 영역이다.</p>
<p><strong>구현:</strong> 시맨틱 SLAM은 일반적으로 딥러닝 기반의 컴퓨터 비전 기술을 SLAM 파이프라인에 통합하여 구현된다. YOLO, SSD와 같은 객체 탐지(Object Detection) 네트워크나, U-Net, SegNet과 같은 시맨틱 분할(Semantic Segmentation) 네트워크를 사용하여, 카메라 이미지로부터 실시간으로 객체의 종류와 위치, 또는 픽셀 단위의 의미론적 레이블을 추출한다.45 이렇게 얻어진 시맨틱 정보는 SLAM을 통해 생성된 기하학적 지도 위에 투영되어, 각 3D 랜드마크나 메시 표면에 의미론적 속성을 부여하게 된다.</p>
<p><strong>이점:</strong></p>
<ul>
<li><strong>고수준 상호작용 및 지능적 작업 수행:</strong> 로봇이 환경을 의미론적으로 이해함으로써, 단순히 길을 찾는 것을 넘어 특정 객체를 조작하거나, 특정 장소에서 특정 작업을 수행하는 등 훨씬 더 지능적인 행동이 가능해진다.55</li>
<li><strong>동적 환경에서의 강건성 향상:</strong> 시맨틱 정보는 SLAM 자체의 성능을 향상시키는 데에도 직접적으로 기여한다. ‘사람’, ’차’와 같이 일반적으로 움직이는 것으로 알려진 객체들을 시맨틱 정보로 명확히 식별하고, 이들로부터 추출된 특징점들을 SLAM의 위치 추정 및 지도 작성 과정에서 선제적으로 배제할 수 있다. 이는 동적 환경에서 SLAM 시스템의 강건성을 크게 향상시킨다.45</li>
<li><strong>데이터 연관의 고도화:</strong> 데이터 연관의 단위를 저수준의 ’특징점’에서 고수준의 ’객체’로 끌어올릴 수 있다. 예를 들어, “이 의자는 아까 다른 로봇이 보았던 그 의자와 동일하다“와 같이 객체 수준에서 데이터 연관을 수행하면, 장기적인 관점에서 훨씬 더 강건하고 신뢰성 있는 지도 작성이 가능해진다.56</li>
</ul>
<p><strong>과제:</strong> 시맨틱 SLAM의 가장 큰 과제는 <strong>계산 비용</strong>이다. 객체 탐지나 시맨틱 분할을 위한 딥러닝 네트워크는 상당한 계산 자원을 소모하므로, 이를 SLAM의 실시간 요구사항과 제한된 온보드 컴퓨팅 자원 내에서 어떻게 효율적으로 통합할 것인지가 핵심적인 연구 주제다.45</p>
<p>이러한 발전은 C-SLAM의 궁극적인 목표를 재정의하고 있다. 전통적인 C-SLAM의 목표가 모든 로봇이 동일한 좌표계 상에서 일관된 ’공유된 기하학적 모델’을 갖는 것이었다면, 시맨틱 C-SLAM(예: Kimera-Multi)은 이를 넘어 ’공유된 세계 이해(Shared World Understanding)’를 구축하는 것으로 나아가고 있다. 이제 로봇들은 단순히 3D 점들의 위치를 교환하는 것이 아니라, “저기에 ’문’이 있다” 또는 “이 구역은 ’도로’이다“와 같은 ’공유된 의미’를 구축하고 교환하기 시작한다.14</p>
<p>이는 엄청난 파급 효과를 가진다. 로봇 A가 ’문’을 발견하고 이 정보를 공유하면, 로봇 B는 그 정보를 바탕으로 “나는 저 ’문’을 통해 다른 방으로 진입할 수 있다“는, 기하학적 정보만으로는 불가능했던 고수준의 행동 계획을 세울 수 있게 된다. 더 나아가, 이는 여러 로봇이 의미론적 지도를 일관되게 병합하고 유지하기 위한 ’공유된 온톨로지(shared ontology)’의 문제를 제기한다. 즉, 로봇 A가 ’의자’라고 인식하는 객체를 로봇 B도 동일하게 ’의자’로 인식하고 이해할 수 있도록 보장하는 기술이 필요해진다.</p>
<p>결론적으로, 시맨틱 C-SLAM의 등장은 C-SLAM의 최종 목표를 단순한 ’공동 매핑’에서 ’공동의 상황 인식 및 이해’로 격상시키고 있다. 지도는 더 이상 탐색을 위한 정적인 배경이 아니라, 로봇 팀이 상호작용하고, 추론하며, 공동의 작업을 수행하는 동적인 ’지식 베이스(knowledge base)’로 진화하고 있다. 이는 진정한 의미의 협력적 지능(collaborative intelligence)으로 나아가는 중요한 이정표이며, 자율 로봇 기술의 미래를 밝히는 핵심 동력이 될 것이다.</p>
<h2>8.  응용 분야 및 결론</h2>
<p>C-SLAM 기술은 그 이론적 정교함을 넘어, 실제 세계의 다양한 분야에서 단일 로봇으로는 해결하기 어려웠던 문제들을 해결하며 그 가치를 입증하고 있다. 특히 광범위한 영역을 신속하게 탐사하고, 위험한 환경에서 강건하게 작동해야 하는 응용 분야에서 C-SLAM의 중요성은 더욱 부각된다.</p>
<h3>8.1  주요 응용 분야</h3>
<ul>
<li><strong>재난 대응 (Disaster Response):</strong> 재난 현장은 통신이 두절되고, GPS 신호가 닿지 않으며, 지형이 급격하게 변하는 등 SLAM 기술에 가장 도전적인 환경 중 하나다. C-SLAM은 이러한 환경에서 핵심적인 역할을 수행한다.</li>
<li><strong>수색 및 구조 (Search and Rescue):</strong> 지진으로 붕괴된 건물이나 터널과 같이 인간 구조대원이 접근하기 위험한 지역에 드론이나 지상 로봇 스웜을 투입할 수 있다. 이 로봇들은 협력적으로 내부 공간의 3D 지도를 신속하게 생성하여 생존자의 위치를 파악하고, 구조대의 안전한 진입 경로를 확보하는 데 결정적인 정보를 제공한다.57</li>
<li><strong>피해 평가 (Damage Assessment):</strong> 홍수, 산불, 태풍 등으로 인해 광범위한 피해가 발생했을 때, 여러 대의 드론이 협력하여 피해 지역 전체의 고해상도 지도를 신속하고 정확하게 작성할 수 있다. 이는 피해 규모를 정량적으로 파악하고, 복구 작업의 우선순위를 결정하며, 자원을 효율적으로 배분하는 데 사용된다.57</li>
<li><strong>물류 지원 (Logistics Support):</strong> 재난으로 인해 도로가 유실되거나 새로운 장애물이 생긴 경우, C-SLAM을 통해 생성된 최신 지도를 바탕으로 구호 물품이나 장비를 수송하는 차량의 경로를 최적화하여 대응 시간을 단축하고 자원 낭비를 최소화할 수 있다.57</li>
<li><strong>행성 및 지하 탐사 (Planetary and Subterranean Exploration):</strong></li>
<li>우주 탐사, 특히 달이나 화성 탐사에서 C-SLAM은 필수적인 기술이다. GPS가 존재하지 않고, 빛이 전혀 없는 달의 용암 동굴(lunar pits and caves)이나 화성의 지하 구조물을 탐사하고 정밀한 3D 지도를 생성하는 데 C-SLAM 기술이 활용된다. NASA의 KNaCK(Kinematic Navigation and Cartography Knapsack) 프로젝트는 이러한 목적을 위해 LiDAR 기반의 SLAM 시스템을 개발하고 있다.60 이렇게 생성된 지도는 미래의 유인 기지 건설, 물과 같은 현지 자원 활용(In-situ resource utilization, ISRU), 그리고 방사선으로부터 안전한 대피소 탐색에 핵심적인 정보를 제공한다.58</li>
<li>지구의 지하 환경 탐사 역시 중요한 응용 분야다. 여러 대의 자율 드릴링 장비나 탐사 로봇이 협력하여 광산 내부와 같이 위험하고 복잡한 환경을 정밀하게 매핑하고, 자원의 분포를 파악하는 데 사용될 수 있다.48</li>
<li><strong>산업 자동화 및 물류 (Industrial Automation and Logistics):</strong></li>
<li>아마존과 같은 대규모 물류 창고나 스마트 팩토리에서는 수백, 수천 대의 자율 이동 로봇(AMR)이 동시에 작업을 수행한다. C-SLAM은 이 로봇들이 서로의 위치를 공유하고, 동적으로 변화하는 작업 환경에 대한 일관된 지도를 유지하며, 충돌 없이 효율적으로 재고를 관리하고 물품을 운송하는 것을 가능하게 한다.30</li>
<li>대규모 건설 현장에서 여러 대의 드론이나 로봇이 협력하여 공사 진행 상황을 주기적으로 모니터링하고, 설계 도면과 비교하여 시공 품질을 검사하는 데에도 활용될 수 있다.</li>
<li><strong>증강 현실 (Augmented Reality, AR):</strong></li>
<li>여러 사용자가 동일한 물리적 공간에서 일관된 AR 경험을 공유하는 ‘공유 AR(Shared AR)’ 환경을 구축하는 데 C-SLAM이 사용된다. 각 사용자의 스마트폰이나 AR 글래스와 같은 장치들이 협력하여 주변 공간을 실시간으로 매핑하고 서로의 상대적인 위치를 정확하게 추정한다. 이를 통해 모든 사용자가 동일한 가상 객체를 동일한 위치에서 보고 상호작용하는 것이 가능해진다.2</li>
</ul>
<h3>8.2  결론 및 미래 전망</h3>
<p><strong>요약:</strong> 본 고찰을 통해 살펴본 바와 같이, C-SLAM은 단일 로봇 SLAM이 직면했던 누적 오차, 탐사 효율성, 강건성의 한계를 극복하기 위한 핵심적인 기술 패러다임으로 자리 잡았다. 여러 로봇의 협력을 통해 시스템 전체의 정확도와 신뢰성을 비약적으로 향상시키는 C-SLAM은 중앙집중형과 분산형이라는 두 가지 주요 아키텍처를 축으로 발전해왔다. 중앙집중형은 자원 효율성과 최적화의 용이성을, 분산형은 강건성과 확장성을 장점으로 가지며, 실제 시스템들은 특정 응용 분야의 요구사항과 제약 조건에 맞춰 이 두 아키터처의 요소를 전략적으로 절충하고 융합하는 방향으로 진화하고 있다.</p>
<p><strong>미래 연구 방향:</strong> C-SLAM 기술은 여전히 해결해야 할 과제와 무한한 잠재력을 동시에 안고 있으며, 다음과 같은 방향으로 연구가 활발히 진행될 것으로 전망된다.</p>
<ul>
<li><strong>장기 자율성 (Long-term Autonomy):</strong> 현재의 SLAM 시스템들은 대부분 단기적인 임무 수행을 가정한다. 그러나 로봇이 수 주, 수 개월, 또는 수 년에 걸쳐 특정 환경에서 지속적으로 작동하기 위해서는, 계절의 변화, 조명의 변화, 건물의 리모델링과 같이 시간이 지남에 따라 점진적 또는 급격하게 변화하는 환경에 맞춰 지도를 지속적으로 업데이트하고, 오래된 정보를 폐기하며, 일관성을 유지하는 기술이 필수적이다.10</li>
<li><strong>알고리즘-하드웨어 공동 설계 (Algorithm-Hardware Co-design):</strong> 딥러닝 기반의 시맨틱 SLAM과 같이 계산 복잡도가 높은 알고리즘을 자원이 제한된 임베디드 장치에서 실시간으로 구동하기 위한 연구가 중요해지고 있다. 이는 단순히 알고리즘을 최적화하는 것을 넘어, 특정 연산을 가속화하는 전용 하드웨어(예: FPGA, ASIC)를 설계하고, 이에 맞춰 알고리즘을 재구성하는 알고리즘-하드웨어 공동 설계 접근법을 필요로 한다.55</li>
<li><strong>학습 기반 엔드투엔드 SLAM (Learning-based End-to-End SLAM):</strong> 전통적인 SLAM이 특징 추출, 데이터 연관, 최적화 등 여러 모듈로 나뉜 파이프라인 구조를 가졌다면, 미래에는 딥러닝을 통해 센서의 원시 데이터로부터 직접 로봇의 포즈와 환경 지도를 추정하는 엔드투엔드(end-to-end) 학습 방식의 연구가 더욱 활발해질 것이다. 이는 인간이 설계한 중간 단계의 표현에 의존하지 않고, 데이터로부터 직접 최적의 해법을 학습할 잠재력을 가지고 있다.</li>
<li><strong>인간-로봇 협력 SLAM (Human-Robot Collaborative SLAM):</strong> 로봇 팀에 인간 작업자가 하나의 ’에이전트’로 참여하여, 인간의 직관과 상황 판단 능력을 SLAM 프로세스에 통합하는 연구가 필요하다. 인간은 로봇이 혼동하는 지각적 중복 문제를 해결해주거나, 지도에 없는 새로운 의미론적 정보를 추가하는 등, 로봇의 인식을 보강하는 중요한 역할을 수행할 수 있다.</li>
</ul>
<p><strong>최종 결론:</strong> C-SLAM 기술은 딥러닝과 시맨틱 기술의 융합을 통해, 단순한 ‘위치 추정 및 지도 작성’ 도구를 넘어서고 있다. 이는 로봇 팀이 복잡하고 동적인 실제 환경을 공동으로 ’이해’하고, 그 이해를 바탕으로 ’상호작용’하며, ’공동의 목표’를 달성하게 하는 **협력적 공간 지능(Collaborative Spatial AI)**의 근간 기술로 발전하고 있다. 이 기술의 지속적인 성숙과 발전은, 자율 로봇이 재난 현장에서부터 우주 공간, 그리고 우리 일상생활에 이르기까지 사회의 다양한 영역에서 핵심적인 역할을 수행하는 미래를 앞당기는 가장 강력한 동력이 될 것이다.</p>
<h2>9. 참고 자료</h2>
<ol>
<li>Simultaneous localization and mapping - Wikipedia, accessed July 31, 2025, https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping</li>
<li>Overview of SLAM. What is SLAM? What is Simultaneous… | by Luis Bermudez | machinevision | Medium, accessed July 31, 2025, https://medium.com/machinevision/overview-of-slam-50b7f49903b7</li>
<li>Understanding SLAM in Robotics and Autonomous Vehicles - Flyability, accessed July 31, 2025, https://www.flyability.com/blog/simultaneous-localization-and-mapping</li>
<li>Overview of Multi-Robot Collaborative SLAM from the Perspective of Data Fusion - MDPI, accessed July 31, 2025, https://www.mdpi.com/2075-1702/11/6/653</li>
<li>The Future of Drone Mapping with SLAM Technology, accessed July 31, 2025, https://www.thedroneu.com/blog/slam-technology/</li>
<li>What Is SLAM (Simultaneous Localization and Mapping)? - MATLAB &amp; Simulink, accessed July 31, 2025, https://nl.mathworks.com/discovery/slam.html</li>
<li>C-SLAM Problem Major Milestones. | Download Scientific Diagram - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/figure/C-SLAM-Problem-Major-Milestones_fig1_361157522</li>
<li>The Future of SLAM: Trends and Challenges - Number Analytics, accessed July 31, 2025, https://www.numberanalytics.com/blog/future-slam-trends-challenges</li>
<li>COLLABORATIVE SLAM FOR GROUND ROBOTICS IN SEARCH AND RESCUE MISSIONS - DiVA portal, accessed July 31, 2025, https://www.diva-portal.org/smash/get/diva2:1869921/FULLTEXT01.pdf</li>
<li>Understanding SLAM: Simultaneous Localization and Mapping in Robotics - Medium, accessed July 31, 2025, https://medium.com/@hexiangnan/understanding-slam-simultaneous-localization-and-mapping-in-robotics-3592eb504f11</li>
<li>Review: Issues and Challenges of Simultaneous Localization and Mapping (SLAM) Technology in Autonomous Robot - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/publication/369646388_Review_Issues_and_Challenges_of_Simultaneous_Localization_and_Mapping_SLAM_Technology_in_Autonomous_Robot</li>
<li>Review: Issues and Challenges of Simultaneous Localization and Mapping (SLAM) Technology in Autonomous Robot, accessed July 31, 2025, https://ijic.utm.my/index.php/ijic/article/download/408/293</li>
<li>Multi S-Graphs: An Efficient Distributed Semantic-Relational Collaborative SLAM - arXiv, accessed July 31, 2025, https://arxiv.org/html/2401.05152v2</li>
<li>Kimera-Multi: a System for Distributed Multi-Robot Metric-Semantic Simultaneous Localization and Mapping | Request PDF - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/publication/355432213_Kimera-Multi_a_System_for_Distributed_Multi-Robot_Metric-Semantic_Simultaneous_Localization_and_Mapping</li>
<li>Swarm-SLAM: Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems - arXiv, accessed July 31, 2025, https://arxiv.org/html/2301.06230v3</li>
<li>DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems - arXiv, accessed July 31, 2025, https://arxiv.org/html/2503.04126v1</li>
<li>Centralized vs. Decentralized vs. Distributed Systems - GeeksforGeeks, accessed July 31, 2025, https://www.geeksforgeeks.org/system-design/comparison-centralized-decentralized-and-distributed-systems/</li>
<li>CCM‐SLAM: Robust and efficient centralized collaborative monocular simultaneous localization and mapping for robotic teams | Request PDF - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/publication/329925889_CCM-SLAM_Robust_and_efficient_centralized_collaborative_monocular_simultaneous_localization_and_mapping_for_robotic_teams</li>
<li>SwarmMap: Scaling Up Real-time Collaborative Visual SLAM at the Edge - USENIX, accessed July 31, 2025, https://www.usenix.org/system/files/nsdi22-paper-xu_jingao.pdf</li>
<li>CCM‐SLAM: Robust and efficient centralized collaborative …, accessed July 31, 2025, https://www.research-collection.ethz.ch/bitstream/20.500.11850/313259/2/2018_CCM_SLAM.pdf</li>
<li>The comparison of decentralized and centralized structure of network communication in different application fields - Atlantis Press, accessed July 31, 2025, https://www.atlantis-press.com/article/125933251.pdf</li>
<li>(PDF) Swarm SLAM: Challenges and Perspectives - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/publication/350127330_Swarm_SLAM_Challenges_and_Perspectives</li>
<li>Swarm SLAM: Challenges and Perspectives - Frontiers, accessed July 31, 2025, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2021.618268/full</li>
<li>Engineering Meets Economics: Shifting, Not Choosing, Between Centralized and Decentralized - Spherical Cow Consulting, accessed July 31, 2025, https://sphericalcowconsulting.com/2025/05/07/engineering-vs-economics/</li>
<li>Visual-SLAM Classical Framework and Key Techniques: A Review - PMC, accessed July 31, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC9227238/</li>
<li>Lecture 23: SLAM, Part I 23.1 Introduction - GitHub, accessed July 31, 2025, https://pages.github.berkeley.edu/EECS-106/sp22-site/assets/scribe_notes/scribe_lec_12B.pdf</li>
<li>Front-end and back-end in a typical SLAM system. - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/figure/Front-end-and-back-end-in-a-typical-SLAM-system_fig1_304163768</li>
<li>Combining SLAM frontend and backend - LEMUR, accessed July 31, 2025, https://uclalemur.com/blog/combining-slam-frontend-and-backend</li>
<li>Comparative analysis of ROS-based centralized methods for conducting collaborative monocular visual SLAM using a pair of UAVs 1. - CLAWAR Association, accessed July 31, 2025, https://clawar.org/wp-content/uploads/2021/02/Clawar2020_Paper_12.pdf</li>
<li>The Complete Guide to SLAM: Origin, Applications, and Comparison of 5 systems - dtLabs, accessed July 31, 2025, https://dt-labs.ai/blog/the-complete-guide-to-slam/</li>
<li>Mastering Graph-Based SLAM in Cognitive Robotics - Number Analytics, accessed July 31, 2025, https://www.numberanalytics.com/blog/graph-based-slam-cognitive-robotics</li>
<li>Index repo for Kimera-Multi system - GitHub, accessed July 31, 2025, https://github.com/MIT-SPARK/Kimera-Multi</li>
<li>A Tutorial on Graph-Based SLAM - Uni Freiburg, accessed July 31, 2025, http://www2.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10titsmag.pdf</li>
<li>(PDF) A tutorial on graph-based SLAM - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/publication/231575337_A_tutorial_on_graph-based_SLAM</li>
<li>SLAM Tutorial - CS@Columbia, accessed July 31, 2025, http://www.cs.columbia.edu/~allen/F17/NOTES/slam_pka.pdf</li>
<li>Novel Insights into the Impact of Graph Structure on SLAM - MIT, accessed July 31, 2025, https://www.mit.edu/~mrrobot/assets/khosoussi14iros.pdf</li>
<li>Graph-based SLAM basics. Simultaneous Localization and Mapping… - Medium, accessed July 31, 2025, https://medium.com/@fatlip/graph-based-slam-basics-f84501525f24</li>
<li>Graph SLAM: From Theory to Implementation - Federico Sarrocco, accessed July 31, 2025, https://federicosarrocco.com/blog/graph-slam-tutorial</li>
<li>Active SLAM: A Review on Last Decade - PMC - PubMed Central, accessed July 31, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC10575033/</li>
<li>Convolutional Neural Network-based Place Recognition - arXiv, accessed July 31, 2025, https://arxiv.org/pdf/1411.1509</li>
<li>[P [R] Deep learning-assisted SLAM to reduce computational : r/MachineLearning - Reddit, accessed July 31, 2025, https://www.reddit.com/r/MachineLearning/comments/1m1fhnt/p_r_deep_learningassisted_slam_to_reduce/</li>
<li>Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems - DSpace@MIT, accessed July 31, 2025, https://dspace.mit.edu/bitstream/handle/1721.1/145301/2106.14386.pdf?sequence=2&amp;isAllowed=y</li>
<li>Towards a Robust Visual Place Recognition in Large-Scale vSLAM Scenarios Based on a Deep Distance Learning - PMC, accessed July 31, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC7796086/</li>
<li>The GraphSLAM Algorithm with Applications to Large-Scale Mapping of Urban Structures - Sebastian Thrun, accessed July 31, 2025, http://robots.stanford.edu/papers/thrun.graphslam.pdf</li>
<li>A Computationally Efficient Semantic SLAM Solution for Dynamic Scenes - MDPI, accessed July 31, 2025, https://www.mdpi.com/2072-4292/11/11/1363</li>
<li>RESEARCH ON SEMANTIC-ASSISTED SLAM IN COMPLEX DYNAMIC INDOOR ENVIRONMENT, accessed July 31, 2025, https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/353/2020/</li>
<li>A semantic SLAM system for dynamic environments - SPIE Digital Library, accessed July 31, 2025, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13401/134010N/A-semantic-SLAM-system-for-dynamic-environments/10.1117/12.3053120.full</li>
<li>Overview of the CCM‐SLAM system architecture. The robotic agent (e.g. - ResearchGate, accessed July 31, 2025, https://www.researchgate.net/figure/Overview-of-the-CCM-SLAM-system-architecture-The-robotic-agent-eg-a-UAV-runs_fig3_329925889</li>
<li>[2106.14386] Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems - arXiv, accessed July 31, 2025, https://arxiv.org/abs/2106.14386</li>
<li>Multi-Camera Visual-Inertial Simultaneous Localization and Mapping for Autonomous Valet Parking - arXiv, accessed July 31, 2025, https://arxiv.org/html/2304.13182v3</li>
<li>Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems, accessed July 31, 2025, https://mit.edu/sparklab/2023/08/25/Kimera-Multi__Robust_Distributed_Dense_Metric-Semantic_SLAM_for_Multi-Robot-Systems.html</li>
<li>Swarm-SLAM: Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems - arXiv, accessed July 31, 2025, https://arxiv.org/pdf/2301.06230</li>
<li>Swarm-SLAM : Sparse Decentralized Collaborative Simultaneous Localization and Mapping Framework for Multi-Robot Systems (2301.06230v3) - Emergent Mind, accessed July 31, 2025, https://www.emergentmind.com/articles/2301.06230</li>
<li>Role of Deep Learning in Loop Closure Detection for Visual and Lidar SLAM: A Survey, accessed July 31, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC7916334/</li>
<li>Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey - arXiv, accessed July 31, 2025, https://arxiv.org/html/2505.12384v1</li>
<li>An Overview on Visual SLAM: From Tradition to Semantic - MDPI, accessed July 31, 2025, https://www.mdpi.com/2072-4292/14/13/3010</li>
<li>Slam Technology on Disaster Response - Scirp.org., accessed July 31, 2025, https://www.scirp.org/journal/paperinformation?paperid=135222</li>
<li>Active SLAM: A Review on Last Decade - MDPI, accessed July 31, 2025, https://www.mdpi.com/1424-8220/23/19/8097</li>
<li>Slam Technology on Disaster Response - Scientific Research Publishing, accessed July 31, 2025, https://www.scirp.org/pdf/wjet2024123_151561550.pdf</li>
<li>The Kinematic Navigation and Cartography Knapsack (KNaCK …, accessed July 31, 2025, https://ntrs.nasa.gov/citations/20230002760</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>