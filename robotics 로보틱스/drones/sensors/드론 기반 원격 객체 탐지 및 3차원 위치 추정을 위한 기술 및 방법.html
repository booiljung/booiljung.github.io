<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:드론 기반 원격 객체 탐지 및 3차원 위치 추정을 위한 기술 및 방법</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>드론 기반 원격 객체 탐지 및 3차원 위치 추정을 위한 기술 및 방법</h1>
                    <nav class="breadcrumbs"><a href="../../../index.html">Home</a> / <a href="../../index.html">로봇공학 (Robotics)</a> / <a href="../index.html">드론 (Drones)</a> / <a href="index.html">드론 센서</a> / <span>드론 기반 원격 객체 탐지 및 3차원 위치 추정을 위한 기술 및 방법</span></nav>
                </div>
            </header>
            <article>
                <h1>드론 기반 원격 객체 탐지 및 3차원 위치 추정을 위한 기술 및 방법</h1>
<h2>1. 서론</h2>
<p>본 안내서는 드론에 탑재된 카메라와 레이저 거리 측정기를 융합하여 원격 객체를 실시간으로 탐지하고, 해당 객체까지의 거리를 측정한 후, 최종적으로 객체의 3차원 절대 좌표(World Coordinate)를 추정하는 시스템 개발에 필요한 기존 기술과 방법론을 심층적으로 분석한다. 이 시스템의 핵심 기술 과제는 제한된 탑재 중량과 전력 환경에서 실시간 인공지능(AI) 추론, 정밀 거리 측정, 그리고 다중 센서 데이터의 시공간적 정합 및 융합을 안정적으로 수행하는 데 있다.1</p>
<p>성공적인 시스템 구현을 위해 요구되는 핵심 기술은 다음과 같이 정의할 수 있다. 첫째, <strong>임베디드 AI 기반 객체 탐지</strong> 기술로, 경량화된 딥러닝 모델을 사용하여 영상 스트림에서 목표 객체를 실시간으로 인식하고 그 위치를 이미지 상에서 특정(localization)해야 한다. 둘째, <strong>레이저 기반 거리 측정</strong> 기술로, 탐지된 객체를 향해 레이저를 조사하여 드론으로부터 객체까지의 정밀한 직선거리(slant range)를 측정해야 한다. 마지막으로, <strong>좌표계 변환 및 위치 추정</strong> 기술은 드론의 자체 위치 및 자세(6-DOF) 정보, 센서 간의 기하학적 관계(extrinsic parameters), 그리고 측정된 거리 값을 통합하여 2D 픽셀 좌표를 3D 월드 좌표로 변환하는 수학적 모델을 수립하는 것을 포함한다.</p>
<p>본 안내서는 시스템 아키텍처 설계부터 하드웨어 선정, 핵심 알고리즘 분석, 그리고 최종 시스템 통합에 이르기까지 전 개발 과정을 체계적으로 안내하는 것을 목적으로 한다. 각 장에서는 이론적 배경, 기술적 대안, 성능 비교, 그리고 실제 구현 시 고려사항을 상세히 다루어 개발팀이 기술적 의사결정을 내리는 데 필요한 포괄적인 정보를 제공한다.</p>
<h2>2.  시스템 아키텍처 및 하드웨어 구성 요소</h2>
<h3>2.1  통합 시스템 구성도</h3>
<p>개발하고자 하는 시스템은 크게 드론 플랫폼, 센서 페이로드, 그리고 지상 관제 시스템(Ground Control System, GCS)의 세 부분으로 구성된다. 이 중 핵심인 센서 페이로드는 실시간 연산을 수행하는 온보드 컴퓨터, 시각 정보를 수집하는 비전 센서(카메라), 거리를 측정하는 레이저 거리 측정기, 그리고 이들을 물리적으로 통합하고 목표물을 조준하는 짐벌(Gimbal) 시스템으로 이루어진다.</p>
<p>데이터의 흐름은 다음과 같다. 먼저 카메라가 촬영한 영상 스트림은 온보드 컴퓨터로 전송된다. 온보드 컴퓨터는 이 영상을 입력받아 객체 탐지 알고리즘을 실행하여 목표물의 이미지 내 위치를 식별한다. 식별된 목표물의 방향으로 짐벌이 정밀하게 제어되면, 레이저 거리 측정기가 목표물까지의 거리를 측정하고 그 값을 다시 온보드 컴퓨터로 전송한다. 동시에, 드론의 비행 컨트롤러(Flight Controller, FC)는 관성 측정 장치(Inertial Measurement Unit, IMU)와 위성 항법 시스템(Global Navigation Satellite System, GNSS)으로부터 드론의 현재 위치 및 자세 데이터를 수신하여 온보드 컴퓨터에 제공한다. 온보드 컴퓨터는 이 모든 정보—객체의 이미지 좌표, 거리, 드론의 위치 및 자세—를 종합하여 최종적으로 객체의 3차원 월드 좌표를 계산하고, 이를 GCS로 전송하여 사용자에게 시각화하거나 추가 임무에 활용한다.4</p>
<h3>2.2  온보드 프로세싱 유닛 선정</h3>
<p>실시간 객체 탐지 모델(예: YOLO)을 초당 30프레임(FPS) 이상으로 안정적으로 구동하기 위해서는 높은 AI 추론 성능(TOPS, Tera Operations Per Second)과 병렬 처리 능력을 갖춘 임베디드 컴퓨터가 필수적이다.6 또한, 드론에 탑재되기 위해서는 저전력 소모, 소형 폼팩터, 그리고 경량이라는 물리적 제약 조건을 반드시 만족해야 한다.</p>
<p>이러한 요구사항을 고려할 때, NVIDIA Jetson 플랫폼은 CUDA-X 소프트웨어 스택과 TensorRT 추론 가속 라이브러리를 통해 AI 모델 실행에 최적화된 환경을 제공하므로 본 프로젝트에 가장 적합한 솔루션으로 판단된다.8 Jetson 시리즈는 성능과 전력 소모에 따라 다양한 라인업을 갖추고 있어 시스템의 요구사항에 맞춰 선택할 수 있다.</p>
<ul>
<li><strong>Jetson Orin Series (AGX, NX, Nano)</strong>: 최신 Ampere 아키텍처를 기반으로 최고의 성능을 제공한다. Jetson AGX Orin은 최대 275 TOPS의 압도적인 성능으로 여러 개의 고해상도 센서 데이터와 복잡한 AI 파이프라인을 동시에 처리할 수 있어, 최고의 성능이 요구되는 시스템에 적합하다.8 반면, Jetson Orin Nano는 최대 67 TOPS의 AI 성능을 매우 작은 폼팩터에서 제공하여, 성능과 전력 효율 간의 균형이 중요한 경량 페이로드에 이상적인 선택지다.8</li>
<li><strong>Jetson Xavier Series (AGX, NX)</strong>: Orin 시리즈 이전 세대이지만 여전히 강력한 성능을 제공한다. 특히 Jetson Xavier NX는 최대 21 TOPS의 성능으로 다수의 최신 신경망을 병렬로 실행할 수 있어, 비용 효율적인 고성능 솔루션으로 고려할 수 있다.8</li>
</ul>
<p>온보드 컴퓨터의 선택은 독립적인 결정이 아니며, 시스템 전체의 성능과 직결되는 중요한 요소다. 예를 들어, 저사양 프로세서인 Jetson Orin Nano를 선택할 경우, 목표 FPS를 맞추기 위해 정확도가 다소 낮은 MobileNet-SSD와 같은 경량 모델을 사용해야 할 수 있다. 이는 객체 탐지의 신뢰도를 낮춰 레이저 조준의 정확성에까지 영향을 미칠 수 있다. 반대로, 고사양 프로세서인 Jetson AGX Orin을 사용하면 YOLOv5와 같이 더 정확하고 강인한 모델을 실시간으로 구동할 수 있어, 전체 감지-추정 파이프라인의 신뢰성을 높일 수 있다. 따라서 하드웨어 예산은 단순한 비용 제약을 넘어, 시스템의 최종 정확도를 결정하는 핵심 변수가 된다. 개발 초기 단계에서는 기능 검증과 알고리즘 개발의 유연성을 위해 Jetson AGX Orin 개발자 키트를 활용하고, 시스템이 안정화된 후 양산 단계에서는 목표 성능과 예산에 맞춰 Orin NX 또는 Orin Nano 모듈로 다운사이징하는 전략이 효과적이다.10</p>
<table><thead><tr><th>모듈명</th><th>AI 성능 (TOPS)</th><th>GPU</th><th>CPU</th><th>메모리</th><th>전력 소모 (W)</th><th>폼팩터</th></tr></thead><tbody>
<tr><td><strong>Jetson AGX Orin 64GB</strong></td><td>275</td><td>2048-core NVIDIA Ampere</td><td>12-core Arm Cortex-A78AE</td><td>64GB</td><td>15-60</td><td>100 x 87 mm</td></tr>
<tr><td><strong>Jetson Orin NX 16GB</strong></td><td>100</td><td>1024-core NVIDIA Ampere</td><td>8-core Arm Cortex-A78AE</td><td>16GB</td><td>10-25</td><td>69.6 x 45 mm</td></tr>
<tr><td><strong>Jetson Orin Nano 8GB</strong></td><td>40</td><td>1024-core NVIDIA Ampere</td><td>6-core Arm Cortex-A78AE</td><td>8GB</td><td>7-15</td><td>69.6 x 45 mm</td></tr>
<tr><td><strong>Jetson AGX Xavier</strong></td><td>32</td><td>512-core NVIDIA Volta</td><td>8-core NVIDIA Carmel Arm</td><td>32GB</td><td>10-30</td><td>100 x 87 mm</td></tr>
<tr><td><strong>Jetson Xavier NX</strong></td><td>21</td><td>384-core NVIDIA Volta</td><td>6-core NVIDIA Carmel Arm</td><td>8GB/16GB</td><td>10-15</td><td>69.6 x 45 mm</td></tr>
</tbody></table>
<h3>2.3  비전 센서(카메라) 선정</h3>
<p>카메라 센서는 시스템의 ‘눈’ 역할을 하므로, 그 성능이 객체 탐지의 성패를 좌우한다. 센서 선정 시 다음의 요소들을 종합적으로 고려해야 한다.</p>
<ul>
<li><strong>인터페이스</strong>: MIPI CSI-2 인터페이스는 USB 방식에 비해 낮은 지연 시간과 높은 대역폭을 제공한다. 이는 고해상도 영상을 압축 없이 실시간으로 처리하는 데 필수적이다. 또한, Jetson 플랫폼에 내장된 ISP(Image Signal Processor)에 직접 연결되어 이미지 전처리(예: 노이즈 제거, 색상 보정)를 하드웨어 수준에서 가속하므로, 메인 CPU의 부하를 크게 줄일 수 있다.11</li>
<li><strong>센서 사양</strong>:</li>
<li><strong>해상도</strong>: 원거리에 있는 작은 객체의 세부 특징까지 명확하게 포착하기 위해서는 8MP(메가픽셀) 이상의 고해상도 센서가 요구된다.13</li>
<li><strong>셔터 유형</strong>: 드론과 목표 객체가 빠르게 움직이는 상황에서는 롤링 셔터(Rolling Shutter) 방식의 센서에서 이미지 왜곡(jelly effect)이 발생할 수 있다. 이를 방지하기 위해 이미지 전체를 동시에 촬영하는 글로벌 셔터(Global Shutter) 방식의 센서가 선호된다.12</li>
<li><strong>저조도 성능</strong>: 야간이나 악천후와 같은 열악한 광원 환경에서도 임무를 수행하기 위해서는 저조도 환경에서 노이즈가 적고 선명한 이미지를 제공하는 센서(예: Sony의 STARVIS 시리즈)를 고려해야 한다.14</li>
<li><strong>모듈 생태계</strong>: Allied Vision, e-con Systems 등 여러 전문 제조사에서 Jetson 플랫폼용 드라이버가 사전 제공되는 다양한 MIPI 카메라 모듈을 출시하고 있다. 이는 개발 초기 단계의 통합 작업을 용이하게 하고, 다양한 센서 옵션 중에서 프로젝트 요구사항에 가장 적합한 제품을 선택할 수 있는 유연성을 제공한다.14</li>
</ul>
<h3>2.4  거리 측정 센서(레이저) 선정</h3>
<p>레이저 거리 측정기는 탐지된 객체까지의 거리를 직접 측정하는 핵심 부품이다. 드론 페이로드에 통합되기 위해서는 다음과 같은 요구사항을 만족해야 한다.</p>
<ul>
<li><strong>물리적 제약</strong>: 드론의 비행 시간에 미치는 영향을 최소화하기 위해 수십 그램 이하의 가벼운 무게와 작은 크기를 가져야 한다.16</li>
<li><strong>성능 요구사항</strong>: 수 미터에서 수 킬로미터에 이르는 넓은 측정 범위를 가지면서도, 위치 추정의 정밀도를 보장하기 위해 센티미터 수준의 측정 정확도가 필요하다.</li>
</ul>
<p>이러한 요구사항을 충족하는 기술은 ToF(Time-of-Flight) 방식의 단일 지점(single-point) 레이저 거리 측정기(LiDAR 또는 Laser Rangefinder)이다. 스캐닝 방식의 LiDAR는 주변 환경 전체의 3D 포인트 클라우드를 생성하는 데 사용되지만, 이 프로젝트에서는 카메라가 식별한 특정 지점까지의 거리만 필요하므로 단일 지점 측정기가 더 효율적이고 비용 효과적이다.</p>
<ul>
<li><strong>경량 모듈</strong>: DFRobot, Benewake, Stemedu와 같은 제조사에서 제공하는 소형 ToF 모듈은 1g에서 10g 내외의 초경량 무게로 10m 이상의 측정 범위를 제공한다. 이러한 모듈은 UART 또는 I2C 통신 인터페이스를 통해 온보드 컴퓨터와 쉽게 연결할 수 있어 프로토타이핑에 적합하다.16</li>
<li><strong>장거리 모듈</strong>: 수백 미터 이상의 장거리 측정이 필요한 경우, 더 높은 출력과 정밀도를 가진 산업용 등급의 모듈을 고려해야 한다. 이는 일반적으로 무게, 크기, 전력 소모의 증가를 수반하므로 드론의 페이로드 용량과 배터리 사양을 고려하여 신중하게 선택해야 한다.</li>
</ul>
<h3>2.5  상용 통합 페이로드 벤치마킹</h3>
<p>기존 상용 제품을 분석하는 것은 개발 목표를 설정하고 기술적 난이도를 가늠하는 데 중요한 기준을 제공한다. 이 분야의 대표적인 제품은 DJI의 Zenmuse H30 시리즈다.</p>
<ul>
<li><strong>DJI Zenmuse H30 Series</strong>: 이 제품은 고배율 줌 카메라(40MP), 광각 카메라(48MP), 레이저 거리 측정기, 그리고 열화상 카메라(H30T 모델)를 하나의 고성능 짐벌 시스템에 완벽하게 통합한 최첨단 페이로드다.19</li>
<li><strong>주요 사양 분석</strong>:</li>
<li><strong>레이저 거리 측정기</strong>: 905nm 파장의 레이저를 사용하며, 최대 3000m의 측정 범위를 가진다. 특히 500m 이내의 거리에서는 <code>±(0.2m + 측정 거리 × 0.15%)</code>라는 매우 높은 정확도를 보여준다.21 이 사양은 본 프로젝트가 목표로 해야 할 성능의 구체적인 기준점을 제시한다.</li>
<li><strong>통합 기능</strong>: 단순히 여러 센서를 장착한 것을 넘어, 줌 카메라와 레이저를 연동하여 화면의 특정 지점을 클릭하면 즉시 거리를 측정하는 ‘Click to Aim’ 기능이나, 다중 센서 데이터를 융합하여 최적의 정보를 기록하는 ‘Smart Capture’ 기능 등을 제공한다.20 이는 고도화된 소프트웨어 융합 기술의 중요성을 명확히 보여준다.</li>
</ul>
<p>DJI Zenmuse H30과 같은 상용 제품을 벤치마킹함으로써 얻는 중요한 시사점은, 최종 제품의 가치가 개별 부품의 성능 합이 아니라 이들의 완벽한 통합—기계적, 전기적, 소프트웨어적 통합—에서 나온다는 것이다. 따라서 성공적인 개발 전략은 모듈식으로 접근하여 단계적으로 통합 수준을 높여가는 것이어야 한다. <strong>1단계</strong>에서는 Jetson 개발자 키트와 같은 상용 부품을 사용하여 핵심 알고리즘(객체 탐지, 좌표 변환)을 검증하는 데 집중한다. <strong>2단계</strong>에서는 검증된 알고리즘을 바탕으로 센서들을 통합하기 위한 맞춤형 PCB 설계, 짐벌 제어 시스템 개발, 센서 융합 소프트웨어 고도화 등 통합 및 최적화에 집중한다. 이러한 단계적 접근 방식은 복잡한 하드웨어 통합 문제를 해결하기 전에 핵심 알고리즘의 불확실성을 먼저 해소함으로써 프로젝트의 리스크를 효과적으로 관리할 수 있게 한다.</p>
<h2>3.  딥러닝 기반 원격 객체 인식 기술</h2>
<h3>3.1  실시간 객체 탐지 알고리즘의 원리</h3>
<p>객체 탐지 알고리즘은 이미지나 비디오에서 특정 객체의 위치(경계 상자, bounding box)와 종류(클래스)를 식별하는 기술이다. 실시간 처리가 요구되는 드론 환경에서는 속도가 매우 중요한 요소이며, 이에 따라 알고리즘은 크게 두 가지 방식으로 나뉜다.</p>
<ul>
<li><strong>1-Stage Detectors</strong>: YOLO(You Only Look Once)와 SSD(Single Shot Detector) 계열이 대표적이다. 이 방식은 입력 이미지를 단 한 번의 신경망 연산으로 처리하여 경계 상자의 위치와 클래스 확률을 동시에 예측한다. 별도의 영역 제안 단계가 없어 구조가 간단하고 속도가 매우 빠르기 때문에, 실시간 처리가 필수적인 임베디드 시스템에 널리 채택된다.5</li>
<li><strong>2-Stage Detectors</strong>: Faster R-CNN 계열이 여기에 속한다. 이 방식은 먼저 객체가 존재할 가능성이 높은 후보 영역을 제안(Region Proposal)하고, 이후 각 후보 영역에 대해 정밀한 분류(Classification)와 위치 보정을 수행하는 2단계 접근법을 사용한다. 일반적으로 1-Stage 방식보다 정확도가 높지만, 연산 과정이 복잡하여 속도가 느리기 때문에 실시간 드론 애플리케이션에는 부적합한 경우가 많다.6</li>
</ul>
<p>대부분의 현대 객체 탐지 모델은 공통적으로 세 가지 주요 부분으로 구성된 아키텍처를 가진다. 이미지의 시각적 특징을 추출하는 <strong>Backbone</strong>, 저수준 특징과 고수준 특징을 결합하여 다양한 크기의 객체를 탐지할 수 있도록 돕는 <strong>Neck</strong>, 그리고 최종적으로 경계 상자와 클래스를 예측하는 <strong>Head</strong>가 그것이다.6</p>
<h3>3.2  임베디드 시스템을 위한 탐지 모델 비교 분석</h3>
<p>드론의 온보드 컴퓨터와 같이 계산 자원이 제한된 환경에서는 모델의 성능(정확도)과 효율성(속도, 모델 크기) 사이의 균형을 맞추는 것이 매우 중요하다.</p>
<ul>
<li>
<p><strong>YOLO (You Only Look Once)</strong>:</p>
</li>
<li>
<p><strong>YOLOv3</strong>: Darknet-53을 백본 네트워크로 사용하며, 세 가지 다른 스케일(해상도)의 피처맵에서 객체를 탐지함으로써 특히 작은 객체에 대한 탐지 성능을 크게 향상시켰다. 속도와 정확도 간의 균형이 잘 잡혀 있어 오랫동안 산업 표준으로 사용되었다.6</p>
</li>
<li>
<p><strong>YOLOv5</strong>: 인기 있는 딥러닝 프레임워크인 PyTorch로 구현되어 개발 편의성과 다른 시스템과의 통합성이 뛰어나다. 또한, nano(n), small(s), medium(m), large(l), extra-large(x) 등 다양한 크기의 모델을 제공하여 사용자가 하드웨어 사양과 요구 성능에 맞춰 최적의 모델을 선택할 수 있는 유연성을 제공한다. 특히 <code>YOLOv5s</code>는 경량화된 모델임에도 불구하고 준수한 성능을 보여 임베디드 장치에서 높은 FPS를 달성할 수 있다.5</p>
</li>
<li>
<p><strong>성능</strong>: 한 연구에 따르면, YOLOv3는 평균 정밀도(mAP) 96.5%에 0.345초의 추론 시간을 보인 반면, 더 무거운 모델인 YOLOv4는 mAP 99.5%에 1.91초의 추론 시간을 기록했다.7 이는 정확도를 높이기 위해 속도를 희생한 경우다. 저사양 임베디드 보드인 Jetson Nano 환경에서</p>
</li>
</ul>
<p><code>YOLOv5s</code>는 15 FPS를 달성하여 실시간 처리의 기준을 만족시켰다.6</p>
<ul>
<li>
<p><strong>MobileNet-SSD</strong>:</p>
</li>
<li>
<p><strong>아키텍처</strong>: Google이 개발한 MobileNet은 ’깊이별 분리 합성곱(depthwise separable convolution)’이라는 혁신적인 기법을 사용하여 모델의 파라미터 수와 연산량을 획기적으로 줄인 경량 백본 네트워크다. 이를 SSD 탐지 헤드와 결합하여 모바일 및 임베디드 장치에 최적화된 실시간 탐지 모델을 구현했다.7</p>
</li>
<li>
<p><strong>성능</strong>: MobileNet-SSD는 가장 빠른 추론 시간(0.145초)을 기록했지만, 정확도(mAP 91.4%)는 YOLO 계열보다 낮게 나타났다.7 Jetson Nano 환경에서는</p>
</li>
</ul>
<p><code>YOLOv5s</code>와 동일한 15 FPS를 달성했으나, mAP는 더 낮아 정확도 면에서는 열세를 보였다.6</p>
<p>드론 애플리케이션에서 ’실시간’은 모호한 개념이 아니라, 일반적으로 초당 15프레임(FPS) 이상이라는 명확한 기준점을 가진다.6 이 기준은 모델 선택 과정에서 중요한 필터 역할을 한다. 아무리 정확도가 높은 모델이라도 이 기준을 충족하지 못하면 실시간 추적 임무에 사용할 수 없다. 예를 들어, YOLOv4는 99.5%의 높은 정밀도를 보이지만 추론 시간이 1.91초(약 0.5 FPS)에 달해 이 프로젝트에는 부적합하다.7 반면, Jetson Nano에서 YOLOv5s와 MobileNet-SSD는 모두 15 FPS를 달성했다. 이 실시간 기준을 통과한 모델들 중에서, 우리는 더 높은 정확도를 가진 모델을 선택하게 된다. 이 경우 YOLOv5s가 MobileNet-SSD보다 높은 mAP를 기록했으므로 더 나은 선택이 된다. 즉, 모델 선정은 “가장 정확한 모델을 선택“하는 것이 아니라, “목표 하드웨어에서 실시간 기준을 만족하는 모델들 중 가장 정확한 모델을 선택“하는 제약 조건 하의 최적화 문제다.</p>
<p>이러한 분석을 종합해 볼 때, 일반적으로 <strong>YOLO 계열이 정확도 면에서 우위</strong>를 점하며, <strong>MobileNet-SSD는 속도와 경량화에 강점</strong>을 보인다. 하지만 YOLOv5s와 같은 최신 경량 YOLO 모델은 MobileNet-SSD와 비슷한 속도를 내면서도 더 높은 정확도를 제공하므로, 본 프로젝트에는 <strong>YOLO 계열의 경량 모델을 우선적으로 고려</strong>하는 것이 가장 합리적인 전략이다.6</p>
<table><thead><tr><th>모델</th><th>백본 네트워크</th><th>mAP (정확도)</th><th>FPS (Jetson Nano)</th><th>모델 크기 (MB)</th></tr></thead><tbody>
<tr><td><strong>YOLOv3-tiny</strong></td><td>Darknet-19</td><td>낮음 (참고치)</td><td>~20-30</td><td>~35</td></tr>
<tr><td><strong>YOLOv4-tiny</strong></td><td>CSPDarknet53-tiny</td><td>중간 (참고치)</td><td>~25-35</td><td>~23</td></tr>
<tr><td><strong>YOLOv5s</strong></td><td>CSPDarknet (P5)</td><td>37.6%</td><td>15</td><td>~14</td></tr>
<tr><td><strong>MobileNet-SSDv2</strong></td><td>MobileNetV2</td><td>33.7%</td><td>15</td><td>~25</td></tr>
</tbody></table>
<h3>3.3  모델 학습 및 최적화</h3>
<p>사전 학습된 모델을 그대로 사용하는 것만으로는 특정 임무 환경에서 최고의 성능을 기대하기 어렵다. 따라서 다음과 같은 학습 및 최적화 과정이 필수적이다.</p>
<ul>
<li><strong>전이 학습 (Transfer Learning)</strong>: COCO와 같이 수백만 개의 이미지를 포함하는 대규모 데이터셋으로 미리 학습된 모델을 기반으로, 사용자가 직접 수집한 특정 객체(예: 특정 종류의 차량, 특정 복장의 사람) 이미지 데이터셋을 이용해 모델을 추가로 학습시키는(미세 조정, fine-tuning) 방식이다. 이는 적은 양의 데이터로도 매우 높은 탐지 성능을 달성할 수 있는 효과적인 방법이다.</li>
<li><strong>임베디드 배포 최적화</strong>: 학습된 딥러닝 모델(예: PyTorch, TensorFlow 포맷)을 그대로 임베디드 장치에 배포하면 추론 속도가 현저히 저하될 수 있다. NVIDIA <strong>TensorRT</strong>와 같은 추론 가속 라이브러리는 이러한 문제를 해결하기 위해 설계되었다. TensorRT는 학습된 모델의 그래프를 최적화하고, 32비트 부동소수점 연산을 16비트나 8비트 정수 연산으로 변환(양자화, quantization)하는 등의 기법을 통해, 정확도 손실을 최소화하면서 추론 속도를 수 배에서 수십 배까지 향상시킨다. 이는 Jetson 플랫폼의 GPU 성능을 극대화하여 실시간 성능 목표를 달성하는 데 핵심적인 과정이다.</li>
</ul>
<p>YOLOv5와 같은 모델 ’군(family)’을 선택하는 것은 장기적인 관점에서 전략적 이점을 제공한다. 개발 초기에는 <code>YOLOv5s</code>와 같은 경량 모델을 Jetson Orin Nano에서 구동하여 프로토타입을 제작할 수 있다. 향후 제품의 성능을 향상시키기 위해 더 강력한 하드웨어(예: Jetson Orin NX)를 채택하게 되면, 동일한 학습 파이프라인과 코드베이스를 그대로 유지한 채 <code>YOLOv5m</code>이나 <code>YOLOv5l</code>과 같이 더 크고 정확한 모델로 쉽게 업그레이드할 수 있다. 이러한 ’모델 확장성’은 전체 소프트웨어 아키텍처를 재설계할 필요 없이 하드웨어 발전에 따라 시스템 성능을 유연하게 확장할 수 있게 해주므로, 제품의 생명주기 전반에 걸쳐 개발 효율성을 높이는 중요한 요소가 된다.</p>
<h2>4.  레이저 기반 거리 추정 기술</h2>
<h3>4.1  기술 1: Time-of-Flight (ToF) 원리</h3>
<p>Time-of-Flight(ToF)는 빛이나 소리와 같은 파동이 특정 거리를 왕복하는 데 걸리는 시간을 측정하여 거리를 계산하는 원리다. 본 프로젝트에서는 레이저를 광원으로 사용하는 광학 ToF 기술을 다룬다.</p>
<ul>
<li><strong>핵심 원리</strong>: 광원(레이저 또는 LED)에서 방출된 빛 펄스가 목표 객체에 반사되어 센서의 수광부로 돌아오는 데 걸리는 시간(<span class="math math-inline">\Delta t</span>)을 정밀하게 측정한다. 빛의 속도(<span class="math math-inline">c</span>)는 진공 중에서 약 초속 30만 km로 일정하므로, 드론과 객체 사이의 거리(<span class="math math-inline">d</span>)는 다음의 간단한 물리 공식으로 계산된다.25<br />
<span class="math math-display">
d = \frac{c \times \Delta t}{2}
</span><br />
여기서 거리를 2로 나누는 이유는 빛이 드론에서 객체까지 갔다가 다시 돌아오는, 즉 거리의 두 배를 이동했기 때문이다.</li>
</ul>
<p>ToF 기술은 측정 방식에 따라 크게 두 가지로 나뉜다.</p>
<ul>
<li><strong>Direct ToF (dToF)</strong>: 직접 ToF 방식은 수 나노초(nanosecond) 길이의 매우 짧은 레이저 펄스를 방출하고, 단일 광자 수준의 미세한 빛도 감지할 수 있는 고감도 센서(SPAD, Single Photon Avalanche Diode)를 사용하여 광자의 비행시간을 직접적으로 측정한다. 야외 환경에서는 태양광과 같은 강력한 배경 노이즈가 존재하는데, dToF 시스템은 수많은 측정값을 수집하여 시간대별로 히스토그램을 생성한다. 이때, 무작위로 분포하는 노이즈와 달리 실제 반사된 신호는 특정 시간대에 집중되므로, 히스토그램에서 가장 높은 빈도를 보이는 값을 실제 거리로 판단하여 노이즈의 영향을 효과적으로 제거한다. 이 방식은 장거리 측정과 주변광이 강한 환경에 특히 유리하다.26</li>
<li><strong>Indirect ToF (iToF)</strong>: 간접 ToF 방식은 일정한 주파수로 변조된 연속적인 빛(continuous wave)을 방출한다. 이 빛이 객체에 반사되어 돌아오면, 원래 방출된 파형과 수신된 파형 간의 위상차(phase shift)가 발생한다. 시스템은 이 위상차를 측정하여 거리를 간접적으로 계산한다. dToF 방식에 비해 시간 측정 회로가 간단하고, 단거리에서 높은 정밀도를 보일 수 있다는 장점이 있다.26</li>
</ul>
<h3>4.2  기술 2: 레이저 삼각측량(Laser Triangulation) 원리</h3>
<p>레이저 삼각측량은 ToF와는 전혀 다른, 기하학적 원리를 기반으로 거리를 측정하는 기술이다.</p>
<ul>
<li><strong>핵심 원리</strong>: 이 방식에서는 레이저, 카메라(센서), 그리고 측정 대상이 하나의 삼각형을 형성한다. 먼저, 레이저를 알려진 각도로 객체 표면에 투사하여 작은 광점(spot)을 만든다. 레이저와 일정한 거리(이를 ‘기선’ 또는 ’baseline’이라 함)만큼 떨어진 위치에 설치된 카메라는 이 광점을 촬영한다. 객체가 카메라로부터 멀어지거나 가까워지면, 삼각형의 형태가 변하면서 카메라의 이미지 센서에 맺히는 광점의 위치도 예측 가능한 방식으로 이동하게 된다. 시스템은 이 이미지 센서 상의 위치 변화를 삼각법 원리를 이용해 정밀한 거리 값으로 환산한다.31</li>
<li><strong>장점</strong>: 마이크로미터(<span class="math math-inline">\mu m</span>) 수준의 매우 높은 정밀도를 달성할 수 있어, 반도체 웨이퍼 검사나 정밀 부품의 품질 관리와 같은 산업 분야에서 널리 사용된다.32</li>
<li><strong>단점</strong>: 측정 원리상 측정 가능 거리가 수 미터 이내로 매우 짧다. 또한, 거울처럼 반사율이 높은 표면이나 투명한 물체는 정확한 측정이 어렵다. 무엇보다도, 태양광과 같은 외부 광원의 간섭에 매우 취약하여 주로 조명이 통제된 실내 환경에서 사용된다는 치명적인 한계가 있다.31</li>
</ul>
<h3>4.3  기술 선정 프레임워크</h3>
<p>ToF와 레이저 삼각측량은 각각의 장단점이 명확하여 적용 분야가 다르다. 본 프로젝트에 가장 적합한 기술을 선정하기 위해 두 기술을 비교 분석할 필요가 있다.</p>
<table><thead><tr><th>특징</th><th>Time-of-Flight (ToF)</th><th>레이저 삼각측량 (Triangulation)</th></tr></thead><tbody>
<tr><td><strong>측정 원리</strong></td><td>빛의 비행시간 측정</td><td>기하학적 삼각법</td></tr>
<tr><td><strong>측정 범위</strong></td><td>중장거리 (수 m ~ 수백 m 이상) 35</td><td>단거리 (수 mm ~ 수 m) 31</td></tr>
<tr><td><strong>정밀도</strong></td><td>밀리미터 ~ 센티미터 수준 35</td><td>마이크로미터 ~ 밀리미터 수준 32</td></tr>
<tr><td><strong>주변광 강인성</strong></td><td>강한 햇빛 등 외부 광원에 비교적 강함 29</td><td>외부 광원 및 표면 반사율에 매우 민감 31</td></tr>
<tr><td><strong>표면 의존도</strong></td><td>객체 색상이나 반사율에 덜 민감함</td><td>객체 표면 특성에 민감함</td></tr>
<tr><td><strong>비용</strong></td><td>일반적으로 더 높음 35</td><td>단거리용은 비용 효율적 35</td></tr>
<tr><td><strong>드론 적용 적합성</strong></td><td><strong>매우 높음</strong> (장거리, 야외 환경)</td><td><strong>매우 낮음</strong> (단거리, 실내 환경)</td></tr>
</tbody></table>
<p>두 기술의 비교는 단순한 ’범위 대 정밀도’의 트레이드오프를 넘어선다. 드론의 주된 운용 환경이 시시각각 조명이 변하는 야외라는 점을 고려하면, 기술 선택의 결정적인 필터는 ’환경 강인성’이 된다. 레이저 삼각측량은 주변광에 대한 취약성 때문에 31 범용적인 야외 드론 애플리케이션에는 근본적으로 부적합하다. 반면, ToF 기술, 특히 펄스광과 히스토그램 필터링 기법을 사용하는 dToF 방식은 야외 환경의 강한 노이즈 속에서도 신뢰성 있는 측정값을 제공하도록 설계되었다.28 따라서 이 프로젝트의 요구사항인 ’원격 객체’의 거리 측정, 즉 수십 미터 이상의 장거리 야외 측정을 위해서는 <strong>ToF 기술이 유일하고 명확한 선택지</strong>다.</p>
<p>또한, 부품을 선정할 때 ‘LiDAR’, ‘ToF 센서’, ‘레이저 거리 측정기’ 등의 용어가 혼용되어 사용되는 점에 유의해야 한다. ’LiDAR’는 종종 주변 환경의 3D 포인트 클라우드를 생성하는 스캐닝 메커니즘을 포함하는 시스템을 의미한다.25 하지만 본 프로젝트에서는 카메라가 식별한 단일 지점까지의 거리만 필요하므로, 스캐닝 기능이 없는 **‘단일 지점 ToF 레이저 거리 측정기(Single-point ToF Laser Rangefinder)’**가 가장 정확하고 효율적인 부품이다. 불필요하게 복잡하고 비싼 스캐닝 LiDAR 시스템을 구매하지 않도록 용어를 명확히 이해하고 부품을 탐색하는 것이 중요하다.</p>
<h2>5.  목표 객체 위치 추정: 알고리즘 및 수학적 모델</h2>
<h3>5.1  좌표계 정의</h3>
<p>객체의 3차원 위치를 정확하게 표현하고 계산하기 위해서는 여러 좌표계 간의 관계를 명확히 정의해야 한다.</p>
<ul>
<li><strong>월드 좌표계 (World Frame, <code>{W}</code>):</strong> 지구에 고정된 절대적인 기준 좌표계. 모든 위치 정보의 최종적인 표현 기준으로 사용된다. 일반적으로 ENU(East-North-Up) 또는 NED(North-East-Down) 좌표계를 사용한다.</li>
<li><strong>드론 동체 좌표계 (Body Frame, <code>{B}</code>):</strong> 드론의 무게 중심을 원점으로 하고, 드론의 진행 방향(전방)을 x축, 오른쪽 날개 방향을 y축, 동체 하방을 z축으로 하는 움직이는 좌표계다. IMU 센서는 이 좌표계를 기준으로 각속도와 가속도를 측정한다.</li>
<li><strong>카메라 좌표계 (Camera Frame, <code>{C}</code>):</strong> 카메라 렌즈의 광학 중심을 원점으로 하고, 카메라가 바라보는 방향(주축)을 z축, 이미지 평면의 수평 방향을 x축, 수직 하방을 y축으로 하는 좌표계다.</li>
<li><strong>이미지 픽셀 좌표계 (Image Frame, <code>{I}</code>):</strong> 2D 이미지 평면상의 좌표계로, 일반적으로 이미지의 좌측 상단을 원점 <code>(0,0)</code>으로 하여 픽셀 단위로 위치를 <code>(u, v)</code>로 표현한다.</li>
</ul>
<h3>5.2  드론 자세 및 위치 추정</h3>
<p>목표 객체의 ‘절대’ 위치를 계산하기 위해서는, 먼저 관측의 주체인 드론 자신의 절대 위치(<span class="math math-inline">P_W^B</span>, 월드 좌표계 기준 드론의 위치)와 절대 자세(<span class="math math-inline">R_W^B</span>, 월드 좌표계에 대한 드론의 회전)를 정확히 알아야 한다. 이 정보는 일반적으로 드론에 탑재된 GNSS(위치 정보 제공)와 IMU(자세 정보 제공) 센서의 데이터를 융합하여 얻는다. IMU는 수백 Hz의 빠른 주기로 자세 변화를 정밀하게 측정하지만 시간이 지남에 따라 오차가 누적(drift)되는 단점이 있다. 반면, GNSS는 수 Hz의 느린 주기로 절대 위치 정보를 제공하지만 드리프트가 없다. 확장 칼만 필터(EKF)와 같은 센서 융합 알고리즘은 두 센서의 장점을 결합하여, IMU로 빠른 움직임을 추정하고 GNSS로 누적 오차를 주기적으로 보정함으로써 안정적이고 정확한 드론의 상태(위치 및 자세)를 실시간으로 추정한다.4</p>
<h3>5.3  핵심 변환 모델: 픽셀에서 월드 좌표로</h3>
<p>이 시스템의 최종 목표는 이미지의 특정 픽셀 좌표 <span class="math math-inline">p_I = [u, v]^T</span>와 레이저로 측정한 거리 <span class="math math-inline">d</span>를 이용하여, 해당 픽셀에 대응하는 목표 객체의 월드 좌표 <span class="math math-inline">P_W^T</span>를 계산하는 것이다. 이 과정은 다음과 같은 4단계의 좌표 변환으로 이루어진다.</p>
<ul>
<li>
<p><strong>1단계: 픽셀 좌표 → 카메라 좌표계 정규화 좌표 (Pixel to Normalized Camera Coordinates)</strong></p>
</li>
<li>
<p>먼저, 2D 픽셀 좌표를 3D 공간상의 방향 벡터로 변환해야 한다. 이는 카메라의 내부 파라미터(intrinsic parameter) 행렬 <span class="math math-inline">K</span>를 사용하여 수행된다. 이 행렬은 카메라의 초점 거리(<span class="math math-inline">f_x, f_y</span>)와 주점(<span class="math math-inline">c_x, c_y</span>) 정보를 담고 있으며, 사전에 카메라 캘리브레이션 과정을 통해 정확하게 측정되어야 한다.41 픽셀 좌표</p>
</li>
</ul>
<p><span class="math math-inline">[u, v, 1]^T</span>에 <span class="math math-inline">K</span>의 역행렬을 곱하면, 초점 거리가 1인 가상의 평면(정규화 이미지 평면, normalized image plane) 상의 좌표 <span class="math math-inline">p_n = [x&#39;, y&#39;, 1]^T</span>로 변환된다. 이 벡터는 카메라의 원점으로부터 목표 객체를 향하는 3차원 방향 벡터의 역할을 한다.</p>
<p><span class="math math-display">
  p_n = K^{-1} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \begin{bmatrix} (u - c_x) / f_x \\ (v - c_y) / f_y \\ 1 \end{bmatrix}
</span></p>
<ul>
<li>
<p><strong>2단계: 거리 정보 융합 → 카메라 좌표계 3D 위치 (Distance Fusion to 3D Camera Coordinates)</strong></p>
</li>
<li>
<p>1단계에서 구한 방향 벡터 <span class="math math-inline">p_n</span>에 레이저로 측정한 거리 <span class="math math-inline">d</span>를 곱하여, 카메라 좌표계 <code>{C}</code> 내에서 목표 객체의 3차원 위치 <span class="math math-inline">P_C^T</span>를 계산한다. 이 단계에서는 레이저 포인터가 카메라의 광축과 정확히 일치한다고 가정한다. (실제로는 두 센서 간의 미세한 오프셋이 존재하며, 이는 5.1절의 외부 파라미터 캘리브레이션을 통해 보정되어야 한다.) 방향 벡터를 자신의 크기로 나누어 단위 벡터로 만든 후 거리를 곱해준다.</p>
<p><span class="math math-display">
P_C^T = d \times \frac{p_n}{\|p_n\|}
</span></p>
</li>
<li>
<p><strong>3단계: 카메라 좌표계 → 드론 동체 좌표계 변환 (Camera to Body Frame)</strong></p>
</li>
<li>
<p>카메라 좌표계에서 계산된 위치 <span class="math math-inline">P_C^T</span>를 드론의 기준 좌표계인 동체 좌표계 <code>{B}</code>로 변환한다. 이를 위해서는 카메라와 드론 동체 간의 상대적인 위치 및 회전 관계를 나타내는 외부 파라미터(extrinsic parameter) <span class="math math-inline">T_B^C</span>가 필요하다. 이 변환 행렬은 단순히 설계 도면상의 이상적인 값이 아니라, 실제 제작 및 조립 과정에서 발생하는 미세한 공차(tolerance)를 모두 포함하는 물리적인 관계를 나타낸다. <span class="math math-inline">R_B^C</span>는 회전 행렬, <span class="math math-inline">t_B^C</span>는 이동 벡터이며, 이 값들 역시 사전에 정밀한 캘리브레이션을 통해 측정되어야 한다.41</p>
<p><span class="math math-display">
P_B^T = R_B^C P_C^T + t_B^C
</span></p>
</li>
<li>
<p><strong>4단계: 드론 동체 좌표계 → 월드 좌표계 변환 (Body to World Frame)</strong></p>
</li>
<li>
<p>마지막으로, 드론 동체 좌표계 기준으로 표현된 목표 객체의 위치 <span class="math math-inline">P_B^T</span>를 절대 기준인 월드 좌표계 <code>{W}</code>로 변환한다. 이 변환에는 4.2절에서 설명한, GNSS/IMU 융합을 통해 실시간으로 추정되는 드론 자신의 월드 좌표계 기준 자세(회전 행렬 <span class="math math-inline">R_W^B</span>)와 위치(위치 벡터 <span class="math math-inline">P_W^B</span>)가 사용된다.</p>
<p><span class="math math-display">
P_W^T = R_W^B P_B^T + P_W^B
</span></p>
</li>
</ul>
<p>이 수학적 파이프라인은 여러 변환의 연속으로, 각 단계의 오차가 다음 단계로 전파되고 증폭되는 구조를 가진다. 예를 들어, IMU 센서에서 발생한 미세한 각도 오차는 부정확한 <span class="math math-inline">R_W^B</span> 행렬을 만들고, 목표물까지의 거리가 멀수록(즉, <span class="math math-inline">d</span>가 클수록) 지렛대 효과(lever-arm effect)에 의해 최종 위치 오차는 수 미터 이상으로 증폭될 수 있다. 마찬가지로, 레이저 거리 측정 오차 36나 외부 파라미터 캘리브레이션 오차 41 역시 최종 결과에 직접적인 영향을 미친다. 따라서 높은 최종 정확도를 달성하기 위해서는 어느 한 센서의 성능에만 의존하는 것이 아니라, IMU 캘리브레이션 43, GNSS/IMU 융합 39 등 시스템 전체에 걸쳐 발생하는 모든 오차 요인을 최소화하고 관리하는 것이 매우 중요하다.</p>
<h3>5.4  Z-Y-X 오일러 각 회전 행렬</h3>
<p>드론의 자세 <span class="math math-inline">R_W^B</span>는 IMU로부터 얻는 Roll(<span class="math math-inline">\psi</span>), Pitch(<span class="math math-inline">\theta</span>), Yaw(<span class="math math-inline">\phi</span>)라는 세 개의 오일러 각(Euler angles)으로 표현되는 경우가 많다. 회전 순서에 따라 여러 종류의 오일러 각이 존재하지만, 항공 분야에서는 일반적으로 Z-Y-X 순서(Yaw 회전 → Pitch 회전 → Roll 회전)가 널리 사용된다. 이 순서에 따른 회전 행렬은 각 축에 대한 기본 회전 행렬의 곱으로 다음과 같이 정의된다.44<br />
<span class="math math-display">
R_W^B = R_z(\phi) R_y(\theta) R_x(\psi)
</span><br />
이를 모두 전개한 행렬식은 다음과 같다.44<br />
<span class="math math-display">
R_W^B = \begin{bmatrix} \cos\phi\cos\theta &amp; \cos\phi\sin\theta\sin\psi - \sin\phi\cos\psi &amp; \cos\phi\sin\theta\cos\psi + \sin\phi\sin\psi \\ \sin\phi\cos\theta &amp; \sin\phi\sin\theta\sin\psi + \cos\phi\cos\psi &amp; \sin\phi\sin\theta\cos\psi - \cos\phi\sin\psi \\ -\sin\theta &amp; \cos\theta\sin\psi &amp; \cos\theta\cos\psi \end{bmatrix}
</span></p>
<h3>5.5  동적 목표물 추적 및 상태 보정</h3>
<p>4.3절의 계산을 통해 얻은 단일 측정값은 각종 센서 노이즈로 인해 참값과 오차를 가질 수 있다. 움직이는 목표물을 추적하거나 정지된 목표물의 위치를 더 정확하게 추정하기 위해서는, 시간에 따라 연속적으로 들어오는 측정값들을 통계적으로 처리하여 최적의 추정치를 얻는 필터링 기법이 필요하다. 이 역할에 가장 널리 사용되는 것이 **칼만 필터(Kalman Filter)**다.46</p>
<ul>
<li><strong>프로세스</strong>: 칼만 필터는 예측(Predict)과 업데이트(Update)의 두 단계를 반복적으로 수행한다.</li>
</ul>
<ol>
<li><strong>예측(Predict)</strong>: 이전 시점의 목표물 상태(위치, 속도)와 등속도 모델과 같은 움직임 모델을 기반으로, 현재 시점의 목표물 상태를 예측한다. 이 예측값은 불확실성을 포함한다.</li>
<li><strong>업데이트(Update)</strong>: 4.3절의 파이프라인을 통해 새로 계산된 측정값과 예측값을 통계적으로 결합(가중 평균)하여 현재 상태를 보정하고 업데이트한다. 이 과정에서 측정값의 불확실성(예: 레이저의 정확도)과 예측의 불확실성을 모두 고려하여, 통계적으로 가장 가능성이 높은 최적의 추정치를 계산한다.46</li>
</ol>
<p>칼만 필터는 단순히 노이즈를 제거하는 스무딩(smoothing) 도구를 넘어, 시스템의 <strong>확률적 융합 엔진</strong>으로서 근본적인 역할을 수행한다. 단일 측정값은 큰 오차를 포함할 수 있지만, 칼만 필터는 시간에 따른 여러 개의 불완전한 측정값들을 융합하여 불확실성을 체계적으로 줄여나간다. 예를 들어, 드론이 급격하게 기동하여 IMU의 불확실성이 커지거나 레이저 센서가 측정 분산이 큰 값을 보고할 때, 필터는 해당 측정값에 낮은 가중치를 부여하여 전체 추정치가 불안정해지는 것을 막는다. 이처럼 칼만 필터는 각 정보 소스의 신뢰도를 동적으로 평가하고 융합함으로써, 단순한 계산기를 넘어 강인한 추정 엔진으로 시스템을 격상시킨다. 목표물의 움직임이나 측정 모델이 비선형적일 경우에는 확장 칼만 필터(EKF)나 무향 칼만 필터(UKF)를 적용할 수 있다.</p>
<h2>6.  시스템 통합 및 구현 고려사항</h2>
<h3>6.1  센서 캘리브레이션</h3>
<p>센서 캘리브레이션은 4.3절의 좌표 변환 수식에 사용되는 모든 변환 행렬(<span class="math math-inline">K</span>, <span class="math math-inline">T_B^C</span>)을 정밀하게 결정하는 과정으로, 시스템 전체 정확도의 성패를 좌우하는 선결 조건이다. 부정확한 캘리브레이션은 모든 측정값에 체계적 오차(systematic error)를 유발하여, 이후의 어떤 정교한 알고리즘으로도 보정할 수 없는 결과를 낳는다.</p>
<ul>
<li><strong>내부 파라미터 캘리브레이션 (Intrinsic Calibration)</strong>: 카메라의 초점 거리, 주점, 렌즈 왜곡 계수 등 고유한 광학적 특성을 측정하는 과정이다. 체커보드와 같은 알려진 패턴을 여러 각도와 거리에서 촬영한 후, OpenCV와 같은 컴퓨터 비전 라이브러리를 사용하여 비교적 쉽게 수행할 수 있다.41</li>
<li><strong>외부 파라미터 캘리브레이션 (Extrinsic Calibration)</strong>: 서로 다른 센서들 간의 3차원 공간상 상대적인 위치와 방향을 결정하는 과정이다. 드론과 센서를 제작하고 조립하는 과정에서 발생하는 미세한 공차로 인해, 설계 도면만으로는 센서 간의 정확한 공간적 관계를 알 수 없다. 따라서 이 관계를 실험적으로 측정하는 외부 파라미터 캘리브레이션은 시스템의 정확도를 보장하기 위한 필수적인 과정이다.</li>
<li><strong>카메라-IMU</strong>: 두 센서 간의 공간적 변환 관계뿐만 아니라 시간적 오프셋(time offset)까지 정밀하게 측정하는 것이 중요하다. Kalibr 48와 같은 오픈소스 툴킷은 이를 위한 표준적인 해결책을 제공한다.</li>
<li><strong>카메라-레이저</strong>: 이 프로젝트의 핵심적인 캘리브레이션 과정이다. 한 가지 정석적인 방법은 캘리브레이션 타겟(예: 체커보드)을 사용하는 것이다. 타겟의 각 모서리 점들은 카메라 이미지에서는 2D 픽셀 좌표로, LiDAR 포인트 클라우드에서는 3D 공간 좌표로 각각 추출된다. 이렇게 얻은 여러 개의 2D-3D 대응점 쌍을 이용하여, 두 센서 좌표계 간의 회전 및 이동 변환 행렬을 최적화 문제(예: Perspective-n-Point, PnP)를 풀어 계산할 수 있다.41 한편, 연구 50에서는 카메라 픽셀 좌표와 LiDAR 측정 좌표 간의 관계를 나타내는 수천 개의 데이터 샘플을 수집한 후, 이들 간의 매핑 함수를 2차 다항식으로 근사(curve fitting)하는 실용적인 접근법을 제시하기도 했다.</li>
</ul>
<p>이처럼 센서 융합 시스템에서 캘리브레이션은 단순한 초기 설정 작업이 아니라, 시스템의 최대 성능을 결정하는 가장 근본적인 토대다. 카메라-IMU 회전각에 단 1도의 오차만 있어도 원거리 목표물의 위치는 수 미터 이상 틀어질 수 있다. 따라서 프로젝트 초기 단계에서 신뢰할 수 있는 캘리브레이션 장비와 절차를 구축하는 데 충분한 시간과 노력을 투입해야 한다.</p>
<h3>6.2  소프트웨어 아키텍처</h3>
<p>복잡한 다중 센서 시스템을 효율적으로 개발하고 유지보수하기 위해서는 모듈화된 소프트웨어 아키텍처가 필수적이다.</p>
<ul>
<li><strong>ROS (Robot Operating System) 기반 설계</strong>: ROS는 로봇 시스템 개발을 위한 사실상의 표준 프레임워크다. 각 기능(예: 카메라 드라이버, 객체 탐지, 위치 추정)을 독립적으로 실행되는 ’노드(Node)’로 구현하고, 이들 노드 간에 표준화된 메시지 형식인 ’토픽(Topic)’을 통해 데이터를 비동기적으로 주고받는 방식을 사용한다. 이러한 구조는 시스템의 모듈화, 코드 재사용성, 그리고 디버깅 용이성을 크게 향상시킨다.</li>
<li><strong>데이터 흐름 예시</strong>:</li>
</ul>
<ol>
<li>카메라 드라이버 노드는 <code>/camera/image_raw</code> 토픽으로 영상 데이터를 발행한다.</li>
<li>YOLO 노드는 이 토픽을 구독하여 객체를 탐지하고, <code>/detector/bounding_boxes</code> 토픽으로 경계 상자 정보를 발행한다.</li>
<li>레이저 드라이버 노드는 <code>/laser/range</code> 토픽으로 거리 데이터를 발행한다.</li>
<li>드론의 비행 컨트롤러 인터페이스 노드는 <code>/imu/data</code>와 <code>/gps/fix</code> 토픽을 발행하고, EKF 노드는 이를 구독하여 <code>/drone/state</code> 토픽으로 드론의 현재 상태를 발행한다.</li>
<li>최종 위치 추정 노드는 <code>/detector/bounding_boxes</code>, <code>/laser/range</code>, <code>/drone/state</code>, 그리고 캘리브레이션 정보가 담긴 <code>/tf</code> 토픽을 모두 구독하여 최종 계산된 목표물의 월드 좌표를 <code>/estimator/target_world_pose</code> 토픽으로 발행한다.</li>
</ol>
<p>ROS를 채택하는 것은 단순한 개발 편의성을 넘어, 프로젝트의 리스크를 관리하고 미래 확장성을 확보하는 전략적 결정이다. 만약 시스템이 단일 프로그램(monolithic application)으로 작성된다면, 최종 위치 값에 오류가 발생했을 때 그 원인이 카메라 드라이버인지, 탐지 알고리즘인지, 좌표 변환 코드인지 찾아내기가 매우 어렵다. ROS를 사용하면 <code>rostopic echo</code>와 같은 도구를 통해 각 파이프라인 단계의 데이터를 실시간으로 확인할 수 있어 문제의 원인을 신속하게 분리하고 해결할 수 있다. 또한, 이러한 모듈성은 향후 시스템 업그레이드를 용이하게 한다. 예를 들어, <code>YOLOv5s</code>를 사용하는 탐지 노드를 더 성능이 좋은 <code>YOLOv8</code> 기반의 노드로 교체하더라도, 동일한 토픽 이름과 메시지 형식을 사용한다면 다른 노드들을 전혀 수정할 필요가 없다. 이는 시스템을 강인하고, 유지보수하기 쉬우며, 미래 기술 발전에 유연하게 대응할 수 있도록 만든다.</p>
<h3>6.3  주요 기술적 과제 및 해결 방안</h3>
<ul>
<li><strong>센서 데이터 동기화 (Synchronization)</strong>: 카메라, 레이저, IMU는 각각 다른 시간과 다른 주기로 데이터를 생성한다. 정확한 위치 추정을 위해서는 특정 시점의 모든 센서 데이터를 정렬해야 한다. 가장 정확한 방법은 하드웨어 트리거를 사용하여 모든 센서가 동일한 순간에 데이터를 획득하도록 하는 것이다. 이것이 불가능할 경우, 모든 데이터에 정밀한 타임스탬프를 기록하고, 소프트웨어적으로 특정 타임스탬프를 기준으로 가장 가까운 시간의 데이터들을 보간(interpolation)하여 사용하는 방법이 있다. ROS는 이러한 시간 기반 데이터 필터링을 위한 라이브러리를 제공한다.39</li>
<li><strong>처리 지연 (Latency)</strong>: 영상 수신부터 객체 탐지, 거리 측정, 위치 계산에 이르는 전체 파이프라인에서 발생하는 지연 시간은 빠르게 움직이는 목표물을 추적할 때 큰 오차를 유발한다. TensorRT를 활용한 모델 최적화, 효율적인 C++ 코드 작성, 그리고 고성능 온보드 컴퓨터 사용을 통해 전체 파이프라인의 지연을 수십 밀리초(ms) 이내로 최소화해야 한다.</li>
<li><strong>GPS 음영 지역 운용 (GPS-Denied Operation)</strong>: 도심의 빌딩 숲이나 실내 환경에서는 GNSS 신호가 불안정해지거나 완전히 수신되지 않을 수 있다.3 이러한 GPS 음영 지역에서 임무를 지속하기 위해서는 드론의 위치를 추정하기 위한 대체 기술이 필요하다. VIO(Visual-Inertial Odometry) 또는 LiDAR-Inertial Odometry와 같은 SLAM(Simultaneous Localization and Mapping) 기술은 카메라나 LiDAR 센서로 주변 환경의 특징점을 추적하고, 이를 IMU 데이터와 융합하여 드론의 상대적인 움직임을 정밀하게 추정한다. 이를 통해 GNSS 없이도 일정 시간 동안 위치를 계속해서 추적할 수 있다. 한 연구에서는 카메라, IMU, 그리고 고도 측정용 1D 레이저를 융합한 RVIO(Range-Visual-Inertial Odometry)를 제안하여 VIO의 고질적인 스케일 드리프트 문제를 효과적으로 억제하는 방법을 보여주었다.51</li>
</ul>
<h2>7. 결론 및 제언</h2>
<p>본 안내서는 드론에 카메라와 레이저 거리 측정기를 탑재하여 원격 객체의 3차원 위치를 추정하는 시스템 개발에 필요한 핵심 기술과 방법론을 종합적으로 분석했다. 분석 결과를 바탕으로 다음과 같은 기술 선택과 개발 로드맵을 제언한다.</p>
<h3>7.1 핵심 기술 선택 요약</h3>
<ul>
<li><strong>온보드 컴퓨터</strong>: 성능, 전력 소모, 비용 간의 최적의 균형을 제공하는 <strong>NVIDIA Jetson Orin Nano 또는 Orin NX</strong>를 선정하는 것이 타당하다.</li>
<li><strong>객체 탐지 알고리즘</strong>: 실시간성과 정확도를 모두 고려했을 때, <strong>YOLOv5s 또는 그 이후의 경량 YOLO 모델</strong>을 기반으로 특정 목표물에 맞게 전이 학습을 수행하는 것이 가장 효과적이다.</li>
<li><strong>거리 측정 기술</strong>: 장거리 측정 능력과 야외 환경에서의 강인성을 고려하여, <strong>ToF(Time-of-Flight) 기반의 단일 지점 레이저 거리 측정기</strong>를 채택해야 한다.</li>
<li><strong>위치 추정 알고리즘</strong>: 각종 센서의 측정 불확실성을 통계적으로 처리하고 동적 목표물을 안정적으로 추적하기 위해 <strong>칼만 필터(또는 EKF) 기반의 센서 융합 알고리즘</strong>을 구현해야 한다.</li>
</ul>
<h3>7.2 개발 로드맵 제안</h3>
<ol>
<li><strong>1단계 (프로토타이핑 및 알고리즘 검증)</strong>: 상용 Jetson 개발자 키트, 웹캠, UART/I2C 인터페이스의 ToF 모듈을 사용하여 벤치탑 환경을 구축한다. 이 환경에서 ROS를 기반으로 4장과 5장에서 기술된 위치 추정 파이프라인의 핵심 로직을 구현하고 그 타당성을 검증하는 데 집중한다.</li>
<li><strong>2단계 (하드웨어 통합 및 캘리브레이션)</strong>: 프로젝트 요구사항에 맞춰 선정한 MIPI 카메라와 경량 ToF 모듈을 짐벌 시스템에 물리적으로 통합한다. 이후, 시스템의 기하학적 모델을 완성하기 위해 정밀한 센서 캘리브레이션 절차를 수립하고, 정확한 외부 파라미터를 획득한다.</li>
<li><strong>3단계 (비행 시험 및 성능 최적화)</strong>: 개발된 페이로드를 실제 드론에 탑재하여 다양한 비행 환경에서 시스템을 테스트한다. 이 과정에서 발생하는 데이터 동기화, 처리 지연, 동적 환경에서의 추적 성능 저하 등 실질적인 문제들을 해결하며 시스템을 안정화하고 고도화한다.</li>
</ol>
<h3>7.3 최종 제언</h3>
<p>본 프로젝트의 성공은 개별 하드웨어의 성능보다 <strong>정밀한 센서 캘리브레이션</strong>과 <strong>강인한 센서 융합 알고리즘</strong> 개발에 달려있다. 특히, 카메라-레이저-IMU 간의 상대적인 위치와 방향을 나타내는 외부 파라미터를 정확하게 추정하는 것이 전체 시스템 정확도를 좌우하는 가장 중요한 요소다. 따라서 개발 초기 단계부터 신뢰성 있는 캘리브레이션 환경을 구축하고, 이를 검증하는 절차를 수립하는 데 충분한 시간과 노력을 투입할 것을 강력히 권고한다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Simultaneous Localization and Mapping (SLAM) and Data Fusion in Unmanned Aerial Vehicles: Recent Advances and Challenges - MDPI, https://www.mdpi.com/2504-446X/6/4/85</li>
<li>Sensor Fusion for Robust UAV Navigation - Xray, https://xray.greyb.com/drones/sensor-fusion-navigation</li>
<li>Precision without GPS: Multi-Sensor Fusion for Autonomous Drone Navigation in Complex Environments - ijircst.org, https://www.ijircst.org/DOC/6-Precision-without-GPS-Multi-Sensor-Fusion-for-Autonomous-Drone-Navigation-in-Complex-Environments.pdf</li>
<li>Unmanned aerial vehicle - Wikipedia, https://en.wikipedia.org/wiki/Unmanned_aerial_vehicle</li>
<li>Basic architecture of drone surveillance system. - ResearchGate, https://www.researchgate.net/figure/Basic-architecture-of-drone-surveillance-system_fig1_354673348</li>
<li>(PDF) Comparison of YOLOv3, YOLOv5s and MobileNet-SSD V2 for …, https://www.researchgate.net/publication/353211011_Comparison_of_YOLOv3_YOLOv5s_and_MobileNet-SSD_V2_for_Real-Time_Mask_Detection</li>
<li>Performance Analysis of YOLOv3, YOLOv4 and MobileNet SSD for Real Time Object Detection - ResearchGate, https://www.researchgate.net/publication/381851712_Performance_Analysis_of_YOLOv3_YOLOv4_and_MobileNet_SSD_for_Real_Time_Object_Detection</li>
<li>Jetson Modules, Support, Ecosystem, and Lineup | NVIDIA Developer, https://developer.nvidia.com/embedded/jetson-modules</li>
<li>Autonomous Machines: The Future of AI - NVIDIA Jetson, https://www.nvidia.com/en-us/autonomous-machines/</li>
<li>Buy the Latest Jetson Products - NVIDIA Developer, https://developer.nvidia.com/buy-jetson</li>
<li>THEIA-CAM MIPI/CSI Camera - JetsonHacks, https://jetsonhacks.com/2024/08/02/theia-cam-mipi-csi-camera/</li>
<li>Edge AI Using Cameras, 3D Time-of-Flight, and NVIDIA Jetson | D3 Embedded, https://www.d3embedded.com/solutions/artificial-intelligence-system-using-2d-3d-mipi-csi-2-cameras-and-nvidia-jetson-modules/</li>
<li>VC MIPI Camera Modules and NVIDIA Jetson Processors, https://www.mipi-modules.com/en/mipi-compatible-processor-boards/nvidia/</li>
<li>Cameras - Silicon Highway, https://www.siliconhighwaydirect.com/category-s/1829.htm</li>
<li>Alvium camera modules for NVIDIA® Jetson™ platform users - Allied Vision, https://www.alliedvision.com/en/products/embedded-vision-camera-modules-for-nvidia-jetson-platform-users/</li>
<li>ToF Laser Ranging Module for Outdoor (15m) - DFRobot, https://www.dfrobot.com/product-2536.html</li>
<li>Ultra-Compact ToF Laser Ranging Sensor (7.8m, 1 gram) - DFRobot, https://www.dfrobot.com/product-2919.html</li>
<li>Stemedu Small TFmini-S Lidar Sensor Benewake 0.1-12m TOF Micro Laser Range Finder Module Lightweight Distance Detector Sensor with UART / I2C Interface for Drone for Arduino for Raspberry Pi Pixhawk - Newegg, https://www.newegg.com/p/3C6-00TP-005N1</li>
<li>Zenmuse H30 Series - DATA SHEET, https://optron.com/dji/wp-content/uploads/2024/06/ZENMUSE-H30-SERIES.pdf</li>
<li>Zenmuse H30 Series - Flagship All-Weather Multi-Sensor Payload - DJI Enterprise, https://enterprise.dji.com/zenmuse-h30-series</li>
<li>Support for Zenmuse H30 Series - DJI, https://www.dji.com/support/product/zenmuse-h30-series</li>
<li>Zenmuse H30 Series - Specs - DJI Enterprise, https://enterprise.dji.com/zenmuse-h30-series/specs</li>
<li>Real-Time Object Detection: Comparing YOLO and SSD Architectures in Surveillance Systems - Science Excel, https://www.sciencexcel.com/articles/WctZlQ6tKQTUDzPNXlUdq1MfOKtAAT10ZNaSZXrZ.pdf</li>
<li>Revolutionizing Real-Time Object Detection: YOLO and MobileNet SSD Integration - Journal of Computing &amp; Biomedical Informatics, https://jcbi.org/index.php/Main/article/download/266/194</li>
<li>Mastering LiDAR with DJI Enterprise: An Introductory Booklet - Insights, https://enterprise-insights.dji.com/blog/lidar-basic-guide</li>
<li>Time-of-Flight principle - Terabee, https://www.terabee.com/time-of-flight-principle/</li>
<li>How LiDAR Works: The Time-of-Flight (ToF) Principle - Move-X, https://www.move-x.ai/news/how-lidar-works-the-time-of-flight-tof-principle</li>
<li>Understanding Time of Flight Systems: TOF,iTOF,and dTOF - LumiMetriC, https://www.lumimetric.com/en/new/TOF-time-of-flight-definition-and-principle.html</li>
<li>Time of Flight System for Distance Measurement and Object Detection - Analog Devices, https://www.analog.com/en/resources/technical-articles/tof-system-for-distance-measurement-and-object-detection.html</li>
<li>Distance measurement | Hamamatsu Photonics, https://www.hamamatsu.com/jp/en/applications/measurement/distance-measurement.html</li>
<li>Principles of Laser Triangulation | Learn About Machine Vision …, https://hermary.com/learning/principles-of-laser-triangulation/</li>
<li>Understanding Triangulation: Future of Measuring - ZEISS, https://www.zeiss.com/metrology/en/explore/topics/triangulation.html</li>
<li>Triangulation measurement: how it works and what to use it for? - Sensor Partners, https://sensorpartners.com/en/knowledge-base/what-is-a-triangulation-or-triangulation/</li>
<li>Laser Triangulation with Embedded Vision, https://www.vision-components.com/en/heading-into-3rd-dimension/</li>
<li>Compare Time Of Flight And Triangulation Laser Sensors &amp; Selection Guide 2025, https://meskernel.net/en/compare-time-of-flight-and-triangulation/</li>
<li>ToF: Time-of-Flight - Overview, Principles, Advantages - AVSystem, https://avsystem.com/blog/linkyfi/time-of-flight</li>
<li>meskernel.net, <a href="https://meskernel.net/en/compare-time-of-flight-and-triangulation/#:~:text=*Range%20Requirements%3A%20For%20long-,added%20value%20in%20challenging%20environments.">https://meskernel.net/en/compare-time-of-flight-and-triangulation/#:~:text=*Range%20Requirements%3A%20For%20long%2D,added%20value%20in%20challenging%20environments.</a></li>
<li>Time Of Flight Sensor Vs LiDAR: Cost, Range &amp; Best Use Cases Compared - Meskernel, https://meskernel.net/en/time-of-flight-sensor-vs-lidar/</li>
<li>A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping - arXiv, https://arxiv.org/pdf/2503.01202</li>
<li>Navigating Skies with Precision: Multi-Sensor Fusion in UAV Navigation, https://www.commercialuavnews.com/surveying/navigating-skies-with-precision-multi-sensor-fusion-in-uav-navigation</li>
<li>Livox-SDK/livox_camera_lidar_calibration: Calibrate the extrinsic parameters between Livox LiDAR and camera - GitHub, https://github.com/Livox-SDK/livox_camera_lidar_calibration</li>
<li>unmannedlab/imu_lidar_calibration: Target-free Extrinsic Calibration of a 3D Lidar and an IMU - GitHub, https://github.com/unmannedlab/imu_lidar_calibration</li>
<li>DJI Mavic 3 Enterprise: How to Calibrate the IMU - YouTube, https://www.youtube.com/watch?v=0-LugVzA0KE</li>
<li>Rotation in the Space∗, https://faculty.sites.iastate.edu/jia/files/inline-files/rotation.pdf</li>
<li>Yaw, pitch, and roll rotations - Steven M. LaValle, https://msl.cs.uiuc.edu/planning/node102.html</li>
<li>Position estimation of a flying target from a camera onboard a UAV using visual tracking - ČVUT DSpace, https://dspace.cvut.cz/bitstream/handle/10467/115332/F3-DP-2024-Morhunenko-Mykola-Position_estimation_of_a_flying_target_from_a_camera_onboard_a_UAV_using_visual_tracking.pdf</li>
<li>Moving Target Geolocation and Trajectory Prediction Using a Fixed-Wing UAV in Cluttered Environments - MDPI, https://www.mdpi.com/2072-4292/17/6/969</li>
<li>ethz-asl/kalibr: The Kalibr visual-inertial calibration toolbox - GitHub, https://github.com/ethz-asl/kalibr</li>
<li>Camera IMU LRF calibration · ethz-asl/kalibr Wiki - GitHub, https://github.com/ethz-asl/kalibr/wiki/Camera-IMU-LRF-calibration</li>
<li>IEEE Conference Paper Template, <a href="https://aast.edu/pheed/staffadminview/pdf_retreive.php?url=5176_70000252775_Multiple+Objects+Detection+and+Localization+using+Data+Fusion_research.pdf&amp;stafftype=staffpdfnew">https://aast.edu/pheed/staffadminview/pdf_retreive.php?url=5176_70000252775_Multiple%20Objects%20Detection%20and%20Localization%20using%20Data%20Fusion_research.pdf&amp;stafftype=staffpdfnew</a></li>
<li>Range–Visual–Inertial Odometry with Coarse-to-Fine Image Registration Fusion for UAV Localization - MDPI, https://www.mdpi.com/2504-446X/7/8/540</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>