<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:농작물 자동 이식 및 파종을 위한 AI 기반 유공 비닐 인식 및 방향 지시 기술 개발</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>농작물 자동 이식 및 파종을 위한 AI 기반 유공 비닐 인식 및 방향 지시 기술 개발</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">로봇공학 (Robotics)</a> / <a href="index.html">자율 이동 로봇</a> / <span>농작물 자동 이식 및 파종을 위한 AI 기반 유공 비닐 인식 및 방향 지시 기술 개발</span></nav>
                </div>
            </header>
            <article>
                <h1>농작물 자동 이식 및 파종을 위한 AI 기반 유공 비닐 인식 및 방향 지시 기술 개발</h1>
<p>2025-11-06, G25DR</p>
<h2>1.  정밀 파종 및 이식을 위한 AI 기반 비전 시스템의 기술적 요구사항</h2>
<h3>1.1  서론: 정밀 농업 자동화의 현주소와 기술적 공백</h3>
<p>정밀 농업(Precision Agriculture)의 구현에 있어, 자동화된 파종 및 이식 시스템은 노동력 절감과 수확량 증대를 위한 핵심 기술로 확고히 자리 잡았다.1 현재 시장을 주도하는 상용 자동화 시스템들은 이 목표를 달성하기 위해 고도로 정밀한 제어 기술을 사용하지만, AI를 활용한 ’실시간 유공 비닐 홀 인식’이라는 과제에 대해서는 각기 다른 방식으로 접근하거나 우회한다.</p>
<p>대표적으로, Kubota의 Agri Robo Rice-Transplanter는 AI 비전 시스템을 통한 ‘홀’ 인식이 아닌, RTK-GPS(Real-Time Kinematic GPS)를 활용한 센티미터(cm)급 위치 결정을 기반으로 ’사전에 정의된 경로’를 정확히 추종하는 방식으로 무인 이앙을 수행한다.3 이 시스템에서 비전 시스템의 역할은 실시간 홀 탐지보다는 장애물 감지나 경로 보조에 국한될 가능성이 크며, 비닐에 미리 뚫린 구멍의 위치를 능동적으로 찾아 대응하지 않는다.</p>
<p>또 다른 상용 시스템인 PlantTape은 AI 비전의 불확실성을 원천적으로 회피하는 기계공학적 접근법을 채택했다.4 이 시스템은 모종이 일정한 간격으로 부착된 ’테이프’를 농기계가 기계적으로 풀어 이식하는 방식으로, 비전 인식 시스템 없이도 높은 정밀도를 달성한다.4</p>
<p>이러한 현존 최첨단 상용 시스템들의 접근 방식은, 실제 농업 현장(field)에서 ‘실시간 유공 비닐 홀 인식 및 대응’ 기술이 높은 강건성(robustness)을 확보하기 매우 어렵다는 현실을 반증한다. 따라서, 본 보고서가 다루는 ‘유공 비닐 인식 및 방향 지시 AI 모델’ 개발은 현존 상용 기술의 한계를 뛰어넘는 차세대 R&amp;D의 핵심 과제(Blue Ocean)라 할 수 있다.</p>
<p>본 보고서는 이러한 기술적 공백을 메우기 위한 구체적인 전략을 제시하는 것을 목적으로 한다. 이를 위해 (A) 열악한 현장 환경에서도 강건하게 작동하는 홀 인식 AI 모델 아키텍처와 (B) 인식된 홀의 위치로 로봇을 정밀하게 유도하는 ‘방향 지시’ 제어 시스템 5을 개발하는 데 필요한 핵심 기술, 제어 전략, 그리고 가장 중요한 데이터 구축 방법론을 체계적으로 분석 및 제시한다.</p>
<h3>1.2  시스템 요구사항 정의: ’인식(Perception)’에서 ’행동(Action)’으로</h3>
<p>자동 이식 및 파종 로봇의 작업 흐름(workflow)은 다음과 같이 명확히 정의될 수 있다:</p>
<ol>
<li>
<p><strong>(전역 항법)</strong> 농기계 본체(트랙터 또는 로봇 플랫폼)가 RTK-GPS 등을 이용해 밭의 이랑을 따라 이동한다.</p>
</li>
<li>
<p><strong>(스캔)</strong> 기계에 장착된 카메라가 비닐 표면을 실시간으로 스캔한다.</p>
</li>
<li>
<p><strong>(인식)</strong> AI 모델이 카메라 이미지 내에서 다음에 작업해야 할 ’유공 비닐 홀’의 2D 또는 3D 위치를 식별한다.</p>
</li>
<li>
<p><strong>(방향 지시)</strong> 인식된 픽셀 좌표를 로봇 매니퓰레이터 또는 이식기가 도달해야 할 실제 세계의 3D 좌표로 변환한다.6</p>
</li>
<li>
<p><strong>(행동)</strong> 이식기 또는 파종기가 해당 위치로 정밀하게 이동하여 5 작업을 수행한다.</p>
</li>
</ol>
<p>이 과정에서 ’인식’과 ‘방향 지시’ 단계는 각각 고유한 기술적 난제들을 포함한다.</p>
<p>인식(Perception)의 과제:</p>
<p>단순히 ’구멍’을 찾는 것을 넘어, 실제 농업 현장의 복잡성을 극복해야 한다.</p>
<ul>
<li>
<p><strong>(가려짐)</strong> 작물의 잎이 홀의 일부 또는 전체를 가리는 경우.7</p>
</li>
<li>
<p><strong>(시각적 모호성)</strong> 흙덩이, 돌, 웅덩이, 그림자가 홀과 시각적으로 매우 유사하여 오인(False Positive)을 유발하는 경우.8</p>
</li>
<li>
<p><strong>(환경 변화)</strong> 다양한 조명 조건(직사광선, 그늘)이나 비와 같은 악천후로 인해 이미지 품질이 저하되는 경우.9</p>
</li>
<li>
<p><strong>(경계 불분명성)</strong> 흙과 비닐의 경계가 명확하지 않은 파종 구멍을 탐지해야 하는 경우.10</p>
</li>
</ul>
<p>방향 지시(Action)의 과제:</p>
<p>인식된 정보를 바탕으로 정밀한 기계적 행동을 수행해야 한다.</p>
<ul>
<li>
<p><strong>(실시간 추적)</strong> 고속으로 이동하는 농기계의 진동 11 속에서도 AI 모델은 홀을 놓치지 않고 실시간으로 추적해야 한다.</p>
</li>
<li>
<p><strong>(좌표 변환)</strong> 카메라가 인식한 2D 이미지 픽셀 좌표를 로봇 팔(end-effector)이 실제로 움직여야 할 3D 물리 공간 좌표로 정확하게 변환해야 한다.6</p>
</li>
<li>
<p><strong>(정밀 제어)</strong> 변환된 3D 좌표까지 로봇 팔을 빠르고 정확하게 이동시키는 비전 서보(Visual Servoing) 제어 기술이 필수적이다.5</p>
</li>
</ul>
<h2>2.  유공 비닐 홀(Hole) 인식을 위한 딥러닝 아키텍처 심층 분석</h2>
<p>유공 비닐 홀을 인식하기 위한 접근법은 크게 ‘속도’ 중심의 객체 탐지와 ‘정밀도’ 중심의 시맨틱 세분화로 나뉜다.</p>
<h3>2.1  접근법 1: 객체 탐지 (Object Detection) - 속도와 효율성 중심 (YOLO 계열)</h3>
<p>YOLO(You Only Look Once) 계열의 아키텍처는 이미지를 한 번만 보는(single-pass) 방식으로 실시간 객체 탐지를 수행한다.12 이러한 압도적인 추론 속도는 농업용 로봇 9이나 자율주행차 12와 같이 빠른 반응 속도가 필수적인 시스템에서 핵심적인 역할을 수행한다.</p>
<ul>
<li>
<p><strong>출력:</strong> 홀의 위치를 ‘바운딩 박스(Bounding Box)’ 13, 즉 사각형 좌표로 반환한다.</p>
</li>
<li>
<p><strong>장점:</strong> 추론 속도(FPS, Frames Per Second)가 매우 빠르다. 로봇이 홀의 대략적인 ’중심점(Centroid)’으로 신속하게 접근하는 데 최적화되어 있다.</p>
</li>
<li>
<p><strong>한계:</strong> 홀의 정확한 ’모양’이나 ‘크기’, ’방향성’을 픽셀 단위로 알 수 없다.15 이는 특히 이식(transplanting) 작업 시, 모종(seedling plug)이 홀에 정확히 맞는지, 또는 홀이 찢어지거나 손상되었는지 판단하기 어렵게 만든다.</p>
</li>
</ul>
<p>이러한 한계를 극복하고 농업 현장의 특수성에 맞춘 YOLO 모델들이 개발되고 있다.</p>
<ul>
<li>특화 모델 1: Mulch-YOLO (YOLOv11 기반)</li>
</ul>
<p>면화밭에 남은 ’플라스틱 멀치 잔여물’을 탐지하기 위해 경량화된 YOLOv11n을 기반으로 개발되었다.16 이 모델은 YOLOv11n 대비 모델 파라미터 수를 24% 줄이면서도, 탐지 정확도(mAP@0.5)는 4.7% 향상시켜 경량화(lightweight)와 고성능을 동시에 달성했다.16 비록 ‘홀’ 탐지가 아닌 ‘잔여물’ 탐지가 목적이지만, 이러한 경량화 접근법은 엣지 디바이스(Edge AI) 적용에 중요한 방향성을 제시한다.17</p>
<ul>
<li>특화 모델 2: YOLO-PH (YOLOv8 기반)</li>
</ul>
<p>더 직접적인 사례로, ‘경계가 불분명한 파종 구멍(indistinct boundary planting holes)’ 탐지라는 10, 본 보고서의 과제와 거의 동일한 문제를 해결하기 위해 YOLOv8을 개선한 모델이다.10 YOLO-PH는 다음과 같은 핵심 모듈을 통해 성능을 극대화했다:</p>
<ol>
<li>
<p><strong>C2f_DyGhostConv 모듈:</strong> 기존 YOLOv8의 C2f 모듈 내 병목(bottleneck) 구조를 DyGhostConv로 대체했다. 이는 GhostNet의 원리를 차용하여 적은 파라미터로 더 많은 유효 특징(feature)을 생성하며, 특히 매우 작거나 변동성이 큰 대상(즉, 파종 구멍)에 대한 특징 추출 능력을 향상시켰다.10</p>
</li>
<li>
<p><strong>ATSS (Adaptive Training Sample Selection):</strong> 홀과 같이 비교적 고정된 형상을 가진 대상에 더 적합한 ATSS 레이블 할당 방식을 채택했다. 이를 통해 훈련 과정에서 긍정(positive) 샘플과 부정(negative) 샘플을 더 정밀하게 선별하여 모델의 탐지 능력을 향상시켰다.10</p>
</li>
<li>
<p><strong>Siblings Detection Head (SD-Head):</strong> 다중 스케일(P3, P4, P5) 특징 맵을 처리하는 탐지 헤드(Detection Head)가 공통의 컨볼루션 모듈을 공유하도록 설계한 경량화 헤드이다. 이로 인해 모델의 연산 부담(FLOPs)을 48.8%나 감소시키면서도, 탐지 속도(FPS)는 26.8 증가시키고 정확도(mAP)까지 1.3% 향상시키는 결과를 달성했다.10</p>
</li>
</ol>
<p>유공 비닐 홀 탐지의 핵심 난제 중 하나는 ’경계의 불분명성(indistinct boundaries)’이다.10 표준 YOLO 모델은 자동차나 사람처럼 명확한 객체에는 강하지만, 흙과 비닐의 경계가 모호한 농업 현장에서는 성능 저하를 겪을 수 있다.8 YOLO-PH의 성공 10은 C2f_DyGhostConv와 같은 모듈이 텍스처(texture)와 미세 특징(fine-grained features) 추출에 더 강하다는 것을 입증한다. 따라서 홀 탐지 모델 개발 시, GhostNet, MobileNet 17 또는 어텐션 메커니즘 18을 백본 네트워크에 결합하여 경량화와 특징 추출 능력 강화를 동시에 추구하는 것이 바람직하다.</p>
<h3>2.2  접근법 2: 시맨틱 세분화 (Semantic Segmentation) - 정밀도와 형태 중심 (U-Net 계열)</h3>
<p>U-Net 아키텍처는 의료 이미지 분석을 위해 개발되었으며, 대칭적인 인코더-디코더(Encoder-Decoder) 구조와 스킵 연결(skip connections)을 특징으로 한다.19 이 구조는 픽셀 단위의 정확한 분류(pixel-level classification)를 가능하게 한다.</p>
<ul>
<li>
<p><strong>출력:</strong> 홀의 위치를 ‘세그멘테이션 마스크(Segmentation Mask)’ 20, 즉 홀에 해당하는 픽셀과 아닌 픽셀을 구분한 정밀한 지도로 반환한다.</p>
</li>
<li>
<p><strong>장점:</strong></p>
</li>
</ul>
<ol>
<li>
<p>홀의 정확한 모양, 크기, 픽셀 면적, 방향(orientation)을 알 수 있다.19</p>
</li>
<li>
<p>바운딩 박스로는 표현하기 힘든, 찢어지거나 불규칙한 형태의 홀도 정확하게 인식한다.</p>
</li>
<li>
<p>YOLO가 객체와 배경 사이의 ’간격(gap)’을 부정확하게 학습하는 경향 15이 있는 반면, U-Net은 정확한 경계를 학습하도록 설계되었다.15</p>
</li>
</ol>
<ul>
<li>
<p><strong>한계:</strong> YOLO 대비 픽셀 단위의 연산을 수행하므로 추론 속도가 상대적으로 느려, 고속 실시간 처리에 불리할 수 있다.19</p>
</li>
<li>
<p><strong>적용:</strong> 위성 이미지 분석에서 토지 피복 19이나 건물 경계를 추출하는 작업, 또는 의료용 OCT 이미지에서 황반 구멍(macular hole) 21을 탐지하는 등, 정밀한 경계가 요구되는 모든 작업에 탁월한 성능을 보인다.</p>
</li>
</ul>
<p>’파종(seeding)’과 ’이식(transplanting)’은 로봇에게 요구하는 정밀도가 근본적으로 다르다.</p>
<p>파종 작업은 씨앗을 홀의 ’중심점’에 떨어뜨리면 되므로, 속도가 빠른 YOLO 접근법이 적합하다.</p>
<p>반면, 이식 작업은 일정한 부피를 가진 모종(plug)이 홀에 물리적으로 삽입되어야 하므로, 홀의 ’면적’과 ’형태’가 작업 성공 여부를 결정한다. 따라서 이식 작업에는 U-Net 기반의 세그멘테이션 접근법이 훨씬 더 높은 가치를 제공한다.</p>
<p>사용자의 요청(“이식 및 파종”)에 완벽히 대응하기 위해서는 이 두 접근법을 결합한 하이브리드 전략이 필요할 수 있다. 예를 들어, (1) YOLO-PH 10와 같이 가볍고 빠른 모델이 홀의 대략적인 위치(RoI, Region of Interest)를 먼저 식별하고, (2) U-Net 19이나 비전 서보 5가 해당 RoI 내부에서만 작동하여 정밀한 마스크를 생성하거나 경계를 추적하는 2-stage 접근법은 속도와 정밀도를 모두 확보할 수 있는 강건한 솔루션이 될 것이다.</p>
<h3>2.3  전통적 기법: 템플릿 매칭 (Template Matching)의 한계</h3>
<p>템플릿 매칭(TM)은 딥러닝 이전의 고전적인 디지털 이미지 처리 기법으로, 미리 정의된 ‘홀 템플릿’ 이미지와 입력 이미지를 비교하여 가장 일치하는 영역을 찾는 방식이다.22</p>
<p>UAV(드론) 멀티스펙트럼 이미지를 이용한 홀 탐지에 이 기법이 적용되어 86%의 평균 정확도를 보인 연구가 있다.22 하지만 이 연구는 템플릿 매칭의 명확한 한계를 동시에 보고한다.</p>
<ul>
<li>
<p><strong>(오인)</strong> 템플릿과 시각적으로 유사한 ’돌(rocky areas)’을 홀로 오인하는 ’위원회 오류(commission errors)’가 다수 발생했다.8</p>
</li>
<li>
<p><strong>(누락)</strong> 잎에 가려지거나(occlusion) 비닐이 손상되어 템플릿과 형태가 달라진 홀을 놓치는 ’누락 오류(omission errors)’가 발생했다.22</p>
</li>
</ul>
<p>결론적으로 템플릿 매칭은 크기, 회전, 조명, 가려짐 등 변화무쌍한 실제 농업 환경 7에 대응하기에는 매우 취약하다. 수동적 특징 공학(manual feature crafting)이 필요한 전통적 기법과 달리, 딥러닝(CNN) 기반 접근법은 데이터로부터 특징을 직접 학습하므로 훨씬 더 정확하고 강건한 성능을 제공한다.24</p>
<h3>2.4 [표 1: 유공 비닐 홀 인식을 위한 AI 모델 아키텍처 비교]</h3>
<table><thead><tr><th><strong>특징</strong></th><th><strong>객체 탐지 (YOLO 계열: YOLO-PH)</strong></th><th><strong>시맨틱 세분화 (U-Net 계열)</strong></th><th><strong>템플릿 매칭 (전통 기법)</strong></th></tr></thead><tbody>
<tr><td><strong>주요 모델</strong></td><td>YOLOv8 12, YOLO-PH 10</td><td>U-Net [15, 19], YUNet [25]</td><td>TM 22</td></tr>
<tr><td><strong>출력 형태</strong></td><td>바운딩 박스 (홀의 중심 좌표)</td><td>픽셀-레벨 마스크 (홀의 정확한 형태/크기)</td><td>일치 점수 및 위치</td></tr>
<tr><td><strong>실시간성 (FPS)</strong></td><td>매우 높음 [9, 10]</td><td>중간~낮음 19</td><td>낮음 24</td></tr>
<tr><td><strong>정밀도 (경계)</strong></td><td>낮음 15</td><td>매우 높음 19</td><td>매우 낮음 22</td></tr>
<tr><td><strong>주요 장점</strong></td><td>실시간 처리, 경량화 10, 중심점 획득</td><td>정확한 형태/크기/방향성 파악 19</td><td>딥러닝 지식 불필요</td></tr>
<tr><td><strong>주요 단점</strong></td><td>형태 부정확, ‘이식’ 적합성 판단 불가</td><td>높은 연산 비용</td><td>조명, 크기, 회전, 가려짐에 극도로 취약 8</td></tr>
<tr><td><strong>적합한 작업</strong></td><td><strong>파종 (Seeding)</strong> (중심점 타격)</td><td><strong>이식 (Transplanting)</strong> (모종 크기 고려)</td><td>제한적 환경에서의 단순 검사 24</td></tr>
<tr><td><strong>현장 강건성</strong></td><td>경계 불분명 시 성능 저하 10</td><td>높음 (단, 데이터셋 의존)</td><td>매우 낮음 8</td></tr>
</tbody></table>
<h2>3.  로봇 ’방향 지시’를 위한 항법 및 제어 전략</h2>
<p>AI 모델이 홀을 ’인식’하는 것은 절반의 성공에 불과하다. 인식된 위치로 로봇을 정확히 ’방향 지시’하는 항법 및 제어 전략이 통합되어야 한다.</p>
<h3>3.1  전역 항법 (Global Navigation): RTK-GPS 기반 경로 추종</h3>
<ul>
<li>
<p><strong>정의:</strong> RTK-GPS(Real-Time Kinematic)는 지상의 고정된 기준국(base station)으로부터 보정 신호를 수신하여, GNSS 위성 신호의 오차를 실시간으로 보정하는 기술이다. 이를 통해 cm 수준(1~2cm)의 초정밀 위치 정확도를 확보할 수 있다.26</p>
</li>
<li>
<p><strong>역할:</strong> 자동 이식 시스템에서 RTK-GPS는 농기계(트랙터, 이앙기) 본체가 밭 전체의 이랑을 따라 정확하게 주행하도록 유도하는 ‘전역 경로 계획기(Global Planner)’ 29 역할을 수행한다.</p>
</li>
<li>
<p><strong>상용 사례: Kubota Agri Robo:</strong> Kubota 이앙기는 RTK-GPS와 IMU(관성 측정 장치)를 퓨전(sensor fusion)하여 11 운전자 없이 3 사전에 생성된 ‘지도’ 3를 따라 정밀하게 주행한다. 이 시스템은 농기계의 자세(heading) 오류를 5도 미만, 측면(lateral) 오류를 10cm 미만으로 유지하며 안정적인 직진 및 회전 주행을 구현한다.3</p>
</li>
<li>
<p><strong>한계:</strong> RTK-GPS는 농기계 ’자신’의 위치는 cm급으로 정밀하게 알지만, 작업 대상인 ’유공 비닐 홀’의 실제 위치는 알지 못한다. 만약 비닐 멀칭이 이랑과 어긋나게 깔렸거나, 이랑 자체가 휘었을 경우, 로봇은 이러한 현장 변수를 인지하지 못하고 ‘잘못된’ GPS 경로 29를 정확하게 추종하는 오류를 범하게 된다. 또한, RTK-GPS는 신호가 차단되는 음영 지역 30이나 예기치 않은 장애물 29에 취약하다는 단점이 있다.</p>
</li>
</ul>
<h3>3.2  지역 정밀 제어 (Local Control): 비전 기반 서보 (Visual Servoing)</h3>
<ul>
<li>
<p><strong>정의:</strong> 비전 서보(Visual Servoing)는 카메라 이미지에서 실시간으로 추출된 시각 정보(visual information)를 피드백 루프에 활용하여 로봇의 엔드 이펙터(end-effector) 자세(pose)를 제어하는 기술이다.6</p>
</li>
<li>
<p><strong>역할:</strong> RTK-GPS가 농기계 본체를 홀 ’근처’로 유도하면, 비전 서보가 ‘지역 제어기(Local Planner)’ 29 역할을 맡는다. 이식기 로봇 팔에 장착된 카메라(Eye-in-Hand) 6가 홀을 직접 포착하고, 이식기가 홀의 중심에 정확히 정렬되도록 로봇 팔을 실시간으로 미세 제어한다.</p>
</li>
<li>
<p><strong>작동 원리:</strong></p>
</li>
<li>
<p><strong>PBVS (Position-Based Visual Servoing):</strong> 이미지 특징(홀의 경계 등)을 3D 공간 좌표로 역산하고, 이 3D 좌표 오차를 최소화하도록 로봇을 제어한다.6</p>
</li>
<li>
<p><strong>IBVS (Image-Based Visual Servoing):</strong> 3D 계산 없이, 이미지 평면(2D) 상의 특징점 오차 6가 0이 되도록 로봇을 제어한다.</p>
</li>
<li>
<p><strong>하이브리드 (HVSC):</strong> 체리 토마토 수확 로봇 5에 적용된 HVSC(Hybrid Visual Servo Control)는 PBVS의 안정성과 IBVS의 강건성을 결합하여, 실제 농업 환경의 불확실성 속에서도 96.25%의 높은 수확 성공률을 달성했다.5</p>
</li>
<li>
<p><strong>한계:</strong> 비전 서보 시스템은 지역적으로는 매우 정밀하지만, 전역 위치를 알 수 없다. 또한 조명 변화, 악천후(비) 9, 먼지, 작물에 의한 가려짐 7 등 시각적 환경 변화에 민감하게 영향을 받는다.29</p>
</li>
</ul>
<h3>3.3  시스템 강건성 확보: RTK-GPS와 Vision의 센서 퓨전 (Sensor Fusion)</h3>
<p>RTK-GPS는 전역적으로 정확하지만 지역 변수(홀의 실제 위치)에 둔감하며 29, 비전 시스템은 지역적으로 정밀하지만 전역 위치를 모르고 환경에 민감하다.29 이처럼 두 기술은 정확히 상호보완적인 장단점을 갖는다.</p>
<p>따라서 자동 이식 시스템의 ’방향 지시’를 위한 최적의 솔루션은 이 두 기술을 ‘융합(Sensor Fusion)’ 32하는 것이다. 융합 시스템은 GPS의 가용성 문제와 비전의 환경 민감성을 동시에 극복하며 최고의 강건성을 확보할 수 있다.34</p>
<p><strong>구현 전략 (3-Stage Hybrid Control):</strong></p>
<ol>
<li>
<p><strong>1단계 (Global Path Following):</strong> RTK-GPS가 ‘전역 계획기(Global Planner)’ 29 역할을 하여, 농기계 본체를 이랑을 따라 사전에 정의된 경로로 유도한다.11</p>
</li>
<li>
<p><strong>2단계 (Local Path Correction):</strong> 딥러닝 비전 모델(FCNN 등)이 ‘지역 계획기(Local Planner)’ 29 역할을 하여, GPS 경로로부터의 이탈(예: 이랑의 휨) 31이나 장애물 29을 감지하고 농기계 본체의 경로를 실시간으로 수정한다.</p>
</li>
<li>
<p><strong>3단계 (Terminal Guidance / Visual Servoing):</strong> 이식기 로봇 팔에 부착된 카메라가 YOLO/U-Net을 통해 홀을 최종 인식하고, 비전 서보 5가 작동하여 로봇 팔을 홀의 정확한 위치로 최종 정렬(fine-tuning)한다.</p>
</li>
</ol>
<p>이러한 통합 솔루션은 목화밭 자율 주행에서 GPS와 비전(FCNN, DWA 알고리즘)을 결합하여 9.5cm 수준의 평균 측면 거리 오차로 강건한 항법을 달성한 연구 29를 통해 그 실효성이 입증된 바 있다.</p>
<h3>3.4 [표 2: ’방향 지시’를 위한 항법 전략 비교]</h3>
<table><thead><tr><th><strong>항법 전략</strong></th><th><strong>RTK-GPS (전역 경로 추종)</strong></th><th><strong>비전 (지역 경로 수정 / 서보)</strong></th><th><strong>GPS-Vision 퓨전 (통합 항법)</strong></th></tr></thead><tbody>
<tr><td><strong>주요 기술</strong></td><td>RTK-GNSS [26, 27], IMU 11</td><td>YOLO/U-Net [10, 19], Visual Servoing 5</td><td>EKF, DWA 29, Pure Pursuit 34</td></tr>
<tr><td><strong>정밀도</strong></td><td>전역적 cm급 [28]</td><td>지역적 mm~cm급 5</td><td>전역적 cm급 + 지역적 정밀 보정 29</td></tr>
<tr><td><strong>역할</strong></td><td>농기계 본체의 ‘경로’ 유도 11</td><td>이식기의 ‘홀’ 정렬 및 장애물 회피 5</td><td>경로 유도 + 홀 정렬 29</td></tr>
<tr><td><strong>장점</strong></td><td>날씨/조명 무관, 24시간 작업 가능 27</td><td>실제 홀 위치/장애물에 반응 29, 저비용 [36]</td><td>상호보완, 높은 강건성 34</td></tr>
<tr><td><strong>단점</strong></td><td>홀/장애물 인식 불가 29, 신호 음영 존재 31</td><td>환경(빛, 비, 먼지)에 민감 [9, 29], 전역 위치 모름</td><td>시스템 복잡도 및 개발 비용 증가</td></tr>
<tr><td><strong>적합 기계</strong></td><td>Kubota 이앙기 3</td><td>저속 정밀 수확 로봇 5</td><td>차세대 고성능 자동 이식기 29</td></tr>
</tbody></table>
<h2>4.  실증적 모델 개발: 데이터셋 구축 및 현장 난제 극복</h2>
<p>성공적인 AI 모델 개발은 알고리즘의 정교함보다 데이터의 질과 양에 달려있다. 특히 농업 환경은 그 어떤 분야보다 예측 불가능한 변수가 많아 데이터 확보가 곧 R&amp;D의 성패를 좌우한다.</p>
<h3>4.1  난관 1: 데이터셋의 부재 및 구축 전략</h3>
<ul>
<li>
<p><strong>문제점:</strong> 유공 비닐 홀 22, 파종 구멍 10, 또는 농업용 잔여 필름 18에 특화된 고품질의 공개 벤치마크 데이터셋은 극히 드물다. 검색되는 데이터셋은 대부분 산업용 홀 38이나 유해 콘텐츠 39 관련으로, 농업 환경과는 무관하다. 따라서 이 분야의 거의 모든 연구는 ‘자체 제작(self-made) 데이터셋’ 18에 의존하고 있다.</p>
</li>
<li>
<p><strong>데이터 수집 전략:</strong></p>
</li>
</ul>
<ol>
<li>
<p><strong>다양한 센서 활용:</strong> UAV(드론)을 이용한 고해상도 항공 이미지(orthophoto) 22와 지상 로봇/농기계에 장착된 RGB 카메라 2 이미지를 모두 확보한다.</p>
</li>
<li>
<p><strong>다양한 환경 확보:</strong> 실제 현장(in-field) 16에서 맑은 날, 흐린 날, 해질녘 등 다양한 조명 조건과 흙먼지, 물기 등 다양한 오염 조건을 포함하여 수집해야 한다.</p>
</li>
<li>
<p><strong>다양한 상태 확보:</strong> 깨끗한 새 비닐, 흙에 덮인 비닐, 찢어지거나 주름진 비닐, 작물 잎에 가려진 비닐 등 가능한 모든 시나리오를 데이터셋에 포함시켜야 한다.7</p>
</li>
</ol>
<p>R&amp;D의 첫 단계는 바로 이러한 포괄적인 ’유공 비닐 홀 데이터셋’을 구축하는 것이 되어야 하며, 이는 모델의 강건성을 결정하는 가장 중요한 자산이 될 것이다.</p>
<h3>4.2  난관 2: 어노테이션 전략 (Bounding Box vs. Segmentation)</h3>
<p>수집된 데이터에 ’정답’을 알려주는 라벨링(어노테이션) 과정은 모델의 목적에 따라 전략적으로 선택해야 한다.</p>
<ul>
<li>
<p><strong>Bounding Box (객체 탐지용):</strong></p>
</li>
<li>
<p><strong>방법:</strong> LabelMe 16 또는 Roboflow 41와 같은 어노테이션 툴을 사용하여 홀 주변에 사각형(bounding box) 13을 그린다.16</p>
</li>
<li>
<p><strong>장점:</strong> 어노테이션 속도가 매우 빠르다. 한 연구에 따르면, 이미지 한 장당 라벨링 시간이 세그멘테이션 마스크(239.7초) 대비 바운딩 박스(38.1초)는 약 6배의 비용 절감 효과가 있다.43 YOLO 포맷 14으로 변환하여 신속한 모델 훈련이 가능하다.</p>
</li>
<li>
<p><strong>단점:</strong> 2.2절에서 논의했듯이, 홀의 정확한 형태나 면적을 반영하지 못한다.20</p>
</li>
<li>
<p><strong>Segmentation Mask (시맨틱 세분화용):</strong></p>
</li>
<li>
<p><strong>방법:</strong> 다각형(Polygon) 13 툴을 사용하여 홀의 경계를 따라 픽셀 단위로 정밀하게 라벨링한다.20</p>
</li>
<li>
<p><strong>장점:</strong> 홀의 정확한 형상 정보를 모델에 제공하여 ‘이식’ 작업의 정밀도를 극대화할 수 있다.</p>
</li>
<li>
<p><strong>단점:</strong> 라벨링에 소요되는 비용과 시간이 기하급수적으로 증가한다.43</p>
</li>
<li>
<p>개발 전략:</p>
</li>
</ul>
<p>두 방식의 장단점이 명확하므로, 단계적 접근이 효율적이다.</p>
<ol>
<li>
<p><strong>(1단계: 파종용 프로토타입)</strong> Bounding Box 16로 신속하게 데이터셋을 구축하여 YOLO-PH 10 기반의 ‘파종’ 모델을 개발한다.</p>
</li>
<li>
<p><strong>(2단계: 이식용 정밀화)</strong> 이식 정밀도 향상이 요구되는 시점에, (a) U-Net 19 학습을 위한 Segmentation 20 데이터를 추가 확보하거나, (b) Bounding Box 라벨만으로 세그멘테이션 마스크를 생성하는 약지도 학습(Weakly Supervised Learning) 44 기법을 도입하여 개발 비용을 절감한다.</p>
</li>
</ol>
<h3>4.3  난관 3: 현장 변수 대응 (가려짐, 오인, 악천후)</h3>
<p>모델의 실제 현장 성능(In-field Performance)을 저해하는 3대 난제와 그 극복 방안은 다음과 같다.</p>
<ul>
<li>
<p><strong>문제 1: 가려짐 (Occlusion)</strong></p>
</li>
<li>
<p><strong>현상:</strong> 모델이 훈련 데이터로 97%의 높은 정확도를 달성했음에도 불구하고, 실제 현장에서 토마토 식물(작물)의 잎이 홀을 가리자 탐지 성능이 급격히 저하되었다.7 이는 훈련 데이터와 실제 현장 데이터 간의 괴리(domain gap)를 보여주는 가장 치명적인 문제이다.</p>
</li>
<li>
<p><strong>해결책:</strong> 데이터 증강(Data Augmentation) 14 단계에서 의도적으로 잎사귀, 흙, 그림자 이미지를 원본 홀 이미지에 합성(overlay)하여 ‘가려진 홀’ 이미지를 대량 생성하고, 이를 훈련 데이터에 포함시켜 모델이 가려짐 상황에 미리 대비하도록 학습시킨다.</p>
</li>
<li>
<p><strong>문제 2: 오인 (False Positives / Commission Errors)</strong></p>
</li>
<li>
<p><strong>현상:</strong> 템플릿 매칭 22은 물론 딥러닝 모델조차 ’돌(rocky areas)’이나 ’풀(grass)’을 홀로 잘못 탐지하는 오류(commission errors)를 범한다.8</p>
</li>
<li>
<p><strong>해결책:</strong> ’하드 네거티브 마이닝(Hard Negative Mining)’이 필요하다. 데이터셋 구축 시, ‘홀’ 뿐만 아니라 오인하기 쉬운 ‘돌’, ‘풀’, ‘그림자’, ‘흙덩이’ 등을 ’Negative Sample (배경)’로 명시적으로 라벨링해야 한다. 이를 통해 모델은 ’홀’을 찾는(detection) 능력과 ’홀이 아닌 것’을 구분하는(classification) 능력을 동시에 학습하게 된다.</p>
</li>
<li>
<p><strong>문제 3: 악천후 (Adverse Weather)</strong></p>
</li>
<li>
<p><strong>현상:</strong> 비(Rain)는 이미지에 줄무늬 노이즈를 추가하고 대비를 낮춰, 포도 송이 탐지(object detection)의 정확도(AP)를 깨끗한 이미지 대비 30% 이상 급격히 감소시켰다.9</p>
</li>
<li>
<p><strong>해결책:</strong> 훈련 데이터에 비 오는 이미지를 단순히 추가(augmentation)하는 것만으로는 충분하지 않았다.9 더 근본적인 해결책은 Cycle-GAN과 같은 생성적 적대 신경망(GAN)을 활용하는 것이다. 입력 이미지를 AI 모델에 통과시키기 전, Cycle-GAN 기반의 ‘비 제거(de-raining)’ 모듈 9을 전처리단에 추가하여, 이미지를 깨끗한 상태로 복원한 후 홀 탐지를 수행한다. 이는 시스템의 전반적인 강건성을 크게 향상시킬 수 있다.</p>
</li>
</ul>
<h2>5.  통합 시스템 구현 및 미래 기술 동향</h2>
<h3>5.1  온디바이스(On-device) 적용: 엣지 AI (Edge AI) 전략</h3>
<ul>
<li>
<p><strong>필요성:</strong> 자동 이식기는 인터넷 연결이 불안정한 농지 한복판에서 실시간으로 작동해야 한다. 모든 카메라 이미지를 원격 클라우드 서버로 전송하고 결과를 받는 방식은 지연 시간(latency)과 통신 비용 문제로 인해 비현실적이다. 따라서, AI 모델은 농기계 자체에 탑재된 ‘엣지 디바이스(Edge Device)’ 45에서 직접 실행되어야 한다.</p>
</li>
<li>
<p><strong>엣지 AI의 이점:</strong> (a) 네트워크 지연 없는 실시간 반응성(low latency), (b) 인터넷 연결이 불필요한 현장 독립성, (c) 민감한 농업 데이터의 프라이버시 보호 17 등이 있다.</p>
</li>
<li>
<p><strong>경량 모델의 중요성:</strong> 엣지 디바이스(예: NVIDIA Jetson, Google Coral)는 데스크톱 GPU에 비해 연산 자원이 극히 제한적이다.17 따라서 엣지 AI 구현을 위해서는 모델의 경량화(lightweight)가 필수적이다.</p>
</li>
<li>
<p><strong>경량 모델 사례:</strong></p>
</li>
<li>
<p><strong>MobileNet V3:</strong> 오렌지 질병 탐지 및 분류 작업에서 24MB에 불과한 적은 메모리 사용량으로도 93%의 높은 분류 정확도를 달성하여 경량화 모델의 효과를 입증했다.17</p>
</li>
<li>
<p><strong>YOLOv8-S:</strong> YOLOv8의 경량화 버전인 YOLOv8-S는 10.9ms라는 극히 빠른 처리 속도와 0.949 (mAP50)라는 높은 탐지 성능을 동시에 보여주며 실시간 엣지 애플리케이션에 매우 적합함을 증명했다.17</p>
</li>
<li>
<p><strong>구현:</strong> 2.1절에서 분석한 YOLO-PH 10가 채택한 C2f_DyGhostConv 및 SD-Head와 같은 기술은 모델의 성능을 유지하면서 파라미터와 연산량을 줄이는 핵심 경량화 전략이며, 이는 엣지 AI 적용에 필수적으로 고려되어야 한다.</p>
</li>
</ul>
<h3>5.2  미래 기술: 다중 작업 학습 (Multi-Task Learning) 모델</h3>
<ul>
<li>
<p><strong>개념:</strong> 현재의 시스템은 ’홀 탐지’라는 단일 작업(Single-Task)에 집중한다. 하지만 미래의 AI 모델은 ‘홀 탐지’, ‘작물 상태 진단’, ‘장애물 인식’ 등 여러 작업을 단일 신경망으로 동시에 처리하는 다중 작업 학습(Multi-Task Learning) 46으로 진화할 것이다.</p>
</li>
<li>
<p>PMJDM 모델 사례:</p>
</li>
</ul>
<p>PMJDM(PlantDisease Multi-task Joint Detection Model)은 이러한 다중 작업 모델의 우수한 농업 적용 사례이다.47 이 모델은 개선된 ConvNeXt 백본 48을 기반으로 ’공유 특징 추출 모듈(Shared Feature Extraction Module)’을 사용하여, ’식물 종 분류(Task 1)’와 ’질병 탐지(Task 2)’라는 두 가지 작업을 동시에 수행한다.48 특히 HOG/LBP와 같은 텍스처 특징 48을 활용하여 복잡한 농업 배경 48에서도 강건하게 작동한다.</p>
<ul>
<li>유공 비닐 탐지 시스템으로의 확장 적용:</li>
</ul>
<p>이식 로봇의 카메라는 이미 비닐과 작물을 동시에 촬영하고 있다. 이 풍부한 시각 데이터를 ’홀 탐지’에만 사용하는 것은 막대한 자원 낭비이다.</p>
<p>PMJDM 48의 아키텍처를 차용하여, ’Task 1: 유공 비닐 홀 탐지(Object Detection)’와 ‘Task 2: 모종/작물 생육 상태 진단(Classification/Segmentation)’ 49을 동시에 수행하는 다중 작업 모델을 개발할 수 있다.</p>
<p>이러한 모델이 구현된다면, 로봇은 유공 비닐에 이식 작업을 수행하는 ‘동시에’ 이미 심어진 주변 작물의 질병 감염 여부 47, 영양 상태(예: 질소 결핍) 50, 또는 수분 스트레스 51를 비파괴적으로 진단할 수 있다. 이는 단순한 자동 이식기를 ’실시간 정밀 농업 데이터 수집 및 진단 플랫폼’으로 격상시키는 핵심 기술이 될 것이다.</p>
<h2>6.  주요 상용 시스템 및 국내외 R&amp;D 동향 분석</h2>
<h3>6.1  글로벌 상용 시스템 현황</h3>
<ul>
<li>
<p>PlantTape 4: 앞서 1.1절에서 분석했듯이, 이 시스템은 AI 비전의 불확실성을 회피하고 100% 기계식 4 정밀 제어에 의존한다. 840개의 모종이 담긴 트레이에서 테이프를 기계적으로 끌어당겨 심는 4 이 방식은, AI 비전이 아닌 기계공학적 설계로 문제를 해결한 성공적인 상용화 사례이다.</p>
</li>
<li>
<p>TTS 자동 이식기 52: 6개의 로봇 팔이 시간당 36,000개의 브뤼셀 스프라우트 모종을 심는 고속 자동화 시스템이다. 이 시스템 역시 개별 ’홀’을 실시간으로 인식하기보다는, 정해진 규격의 트레이에서 모종을 뽑아 정해진 ’좌표’에 심는 방식에 가깝다.</p>
</li>
<li>
<p>Keyence / MVTec (산업용 비전) 53: 이들은 농업 분야는 아니지만, Keyence 53와 MVTec 54 같은 머신 비전 전문 기업들은 이미 오래전부터 플라스틱 부품의 레이저 드릴 홀(μm 단위) 검사 54 등 정밀한 ‘홀 탐지’ 솔루션을 산업 현장에 공급하고 있다. 이는 ‘홀 탐지’ 기술 자체가 이미 성숙해 있음을 보여준다. 진짜 과제는, 공장과 같은 정형화된 환경이 아닌, 조명, 먼지, 오염 등 비정형적인 ‘농업 현장’ 8에 이 기술을 강건하게 적용하는 것이다.</p>
</li>
</ul>
<h3>6.2  국내외 R&amp;D 동향</h3>
<ul>
<li>
<p>대동(Daedong) 55: 대동은 서울 R&amp;D 센터를 중심으로 스마트팜 솔루션을 적극 개발 중이다.55 특히 ‘AI 생육 모니터링 로봇’ 55은 RGB 카메라를 이용한 ‘비전 솔루션’ 55을 통해 작물의 생육 정보를 획득(Acquisition of growth information) 55한다. 이는 5.2절에서 제시한 다중 작업 모델의 ‘생육 상태 진단’ 태스크와 직접적으로 연결되는 R&amp;D 방향이다.</p>
</li>
<li>
<p>KIST 스마트팜 융합 연구센터 49:</p>
</li>
<li>
<p>KIST 강릉분원 스마트팜 융합 연구센터는 ‘데이터 기반 기능성 식물 최적 생산 스마트팜 플랫폼’ 개발에 집중하고 있다.50</p>
</li>
<li>
<p>KIST 로봇 비전 그룹 56은 작물들이 고밀도로 얽힌 스마트팜 환경에서도 개별 작물을 정확히 식별하기 위해, RGB 이미지와 깊이(Depth) 이미지를 퓨전(image fusion)하는 기술을 연구한다.56</p>
</li>
<li>
<p>최근(2024년) KIST 스마트팜 연구센터가 참여한 연구 49에서는 Vision Transformer(ViT)와 1D-CNN을 사용하여, 저비용 RGB 이미지(비파괴적)만으로 상추의 생체중(shoot fresh weight), 건조중, 엽면적 등 핵심 생육 정보를 정확하게 예측(prediction)하는 데 성공했다.</p>
</li>
</ul>
<p>국내외 주요 R&amp;D 동향을 분석한 결과, 현재의 기술 개발은 ’노지(Field)’의 ’홀 탐지’보다는 ‘스마트팜(Greenhouse)’ 55이라는 제어된 환경 내부의 ‘작물 생육 모니터링 및 예측’ 49에 집중되는 경향을 보인다.</p>
<p>이는 제어된 환경이 비전 기술을 적용하고 검증하기에 더 용이하기 때문으로 풀이된다. 따라서 사용자가 목표로 하는 ‘노지’ 환경의 ‘유공 비닐 홀’ 탐지는 상대적으로 더 높은 기술적 난이도를 가지며, 시장 선점의 기회가 있는 분야이다. KIST 등 49에서 활발히 개발 중인 ’작물 생육 예측 모델’을 ’홀 탐지 모델’과 결합(Multi-Task) 48하는 것은, 경쟁사와 차별화되는 강력한 기술적 우위를 확보하는 전략이 될 것이다.</p>
<h2>7.  성공적인 개발 및 도입을 위한 기술적 제언</h2>
<p>본 보고서는 농작물 자동 이식 및 파종을 위한 AI 모델 개발이 단순한 ‘홀 탐지 알고리즘’ 개발이 아닌, ‘인식(Perception)’, ‘제어(Control)’, ’데이터(Data)’가 융합된 복합 시스템 엔지니어링의 과제임을 명확히 하였다. 성공적인 시스템 개발 및 현장 도입을 위해 다음 4가지 핵심 전략을 제언한다.</p>
<ol>
<li>’하이브리드 항법(Hybrid Navigation)’을 기본 아키텍처로 채택하라.</li>
</ol>
<p>RTK-GPS 단독 27 또는 비전 단독 29 시스템은 명확한 한계를 갖는다. RTK-GPS의 전역적 경로 추종 능력과 비전 시스템의 지역적 환경 보정 능력을 결합한 ‘GPS-Vision 퓨전’ 29을 시스템의 기본 항법 아키텍처로 채택해야 한다. GPS는 농기계 본체를 이랑으로 유도하고, 비전 시스템은 (a) 이랑의 휨이나 장애물을 감지하여 본체 경로를 수정하며, (b) 로봇 팔에 장착된 카메라는 비전 서보 5를 통해 이식기를 실제 홀의 위치로 최종 보정해야 한다.</p>
<ol start="2">
<li>‘실패 사례(Failure Case)’ 중심의 데이터셋을 구축하라.</li>
</ol>
<p>모델의 강건성은 알고리즘이 아닌 데이터에서 나온다. 깨끗하고 명확한 홀 이미지 24는 실전에서 무의미하다. 실제 현장의 핵심 실패 원인인 (a) 작물 잎에 의한 가려짐(Occlusion) 7, (b) 돌, 흙, 풀과의 오인(False Positives) 8, (c) 비, 먼지 등 악천후 9를 의도적으로 수집하고 증강(Cycle-GAN 등)한 ‘하드 네거티브(Hard Negative)’ 중심의 데이터셋을 구축하는 데 R&amp;D 자원의 절반 이상을 투입해야 한다.</p>
<ol start="3">
<li>작업 목적에 맞는 ’AI 모델’과 ’어노테이션’을 전략적으로 선택하라.</li>
</ol>
<p>’파종(Seeding)’과 ’이식(Transplanting)’은 요구 정밀도가 다르다.</p>
<ul>
<li>
<p><strong>파종:</strong> 홀의 중심점만 알면 되므로, 라벨링 비용이 6배 저렴하고 43 속도가 빠른 YOLO-PH 10 기반의 ‘바운딩 박스(Bounding Box)’ 16 접근이 효율적이다.</p>
</li>
<li>
<p><strong>이식:</strong> 모종의 크기(plug size)를 고려한 정밀한 삽입이 필요하므로, 홀의 정확한 형상을 아는 것이 중요하다. U-Net 19 기반의 ‘세그멘테이션 마스크(Segmentation Mask)’ 20 접근이 궁극적으로 필요하며, 이는 정밀한 비전 서보 5 제어의 핵심 입력이 된다.</p>
</li>
</ul>
<ol start="4">
<li><strong>’엣지 AI’와 ’다중 작업 모델’로 시스템의 가치를 극대화하라.</strong></li>
</ol>
<ul>
<li>
<p><strong>(구현)</strong> 개발 초기부터 MobileNet 17 또는 GhostNet 10 기반의 경량화 10 아키텍처를 적용하여, 엣지 AI 17 디바이스에 탑재 가능한 모델을 목표로 해야 한다.</p>
</li>
<li>
<p><strong>(미래 전략)</strong> 시스템이 성숙되면, ’홀 탐지(Task 1)’와 ‘작물 생육 진단(Task 2)’ 49을 동시에 수행하는 다중 작업 모델 48로 고도화하는 로드맵을 수립하라. 이는 단순한 자동 이식기를 ’데이터 기반 정밀 농업 솔루션 플랫폼’으로 진화시켜, 시장에서 압도적인 기술적 차별성을 확보하는 핵심 전략이 될 것이다.</p>
</li>
</ul>
<h2>8. Works cited</h2>
<ol>
<li>Agriculture robot example: TTS automatic transplanter - ducksize, accessed November 6, 2025, https://www.ducksize.com/automatic-transplanter-robot</li>
<li>AI-Assisted Vision for Agricultural Robots - MDPI, accessed November 6, 2025, https://www.mdpi.com/2624-7402/4/3/43</li>
<li>The Agri Robo Rice-Transplanter Makes Planting Beautiful …, accessed November 6, 2025, https://www.kubota.com/innovation/our-stories/automated-transplanter.html</li>
<li>PlantTape Automated Transplanting System - PlantTape, accessed November 6, 2025, https://www.planttape.com/</li>
<li>Hybrid Visual Servo Control of a Robotic Manipulator for Cherry …, accessed November 6, 2025, https://www.mdpi.com/2076-0825/12/6/253</li>
<li>(PDF) Visual Servoing Design and Control for Agriculture Robot; a Review - ResearchGate, accessed November 6, 2025, https://www.researchgate.net/publication/330295720_Visual_Servoing_Design_and_Control_for_Agriculture_Robot_a_Review</li>
<li>Developing AI Smart Sprayer for Punch-Hole Herbicide Application in Plasticulture Production System - MDPI, accessed November 6, 2025, https://www.mdpi.com/2624-7402/7/1/2</li>
<li>Identification of Holes in Plastic Mulch Based on UAV Multispectral Image Using Template Matching Algorithm - AIP Publishing, accessed November 6, 2025, https://pubs.aip.org/aip/acp/article-pdf/doi/10.1063/5.0181351/18255283/030020_1_5.0181351.pdf</li>
<li>Robust Visual Servoing for Precision Agriculture Tasks using …, accessed November 6, 2025, https://agelosk.github.io/pdf/ICRA.pdf</li>
<li>Detecting Planting Holes Using Improved YOLO-PH Algorithm with …, accessed November 6, 2025, https://www.mdpi.com/2072-4292/17/15/2614</li>
<li>Development of an automatically guided rice transplanter using RTK-GNSS and IMU, accessed November 6, 2025, https://www.researchgate.net/publication/327609706_Development_of_an_automatically_guided_rice_transplanter_using_RTK-GNSS_and_IMU</li>
<li>A Comprehensive Review of YOLO Architectures in Computer Vision: From YOLOv1 to YOLOv8 and YOLO-NAS - MDPI, accessed November 6, 2025, https://www.mdpi.com/2504-4990/5/4/83</li>
<li>데이터 어노테이션 기법: AI 학습을 위한 6가지 핵심 방법론 - GilliLab - TechLog, accessed November 6, 2025, <a href="https://rupijun.tistory.com/entry/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%96%B4%EB%85%B8%ED%85%8C%EC%9D%B4%EC%85%98-%EA%B8%B0%EB%B2%95-AI-%ED%95%99%EC%8A%B5%EC%9D%84-%EC%9C%84%ED%95%9C-6%EA%B0%80%EC%A7%80-%ED%95%B5%EC%8B%AC-%EB%B0%A9%EB%B2%95%EB%A1%A0">https://rupijun.tistory.com/entry/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%96%B4%EB%85%B8%ED%85%8C%EC%9D%B4%EC%85%98-%EA%B8%B0%EB%B2%95-AI-%ED%95%99%EC%8A%B5%EC%9D%84-%EC%9C%84%ED%95%9C-6%EA%B0%80%EC%A7%80-%ED%95%B5%EC%8B%AC-%EB%B0%A9%EB%B2%95%EB%A1%A0</a></li>
<li>Data Annotation and Preparation for Object Detection using YOLOv8 - Medium, accessed November 6, 2025, https://medium.com/@KaziMushfiq1234/data-annotation-and-preparation-for-object-detection-using-yolov8-d7546e7cc41c</li>
<li>(PDF) A comparative analysis of YOLOv8 and U-Net image segmentation approaches for transmission electron micrographs of polycrystalline thin films - ResearchGate, accessed November 6, 2025, https://www.researchgate.net/publication/393980973_A_comparative_analysis_of_YOLOv8_and_U-Net_image_segmentation_approaches_for_transmission_electron_micrographs_of_polycrystalline_thin_films</li>
<li>Mulch-YOLO: Improved YOLOv11 for Real-Time Detection of Mulch …, accessed November 6, 2025, https://www.mdpi.com/2076-3417/15/21/11604</li>
<li>[논문 리뷰] Edge-AI for Agriculture: Lightweight Vision Models for …, accessed November 6, 2025, https://www.themoonlight.io/ko/review/edge-ai-for-agriculture-lightweight-vision-models-for-disease-detection-in-resource-limited-settings</li>
<li>Mapping Plastic Mulched Film based on Deep Learning and Multi-Source Satellite Imagery, accessed November 6, 2025, https://www.researchgate.net/publication/393692655_Mapping_Plastic_Mulched_Film_based_on_Deep_Learning_and_Multi-Source_Satellite_Imagery</li>
<li>Geospatial Object Detection with YOLO and U-Net in Satellite Imagery, accessed November 6, 2025, https://www.geowgs84.ai/post/geospatial-object-detection-with-yolo-and-u-net-in-satellite-imagery</li>
<li>Why Are Image Segmentation Maps Superior to Bounding Boxes? - Voxel51, accessed November 6, 2025, https://voxel51.com/blog/why-are-image-segmentation-maps-superior-to-bounding-boxes</li>
<li>Comparative Analysis of Deep Learning Architectures for Macular Hole Segmentation in OCT Images: A Performance Evaluation of U-Net Variants - MDPI, accessed November 6, 2025, https://www.mdpi.com/2313-433X/11/2/53</li>
<li>(PDF) Identification of holes in plastic mulch based on UAV multispectral image using template matching algorithm - ResearchGate, accessed November 6, 2025, https://www.researchgate.net/publication/376434148_Identification_of_holes_in_plastic_mulch_based_on_UAV_multispectral_image_using_template_matching_algorithm</li>
<li>HOLE DETECTION IN PLASTIC MULCH USING TEMPLATE MATCHING AND MACHINE LEARNING ALGORITHMS - Semantic Scholar, accessed November 6, 2025, https://pdfs.semanticscholar.org/495a/5a9cda591ebe84aea4b9d684aba8089dae95.pdf</li>
<li>[2509.05813] LabelImg: CNN-Based Surface Defect Detection - arXiv, accessed November 6, 2025, https://arxiv.org/abs/2509.05813</li>
<li>RTK GPS Farming: Real-Time Kinematic Tech In Agriculture - Farmonaut, accessed November 6, 2025, https://farmonaut.com/precision-farming/precision-agriculture-11-tech-innovations-boosting-farm-yields</li>
<li>RTK Makes Precision Agriculture Simple - AgritechTomorrow, accessed November 6, 2025, https://www.agritechtomorrow.com/news/2023/09/13/rtk-makes-precision-agriculture-simple/14836/</li>
<li>Comparison of positional accuracy between RTK and RTX GNSS gased on the autonomous agricultural vehicles under field… - idUS, accessed November 6, 2025, https://idus.us.es/bitstreams/933fe015-b6c6-42e4-ab76-109c0b314933/download</li>
<li>The integration of GPS and visual navigation for autonomous navigation of an Ackerman steering mobile robot in cotton fields - PMC - NIH, accessed November 6, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11045887/</li>
<li>The Integration of GPS/BDS Real-Time Kinematic Positioning and Visual–Inertial Odometry Based on Smartphones - MDPI, accessed November 6, 2025, https://www.mdpi.com/2220-9964/10/10/699</li>
<li>A Vision-Based Road Detection System for the Navigation of an Autonomous Tractor - CABI Digital Library, accessed November 6, 2025, https://www.cabidigitallibrary.org/doi/pdf/10.5555/20220278647</li>
<li>Vision-RTK 2: precise positioning under all conditions - Fixposition, accessed November 6, 2025, https://www.fixposition.com/pages/vision-rtk2</li>
<li>Merge Fuzzy Visual Servoing and GPS-Based Planning to Obtain a Proper Navigation Behavior for a Small Crop-Inspection Robot - MDPI, accessed November 6, 2025, https://www.mdpi.com/1424-8220/16/3/276</li>
<li>The integration of GPS and visual navigation for autonomous navigation of an Ackerman steering mobile robot in cotton fields - Frontiers, accessed November 6, 2025, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1359887/full</li>
<li>Autonomous Navigation of an Agricultural Robot Using RTK GPS and Pixhawk | Request PDF - ResearchGate, accessed November 6, 2025, https://www.researchgate.net/publication/346859007_Autonomous_Navigation_of_an_Agricultural_Robot_Using_RTK_GPS_and_Pixhawk</li>
<li>YOLOv5-Atn: An Algorithm for Residual Film Detection in Farmland Combined with an Attention Mechanism - MDPI, accessed November 6, 2025, https://www.mdpi.com/1424-8220/23/16/7035</li>
<li>Holes Object Detection - Dataset Ninja, accessed November 6, 2025, https://datasetninja.com/holes-object-detection</li>
<li>HOD: A Benchmark Dataset for Harmful Object Detection - GitHub, accessed November 6, 2025, https://github.com/poori-nuna/HOD-Benchmark-Dataset</li>
<li>[2212.09981] Benchmarking person re-identification datasets and approaches for practical real-world implementations - arXiv, accessed November 6, 2025, https://arxiv.org/abs/2212.09981</li>
<li>Roboflow Annotate: Label Images Faster Than Ever, accessed November 6, 2025, https://roboflow.com/annotate</li>
<li>How to Perform Data Annotation using Roboflow - YouTube, accessed November 6, 2025, https://www.youtube.com/watch?v=HId5D6tjrRs</li>
<li>Learning Few-Shot Segmentation From Bounding Box Annotations - CVF Open Access, accessed November 6, 2025, https://openaccess.thecvf.com/content/WACV2023/papers/Han_Learning_Few-Shot_Segmentation_From_Bounding_Box_Annotations_WACV_2023_paper.pdf</li>
<li>DeepCut: Object Segmentation from Bounding Box Annotations using Convolutional Neural Networks - PMC - NIH, accessed November 6, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC7115996/</li>
<li>엣지 AI(3)-농산업 비전인식 도입 활발, 첨단 ‘농로봇’ 진입 박차 - e4ds news, accessed November 6, 2025, https://www.e4ds.com/sub_view.asp?ch=1&amp;t=0&amp;idx=17304</li>
<li>Multi-task learning model for agricultural pest detection from crop-plant imagery: A Bayesian approach - Murdoch University - Research Portal, accessed November 6, 2025, https://researchportal.murdoch.edu.au/esploro/outputs/journalArticle/Multi-task-learning-model-for-agricultural-pest/991005637962707891</li>
<li>PMJDM: a multi-task joint detection model for plant disease identification - PubMed, accessed November 6, 2025, https://pubmed.ncbi.nlm.nih.gov/40475903/</li>
<li>PMJDM: a multi-task joint detection model for plant disease … - NIH, accessed November 6, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC12137281/</li>
<li>Continuous Growth Monitoring and Prediction with 1D Convolutional Neural Network Using Generated Data with Vision Transformer - PubMed, accessed November 6, 2025, https://pubmed.ncbi.nlm.nih.gov/39520028/</li>
<li>Center Introduction</li>
<li>Benchmarking the Agronomic Performance of Biodegradable Mulches against Polyethylene Mulch Film: A Meta-Analysis - MDPI, accessed November 6, 2025, https://www.mdpi.com/2073-4395/10/10/1618</li>
<li>Fully Automated Planting of Brussels Sprouts w/ 6-row TTS Transplanter - Fendt 724, accessed November 6, 2025, https://www.youtube.com/watch?v=qgh4Bn23lVQ</li>
<li>Vision Systems for the Commodities Industry | KEYENCE America, accessed November 6, 2025, https://www.keyence.com/products/vision/vision-sys/industries/commodities/</li>
<li>Vision system inspects laser-drilled holes - Fisher Smith LLP - MVTec Software, accessed November 6, 2025, https://www.mvtec.com/application-areas/success-stories/vision-system-inspects-laser-drilled-holes-fisher-smith-llp</li>
<li>Innovation &gt; Smart Farming - Daedong Kioti, accessed November 6, 2025, https://daedong-kioti.com/innovation/smartfarm</li>
<li>Research - KIST Robot Vision Lab, accessed November 6, 2025, https://kistrobot.vision/research/</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>