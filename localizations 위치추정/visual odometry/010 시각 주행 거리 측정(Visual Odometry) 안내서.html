<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:시각 주행 거리 측정(Visual Odometry) 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>시각 주행 거리 측정(Visual Odometry) 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">위치 추정 (Localization)</a> / <a href="index.html">Visual Odemtry</a> / <span>시각 주행 거리 측정(Visual Odometry) 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>시각 주행 거리 측정(Visual Odometry) 안내서</h1>
<h2>1.  시각 주행 거리 측정(VO)의 서론</h2>
<h3>1.1  VO의 정의 및 기본 원리</h3>
<p>시각 주행 거리 측정(Visual Odometry, VO)은 로봇이나 차량과 같은 이동체에 장착된 하나 이상의 카메라로부터 입력되는 연속적인 이미지 시퀀스를 분석하여, 이동체의 자체 움직임(egomotion)을 점진적으로 추정하는 기술이다.1 VO의 핵심 목표는 연속된 카메라 포즈(pose) 간의 상대적인 이동(translation)과 회전(rotation)을 계산하는 것이다.1</p>
<p>VO의 기본 원리는 이미지 평면상에서 관찰되는 특징들의 겉보기 움직임을 통해 실제 3차원 공간에서의 카메라 움직임을 추론하는 것이다.3 이 과정은 일반적으로 이미지 내에서 식별이 용이하고 추적이 가능한 주요 지점(salient features)이나 픽셀 패턴을 찾고, 이들이 다음 이미지 프레임에서 어떻게 이동했는지를 추적함으로써 이루어진다. 이 2차원적인 움직임 벡터들의 집합은 카메라의 3차원 공간상에서의 변위와 방향 전환을 추정하는 데 사용된다.</p>
<p>VO 시스템이 효과적으로 작동하기 위해서는 몇 가지 전제 조건이 충족되어야 한다. 우선, 주변 환경은 대부분 정적(static)이어야 하며, 신뢰할 수 있는 시각 정보를 추출할 수 있도록 충분한 질감(texture)과 조명이 확보되어야 한다. 또한, 연속된 이미지 프레임들은 특징점 추적이 가능하도록 충분한 영역을 서로 공유(overlap)해야 한다.1</p>
<h3>1.2  주행 거리 측정의 종류</h3>
<p>주행 거리 측정(Odometry) 기술은 이동체의 위치 변화를 추정하는 다양한 방법을 포함하며, VO는 그중 시각 정보에 의존하는 한 갈래이다. 다른 주요 방식들과 비교할 때 VO의 장단점은 명확해진다.</p>
<ul>
<li>
<p><strong>바퀴 주행 거리 측정 (Wheel Odometry):</strong> 가장 전통적인 방식으로, 바퀴에 부착된 회전 엔코더(rotary encoder)를 사용하여 바퀴의 회전수를 측정하고 이를 통해 이동 거리를 추정한다.5 이 방식의 치명적인 약점은 고르지 않은 지형이나 마찰이 적은 표면에서 발생하는 바퀴 미끄러짐(wheel slip) 현상에 매우 취약하다는 점이다.2</p>
</li>
<li>
<p><strong>관성 주행 거리 측정 (Inertial Odometry):</strong> 관성 측정 장치(Inertial Measurement Unit, IMU)에 내장된 가속도계와 자이로스코프를 이용하여 각각 선형 가속도와 각속도를 측정한다.6 빠른 움직임을 추적하는 데는 탁월하지만, 측정된 가속도 값을 두 번 적분하여 위치를 계산하는 과정에서 노이즈가 누적되어 시간이 지남에 따라 심각한 오차 누적(drift)이 발생한다.</p>
</li>
<li>
<p><strong>시각 주행 거리 측정 (Visual Odometry):</strong> VO는 바퀴 미끄러짐과 같은 문제의 영향을 받지 않으며, 다른 방식에 비해 더 정확한 궤적 추정이 가능하다. 보고에 따르면 상대적 위치 오차는 0.1%에서 2% 범위에 이른다.2 이러한 특성 덕분에 VO는 GPS 신호가 잡히지 않는 실내나 수중과 같은 환경에서 다른 주행 거리 측정 기술을 보완하거나 대체하는 강력한 수단으로 활용된다.8</p>
</li>
</ul>
<p>이 기술들의 발전 과정은 불확실성과 오차 누적을 점차 더 큰 규모에서 관리하려는 필요성에 의해 주도되었다. 바퀴 주행 거리 측정은 환경과의 물리적 상호작용(미끄러짐)으로 인해 실패하고, VO는 이를 극복하지만 시각적 오차 누적이라는 자체적인 문제점을 가진다. 이후 등장하는 관성 센서와의 융합(VIO)이나 전역적 최적화(V-SLAM)는 이러한 한계를 극복하기 위한 자연스러운 기술적 진화 과정으로 볼 수 있다.</p>
<h3>1.3  VO, V-SLAM, VIO의 관계 및 차이점</h3>
<p>VO는 종종 시각 동시적 위치 추정 및 지도 작성(Visual SLAM, V-SLAM)이나 시각-관성 주행 거리 측정(Visual-Inertial Odometry, VIO)과 혼용되지만, 이들은 목표와 기능 면에서 뚜렷한 차이가 있다.</p>
<ul>
<li>
<p><strong>VO (지역적 일관성):</strong> VO의 주된 목표는 카메라의 경로를 점진적으로, 즉 한 포즈씩 순차적으로 복원하는 것이다. 따라서 VO는 주로 *지역적 일관성(local consistency)*에 초점을 맞추어 짧은 시간 동안의 궤적 정확도를 보장한다. 하지만 일반적으로 루프 폐쇄(loop closure)나 전역적인 지도 최적화를 수행하지 않기 때문에, 시간이 지남에 따라 오차 누적을 피할 수 없다.10 VO는 종종 더 큰 V-SLAM 시스템의 핵심적인 프론트엔드(front-end) 구성 요소로 간주된다.4</p>
</li>
<li>
<p><strong>V-SLAM (전역적 일관성):</strong> SLAM은 미지의 환경에 대한 전역적으로 일관된 지도(globally consistent map)를 작성하는 동시에, 그 지도 내에서 이동체의 현재 위치를 추적하는 것을 목표로 한다.10 VO와 구별되는 V-SLAM의 핵심 기능은</p>
</li>
</ul>
<p><strong>루프 폐쇄 감지</strong>이다. 이는 시스템이 이전에 방문했던 장소를 다시 인식하고, 이를 통해 그동안 누적된 오차를 보정하여 위상학적으로 정확한 전역 지도를 구축하는 능력이다.10 V-SLAM은 전체 지도와 궤적이라는 더 큰 상태 공간을 유지해야 하므로 VO보다 계산 복잡도가 높다.11</p>
<ul>
<li>
<p><strong>VIO (센서 융합):</strong> VIO는 VO-SLAM 스펙트럼상의 독립적인 범주라기보다는 센서 융합 기술에 가깝다. VIO는 카메라의 시각 데이터와 IMU의 측정값을 결합한다.7 IMU는 고주파의 움직임 데이터와 실제 미터 단위의 스케일(metric scale) 정보를 제공하여, 빠른 움직임이나 질감이 부족한 환경, 모션 블러 상황에서 카메라의 단점을 보완한다. 특히 단일 카메라 VO의 고질적인 문제인 스케일 모호성(scale ambiguity)을 해결하는 데 결정적인 역할을 한다.6 VIO는 VO와 V-SLAM 시스템 모두의 동작 추정 엔진으로 사용될 수 있다.</p>
</li>
<li>
<p><strong>움직임으로부터 구조 복원 (Structure from Motion, SfM):</strong> SfM은 관련 분야이지만 뚜렷한 차이가 있다. SfM은 주로 오프라인(offline) 프로세스로, 정렬되지 않은(unordered) 이미지 집합으로부터 3차원 장면을 복원한다. 실시간 자기 위치 추정보다는 고품질의 3차원 재구성에 더 중점을 둔다.10</p>
</li>
</ul>
<table><thead><tr><th>구분</th><th>시각 주행 거리 측정 (VO)</th><th>시각 SLAM (V-SLAM)</th><th>시각-관성 주행 거리 측정 (VIO)</th></tr></thead><tbody>
<tr><td><strong>주요 목표</strong></td><td>지역적 궤적 추정</td><td>전역적 위치 및 지도 추정</td><td>센서 융합을 통한 강인한 궤적 추정</td></tr>
<tr><td><strong>지도 활용</strong></td><td>추적을 위한 지역적, 임시적 지도</td><td>영구적, 전역적으로 일관된 지도</td><td>적용 대상(VO/SLAM)에 따라 다름</td></tr>
<tr><td><strong>오차 누적 처리</strong></td><td>장기적 오차 누적에 취약, 보정 없음</td><td>루프 폐쇄 및 전역 최적화로 오차 보정</td><td>IMU 통합으로 오차 누적 감소 및 스케일 제공</td></tr>
<tr><td><strong>계산 복잡도</strong></td><td>낮음 11</td><td>높음 12</td><td>중간-높음 (융합 방식에 따라 다름)</td></tr>
<tr><td><strong>핵심 기능</strong></td><td>점진적 포즈 추정</td><td>루프 폐쇄 감지</td><td>카메라-IMU 데이터 융합</td></tr>
</tbody></table>
<h2>2.  VO의 기하학적 기반: 다중 시점 기하학</h2>
<p>VO 시스템은 여러 시점에서 촬영된 이미지들 간의 기하학적 관계를 설명하는 다중 시점 기하학(multi-view geometry)이라는 수학적 원리에 깊이 뿌리내리고 있다. 이 기하학적 제약을 이해하는 것은 VO 알고리즘을 설계하고 분석하는 데 필수적이다.</p>
<h3>2.1  에피폴라 기하학 (Epipolar Geometry)</h3>
<p>에피폴라 기하학은 3차원 장면에 대한 두 시점 간의 내재적인 투영 기하학을 설명한다.15 이는 장면의 구체적인 3차원 구조 정보 없이도, 두 이미지에 나타난 대응점들의 위치 관계에 강력한 제약을 제공한다.</p>
<ul>
<li>
<p><strong>주요 용어</strong> 15:</p>
</li>
<li>
<p><strong>카메라 중심 (<span class="math math-inline">O_1</span>, <span class="math math-inline">O_2</span>):</strong> 두 카메라의 광학적 중심.</p>
</li>
<li>
<p><strong>기선 (Baseline):</strong> 두 카메라 중심을 연결하는 선분.</p>
</li>
<li>
<p><strong>에피폴라 평면 (Epipolar Plane):</strong> 3차원 공간상의 한 점 <span class="math math-inline">P</span>와 두 카메라 중심 <span class="math math-inline">O_1</span>, <span class="math math-inline">O_2</span>를 모두 포함하는 평면.</p>
</li>
<li>
<p><strong>에피폴 (Epipole, <span class="math math-inline">e</span>, <span class="math math-inline">e&#39;</span>):</strong> 기선이 각 이미지 평면과 만나는 점. 한 이미지의 에피폴은 다른 카메라 중심의 투영점이다.</p>
</li>
<li>
<p><strong>에피폴라 선 (Epipolar Line, <span class="math math-inline">l</span>, <span class="math math-inline">l&#39;</span>):</strong> 에피폴라 평면이 각 이미지 평면과 교차하여 만들어지는 선. 한 이미지 내의 모든 에피폴라 선은 그 이미지의 에피폴에서 만난다.</p>
</li>
<li>
<p><strong>에피폴라 제약 (Epipolar Constraint):</strong> 첫 번째 이미지의 한 점 <span class="math math-inline">p</span>에 대응하는 두 번째 이미지의 점 <span class="math math-inline">p&#39;</span>는 반드시 두 번째 이미지의 에피폴라 선 <span class="math math-inline">l&#39;</span> 위에 존재해야 한다.16 이 제약 조건은 특징점 매칭을 위한 탐색 공간을 2차원 이미지 전체에서 1차원 직선으로 획기적으로 줄여주어, 계산 효율성과 정확도를 크게 향상시킨다.</p>
</li>
</ul>
<h3>2.2  본질 행렬(Essential Matrix)과 기본 행렬(Fundamental Matrix)</h3>
<p>본질 행렬과 기본 행렬은 에피폴라 기하학을 대수적으로 표현하는 3x3 행렬이다.15</p>
<ul>
<li>
<p><strong>본질 행렬 (Essential Matrix, E):</strong></p>
</li>
<li>
<p><strong>정의:</strong> 카메라가 보정(calibrated)되었다고 가정하고, <em>정규화된 이미지 좌표(normalized image coordinates)</em> 상의 대응점들을 관계 짓는다.17 이 행렬은 두 카메라 간의 상대적인 회전 <span class="math math-inline">R</span>과 (스케일을 제외한) 이동 <span class="math math-inline">t</span> 정보를 담고 있다.</p>
</li>
<li>
<p><strong>유도 (공면 조건):</strong> 3차원 점 <span class="math math-inline">P</span>의 정규화된 이미지 좌표를 각각 <span class="math math-inline">p</span>와 <span class="math math-inline">p&#39;</span>라 하자. 두 카메라 중심과 점 <span class="math math-inline">P</span>를 잇는 벡터 <span class="math math-inline">p</span>, <span class="math math-inline">Rp&#39;</span>와 카메라 간의 이동 벡터 <span class="math math-inline">t</span>는 동일 평면(에피폴라 평면)에 존재한다. 따라서 세 벡터의 스칼라 삼중적은 0이 된다: <span class="math math-inline">p^T (t \times Rp&#39;) = 0</span>.15 외적 <span class="math math-inline">t \times</span>는 반대칭 행렬(skew-symmetric matrix) <span class="math math-inline">[t]_x</span>로 표현할 수 있으므로, 식은 <span class="math math-inline">p^T [t]_x R p&#39; = 0</span>이 된다. 여기서 <strong>본질 행렬</strong>은 <span class="math math-inline">E = [t]_x R</span>로 정의된다.15</p>
</li>
<li>
<p><strong>에피폴라 제약식:</strong> <span class="math math-inline">p^T E p&#39; = 0</span>.15</p>
</li>
<li>
<p><strong>특성:</strong> <span class="math math-inline">E</span>는 5 자유도(회전 3, 이동 방향 2)를 가지며, 랭크(rank)는 2이다.21</p>
</li>
<li>
<p><strong>기본 행렬 (Fundamental Matrix, F):</strong></p>
</li>
<li>
<p><strong>정의:</strong> 본질 행렬을 일반화하여, 보정되지 않은(uncalibrated) 카메라의 <em>픽셀 좌표</em>에 적용된다.17 한 이미지의 점을 다른 이미지의 에피폴라 선으로 사상(mapping)하는 관계를 나타낸다.16</p>
</li>
<li>
<p><strong>유도:</strong> 픽셀 좌표 <span class="math math-inline">x</span>, <span class="math math-inline">x&#39;</span>와 카메라 내부 파라미터 행렬 <span class="math math-inline">K</span>, <span class="math math-inline">K&#39;</span>가 주어졌을 때, 정규화된 좌표는 <span class="math math-inline">p = K^{-1}x</span>, <span class="math math-inline">p&#39; = K&#39;^{-1}x&#39;</span>이다. 이를 본질 행렬 제약식에 대입하면 <span class="math math-inline">(K^{-1}x)^T E (K&#39;^{-1}x&#39;) = 0</span>이 되고, 정리하면 <span class="math math-inline">x^T K^{-T} E K&#39;^{-1} x&#39; = 0</span>이 된다. 여기서 <strong>기본 행렬</strong>은 <span class="math math-inline">F = K^{-T} E K&#39;^{-1} = K^{-T} [t]_x R K&#39;^{-1}</span>로 정의된다.15</p>
</li>
<li>
<p><strong>에피폴라 제약식:</strong> <span class="math math-inline">x&#39;^T F x = 0</span>.17</p>
</li>
<li>
<p><strong>특성:</strong> <span class="math math-inline">F</span>는 7 자유도(9개 원소에서 스케일 불변성 1, <span class="math math-inline">\det(F)=0</span> 제약 1 제외)를 가지며, 랭크는 2이다.20</p>
</li>
</ul>
<p>이 두 행렬은 최소 8쌍의 대응점만 있으면 8점 알고리즘(8-point algorithm)을 통해 선형적으로 추정할 수 있다. 실제로는 부정확한 매칭(outlier)에 강인하게 대처하기 위해 RANSAC(Random Sample Consensus)과 같은 기법과 함께 사용된다.2</p>
<h3>2.3  호모그래피 (Homography)</h3>
<ul>
<li>
<p><strong>정의:</strong> 호모그래피 <span class="math math-inline">H</span>는 관찰된 모든 3차원 점들이 하나의 평면 위에 존재할 때, 두 이미지상의 대응점들을 관계 짓는 3x3 행렬이다.25 또한, 장면의 구조와 상관없이 카메라가 순수하게 회전만 하는 경우에도 적용될 수 있다.</p>
</li>
<li>
<p><strong>수학적 모델:</strong> 동차 좌표(homogeneous coordinates)로 표현된 대응점 <span class="math math-inline">x</span>와 <span class="math math-inline">x&#39;</span>에 대해 <span class="math math-inline">x&#39; = Hx</span> 관계가 성립한다. <span class="math math-inline">H</span>는 8 자유도를 가지며, 최소 4쌍의 대응점을 이용하여 직접 선형 변환(Direct Linear Transform, DLT) 알고리즘으로 추정할 수 있다.27</p>
</li>
<li>
<p><strong>유도:</strong> 평면 장면에서의 호모그래피 행렬은 <span class="math math-inline">H = K&#39; (R - \frac{tn^T}{d}) K^{-1}</span>로 주어진다. 여기서 <span class="math math-inline">R</span>과 <span class="math math-inline">t</span>는 카메라의 움직임, <span class="math math-inline">n</span>은 평면의 법선 벡터, <span class="math math-inline">d</span>는 카메라 원점으로부터 평면까지의 거리이다.26</p>
</li>
<li>
<p><strong>VO에서의 활용:</strong> 평평한 도로를 주행하거나 실내의 바닥/벽을 보는 등 평면이 지배적인 환경, 또는 드론이 제자리에서 회전하는 상황에서는 호모그래피 행렬을 분해하여 움직임을 추정하는 것이 더 안정적이다. 본질/기본 행렬은 모든 특징점이 평면에 있을 때 불안정해질 수 있기 때문이다. 따라서 강인한 VO 시스템은 장면의 기하학적 구조를 실시간으로 판단하여 호모그래피 모델과 기본 행렬 모델 중 더 적합한 것을 동적으로 선택하는 전략을 사용한다.28 이는 VO의 기하학적 기반이 단일 모델이 아닌, 상황에 맞게 최적의 도구를 선택해야 하는 도구 상자와 같음을 시사한다.</p>
</li>
</ul>
<h2>3.  시각 주행 거리 측정 파이프라인</h2>
<p>VO 시스템은 이미지 입력부터 최종 궤적 출력까지 여러 단계를 거치는 파이프라인 구조를 가진다. 각 단계에서는 다양한 알고리즘적 선택이 가능하며, 이는 시스템 전체의 성능과 특성을 결정한다.</p>
<h3>3.1  카메라 모델 및 구성</h3>
<p>VO 시스템의 가장 첫 번째 설계 결정은 사용할 카메라의 종류를 선택하는 것이며, 이는 전체 파이프라인의 구조를 좌우한다.</p>
<ul>
<li>
<p><strong>단일 카메라 (Monocular):</strong></p>
</li>
<li>
<p><strong>장점:</strong> 저렴한 비용, 간단한 구성, 작은 크기.29</p>
</li>
<li>
<p><strong>단점:</strong> 고질적인 <strong>스케일 모호성(scale ambiguity)</strong> 문제를 가진다. 단일 카메라는 각도만 측정할 수 있으므로, 가까이 있는 작은 물체와 멀리 있는 큰 물체를 구분할 수 없다. 따라서 복원된 궤적과 지도는 실제 크기를 알 수 없는, 임의의 스케일을 갖게 된다.28</p>
</li>
<li>
<p><strong>스테레오 카메라 (Stereo):</strong></p>
</li>
<li>
<p><strong>원리:</strong> 알려진 거리(기선)만큼 떨어진 두 대의 카메라를 사용한다. 좌우 영상에서 동일한 점을 찾아 삼각측량(triangulation) 원리를 이용해 해당 점의 3차원 위치를 직접 계산할 수 있다.30</p>
</li>
<li>
<p><strong>장점:</strong> 스케일 모호성 문제를 해결하여 실제 미터 단위의 궤적과 지도를 생성할 수 있다.30 깊이 추정 정확도가 일반적으로 단일 카메라 방식보다 높다.29</p>
</li>
<li>
<p><strong>단점:</strong> 더 비싸고 복잡한 보정 과정이 필요하며, 계산 비용이 높다. 또한, 질감이 없는 영역에서는 매칭이 실패하고, 거리가 먼 물체에 대해서는 시차(disparity)가 거의 없어 깊이 추정 성능이 저하된다.30</p>
</li>
<li>
<p><strong>RGB-D 카메라 (Depth Camera):</strong></p>
</li>
<li>
<p><strong>원리:</strong> 컬러(RGB) 이미지와 함께 픽셀별 깊이(Depth) 정보를 직접 제공한다. 구조광(Structured Light)이나 비행시간 측정(Time-of-Flight, ToF) 기술을 사용한다.36</p>
</li>
<li>
<p><strong>장점:</strong> 조밀하고 직접적인 깊이 측정값을 제공하여 3차원 재구성 과정을 크게 단순화한다.</p>
</li>
<li>
<p><strong>단점:</strong> 측정 가능 거리가 짧고, 직사광선에 취약하며(특히 구조광 방식), 반사되거나 투명한 표면에서는 깊이 측정이 실패할 수 있다.37</p>
</li>
</ul>
<table><thead><tr><th>구분</th><th>단일 카메라 (Monocular)</th><th>스테레오 카메라 (Stereo)</th><th>RGB-D 카메라</th></tr></thead><tbody>
<tr><td><strong>비용 및 복잡도</strong></td><td>낮음</td><td>중간</td><td>중간-높음</td></tr>
<tr><td><strong>스케일 추정</strong></td><td>모호함 (상대 스케일)</td><td>절대값 (미터 단위)</td><td>절대값 (미터 단위)</td></tr>
<tr><td><strong>깊이 추정</strong></td><td>간접적 (움직임 기반)</td><td>직접적 (시차 기반)</td><td>직접적 (센서 측정)</td></tr>
<tr><td><strong>작동 범위</strong></td><td>장거리</td><td>중거리 (기선에 의존)</td><td>단거리</td></tr>
<tr><td><strong>질감 의존성</strong></td><td>높음 (특징점 필요)</td><td>높음 (매칭 필요)</td><td>낮음 (질감 무관)</td></tr>
<tr><td><strong>실외 성능</strong></td><td>좋음</td><td>좋음</td><td>나쁨 (햇빛 간섭)</td></tr>
</tbody></table>
<h3>3.2  특징점 기반(간접) 방식 (Feature-Based / Indirect Methods)</h3>
<p>이 방식은 VO의 “고전적인” 접근법으로, 명시적인 특징점 추출 및 매칭 단계를 거친다.2</p>
<ol>
<li>
<p><strong>특징점 검출 (Feature Detection):</strong> 이미지에서 반복적으로 검출 가능하고 식별이 용이한 지점(코너, 블롭 등)을 찾는다. Harris 코너 23, FAST 42, SIFT 2, SURF 3, ORB 3 등의 알고리즘이 사용된다. SIFT/SURF는 스케일과 회전 변화에 강인하지만 계산량이 많고, ORB는 FAST 검출기와 BRIEF 기술자를 결합하여 실시간 응용에 적합한 빠른 속도를 제공한다.3</p>
</li>
<li>
<p><strong>특징점 기술 (Feature Description):</strong> 각 특징점 주변 영역의 외형 정보를 요약하는 벡터(descriptor)를 생성한다. 이 기술자는 조명이나 시점 변화에도 강인해야 한다. SIFT, SURF, ORB 등은 각각 고유한 기술자 정의 방식을 가진다.2</p>
</li>
<li>
<p><strong>특징점 매칭 (Feature Matching):</strong> 한 프레임의 기술자들을 다음 프레임의 기술자들과 비교하여 대응 관계를 찾는다. Brute-force 방식이나 FLANN(Fast Library for Approximate Nearest Neighbors)과 같은 근사 최근접 이웃 탐색 알고리즘이 사용된다.16 모호한 매칭을 제거하기 위해 가장 가까운 매칭과 두 번째로 가까운 매칭 간의 거리 비율을 확인하는 비율 테스트(Ratio Test)가 널리 쓰인다.3</p>
</li>
<li>
<p><strong>오차점 제거 (Outlier Rejection):</strong> 매칭 결과에는 필연적으로 잘못된 대응(outlier)이 포함된다. RANSAC(Random Sample Consensus)은 다수의 데이터 속에서 특정 기하학 모델(예: 기본 행렬)과 일관성을 갖는 정상 데이터(inlier) 집합을 찾는 표준적인 방법이다.4</p>
</li>
</ol>
<h3>3.3  직접 방식 (Direct Methods)</h3>
<p>직접 방식은 특징점 검출 및 기술 단계를 생략하고, 픽셀의 밝기 값을 직접 사용하여 동작을 추정한다.13</p>
<ul>
<li>
<p><strong>원리:</strong> 밝기 항상성(brightness constancy)을 가정하고, 두 이미지 간 대응되는 픽셀들의 밝기 값 차이, 즉 *광도 오차(photometric error)*를 최소화하는 방향으로 카메라의 움직임을 추정한다.40</p>
</li>
<li>
<p><strong>종류:</strong></p>
</li>
<li>
<p><strong>조밀(Dense) 방식:</strong> 이미지의 모든 픽셀을 사용한다. 계산 비용이 매우 높지만 조밀한 3차원 구조를 얻을 수 있다.</p>
</li>
<li>
<p><strong>준조밀(Semi-Dense) 방식:</strong> 밝기 변화가 큰 픽셀(엣지 등)만을 사용한다. 조밀 방식과 희소 방식의 절충안이다 (예: LSD-SLAM 40).</p>
</li>
<li>
<p><strong>희소(Sparse) 방식:</strong> 희소한 픽셀 집합을 사용하지만, 여전히 광도 오차를 직접 최적화한다 (예: DSO 40).</p>
</li>
<li>
<p><strong>장점:</strong> 특징점 검출기가 실패할 수 있는 질감이 부족한 영역(예: 벽면의 엣지)의 정보도 활용할 수 있다. 특징점 검출 과정에서 발생하는 정보 손실이 없어 잠재적으로 더 높은 정확도를 가질 수 있다.40</p>
</li>
<li>
<p><strong>단점:</strong> 조명 변화나 카메라의 자동 노출 기능과 같은 광도 변화에 매우 민감하다. 수렴을 위해 움직임에 대한 좋은 초기 추정치가 필요하다.45</p>
</li>
</ul>
<table><thead><tr><th>특성</th><th>특징점 기반 (간접) 방식</th><th>직접 방식</th></tr></thead><tbody>
<tr><td><strong>핵심 원리</strong></td><td>기하학적 재투영 오차 최소화</td><td>광도 오차 최소화</td></tr>
<tr><td><strong>사용 정보</strong></td><td>희소한 특징점</td><td>모든 픽셀(조밀) 또는 일부 픽셀(준조밀/희소)</td></tr>
<tr><td><strong>조명 변화 강인성</strong></td><td>높음 (불변 기술자 덕분) 40</td><td>낮음 (밝기 항상성 가정 위배)</td></tr>
<tr><td><strong>저질감 환경 성능</strong></td><td>나쁨 (검출할 특징점 없음)</td><td>좋음 (엣지/그래디언트 활용 가능) 40</td></tr>
<tr><td><strong>계산 비용</strong></td><td>특징점 수에 의존</td><td>높음 (조밀) ~ 중간 (희소)</td></tr>
<tr><td><strong>정확도</strong></td><td>특징점 검출 정밀도에 제한</td><td>잠재적으로 더 높음 (서브픽셀 단위) 45</td></tr>
<tr><td><strong>대표 알고리즘</strong></td><td>ORB-SLAM 44</td><td>LSD-SLAM, DSO 40</td></tr>
</tbody></table>
<h3>3.4  동작 추정 (Motion Estimation)</h3>
<p>신뢰할 수 있는 2D-2D 또는 3D-2D 대응점 집합이 확보되면, 카메라의 동작(회전 <span class="math math-inline">R</span>과 이동 <span class="math math-inline">t</span>)을 계산할 수 있다.</p>
<ul>
<li>
<p><strong>2D-2D 대응 (단일/미보정 스테레오):</strong> 2차원 대응점들로부터 본질/기본 행렬을 추정한 뒤, 본질 행렬 <span class="math math-inline">E</span>를 분해하여 4개의 가능한 $$ 해를 얻는다.3 이 중 올바른 해는 임의의 점을 삼각측량하여 두 카메라 모두의 앞에 위치하는지(깊이가 양수인지) 확인함으로써 찾을 수 있다.</p>
</li>
<li>
<p><strong>3D-2D 대응 (스테레오/RGB-D/초기화된 단일):</strong> 3차원 지도 점들과 현재 프레임에서의 2차원 투영점들 간의 대응 관계가 주어졌을 때 카메라의 포즈를 찾는 문제로, 이를 PnP(Perspective-n-Point) 문제라고 한다.1 최소 3개의 대응점(P3P)이 필요하며, 일반적으로 RANSAC 루프 내에서 수행하여 오차점에 강인하게 만든다.1</p>
</li>
<li>
<p><strong>3D-3D 대응 (스테레오/RGB-D):</strong> 두 시점에서 얻은 3차원 점 집합들 간의 대응 관계가 주어졌을 때, 이들을 정렬하는 변환 $$를 찾는 문제이다. 반복적 최근접 점(Iterative Closest Point, ICP) 알고리즘이나 특이값 분해(SVD)를 통해 직접 해를 구할 수 있다.</p>
</li>
</ul>
<h3>3.5  삼각측량 및 스케일 문제</h3>
<ul>
<li>
<p><strong>삼각측량 (Triangulation):</strong> 알려진 카메라 포즈를 가진 두 개 이상의 이미지에서 한 점의 투영 위치가 주어졌을 때, 그 점의 3차원 좌표를 결정하는 과정이다.18 3차원 지도를 구축하는 데 필수적이다.</p>
</li>
<li>
<p><strong>단일 카메라 VO의 스케일 모호성:</strong> 이는 단일 카메라 VO의 가장 큰 난제이다. 단일 카메라는 3차원 세계를 2차원 평면에 투영하면서 깊이 정보를 잃어버리기 때문에, 재구성된 궤적과 지도의 형태는 정확하지만 그 절대적인 크기, 즉 스케일은 알 수 없다.28 이 스케일은 시간이 지남에 따라 변동(drift)할 수도 있다.</p>
</li>
<li>
<p><strong>스케일 모호성 해결 방안:</strong></p>
</li>
<li>
<p><strong>초기화:</strong> 초기 움직임의 크기를 임의의 값(예: 1)으로 가정하여 전체 시스템의 기준 스케일로 삼는다.38</p>
</li>
<li>
<p><strong>센서 융합:</strong> IMU(VIO)나 바퀴 엔코더와 같이 미터 단위 정보를 제공하는 센서와 융합하는 것이 가장 강인한 해결책이다.38</p>
</li>
<li>
<p><strong>장면 제약:</strong> 카메라가 지면으로부터 알려진 높이에 있다고 가정하거나 28, 알려진 크기의 물체(자동차, 표지판 등)를 인식하여 스케일을 추정한다.28</p>
</li>
<li>
<p><strong>사전 정보 활용:</strong> 기존에 구축된 3차원 도시 모델과 매칭하여 스케일을 얻을 수도 있다.28</p>
</li>
</ul>
<h3>3.6  최적화: 번들 조정 (Bundle Adjustment)</h3>
<ul>
<li>
<p><strong>원리:</strong> 번들 조정(BA)은 모든 3차원 지도 점들의 위치와 모든 카메라의 포즈를 동시에 최적화하여, 전체 *재투영 오차(reprojection error)*를 최소화하는 비선형 최적화 기법이다.14 재투영 오차는 지도 점을 특정 카메라 포즈로 이미지 평면에 투영했을 때의 계산된 위치와 실제 관측된 특징점 위치 간의 거리이다.</p>
</li>
<li>
<p><strong>전역적 BA (Global BA):</strong> 전체 궤적과 지도를 한 번에 최적화한다. 오프라인 SfM에서는 정확도의 표준으로 여겨지지만, 실시간 VO에는 계산 비용이 너무 크다.14</p>
</li>
<li>
<p><strong>지역적 BA / 슬라이딩 윈도우 BA (Local BA / Sliding Window BA):</strong> 실시간 성능을 달성하기 위해, VO/SLAM 시스템은 최근의 일부 카메라 포즈와 이들이 관측하는 지도 점들로 구성된 지역적인 창(window) 내에서만 최적화를 수행한다.10 오래된 포즈와 점들은 고정되거나 주변화(marginalize out)된다. 이는 전역 최적화의 비용 없이 지역적인 오차 누적을 효과적으로 보정하여 정확도와 효율성 사이의 균형을 맞춘다.32</p>
</li>
</ul>
<p>결론적으로, VO 파이프라인은 불확실성이 전파되고 누적되는 연쇄적인 추정 시스템이다. 초기 센서 선택부터 최종 최적화 전략에 이르기까지, 각 단계는 실시간성, 강인성, 정확도라는 상충하는 요구사항 사이의 균형을 맞추는 일련의 공학적 절충안으로 구성된다.</p>
<h2>4.  고급 주제 및 최신 연구 동향</h2>
<p>기본적인 VO 파이프라인의 한계를 극복하기 위해 다양한 고급 기법과 새로운 연구들이 활발히 진행되고 있다. 이들은 오차 누적, 동적 환경, 센서 실패와 같은 핵심적인 문제들을 해결하는 데 초점을 맞춘다.</p>
<h3>4.1  시각-관성 주행 거리 측정 (Visual-Inertial Odometry, VIO)</h3>
<p>VIO는 카메라와 IMU의 장점을 결합하여 각 센서의 단점을 상호 보완하는 강력한 기술이다. 카메라는 환경에 대한 (단기적으로) 오차 누적이 없는 관측을 제공하는 반면, IMU는 고주파의 움직임 데이터, 미터 단위의 스케일, 중력 방향 정보를 제공하여 시각적 특징이 부족하거나 빠른 움직임으로 인해 이미지가 흐릿해지는 상황에서도 시스템의 강인성을 유지한다.6</p>
<ul>
<li>
<p><strong>융합 방식:</strong></p>
</li>
<li>
<p><strong>필터 기반 (Filtering-based):</strong> 확장 칼만 필터(EKF)를 기반으로 하는 강결합(tightly-coupled) 방식인 MSCKF(Multi-State Constraint Kalman Filter)는 카메라 특징점을 과거 상태에 대한 제약 조건으로 활용한다. 이는 계산적으로 효율적이다.6</p>
</li>
<li>
<p><strong>최적화 기반 (Optimization-based):</strong> 슬라이딩 윈도우 내의 IMU 측정값과 시각 정보를 함께 비선형 최적화하여 포즈, 속도, IMU 편향, 특징점 위치를 동시에 추정한다. 일반적으로 필터 기반 방식보다 정확하지만 계산 요구량이 더 높다.7</p>
</li>
</ul>
<p>VIO의 가장 중요한 이점은 IMU의 가속도계 측정값을 통해 미터 단위의 스케일을 관측 가능하게 만들어, 단일 카메라 VO의 근본적인 문제를 해결한다는 점이다.6</p>
<h3>4.2  동적 환경에서의 VO (VO in Dynamic Environments)</h3>
<p>대부분의 VO 알고리즘은 ’세상은 정적이다’라는 기본 가정을 전제로 한다. 그러나 자동차, 보행자 등 움직이는 객체들은 카메라의 자체 움직임과 무관한 특징점 이동을 유발하여 잘못된 포즈 추정의 원인이 된다.54</p>
<ul>
<li>
<p><strong>해결 방안:</strong></p>
</li>
<li>
<p><strong>기하학적 오차점 제거:</strong> RANSAC은 일부 동적 특징점을 제거할 수 있지만, 장면의 상당 부분이 동적일 경우 실패한다.</p>
</li>
<li>
<p><strong>의미론적 분할 (Semantic Segmentation):</strong> 딥러닝 모델을 사용하여 ‘사람’, ’자동차’와 같은 동적 객체를 픽셀 단위로 식별하고 분할한다. 이렇게 분할된 영역을 마스킹하여 VO 시스템이 정적인 배경의 특징점만을 사용하도록 강제할 수 있다.54</p>
</li>
<li>
<p><strong>객체 수준 SLAM (Object-level SLAM):</strong> 더 발전된 접근법으로, 동적 객체를 명시적으로 모델링하고 추적하여 이들을 하나의 독립된 랜드마크로 다룬다. 이를 통해 장면을 더 풍부하게 이해하고 강인성을 높인다.58</p>
</li>
</ul>
<h3>4.3  딥러닝 기반 VO (Deep Learning-based VO)</h3>
<p>딥러닝 기반 VO는 전통적인 파이프라인의 수작업으로 설계된 구성 요소들을 학습된 함수로 대체하여, 도전적인 환경에서의 성능과 강인성을 향상시키는 것을 목표로 한다.59</p>
<ul>
<li>
<p><strong>접근 방식:</strong></p>
</li>
<li>
<p><strong>종단간(End-to-End) 학습:</strong> 이미지 시퀀스를 입력받아 카메라 포즈를 직접 회귀(regress)하는 신경망. 주로 재투영 오차를 최소화하는 자기 지도 학습(self-supervised learning) 방식으로 훈련된다.60</p>
</li>
<li>
<p><strong>하이브리드 (Hybrid) 방식:</strong> 딥러닝을 사용하여 전통적인 파이프라인의 특정 모듈을 강화한다. 예를 들어, CNN을 특징 검출 및 기술에 사용하거나, 단일 이미지로부터 조밀한 깊이 맵을 추정하여 이를 기하학 기반의 백엔드에 입력으로 제공하는 방식이다.57</p>
</li>
<li>
<p><strong>다중 작업 학습 (Multi-Task Learning):</strong> 최신 프레임워크들은 깊이, 광학 흐름, 카메라 포즈, 의미론적 분할 등 여러 작업을 단일 네트워크 내에서 공동으로 학습한다. 각 작업 간의 일관성을 강력한 자기 지도 신호로 활용하여 성능을 극대화한다.57</p>
</li>
</ul>
<h3>4.4  이벤트 카메라 기반 VO (Event Camera-based VO)</h3>
<p>이벤트 카메라는 고정된 프레임률로 전체 이미지를 촬영하는 대신, 각 픽셀이 밝기 변화를 감지할 때마다 비동기적으로 ’이벤트’를 보고하는 생체 모방형 센서이다.61</p>
<ul>
<li>
<p><strong>VO에서의 장점:</strong></p>
</li>
<li>
<p><strong>높은 시간 해상도 (µs):</strong> 모션 블러를 근본적으로 제거하여 극도로 빠른 움직임도 정확하게 추적할 수 있다.62</p>
</li>
<li>
<p><strong>높은 동적 범위 (HDR &gt; 120dB):</strong> 기존 카메라가 포화되거나 노출 부족을 겪는 극단적인 조명 환경에서도 작동 가능하다.61</p>
</li>
<li>
<p><strong>낮은 지연 시간 및 전력 소모:</strong> 데이터가 희소하고 필요할 때만 생성되므로 자원이 제한된 플랫폼에 이상적이다.</p>
</li>
<li>
<p><strong>알고리즘:</strong> 프레임 기반 알고리즘에서 벗어난 새로운 패러다임을 요구한다. 대표적으로, 이벤트들을 특정 움직임으로 되감았을 때 생성되는 이미지의 대비(contrast)를 최대화하는 방식으로 동작을 추정하거나, 시공간 이벤트 스트림에서 직접 특징을 추적하는 방법 등이 있다.65</p>
</li>
</ul>
<p>이러한 고급 주제들은 VO 연구가 고전적인 파이프라인의 가정이 무너지는 ’엣지 케이스’를 해결하는 방향으로 나아가고 있음을 보여준다. 이는 순수한 기하학적, 단일 센서 시스템에서 벗어나, 실제 세계의 복잡성에 대응할 수 있는 강인한 다중 모드, 학습 증강 인식 시스템으로 발전하는 명확한 추세를 나타낸다.</p>
<h2>5.  응용 분야 및 평가</h2>
<p>VO 기술은 이론적 논의를 넘어 다양한 실제 응용 분야에서 핵심적인 역할을 수행하고 있으며, 그 성능은 표준화된 벤치마크를 통해 과학적으로 평가된다.</p>
<h3>5.1  주요 응용 분야</h3>
<ul>
<li>
<p><strong>로보틱스:</strong> 실내나 수중과 같이 GPS 사용이 불가능한 환경에서 로봇 항법에 필수적이다.1 화성 탐사 로버가 VO를 활용한 대표적인 사례이다.13</p>
</li>
<li>
<p><strong>자율 주행:</strong> GPS, LiDAR, 바퀴 주행 거리 측정 기술을 보완하여, 특히 GPS 신호가 불안정한 도심 협곡(urban canyon) 등에서 강인하고 비용 효율적인 위치 추정 솔루션을 제공한다.8</p>
</li>
<li>
<p><strong>드론 (UAVs):</strong> 실내 또는 저고도 비행 시 자율 항법을 가능하게 하여 정밀 농업, 창고 자동화, 최종 단계 배송(last-mile delivery)과 같은 분야를 혁신하고 있다.9</p>
</li>
<li>
<p><strong>증강/가상 현실 (AR/VR):</strong> 사용자의 머리 움직임(6-DOF)을 정밀하게 추적하여 가상 콘텐츠를 현실 세계에 자연스럽게 겹치거나 안정적인 가상 세계를 렌더링하는 데 결정적인 역할을 한다. 최신 모바일 AR/VR 기기에서는 VIO가 지배적인 기술로 사용된다.1</p>
</li>
</ul>
<h3>5.2  벤치마크 데이터셋</h3>
<p>벤치마크 데이터셋은 다양한 VO/SLAM 알고리즘의 성능을 객관적으로 비교하기 위한 표준화된 데이터와 평가 지표를 제공한다.</p>
<ul>
<li>
<p><strong>KITTI 데이터셋:</strong></p>
</li>
<li>
<p><strong>초점:</strong> 자율 주행.69</p>
</li>
<li>
<p><strong>센서:</strong> 스테레오 카메라(컬러/흑백), Velodyne LiDAR, GPS/IMU.69</p>
</li>
<li>
<p><strong>환경:</strong> 독일 카를스루에 시의 도심, 시골, 고속도로 환경.</p>
</li>
<li>
<p><strong>평가:</strong> 추정된 궤적을 GPS/IMU로 측정한 실제 경로(ground truth)와 비교한다. 다양한 주행 거리와 속도 구간에 대한 평균 이동 오차(%)와 회전 오차(도/미터)를 지표로 사용한다.72</p>
</li>
<li>
<p><strong>EuRoC MAV 데이터셋:</strong></p>
</li>
<li>
<p><strong>초점:</strong> 소형 무인 항공기(MAV) / 드론.73</p>
</li>
<li>
<p><strong>센서:</strong> 글로벌 셔터 스테레오 카메라, 동기화된 MEMS IMU.74</p>
</li>
<li>
<p><strong>환경:</strong> 산업용 기계실과 Vicon 모션 캡처 룸에서 촬영되었으며, 움직임 속도, 조명, 질감에 따라 쉬움(easy), 중간(medium), 어려움(difficult) 난이도의 시퀀스를 제공한다.73</p>
</li>
<li>
<p><strong>평가:</strong> Leica 레이저 트래커나 Vicon 시스템으로부터 얻은 매우 정확한 실제 경로를 제공하여, 특히 VIO 알고리즘 벤치마킹에 이상적이다.73</p>
</li>
<li>
<p><strong>TUM RGB-D 데이터셋:</strong></p>
</li>
<li>
<p><strong>초점:</strong> 실내 로보틱스를 위한 RGB-D SLAM.75</p>
</li>
<li>
<p><strong>센서:</strong> Microsoft Kinect 센서(RGB, 깊이, 가속도계).75</p>
</li>
<li>
<p><strong>환경:</strong> 다양한 실내 사무실/책상 환경.</p>
</li>
<li>
<p><strong>평가:</strong> 고정밀 모션 캡처 시스템으로 얻은 실제 경로를 제공하며, 절대 궤적 오차(Absolute Trajectory Error, ATE)와 상대 포즈 오차(Relative Pose Error, RPE)를 표준 평가 지표로 제안했다.75</p>
</li>
</ul>
<p>이러한 고품질 데이터셋의 등장은 지난 10년간 VO 분야의 급속한 발전을 이끈 핵심적인 촉매제 역할을 했다. 연구실 수준의 알고리즘들이 실제 환경의 다양한 도전에 직면하게 되면서, 더 강인하고 정교한 기술(VIO, 딥러닝 기반 방법 등)의 개발을 촉진하는 선순환 구조를 형성했다.</p>
<h2>6.  결론 및 향후 과제</h2>
<h3>6.1  VO 기술의 요약 및 한계</h3>
<p>VO는 점진적인 동작 추정을 위한 강력하고 비용 효율적인 기술이다. 기본적인 기하학 원리에서 출발하여 센서 융합과 머신러닝을 통합하는 정교한 시스템으로 발전해왔다. 성공적인 VO 시스템 구축은 카메라 종류, 알고리즘 접근법(특징점 기반 대 직접 방식), 최적화 전략 등 여러 핵심적인 요소들 간의 절충안을 찾는 과정이다.</p>
<p>그럼에도 불구하고 VO는 다음과 같은 근본적인 한계를 내포하고 있다.</p>
<ul>
<li>
<p><strong>오차 누적 (Drift):</strong> 상대적인 움직임을 누적하여 위치를 추정하는 추측 항법(dead-reckoning)의 일종이므로, 작은 오차가 시간이 지남에 따라 계속 쌓여 전역 궤적의 오차가 무한정 커질 수 있다. 이는 완전한 SLAM 시스템과 비교했을 때 VO의 가장 큰 제약이다.</p>
</li>
<li>
<p><strong>환경 민감성:</strong> 성능이 충분한 질감, 안정적인 조명, 대부분이 정적인 장면과 같은 환경 조건에 크게 의존한다.</p>
</li>
<li>
<p><strong>스케일 모호성:</strong> 순수 단일 카메라 시스템에서 지속적으로 발생하는 문제로, 외부 정보 없이는 절대적인 스케일을 결정할 수 없다.</p>
</li>
</ul>
<h3>6.2  미래 연구 방향</h3>
<p>VO 기술의 발전을 위해 다음과 같은 연구 방향이 중요하게 다뤄지고 있다.</p>
<ul>
<li>
<p><strong>장기적 강인성:</strong> 날씨 변화, 낮과 밤의 주기 등 다양하고 변화무쌍한 환경에서 장시간 동안 안정적으로 작동할 수 있는 방법을 개발하는 연구.68</p>
</li>
<li>
<p><strong>다중 센서와의 강결합 융합:</strong> VIO를 넘어 LiDAR, 열화상, 레이더 등 다른 종류의 센서와 시각 정보를 긴밀하게 결합하여 모든 날씨와 조건에서 작동하는 전천후 위치 추정 시스템을 구축하는 연구.</p>
</li>
<li>
<p><strong>딥러닝의 일반화 성능 향상:</strong> 학습 기반 VO 방법들이 훈련 데이터에서 보지 못한 새로운 환경이나 카메라에 대해서도 잘 작동하도록 일반화 성능을 높이는 연구. 온라인 적응 및 자기 지도 학습이 핵심적인 연구 분야이다.60</p>
</li>
<li>
<p><strong>의미론적 이해:</strong> 점과 픽셀을 추적하는 수준을 넘어, 객체를 추적하고 장면의 기능적 구성을 이해하는 등 고수준의 장면 이해를 통합하여 더 의미 있고 강인한 위치 추정을 수행하는 연구.56</p>
</li>
<li>
<p><strong>이벤트 기반 비전의 성숙:</strong> 이벤트 카메라의 장점을 최대한 활용하고 고속 및 HDR 응용 분야에서 주류 기술로 자리 잡을 수 있도록 관련 VO 알고리즘을 성숙시키는 연구.61</p>
</li>
</ul>
<h2>7. 참고 자료</h2>
<ol>
<li>(PDF) Visual Odometry [Tutorial] - ResearchGate, https://www.researchgate.net/publication/220556161_Visual_Odometry_Tutorial</li>
<li>Visual Odometry: Part II Matching, Robustness, Optimization, and Applications - zora.uzh.ch, https://www.zora.uzh.ch/71030/1/Fraundorfer_Scaramuzza_Visual_odometry.pdf</li>
<li>Visual Odometry: A practical guide | by Rushideshmukh - Medium, https://medium.com/@rushideshmukh23/visual-odometry-a-practical-guide-71ddff1687cd</li>
<li>Visual Odometry Tutorial - John Lambert, https://johnwlambert.github.io/vo/</li>
<li>Visual Slam vs. Visual Odometry – How Does Visual Odometry Work? - Inertial Sense, https://inertialsense.com/visual-slam-vs-visual-odometry-how-does-visual-odometry-work/</li>
<li>P4 - Visual Inertial Odometry - PeAR WPI, https://pear.wpi.edu/img/teaching/rbe549/spring2023/studentoutputs/p4ph2/sankaruday.pdf</li>
<li>How Visual Inertial Odometry (VIO) Works - Think Autonomous., https://www.thinkautonomous.ai/blog/visual-inertial-odometry/</li>
<li>Visual Odometry For Autonomous Vehicles. - International Journal of Advanced Research, http://www.journalijar.com/article/29806/visual-odometry-for-autonomous-vehicles.</li>
<li>Autonomous Drone Visual Odometry - Meegle, https://www.meegle.com/en_us/topics/autonomous-drones/autonomous-drone-visual-odometry</li>
<li>Visual Odometry vs. Visual SLAM vs. Structure-from-Motion | by …, https://guvencetinkaya.medium.com/visual-odometry-vs-visual-slam-cdda75df592</li>
<li>Visual Odometry Vs Visual SLAM - YouTube, https://www.youtube.com/watch?v=0BHfZnhwo34</li>
<li>What is the difference between SLAM and vio? - BytePlus, https://www.byteplus.com/en/topic/110737</li>
<li>Visual odometry - Wikipedia, https://en.wikipedia.org/wiki/Visual_odometry</li>
<li>Visual_Odometry_Tutorial.pdf - Robotics and Perception Group, https://rpg.ifi.uzh.ch/docs/Visual_Odometry_Tutorial.pdf</li>
<li>CS231A Course Notes 3: Epipolar Geometry, https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf</li>
<li>Epipolar Geometry - OpenCV Documentation, https://docs.opencv.org/3.4/da/de9/tutorial_py_epipolar_geometry.html</li>
<li>aircconline.com, https://aircconline.com/ijcsit/V14N1/14122ijcsit02.pdf</li>
<li>Epipolar Geometry - OpenCV Documentation, https://docs.opencv.org/4.x/da/de9/tutorial_py_epipolar_geometry.html</li>
<li>Epipolar Geometry - Carnegie Mellon University, https://www.cs.cmu.edu/~16385/s15/lectures/Lecture18.pdf</li>
<li>Epipolar Geometry and the Fundamental Matrix, https://engineering.purdue.edu/kak/computervision/ECE661Folder/Lecture23.pdf</li>
<li>Essential and Fundamental Matrices Epipolar Geometry Epipolar Geometry This Lecture… Essential Matrix Essential Ma - Penn State, https://www.cse.psu.edu/~rtc12/CSE486/lecture19_6pp.pdf</li>
<li>Difference Between Fundamental Matrix and Essential Matrix | Baeldung on Computer Science, https://www.baeldung.com/cs/fundamental-matrix-vs-essential-matrix</li>
<li>Monocular Visual Odometry - Mayank Mittal, https://mayankm96.github.io/assets/documents/projects/VA4MR_Mini_Project.pdf</li>
<li>Fundamental matrix (computer vision) - Wikipedia, https://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision)</li>
<li>Homography Estimation - Papers With Code, https://paperswithcode.com/task/homography-estimation/codeless</li>
<li>Homography (computer vision) - Wikipedia, https://en.wikipedia.org/wiki/Homography_(computer_vision)</li>
<li>41 Homographies - Foundations of Computer Vision, https://visionbook.mit.edu/homography.html</li>
<li>Illustration of the scale factor issue in monocular visual odometry… - ResearchGate, https://www.researchgate.net/figure/llustration-of-the-scale-factor-issue-in-monocular-visual-odometry-adapted-from-Gakne_fig1_342786726</li>
<li>Review of monocular depth estimation methods - SPIE Digital Library, https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-34/issue-02/020901/Review-of-monocular-depth-estimation-methods/10.1117/1.JEI.34.2.020901.full</li>
<li>Monocular vs Stereo vs Monochrome Camera | by Abhishek Jain - Medium, https://medium.com/@abhishekjainindore24/monocular-vs-stereo-vs-monochrome-camera-16612bf4358b</li>
<li>Visual Odometry (VO), http://www.cs.toronto.edu/~urtasun/courses/CSC2541/03_odometry.pdf</li>
<li>Monocular Visual Odometry - MATLAB &amp; Simulink - MathWorks, https://uk.mathworks.com/help/vision/ug/monocular-visual-odometry.html</li>
<li>Reducing Scale Drift in Monocular Visual Odometry with BEV Representation - arXiv, https://arxiv.org/html/2411.10195v1</li>
<li>Introduction to Epipolar Geometry and Stereo Vision - LearnOpenCV, https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/</li>
<li>What is the difference between monocular and stereo image?, https://dsp.stackexchange.com/questions/24268/what-is-the-difference-between-monocular-and-stereo-image</li>
<li>Best Stereo Camera for Depth Perception in Imaging and Robotics - Foamcoreprint.com, https://www.foamcoreprint.com/blog/best-stereo-camera-for-depth-perception-in-imaging-and-robotics</li>
<li>3D cameras in 2022: choosing a camera for CV project | by Anton Maltsev | Medium, https://medium.com/@zlodeibaal/3d-cameras-in-2022-choosing-a-camera-for-cv-project-6eb6fcc67948</li>
<li>Monocular VO Scale Ambiguity Resolution Using an Ultra Low-Cost Spike Rangefinder - Scirp.org, https://www.scirp.org/journal/paperinformation?paperid=106397</li>
<li>What is the difference between stereo camera and RGBD camera? - Reddit, https://www.reddit.com/r/computervision/comments/mtgir1/what_is_the_difference_between_stereo_camera_and/</li>
<li>Feature-based or Direct: An Evaluation of Monocular Visual Odometry - ResearchGate, https://www.researchgate.net/publication/316875491_Feature-based_or_Direct_An_Evaluation_of_Monocular_Visual_Odometry</li>
<li>Mini-project: A Visual Odometry Pipeline - Kexin Shi, https://tenhearts.github.io/assets/pdf/vo_report.pdf</li>
<li>Visual Odmetry from scratch - A tutorial for beginners - Avi Singh’s, https://avisingh599.github.io/vision/visual-odometry-full/</li>
<li>Visual Odometry – Abhishek Portfolio - Abhishek Sankar, https://abhishek-sankar.com/visual-odometry/</li>
<li>The difference between direct dense method and feature-based sparse… - ResearchGate, https://www.researchgate.net/figure/The-difference-between-direct-dense-method-and-feature-based-sparse-method-Feature-based_fig4_224252357</li>
<li>[1804.05422] FDMO: Feature Assisted Direct Monocular Odometry - arXiv, https://arxiv.org/abs/1804.05422</li>
<li>Lecture 5: calibration, triangulation, and epipolar geometry - TTIC 31040: Introduction to Computer Vision, https://dl.ttic.edu/pals/31040/lectures/lec5.pdf</li>
<li>Monocular Visual Odometry using a Planar Road Model to Solve Scale Ambiguity, https://www.ri.cmu.edu/publications/monocular-visual-odometry-using-a-planar-road-model-to-solve-scale-ambiguity/</li>
<li>Resolving scale ambiguity for monocular visual odometry - Semantic Scholar, https://www.semanticscholar.org/paper/Resolving-scale-ambiguity-for-monocular-visual-Choi-Park/29287871c02ca75fd88df5ca297bccae52d7de62</li>
<li>Solving the scale ambiguity in monocular SLAM : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/3zovt9/solving_the_scale_ambiguity_in_monocular_slam/</li>
<li>Visual Odometry Using Sparse Bundle Adjustment on an Autonomous Outdoor Vehicle - TU Chemnitz, https://www.tu-chemnitz.de/etit/proaut/publications/ams05-visualodometry.pdf</li>
<li>Weighted Local Bundle Adjustment and Application to Odometry and Visual SLAM Fusion - BMVA Archive, https://www.bmva-archive.org.uk/bmvc/2010/conference/paper25/abstract25.pdf</li>
<li>BALM: Bundle Adjustment for Lidar Mapping - YouTube, https://www.youtube.com/watch?v=d8R7aJmKifQ</li>
<li>Local bundle adjustment when camera is added. Only surrounded points… - ResearchGate, https://www.researchgate.net/figure/Local-bundle-adjustment-when-camera-is-added-Only-surrounded-points-and-cameras-are_fig6_232651073</li>
<li>ROBUST VISUAL-INERTIAL ODOMETRY IN DYNAMIC …, https://elib.dlr.de/136795/1/isprs-annals-V-2-2020-435-2020.pdf</li>
<li>RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in Dynamic Environments - arXiv, https://arxiv.org/html/2310.15072v3</li>
<li>VSO: Visual Semantic Odometry - CVF Open Access, https://openaccess.thecvf.com/content_ECCV_2018/papers/Konstantinos-Nektarios_Lianos_VSO_Visual_Semantic_ECCV_2018_paper.pdf</li>
<li>Deep Learning-based Visual Odometry and SLAM | by Yu Huang …, https://yuhuang-63908.medium.com/deep-learning-based-visual-odometry-and-slam-4a8d0f8f907</li>
<li>VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks - arXiv, https://arxiv.org/html/2402.13609v1</li>
<li>Unsupervised Deep Learning-Based RGB-D Visual Odometry - MDPI, https://www.mdpi.com/2076-3417/10/16/5426</li>
<li>Generalizing to the Open World: Deep Visual Odometry With Online Adaptation, https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Generalizing_to_the_Open_World_Deep_Visual_Odometry_With_Online_CVPR_2021_paper.pdf</li>
<li>Event-based Sensor Fusion and Application on Odometry: A Survey - arXiv, https://arxiv.org/html/2410.15480v1</li>
<li>Event-based feature tracking in a visual inertial odometry framework - Frontiers, https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2023.994488/full</li>
<li>Stereo Event-Based Visual–Inertial Odometry - MDPI, https://www.mdpi.com/1424-8220/25/3/887</li>
<li>Asynchronous Visual-Inertial Odometry for Event Cameras - ISPRS - The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, https://isprs-archives.copernicus.org/articles/XLVIII-4-2024/651/2024/isprs-archives-XLVIII-4-2024-651-2024.pdf</li>
<li>Event-Based Visual Inertial Odometry - CVF Open Access, https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Event-Based_Visual_Inertial_CVPR_2017_paper.pdf</li>
<li>Event Camera-based Visual Odometry for Dynamic Motion Tracking of a Legged Robot Using Adaptive Time Surface - UMass Amherst, https://www.umass.edu/robotics/sites/default/files/2023-10/shifanIROS.pdf</li>
<li>Spatiotemporal Registration for Event-Based Visual Odometry - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Spatiotemporal_Registration_for_Event-Based_Visual_Odometry_CVPR_2021_paper.pdf</li>
<li>Evaluating Visual Odometry Methods for Autonomous Driving in Rain - arXiv, https://arxiv.org/html/2309.05249v3</li>
<li>The KITTI Vision Benchmark Suite - Andreas Geiger, https://www.cvlibs.net/datasets/kitti/</li>
<li>Exploring KITTI Dataset: Visual Odometry for autonomous vehicles | by Jaimin-k | Medium, https://medium.com/@jaimin-k/exploring-kitti-visual-ododmetry-dataset-8ac588246cdc</li>
<li>KITTI Vision Benchmark Suite - Registry of Open Data on AWS, https://registry.opendata.aws/kitti/</li>
<li>V-LOAM - The KITTI Vision Benchmark Suite, https://www.cvlibs.net/datasets/kitti/eval_odometry_detail.php?&amp;result=5f6b5be438aea2372a7e0bd958ef381f96856045</li>
<li>The EuRoC MAV Datasets - ResearchGate, https://www.researchgate.net/profile/Michael_Burri2/publication/291954561_The_EuRoC_micro_aerial_vehicle_datasets/links/56af0c6008ae19a38516937c.pdf</li>
<li>kmavvisualinertialdatasets – ASL Datasets, https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets</li>
<li>TUM RGB-D SLAM Dataset and Benchmark - COVE - Computer Vision Exchange, https://cove.thecvf.com/datasets/867</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>