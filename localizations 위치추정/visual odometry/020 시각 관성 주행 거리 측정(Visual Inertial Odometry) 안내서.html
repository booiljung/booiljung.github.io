<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:시각 관성 주행 거리 측정(Visual Inertial Odometry) 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>시각 관성 주행 거리 측정(Visual Inertial Odometry) 안내서</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">위치 추정 (Localization)</a> / <a href="index.html">Visual Odemtry</a> / <span>시각 관성 주행 거리 측정(Visual Inertial Odometry) 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>시각 관성 주행 거리 측정(Visual Inertial Odometry) 안내서</h1>
<h2>1. 서론: 시각 관성 주행 거리 측정의 이해</h2>
<h3>1.1 VIO의 정의 및 핵심 목표</h3>
<p>시각 관성 주행 거리 측정(Visual Inertial Odometry, VIO)은 하나 이상의 카메라와 관성 측정 장치(Inertial Measurement Unit, IMU)로부터 얻은 데이터를 융합하여, 로봇이나 드론과 같은 에이전트의 상태(state), 즉 3차원 공간에서의 자세(pose)와 속도(velocity)를 추정하는 일련의 과정을 의미한다.1 이 기술의 궁극적인 목표는 GPS(Global Positioning System)와 같은 외부 위치 결정 시스템에 의존하지 않거나, 해당 신호가 불안정하거나 수신 불가능한 환경에서도 정확하고 강건한 6자유도(6-Degrees of Freedom, 6-DoF) 모션 트래킹을 실시간으로 제공하는 것이다.3</p>
<p>VIO는 단순히 에이전트의 위치를 파악하는 것을 넘어, 로컬 시작점(local starting position)을 기준으로 한 상대적인 3D 자세(위치 및 방향)와 속도를 정밀하게 추정한다.3 이는 시간이 지남에 따라 필연적으로 발생하는 누적 오차(drift)를 최소화하면서, 외부의 도움 없이 스스로의 움직임을 지속적으로 추적하는 ’주행 거리 측정(odometry)’의 한 형태이다.4 따라서 VIO 시스템의 성능은 정확성(accuracy), 강건성(robustness), 그리고 실시간성(real-time performance)이라는 세 가지 핵심 지표로 평가된다.</p>
<h3>1.2 카메라와 IMU의 상보적 관계: 왜 두 센서를 결합하는가?</h3>
<p>VIO의 핵심 철학은 서로 다른 물리적 원리를 기반으로 동작하는 두 센서, 즉 카메라와 IMU의 장단점을 상호 보완하여 시너지를 창출하는 데 있다. 어느 한 센서만으로는 강건한 상태 추정이 어려운 상황을 두 센서의 융합을 통해 극복한다.</p>
<p>카메라의 강점과 약점</p>
<p>카메라는 인간의 눈처럼 주변 환경에 대한 풍부하고 직관적인 시각 정보를 제공한다. 이미지 내의 특징점(feature)이나 픽셀 밝기 패턴을 추적함으로써, 드리프트가 비교적 적은 정확한 위치 추정이 가능하다.6 그러나 카메라는 본질적인 한계를 지닌다. 우선, 주변 조명 조건의 변화에 매우 민감하여 급격한 밝기 변화나 어두운 환경에서는 성능이 급격히 저하된다.1 또한, 벽이나 복도와 같이 특징이 없는(texture-less) 환경에서는 추적할 시각적 단서가 부족하여 위치를 잃기 쉽다.4 가장 큰 문제 중 하나는 빠른 움직임이 발생할 때 이미지에 모션 블러(motion blur)가 생겨 특징점 추적이 실패하는 것이다.3 더불어, 단 하나의 카메라(monocular camera)만을 사용하는 경우, 관측된 장면의 실제 크기, 즉 절대적인 스케일(absolute scale)을 알 수 없는 ’스케일 모호성(scale ambiguity)’이라는 근본적인 문제를 내포한다.5</p>
<p>IMU의 강점과 약점</p>
<p>IMU는 내장된 가속도계와 자이로스코프를 통해 에이전트의 선형 가속도와 각속도를 매우 높은 주파수(수백~수천 Hz)로 측정한다.4 이는 카메라가 처리하기 어려운 매우 빠른 움직임이나 급격한 회전 상황에서도 안정적으로 모션을 측정할 수 있음을 의미한다. 그러나 IMU는 치명적인 약점을 가지고 있다. 측정값에 필연적으로 노이즈(noise)와 바이어스(bias)라는 오차가 포함되어 있는데, 이 측정값을 적분하여 자세와 속도, 위치를 계산하는 과정에서 오차가 시간에 따라 기하급수적으로 누적된다.7 특히 위치는 가속도를 두 번 적분해야 하므로, 작은 바이어스 오차라도 시간이 흐르면 엄청난 위치 오차로 증폭되어 심각한 드리프트(drift) 현상을 유발한다.11</p>
<p>융합의 시너지: 상호 보완을 통한 강건성 확보</p>
<p>VIO는 이 두 센서의 장점을 결합하고 단점을 상쇄하는 방식으로 동작한다. IMU는 카메라가 모션 블러나 텍스처 부족으로 인해 제 기능을 못하는 짧은 구간 동안 높은 주파수의 측정값을 기반으로 상태를 예측(propagate)하여 추적의 연속성을 보장한다.3 반대로, 카메라는 주변 환경의 정적인 특징점들을 관측함으로써 IMU의 누적 오차를 주기적으로 보정하고 드리프트를 억제하는 역할을 한다. 특히, IMU의 가속도계는 중력 벡터를 측정할 수 있는데, 이는 지구를 기준으로 한 절대적인 방향 정보를 제공하여 시스템의 롤(roll) 및 피치(pitch) 각도를 관측 가능하게 만들고, 단안 카메라의 스케일 모호성 문제를 해결하는 데 결정적인 단서를 제공한다.5 이처럼 VIO는 단순히 두 센서의 정보를 더하는 것이 아니라, 각 센서가 다른 센서의 약점을 보완하는 유기적인 융합을 통해 개별 센서만으로는 달성할 수 없는 수준의 정확성과 강건성을 확보하는 기술이다.</p>
<h3>1.3 VIO, VO, SLAM의 관계 및 GPS/LiDAR 대비 VIO의 장점</h3>
<p>VIO는 로봇 공학 및 컴퓨터 비전 분야의 다른 위치 추정 기술들과 밀접한 관련을 맺고 있다.</p>
<ul>
<li>
<p><strong>VO (Visual Odometry)와의 관계:</strong> VO는 카메라 이미지만을 사용하여 모션을 추정하는 기술로, VIO의 핵심 구성 요소 중 하나이다.6 VIO는 VO의 추정치에 IMU 정보를 통합하여 강건성과 정확도를 대폭 향상시킨 확장된 개념으로 볼 수 있다.6</p>
</li>
<li>
<p><strong>SLAM (Simultaneous Localization and Mapping)과의 관계:</strong> VIO는 종종 시각 기반 SLAM(vSLAM) 시스템의 프론트엔드(front-end), 즉 실시간 모션 추적을 담당하는 부분으로 활용된다.8 VIO가 주로 현재의 자세를 추정하는 데 집중하는 반면, SLAM은 장기적인 관점에서 일관성 있는 지도를 작성하고(mapping), 이전에 방문했던 장소를 재인식하여(loop closure) 누적된 오차를 보정하며, 필요시 지도 내에서 자신의 위치를 다시 찾는(relocalization) 기능까지 포함하는 더 포괄적인 개념이다.13</p>
</li>
<li>
<p><strong>GPS/LiDAR 대비 장점:</strong> VIO는 기존의 주력 위치 추정 기술인 GPS나 LiDAR에 비해 여러 가지 뚜렷한 장점을 가진다. 가장 큰 장점은 GPS 신호가 도달하지 않는 실내, 지하, 터널, 수중, 고층 빌딩 숲과 같은 ’GPS 음영 지역(GNSS-denied environments)’에서 독자적으로 작동할 수 있다는 점이다.1 또한, 카메라와 IMU는 LiDAR 센서나 고정밀 GPS 수신기에 비해 가격이 매우 저렴하고, 크기가 작고 가벼우며, 전력 소모가 적다.1 이러한 특성 덕분에 VIO는 무게와 배터리에 제약이 큰 소형 드론, 스마트폰, AR/VR 헤드셋 등 다양한 플랫폼에 쉽게 탑재될 수 있다.</p>
</li>
</ul>
<table><thead><tr><th>기술 (Technology)</th><th>원리 (Principle)</th><th>장점 (Pros)</th><th>단점 (Cons)</th><th>주요 응용 분야 (Applications)</th></tr></thead><tbody>
<tr><td><strong>VIO</strong></td><td>카메라 + IMU 융합</td><td>저비용, 저전력, 소형, GPS 음영 지역 동작</td><td>시간에 따른 오차 누적(드리프트)</td><td>드론, AR/VR, 실내 로봇</td></tr>
<tr><td><strong>VO</strong></td><td>카메라 이미지 시퀀스</td><td>저비용, 환경 텍스처 정보 활용</td><td>스케일 모호성(단안), 조명/속도에 민감</td><td>로봇 청소기, 저속 모바일 로봇</td></tr>
<tr><td><strong>GPS</strong></td><td>위성 신호 삼각 측량</td><td>전역 절대 위치 제공, 드리프트 없음</td><td>실내/빌딩 숲 사용 불가, 낮은 업데이트 주기</td><td>실외 내비게이션, 차량 추적</td></tr>
<tr><td><strong>LiDAR Odometry</strong></td><td>레이저 스캔 정합</td><td>고정밀, 조명 불변</td><td>고비용, 고전력, 악천후(안개, 비)에 취약</td><td>자율주행차, 고정밀 3D 맵핑</td></tr>
</tbody></table>
<h3>1.4 주요 응용 분야</h3>
<p>VIO 기술의 고유한 장점들은 다양한 첨단 기술 분야에서 핵심적인 역할을 수행하도록 이끌었다.</p>
<ul>
<li>
<p><strong>자율 드론 및 로보틱스:</strong> GPS 없이도 복잡한 실내외 환경을 안정적으로 비행하고 임무를 수행하는 자율 드론 및 로봇의 핵심 내비게이션 기술로 사용된다.1 특히, 교량 하부 점검, 실내 시설물 검사, 재난 현장 탐사 등 GPS 사용이 불가능한 시나리오에서 필수적이다.4 Skydio 사의 자율 비행 드론은 VIO 기술이 성공적으로 상용화된 대표적인 사례다.2</p>
</li>
<li>
<p><strong>증강현실(AR) 및 가상현실(VR):</strong> 사용자의 머리 움직임을 지연 없이 정밀하게 추적하여, 가상의 객체가 현실 세계에 안정적으로 고정되어 있는 것처럼 보이게 하는 데 결정적인 역할을 한다.4 AR 안경이나 모바일 AR 애플리케이션의 몰입감과 현실감을 높이는 기반 기술이다.</p>
</li>
<li>
<p><strong>자율 주행:</strong> 도심의 터널, 지하 주차장, 고층 빌딩 사이 등 GPS 신호가 약하거나 끊기는 구간에서 차량의 위치를 끊김 없이 정밀하게 추적하여 자율 주행 시스템의 안정성을 높인다.4</p>
</li>
<li>
<p><strong>3D 모델링 및 인프라 검사:</strong> GPS의 도움 없이도 드론이나 로봇을 이용해 건물, 교량, 터널과 같은 대규모 구조물의 정밀한 3D 모델을 생성하고, 시간에 따른 변화나 손상을 감지하는 검사 작업에 활용된다.4</p>
</li>
</ul>
<h2>2. 핵심 구성 요소 1: 시각 주행 거리 측정 (Core Component 1: Visual Odometry)</h2>
<p>시각 주행 거리 측정(VO)은 VIO 시스템의 ’눈’에 해당하는 부분으로, 연속적인 카메라 이미지로부터 카메라 자체의 움직임, 즉 자세(pose) 변화를 추정하는 기술이다.6 이는 VIO의 기반을 이루는 핵심적인 구성 요소다.</p>
<h3>2.1 VO의 기본 원리: 특징점 기반 방식 vs. 직접 방식</h3>
<p>VO 알고리즘은 이미지 정보를 처리하는 방식에 따라 크게 두 가지, 그리고 이 둘을 절충한 하이브리드 방식으로 나뉜다.</p>
<ul>
<li><strong>특징점 기반(Feature-based) 방식:</strong> 이 방식은 인간의 시각 인지 과정과 유사하게, 이미지에서 눈에 띄는 부분, 즉 ‘특징점’(예: 코너, 모서리)을 먼저 찾고 이를 추적하는 2단계 접근법을 사용한다.6</li>
</ul>
<ol>
<li>
<p><strong>특징점 추출 및 기술(Detection &amp; Description):</strong> Harris, Shi-Tomasi, SIFT, ORB와 같은 알고리즘을 사용하여 이미지 내에서 안정적으로 추적할 수 있는 특징점을 찾고, 각 특징점 주변의 픽셀 패턴을 요약한 ’기술자(descriptor)’를 생성한다.</p>
</li>
<li>
<p><strong>특징점 매칭 및 추적(Matching &amp; Tracking):</strong> 연속된 프레임 간에 기술자의 유사도를 비교하여 동일한 3D 지점에 해당하는 특징점 쌍을 찾는다.</p>
</li>
<li>
<p>모션 추정(Motion Estimation): 매칭된 특징점 쌍들의 2D 이미지 상의 움직임(optical flow)을 분석하고, 에피폴라 기하학(epipolar geometry)과 같은 원리를 이용하여 이 움직임을 유발한 카메라의 3D 회전과 (스케일 미정의) 평행 이동을 계산한다.</p>
</li>
</ol>
<p>이 방식은 조명 변화나 빠른 움직임에 상대적으로 강건하며, SLAM으로 확장하여 장기적인 맵을 구축하기에 용이하다는 장점이 있다.16 그러나 특징점 추출 및 매칭 과정에서 상당한 계산 비용이 소요된다는 단점이 있다.17</p>
<ul>
<li><strong>직접(Direct) 방식:</strong> 이 방식은 특징점이라는 중간 단계를 거치지 않고, 이미지의 픽셀 밝기(intensity) 값 자체를 직접 사용하여 카메라의 움직임을 추정한다.6</li>
</ul>
<ol>
<li>
<p><strong>광도 일관성 가정(Photometric Consistency Assumption):</strong> 3D 공간의 한 점은 어느 각도에서 보더라도 그 밝기가 일정하게 유지된다는 가정을 기반으로 한다.</p>
</li>
<li>
<p><strong>오차 최소화(Error Minimization):</strong> 한 프레임의 픽셀들을 추정된 카메라 모션에 따라 다른 프레임으로 투영했을 때, 두 픽셀의 밝기 값 차이, 즉 ’광도 오차(photometric error)’가 최소가 되는 카메라 모션을 찾는다.17</p>
</li>
</ol>
<p>직접 방식은 텍스처가 부족한 환경에서도 픽셀 기울기(gradient)만 있다면 작동할 수 있으며, 특징점 추출 과정이 없어 계산적으로 더 효율적일 수 있다.17 하지만 광도 일관성 가정이 깨지는 급격한 조명 변화나 카메라의 자동 노출 기능에 매우 취약하며, 수렴 반경이 좁아 큰 움직임을 추정하기 위해서는 좋은 초기 추정값이 필요하다는 단점이 있다.17</p>
<ul>
<li><strong>하이브리드/준직접(Semi-direct) 방식:</strong> 두 방식의 장점을 결합하려는 시도도 있다. 대표적인 예로 SVO(Semi-direct Visual Odometry)는, 특징점을 추출하되 기술자를 사용한 매칭 대신 그 주변 픽셀들의 밝기 정보를 직접 방식처럼 사용하여 모션을 추정함으로써 속도와 강건성의 균형을 맞춘다.1</li>
</ul>
<h3>2.2 카메라 모델: 핀홀 카메라와 투영 기하학</h3>
<p>VO의 수학적 기반을 이해하기 위해서는 3차원 공간의 점이 2차원 이미지 평면에 어떻게 맺히는지를 설명하는 카메라 모델이 필수적이다. 가장 널리 사용되는 모델은 핀홀 카메라 모델(pinhole camera model)이다.18</p>
<p>이 모델에 따르면, 월드 좌표계에 있는 3D 점 <span class="math math-inline">\mathbf{X}_w =^T</span>는 카메라의 투영 과정을 거쳐 이미지 평면 위의 2D 픽셀 좌표 <span class="math math-inline">\mathbf{x}_p = [u, v]^T</span>로 변환된다. 이 복잡한 3D-2D 변환 과정은 동차 좌표계(homogeneous coordinates)와 행렬 곱셈을 사용하여 다음과 같은 간결한 선형 방정식으로 표현할 수 있다.</p>
<p><span class="math math-display">
\lambda \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} = \mathbf{K} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = \mathbf{P} \mathbf{X}_w
</span><br />
이 방정식의 각 요소는 다음과 같은 의미를 가진다.</p>
<ul>
<li>
<p><span class="math math-inline">\mathbf{P}</span>: 3×4 크기의 카메라 투영 행렬(projection matrix)이다.</p>
</li>
<li>
<p><span class="math math-inline">\lambda</span>: 점의 깊이(depth)에 비례하는 스케일 팩터이다.</p>
</li>
<li>
<p><span class="math math-inline">\mathbf{K}</span>: 카메라 내부 파라미터 행렬(intrinsic matrix)로, 카메라 렌즈와 센서의 고유한 광학적 특성을 담고 있다. 초점 거리(<span class="math math-inline">f_x, f_y</span>), 주점(principal point, <span class="math math-inline">c_x, c_y</span>), 비대칭 계수(skew, <span class="math math-inline">s</span>)로 구성된다.19</p>
<p><span class="math math-display">
\mathbf{K} = \begin{bmatrix} f_x &amp; s &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
</span></p>
</li>
<li>
<p>$ [\mathbf R|t]$: 카메라 외부 파라미터 행렬(extrinsic matrix)로, 월드 좌표계에 대한 카메라의 상대적인 자세를 나타낸다. $ \mathbf{R} $은 3×3 회전 행렬(rotation matrix)이고, $ \mathbf{t} $는 3×1 평행 이동 벡터(translation vector)이다.19<br />
<span class="math math-display">
[\mathbf R|t] = \begin{bmatrix} r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\ r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\ r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \end{bmatrix}
</span><br />
VO와 VIO의 궁극적인 목표는 매시간 변화하는 바로 이 외부 파라미터, 즉 카메라의 자세 $$를 추정하는 것이다.</p>
</li>
</ul>
<h3>단안(Monocular) VO의 본질적 한계: 스케일 모호성(Scale Ambiguity)</h3>
<p>단 하나의 카메라를 사용하는 단안(monocular) VO 시스템은 인간의 한쪽 눈과 마찬가지로, 단일 이미지로부터 깊이 정보를 직접적으로 얻을 수 없다는 근본적인 한계를 가진다.5 이로 인해 카메라의 움직임과 장면의 구조를 실제 크기가 아닌, 상대적인 비율까지만 복원할 수 있는 ‘스케일 모호성’ 문제가 발생한다.7</p>
<p>예를 들어, 카메라가 1 m 떨어진 곳에서 10 cm를 움직이는 상황과, 10 m 떨어진 곳에서 1 m를 움직이는 상황은 이미지 상에서 동일한 시각적 변화를 만들어낸다.5 VO 알고리즘은 이 두 상황을 구분할 수 없으며, 따라서 추정된 궤적의 절대적인 크기(미터 단위)를 결정할 수 없다. 이 문제는 시간이 지남에 따라 추정된 스케일에 오차가 누적되는 ‘스케일 드리프트(scale drift)’ 현상으로 이어져, 궤적의 형태는 정확하더라도 그 크기가 실제와는 완전히 달라지는 결과를 초래할 수 있다.9</p>
<h3>스케일 모호성 해결 방안</h3>
<p>이러한 단안 VO의 근본적인 한계를 극복하기 위해 여러 가지 방법이 제안되었다.</p>
<ul>
<li>
<p><strong>IMU 융합 (VIO):</strong> 가장 효과적이고 널리 사용되는 해결책이다. IMU의 가속도계는 중력을 포함한 실제 물리적 가속도를 미터/초^2(<span class="math math-inline">m/s^2</span>) 단위로 측정한다. 이 절대적인 물리량을 시각 정보와 결합함으로써, 시스템은 전체 궤적의 절대 스케일을 결정할 수 있게 된다.5 이는 VIO가 VO에 비해 갖는 가장 중요한 장점 중 하나이다.</p>
</li>
<li>
<p><strong>스테레오/RGB-D 카메라 사용:</strong> 두 개의 카메라(스테레오)를 사용하면, 좌우 이미지 간의 시차(disparity)를 이용한 삼각 측량(triangulation)을 통해 직접적으로 3D 점의 깊이를 계산할 수 있다.9 RGB-D 카메라는 적외선 패턴 등을 이용하여 직접 깊이 맵을 제공하므로 스케일 문제가 원천적으로 발생하지 않는다.5</p>
</li>
<li>
<p><strong>환경에 대한 가정:</strong> 차량 기반 VO의 경우, 카메라의 지상고가 일정하다고 가정하거나, 주행하는 도로가 평면이라고 가정하여 스케일을 추정하는 연구도 있다.9</p>
</li>
<li>
<p><strong>알려진 크기의 물체 활용:</strong> 장면 내에 실제 크기를 알고 있는 물체(예: 특정 패턴의 마커)가 있다면, 이를 기준으로 전체 스케일을 결정할 수 있다.</p>
</li>
<li>
<p><strong>루프 폐쇄 (SLAM):</strong> SLAM 시스템에서는 이전에 방문했던 장소를 재인식하고 루프를 닫는 과정에서 전체 궤적의 기하학적 일관성을 최적화하며 누적된 스케일 드리프트를 보정할 수 있다.5</p>
</li>
</ul>
<h2>핵심 구성 요소 2: 관성 측정 장치 (Core Component 2: Inertial Measurement Unit)</h2>
<p>관성 측정 장치(IMU)는 VIO 시스템의 ’전정 기관’에 해당하는 부분으로, 카메라가 제공할 수 없는 동적인 모션 정보를 제공하여 시스템의 강건성과 정확성을 비약적으로 향상시킨다.</p>
<h3>IMU의 작동 원리: 가속도계와 자이로스코프</h3>
<p>IMU는 일반적으로 서로 직교하는 세 축에 대해 각각 선형 가속도와 각속도를 측정하는 센서들로 구성된다.</p>
<ul>
<li>
<p><strong>가속도계(Accelerometer):</strong> 이 센서는 뉴턴의 운동 법칙에 기반하여 물체의 ’고유 가속도(proper acceleration)’를 측정한다.22 이는 관성 좌표계에 대한 좌표 가속도와는 다른 개념으로, 중력을 포함한 모든 비관성력(non-gravitational forces)의 합, 즉 ’비력(specific force)’을 측정하는 것이다. 따라서 자유 낙하하는 가속도계는 0을 측정하는 반면, 지표면에 정지해 있는 가속도계는 중력에 저항하여 자신을 지탱하는 지면의 수직 항력을 측정하게 되며, 이는 중력 가속도 <span class="math math-inline">g</span> (약 9.8m/s2)가 위쪽 방향으로 작용하는 것으로 나타난다. 즉, 가속도계의 측정값 <span class="math math-inline">\tilde{\mathbf{a}}</span>는 IMU의 몸체(body) 좌표계에서 다음과 같이 모델링된다.13<br />
<span class="math math-display">
\tilde{\mathbf{a}}_b = \mathbf{R}_{bw}(\mathbf{a}_w - \mathbf{g}_w) + \mathbf{b}_a + \mathbf{n}_a
</span><br />
여기서 <span class="math math-inline">\mathbf{a}_w</span>는 월드 좌표계에서의 실제 가속도, <span class="math math-inline">\mathbf{g}_w</span>는 월드 좌표계에서의 중력 벡터, <span class="math math-inline">\mathbf{R}_{bw}</span>는 월드에서 바디 좌표계로의 회전 행렬, <span class="math math-inline">\mathbf{b}_a</span>와 <span class="math math-inline">\mathbf{n}_a</span>는 각각 바이어스와 노이즈다. 이 중력 항의 존재가 VIO에서 매우 중요한 역할을 한다.</p>
</li>
<li>
<p><strong>자이로스코프(Gyroscope):</strong> 이 센서는 코리올리 효과(Coriolis effect)와 같은 물리적 원리를 이용하여 센서의 몸체 좌표계에 대한 3축 각속도 <span class="math math-inline">\boldsymbol{\omega}_b</span>를 측정한다.4 측정값 <span class="math math-inline">\tilde{\boldsymbol{\omega}}_b</span>는 다음과 같이 모델링된다.<br />
<span class="math math-display">
\tilde{\boldsymbol{\omega}}_b = \boldsymbol{\omega}_b + \mathbf{b}_g + \mathbf{n}_g
</span><br />
여기서 <span class="math math-inline">\mathbf{b}_g</span>와 <span class="math math-inline">\mathbf{n}_g</span>는 각각 자이로스코프의 바이어스와 노이즈다.</p>
</li>
</ul>
<h3>IMU 운동학 모델 (IMU Kinematics Model)</h3>
<p>IMU 측정값을 사용하여 시간에 따른 시스템의 상태, 즉 자세(orientation), 속도(velocity), 위치(position)가 어떻게 변화하는지를 수학적으로 기술하는 것이 운동학 모델이다.</p>
<ul>
<li>
<p><strong>연속 시간 모델 (Continuous-time Model):</strong> 월드 좌표계를 기준으로 한 IMU의 동역학은 다음과 같은 미분 방정식으로 표현된다.24</p>
<ul>
<li>
<p><strong>위치:</strong> <span class="math math-inline">\dot{\mathbf{p}}_w(t) = \mathbf{v}_w(t)</span></p>
</li>
<li>
<p><strong>속도:</strong> <span class="math math-inline">\dot{\mathbf{v}}_w(t) = \mathbf{a}_w(t) = \mathbf{R}_{wb}(t)(\tilde{\mathbf{a}}_b(t) - \mathbf{b}_a(t) - \mathbf{n}_a(t)) + \mathbf{g}_w</span></p>
</li>
<li>
<p><strong>자세 (회전 행렬):</strong> <span class="math math-inline">\dot{\mathbf{R}}_{wb}(t) = \mathbf{R}_{wb}(t) \lfloor \tilde{\boldsymbol{\omega}}_b(t) - \mathbf{b}_g(t) - \mathbf{n}_g(t) \rfloor_{\times}</span></p>
</li>
<li>
<p><strong>자세 (쿼터니언):</strong> <span class="math math-inline">\dot{\mathbf{q}}_{wb}(t) = \frac{1}{2} \mathbf{q}_{wb}(t) \otimes \begin{bmatrix} 0 \\ \tilde{\boldsymbol{\omega}}_b(t) - \mathbf{b}_g(t) - \mathbf{n}_g(t) \end{bmatrix}</span></p>
</li>
</ul>
<p>여기서 <span class="math math-inline">\mathbf{R}_{wb}</span>와 <span class="math math-inline">\mathbf{q}_{wb}</span>는 월드 좌표계에 대한 바디 좌표계의 자세를 나타내는 회전 행렬과 쿼터니언이며, <span class="math math-inline">\lfloor \cdot \rfloor_{\times}</span>는 벡터를 반대칭 행렬(skew-symmetric matrix)로 변환하는 연산자, <span class="math math-inline">\otimes</span>는 쿼터니언 곱셈을 의미한다. VIO 시스템은 이 미분 방정식을 수치적으로 적분하여 IMU 측정값으로부터 상태를 예측한다.</p>
</li>
</ul>
<h3>IMU 오차 모델: 바이어스(Bias)와 노이즈(Noise)의 수학적 모델링</h3>
<p>실제 IMU 측정값은 참값에 여러 오차 요인이 더해진 형태로 나타나며, VIO의 성능은 이러한 오차를 얼마나 정확하게 모델링하고 보상하느냐에 크게 좌우된다.12 IMU의 확률적 오차(stochastic errors)는 주로 백색 소음과 바이어스로 나뉜다.</p>
<ul>
<li>
<p><strong>백색 가우시안 노이즈 (White Gaussian Noise):</strong> 이는 센서의 단기적인(short-term) 랜덤 오차로, 열 잡음이나 기계적 진동 등에 의해 발생한다. 수학적으로는 평균이 0이고 특정 분산을 갖는 가우시안 분포를 따르며, 시간적으로 상관관계가 없는(uncorrelated) 신호로 모델링된다.27 이 노이즈를 적분하면 그 오차가 랜덤 워크(random walk) 형태로 누적되는데, 이 때문에 자이로스코프 노이즈는 ‘각속도 랜덤 워크(Angular Random Walk, ARW)’, 가속도계 노이즈는 ’속도 랜덤 워크(Velocity Random Walk, VRW)’라는 지표로 표현되기도 한다.29 이 값은 IMU 데이터시트에서 ’노이즈 밀도(Noise Density)’로 주어진다.</p>
</li>
<li>
<p><strong>바이어스 (Bias):</strong> 이는 측정값의 장기적인(long-term) 드리프트를 유발하는 주된 원인으로, 센서 출력에 더해지는 느리게 변하는 오프셋(offset)이다.</p>
<ul>
<li><strong>바이어스 불안정성 (Bias Instability):</strong> 시간에 따라 천천히 변동하는 바이어스 성분으로, 온도 변화나 내부 부품의 스트레스 등으로 인해 발생한다.26 VIO에서는 이 변화를 추적하기 위해 바이어스를 ‘랜덤 워크(Random Walk)’ 프로세스로 모델링하는 것이 일반적이다.26 즉, 바이어스의 시간 변화율(<span class="math math-inline">\dot{\mathbf{b}}(t)</span>)이 백색 가우시안 노이즈(<span class="math math-inline">\mathbf{n}_{bw}(t)</span>)라고 가정한다.<br />
<span class="math math-display">
\dot{\mathbf{b}}(t) = \mathbf{n}_{bw}(t), \quad \mathbf{n}_{bw} \sim \mathcal{N}(0, \sigma_{bw}^2 \mathbf{I})
</span><br />
이 모델 덕분에 바이어스를 시스템의 상태 변수(state variable)에 포함시켜 칼만 필터나 최적화 프레임워크를 통해 온라인으로 실시간 추정 및 보상이 가능해진다.</li>
</ul>
</li>
</ul>
<p>이러한 오차 모델링은 VIO 시스템의 근간을 이룬다. 특히 가속도를 두 번 적분하여 위치를 얻는 과정은 오차를 증폭시키는 경향이 매우 크다. 가속도계에 작은 상수 바이어스 <span class="math math-inline">\mathbf{b}_a</span>가 존재한다고 가정하면, 이를 두 번 적분했을 때 위치 오차는 <span class="math math-inline">\frac{1}{2}\mathbf{b}_a t^2</span>에 비례하여 시간에 따라 이차 함수적으로(quadratically) 증가한다. 이것이 바로 IMU 단독 항법이 장시간 동안 불가능한 근본적인 이유이며, 주기적인 외부 관측(카메라 등)을 통한 오차 보정이 필수적인 이유다. VIO에서 카메라는 바로 이 폭발적으로 증가하는 관성 오차를 바로잡는 ‘닻(anchor)’ 역할을 수행한다.</p>
<table><thead><tr><th>파라미터 (Parameter)</th><th>기호 (Symbol)</th><th>물리적 의미 (Physical Meaning)</th><th>일반적인 단위 (Common Units)</th><th>수학적 모델 (Mathematical Model)</th></tr></thead><tbody>
<tr><td>각속도 랜덤 워크 (Angular Random Walk)</td><td><span class="math math-inline">\mathbf{n}_g</span></td><td>자이로스코프의 단기적 백색 소음</td><td><code>rad/s/$\sqrt{Hz}$ or </code>deg/<span class="math math-inline">\sqrt{hr}</span></td><td><span class="math math-inline">\mathbf{n}_g \sim \mathcal{N}(0, \sigma_g^2 \mathbf{I})</span></td></tr>
<tr><td>속도 랜덤 워크 (Velocity Random Walk)</td><td><span class="math math-inline">\mathbf{n}_a</span></td><td>가속도계의 단기적 백색 소음</td><td><code>m/s$^2$/$\sqrt{Hz}$ or </code>g/<span class="math math-inline">\sqrt{Hz}</span></td><td><span class="math math-inline">\mathbf{n}_a \sim \mathcal{N}(0, \sigma_a^2 \mathbf{I})</span></td></tr>
<tr><td>자이로 바이어스 불안정성 (Gyro Bias Instability)</td><td><span class="math math-inline">\mathbf{b}_g</span></td><td>자이로스코프의 장기적, 느리게 변하는 바이어스</td><td><code>rad/s</code> or <code>deg/hr</code></td><td><span class="math math-inline">\dot{\mathbf{b}}_g(t) = \mathbf{n}_{bg}(t), \quad \mathbf{n}_{bg} \sim \mathcal{N}(0, \sigma_{bg}^2 \mathbf{I})</span></td></tr>
<tr><td>가속도계 바이어스 불안정성 (Accel Bias Instability)</td><td><span class="math math-inline">\mathbf{b}_a</span></td><td>가속도계의 장기적, 느리게 변하는 바이어스</td><td><code>m/s$^2$ or </code>mg`</td><td><span class="math math-inline">\dot{\mathbf{b}}_a(t) = \mathbf{n}_{ba}(t), \quad \mathbf{n}_{ba} \sim \mathcal{N}(0, \sigma_{ba}^2 \mathbf{I})</span></td></tr>
</tbody></table>
<h2>VIO 상태 추정 방법론 (VIO State Estimation Methodologies)</h2>
<p>VIO의 핵심은 시각 정보와 관성 정보를 효과적으로 융합하여 최적의 상태를 추정하는 백엔드(back-end) 알고리즘에 있다. 이 방법론은 크게 필터 기반 접근법과 최적화 기반 접근법으로 양분된다.17 두 방법론은 정보를 처리하고 불확실성을 관리하는 방식에서 근본적인 차이를 보이며, 이는 정확도, 계산 효율성, 강건성 측면에서 뚜렷한 장단점으로 나타난다.</p>
<h3>필터 기반 VIO (Filter-based VIO)</h3>
<p>필터 기반 VIO는 베이즈 필터(Bayesian filter), 특히 확장 칼만 필터(Extended Kalman Filter, EKF)를 기반으로 동작한다. 이 접근법의 핵심 철학은 시스템의 현재 상태(state)와 그 불확실성(covariance)만을 유지하며, 새로운 측정값이 들어올 때마다 이를 점진적으로 업데이트하는 것이다.17</p>
<ul>
<li>
<p><strong>원리:</strong> 필터는 두 단계, 즉 예측(prediction)과 보정(update)을 반복한다.</p>
<ol>
<li>
<p><strong>예측 단계:</strong> 높은 주파수의 IMU 측정값을 사용하여 IMU 운동학 모델에 따라 현재 상태를 다음 시간 단계의 상태로 전파(propagate)한다. 이 과정에서 상태의 불확실성도 함께 예측된다.</p>
</li>
<li>
<p><strong>보정 단계:</strong> 낮은 주파수의 카메라 이미지가 들어오면, 이미지에서 특징점을 관측하고 이를 현재 예측된 상태와 비교한다. 이 둘 사이의 오차(잔차, residual)를 이용하여 예측된 상태와 불확실성을 보정한다.</p>
</li>
</ol>
</li>
<li>
<p><strong>장점:</strong> 이 방식은 오직 현재 상태에 대한 정보만 유지하므로 계산량이 적고 메모리 요구량이 낮다. 따라서 연산 능력이 제한된 소형 드론이나 모바일 기기와 같은 임베디드 시스템에 적용하기에 매우 유리하다.14</p>
</li>
<li>
<p><strong>단점:</strong> 필터 기반 방식의 가장 큰 약점은 ’선형화 오차(linearization error)’에 있다. EKF는 비선형적인 운동학 모델과 관측 모델을 현재 상태 추정치를 중심으로 1차 테일러 급수 전개를 통해 선형화한다. 이 과정에서 발생하는 근사 오차는 한번 발생하면 필터의 상태에 영구적으로 “구워져(baked in)” 다시는 수정할 수 없다.34 이 오차가 시간에 따라 누적되면, 필터가 자신의 불확실성을 과소평가하게 되는 ‘비일관성(inconsistency)’ 문제로 이어져 결국 추정 성능이 저하되거나 발산할 수 있다.14 대표적인 필터 기반 알고리즘인 MSCKF(Multi-State Constraint Kalman Filter)는 특징점의 3D 위치를 상태 벡터에서 명시적으로 제거하여 계산 복잡도를 크게 줄였지만, 여전히 선형화 오차 문제로부터 자유롭지 못하다.14</p>
</li>
</ul>
<h3>최적화 기반 VIO (Optimization-based VIO)</h3>
<p>최적화 기반 VIO는 필터와 달리, 현재뿐만 아니라 과거의 일정 기간 동안의 상태들을 모두 변수로 간주하고, 이 기간 동안 수집된 모든 측정값(IMU, 카메라)을 가장 잘 만족시키는 최적의 상태 궤적(trajectory)을 찾는 접근법이다.1</p>
<ul>
<li>
<p><strong>원리:</strong> 이 방식은 일반적으로 ‘슬라이딩 윈도우 최적화(sliding window optimization)’ 기법을 사용한다.</p>
<ol>
<li>
<p><strong>상태 변수 정의:</strong> 최근 N개의 카메라 프레임에 해당하는 자세, 속도, IMU 바이어스와 해당 프레임들에서 관측된 3D 특징점들의 위치를 모두 최적화할 상태 변수로 정의한다.</p>
</li>
<li>
<p><strong>비용 함수 구성:</strong> 비용 함수(cost function)는 두 종류의 오차 항으로 구성된다. 첫째는 IMU 측정값과 추정된 상태 궤적 사이의 불일치를 나타내는 ’IMU 오차 항’이고, 둘째는 3D 특징점을 각 카메라 자세로 재투영(re-project)했을 때의 이미지 상의 오차를 나타내는 ’시각 재투영 오차 항’이다.</p>
</li>
<li>
<p><strong>비선형 최적화:</strong> 이 비용 함수의 총합을 최소화하는 상태 변수들의 값을 비선형 최소 제곱법(non-linear least squares), 예를 들어 가우스-뉴턴(Gauss-Newton) 또는 레벤버그-마쿼트(Levenberg-Marquardt) 알고리즘을 사용하여 반복적으로 찾는다.</p>
</li>
</ol>
</li>
<li>
<p><strong>장점:</strong> 이 방식의 가장 큰 장점은 여러 프레임에 걸친 정보를 동시에 고려하여 전역적인 최적해를 찾는다는 점이다. 특히, 최적화 과정에서 필요하다면 과거 상태에 대한 선형화 지점을 현재의 더 나은 추정치를 기반으로 반복적으로 수정하는 ’재선형화(re-linearization)’가 가능하다.1 이 덕분에 선형화 오차의 누적을 효과적으로 억제할 수 있어, 필터 기반 방식보다 일반적으로 훨씬 높은 정확도를 달성한다.16</p>
</li>
<li>
<p><strong>단점:</strong> 슬라이딩 윈도우 내의 모든 상태와 측정값을 처리해야 하므로 계산 비용이 매우 높고 복잡하다.34 윈도우가 움직일 때 가장 오래된 상태를 제거하기 위해 ’마지널라이제이션(marginalization)’이라는 기법을 사용하는데, 이 과정 자체가 계산적으로 복잡하고 정보 손실을 유발하여 시스템의 희소성(sparsity)을 해칠 수 있다.34 VINS-Mono, OKVIS 등이 이 방식에 기반한 대표적인 알고리즘이다.32</p>
</li>
</ul>
<h3>방법론 비교 분석</h3>
<p>두 방법론의 선택은 적용하려는 시스템의 요구사항, 특히 가용한 컴퓨팅 자원과 요구되는 정확도 수준 사이의 균형에 따라 결정된다.</p>
<ul>
<li>
<p><strong>정확도:</strong> 재선형화의 이점 덕분에 일반적으로 최적화 기반 방법이 필터 기반 방법보다 우수한 정확도를 보인다.1</p>
</li>
<li>
<p><strong>계산 복잡도 및 효율성:</strong> 필터 기반 방법은 현재 상태만 다루기 때문에 계산적으로 훨씬 가볍고 효율적이다. 실시간 성능이 매우 중요한 저사양 임베디드 시스템에서는 필터 기반 접근법이 유일한 선택일 수 있다.14</p>
</li>
<li>
<p><strong>일관성:</strong> 필터는 한번 잘못된 선형화로 인해 발생한 오차를 되돌릴 수 없어 비일관성 문제에 더 취약하다. 반면, 최적화 방법은 윈도우 내에서 과거의 결정을 재평가하고 수정할 수 있는 능력이 있어 더 나은 일관성을 유지하는 경향이 있다.34</p>
</li>
<li>
<p><strong>정보 관리 방식의 근본적 차이:</strong> 이 둘의 차이는 단순히 정확도와 속도의 트레이드오프를 넘어선다. 필터는 마르코프 가정(Markov assumption)에 기반하여, 현재 상태가 과거의 모든 정보를 요약하고 있다고 믿는다. 이는 효율적이지만, 한번의 잘못된 결정이 시스템 전체를 오염시킬 수 있는 취약한 구조다. 반면, 최적화 방법은 윈도우 내에서는 비마르코프적(non-Markovian)으로, 과거 측정값들 간의 원본 제약 조건을 그대로 유지한다. 이는 새로운 정보가 들어왔을 때 과거의 실수를 바로잡을 수 있는 유연성과 강건성을 부여하지만, 그 대가로 높은 계산 비용을 치르게 된다.</p>
</li>
</ul>
<table><thead><tr><th>기준 (Criterion)</th><th>필터 기반 (Filter-based)</th><th>최적화 기반 (Optimization-based)</th></tr></thead><tbody>
<tr><td><strong>핵심 원리</strong></td><td>현재 상태만 추정 (확장 칼만 필터)</td><td>일정 시간 창 내의 모든 상태를 동시 최적화</td></tr>
<tr><td><strong>정확도</strong></td><td>상대적으로 낮음 (선형화 오차 누적)</td><td>상대적으로 높음 (재선형화 가능)</td></tr>
<tr><td><strong>계산 복잡도</strong></td><td>낮음 (실시간 임베디드 시스템에 유리)</td><td>높음 (고성능 컴퓨팅 자원 필요)</td></tr>
<tr><td><strong>메모리 사용량</strong></td><td>낮음 (현재 상태만 저장)</td><td>높음 (상태 및 측정값의 이력 저장)</td></tr>
<tr><td><strong>일관성</strong></td><td>선형화 오차로 인해 비일관성 문제 발생 가능</td><td>더 나은 일관성 유지</td></tr>
<tr><td><strong>대표 알고리즘</strong></td><td>MSCKF 14, ROVIO 32</td><td>VINS-Mono 32, OKVIS 35, PL-VIO 35</td></tr>
</tbody></table>
<h2>핵심 이론 심화: IMU 사전적분 (IMU Preintegration)</h2>
<p>최적화 기반 VIO의 높은 정확도에도 불구하고, 이를 실시간으로 구현하는 데에는 한 가지 큰 계산적 장애물이 존재했다. 바로 IMU 측정값을 처리하는 방식이다. IMU 사전적분(IMU preintegration)은 이 문제를 해결하고 현대 최적화 기반 VIO를 실용적으로 만든 핵심적인 이론적 돌파구다.</p>
<h3>사전적분의 필요성: 재선형화 시 반복 적분 문제 해결</h3>
<p>최적화 기반 VIO의 백엔드는 비용 함수를 최소화하기 위해 상태 변수(자세, 속도 등)를 반복적으로 업데이트한다. IMU 운동학 방정식은 현재의 자세 <span class="math math-inline">\mathbf{R}(t)</span>와 바이어스 <span class="math math-inline">\mathbf{b}(t)</span>에 직접적으로 의존한다. 문제는 이 자세와 바이어스가 바로 최적화 과정에서 계속해서 변하는 상태 변수라는 점이다.13</p>
<p>카메라 프레임은 보통 10-30 Hz로 들어오는 반면, IMU 데이터는 200-1000 Hz로 훨씬 더 높은 주파수로 수집된다. 이는 두 카메라 프레임 사이에 수십에서 수백 개의 IMU 측정값이 존재함을 의미한다. 만약 최적화의 각 반복 단계에서 상태 변수 추정치가 약간 업데이트될 때마다, 이 수많은 IMU 측정값들을 처음부터 다시 적분(re-integration)해야 한다면 엄청난 계산 부담이 발생하여 실시간 처리가 불가능해진다.1 사전적분은 바로 이 반복적인 재적분 문제를 피하기 위해 고안된 기법이다.</p>
<h3>이론 및 수학적 유도</h3>
<p>사전적분의 핵심 아이디어는 IMU 측정값들을 전역 월드 좌표계가 아닌, 이전 키프레임 <span class="math math-inline">k</span>의 몸체(body) 좌표계를 기준으로 미리 적분해두는 것이다. 이렇게 하면 두 키프레임 <span class="math math-inline">k</span>와 <span class="math math-inline">k+1</span> 사이의 상대적인 움직임을 나타내는 단일 제약(constraint)을 만들 수 있으며, 이 제약은 오직 두 프레임 사이의 IMU 측정값에만 의존하게 된다.36 즉, 월드 좌표계에 대한 키프레임</p>
<p><span class="math math-inline">k</span>의 자세가 최적화 과정에서 아무리 바뀌더라도 이 사전적분된 값은 재계산할 필요가 없다.</p>
<p>두 키프레임 <span class="math math-inline">b_k</span>와 <span class="math math-inline">b_{k+1}</span> 사이의 시간 간격 <span class="math math-inline">[t_k, t_{k+1}]</span> 동안의 IMU 측정값(<span class="math math-inline">\tilde{\mathbf{a}}_t, \tilde{\boldsymbol{\omega}}_t</span>)이 주어졌을 때, 사전적분된 측정값 <span class="math math-inline">\Delta \mathbf{R}_{k+1}^k, \Delta \mathbf{v}_{k+1}^k, \Delta \mathbf{p}_{k+1}^k</span>는 다음과 같이 점진적으로 계산된다. 이들은 <span class="math math-inline">b_k</span> 프레임을 기준으로 한 상대적인 회전, 속도 변화, 위치 변화를 나타낸다.</p>
<ul>
<li>
<p><strong>회전 사전적분:</strong><br />
<span class="math math-display">
\Delta \mathbf{R}_{t+\delta t}^k = \Delta \mathbf{R}_t^k \text{Exp}((\tilde{\boldsymbol{\omega}}_t - \mathbf{b}_g)\delta t)
</span></p>
</li>
<li>
<p><strong>속도 사전적분:</strong><br />
<span class="math math-display">
\Delta \mathbf{v}_{t+\delta t}^k = \Delta \mathbf{v}_t^k + \Delta \mathbf{R}_t^k (\tilde{\mathbf{a}}_t - \mathbf{b}_a)\delta t
</span></p>
</li>
<li>
<p><strong>위치 사전적분:</strong></p>
</li>
</ul>
<p><span class="math math-display">
\Delta \mathbf{p}_{t+\delta t}^k = \Delta \mathbf{p}_t^k + \Delta \mathbf{v}_t^k \delta t + \frac{1}{2}\Delta \mathbf{R}_t^k (\tilde{\mathbf{a}}_t - \mathbf{b}_a)\delta t^2
  </span><br />
여기서 <span class="math math-inline">\text{Exp}(\cdot)</span>는 회전 벡터를 SO(3) 상의 회전 행렬로 변환하는 지수 맵(exponential map)이다.</p>
<p>이렇게 계산된 사전적분 항들을 사용하여, 두 키프레임의 상태 변수들을 연결하는 IMU 오차(잔차) 항을 정의할 수 있다.38 예를 들어, 위치에 대한 잔차는 다음과 같이 표현된다.</p>
<p><span class="math math-display">
\mathbf{r}_{\mathbf{p}} = \mathbf{R}_{w}^{b_k} ( \mathbf{p}_{b_{k+1}}^w - \mathbf{p}_{b_k}^w - \mathbf{v}_{b_k}^w \Delta t_k + \frac{1}{2}\mathbf{g}^w \Delta t_k^2 ) - \Delta \mathbf{p}_{k+1}^k
</span><br />
이 잔차항은 이제 키프레임의 상태 변수와 사전적분된 측정값 사이의 불일치를 나타내며, 최적화 과정에서 최소화의 대상이 된다.</p>
<p>최적화 과정에서 IMU 바이어스 추정치 <span class="math math-inline">\mathbf{b}_a, \mathbf{b}_g</span>가 업데이트되면, 사전적분된 값을 처음부터 재계산하는 대신, 바이어스 변화가 사전적분 값에 미치는 영향을 1차 테일러 근사를 통해 선형적으로 보정해준다. 이는 계산 효율을 극대화한다.25</p>
<p>이러한 접근 방식은 고주파의 관성 동역학을 저주파의 기하학적 제약(카메라)과 분리하는 효과를 가져온다. 즉, 고주파 IMU 측정값들은 사전적분을 통해 두 키프레임 사이의 상대적인 움직임을 요약하는 단일 ’요소(factor)’로 압축된다. 최적화 시스템은 이제 이 압축된 정보만을 다루면 되므로, 전체 문제의 복잡성이 크게 줄어들어 실시간 처리가 가능해지는 것이다.</p>
<h3>불확실성 전파(Uncertainty Propagation)</h3>
<p>IMU 측정값에는 노이즈가 포함되어 있으므로, 이를 적분하는 과정에서 불확실성이 누적되고 커진다. 사전적분 이론은 이러한 불확실성이 어떻게 전파되는지를 수학적으로 엄밀하게 다룬다. 각 사전적분된 측정값(<span class="math math-inline">\Delta \mathbf{R}, \Delta \mathbf{v}, \Delta \mathbf{p}</span>)에 대한 공분산 행렬(covariance matrix)을 함께 계산한다. 이는 연속 시간 오차 상태 동역학(continuous-time error state dynamics)을 선형화하고 적분하여 유도된다.25</p>
<p>이렇게 계산된 공분산 행렬은 최적화 과정에서 각 IMU 제약의 신뢰도를 나타내는 정보 행렬(information matrix, 공분산의 역행렬)로 사용된다. 즉, 노이즈가 많이 누적되어 불확실성이 큰 사전적분 측정값은 최적화에 미치는 영향(가중치)이 작아지고, 반대로 짧은 시간 동안의 측정으로 불확실성이 작은 값은 더 큰 영향을 미치게 된다. 이는 전체 시스템의 정확성과 강건성을 높이는 데 결정적인 역할을 한다.</p>
<h2>실용적 구현 과제 (Practical Implementation Challenges)</h2>
<p>VIO 시스템을 이론에서 현실로 구현하기 위해서는 여러 가지 실용적인 문제들을 해결해야 한다. 이 중 시스템 초기화, 센서 동기화, 그리고 이상치 제거는 VIO 시스템의 성능과 안정성을 좌우하는 가장 중요한 과제들이다.</p>
<h3>시스템 초기화 (Initialization): 스케일, 중력, 속도, 바이어스 추정 절차</h3>
<p>VIO 시스템이 동작을 시작하기 전에, 상태 변수들에 대한 합리적인 초기값을 반드시 추정해야 한다. 이 초기화 과정의 성공 여부는 전체 시스템의 수렴과 안정성에 지대한 영향을 미친다. 잘못된 초기값은 추정기가 부정확한 값으로 수렴하거나 심지어 발산하는 원인이 될 수 있다.15 특히, AR/VR과 같이 사용자가 즉각적인 반응을 기대하는 응용 분야에서는 동적인 움직임 중에도 수백 밀리초 내에 빠르고 정확하게 초기화가 완료되어야 한다.15</p>
<ul>
<li>
<p><strong>초기화 목표:</strong> 초기화 단계의 핵심 목표는 다음과 같은 네 가지 주요 변수를 추정하는 것이다.40</p>
<ol>
<li>
<p><strong>시각적 스케일(Scale):</strong> 단안 VIO의 스케일 모호성을 해결하기 위한 절대적인 크기.</p>
</li>
<li>
<p><strong>중력 벡터(Gravity Vector):</strong> 월드 좌표계에서의 중력의 방향과 크기. 이는 시스템의 롤, 피치 각도를 결정하는 기준이 된다.</p>
</li>
<li>
<p><strong>초기 속도(Velocity):</strong> 시작 시점의 IMU 속도.</p>
</li>
<li>
<p><strong>IMU 바이어스(IMU Biases):</strong> 가속도계와 자이로스코프의 바이어스. 특히 자이로스코프 바이어스는 회전 추정에 큰 영향을 미치므로 정확한 추정이 중요하다.</p>
</li>
</ol>
</li>
<li>
<p><strong>초기화 절차 (VINS-Mono 기준):</strong> 많은 현대 VIO 시스템이 VINS-Mono에서 제안된 절차와 유사한 방식을 따른다. 이는 시각 정보만으로 상대적인 움직임을 먼저 복원한 뒤, 이를 관성 정보와 정렬하는 2단계 접근법이다.</p>
<ol>
<li>
<p><strong>시각 기반 Structure from Motion (SfM):</strong> 먼저, 짧은 시간(예: 2초) 동안의 카메라 이미지만을 사용하여 시각적 VO를 수행한다. 이를 통해 여러 카메라 프레임 간의 상대적인 자세와 3D 특징점들의 위치를 (실제 크기는 알 수 없는) 스케일 미정 상태로 복원한다.40 이 과정이 성공하기 위해서는 카메라가 순수 회전만 하지 않고 충분한 평행 이동을 하여 특징점들 간에 시차(parallax)가 발생해야 한다.41</p>
</li>
<li>
<p><strong>시각-관성 정렬 (Visual-Inertial Alignment):</strong> 다음으로, SfM을 통해 얻은 상대적인 시각적 움직임과, 같은 시간 동안 IMU 사전적분을 통해 얻은 상대적인 관성 움직임을 정렬하여 위에서 언급한 네 가지 핵심 변수를 추정한다.</p>
<ul>
<li>
<p><strong>자이로스코프 바이어스 추정:</strong> SfM으로 구한 프레임 간의 상대 회전과, IMU 사전적분으로 구한 상대 회전 사이의 차이는 주로 자이로스코프 바이어스에 의해 발생한다. 이 관계를 이용하여 여러 프레임에 걸쳐 선형 최소 제곱법(linear least squares)으로 바이어스를 추정할 수 있다.40</p>
</li>
<li>
<p><strong>속도, 중력, 스케일 추정:</strong> 자이로 바이어스가 보정된 IMU 사전적분 값(위치, 속도)과 SfM 결과(위치) 사이에는 스케일, 초기 속도, 중력 벡터에 대한 선형적인 관계가 성립한다. 이 관계를 여러 프레임에 대해 연립방정식으로 구성하고 풀면 이 세 변수의 초기값을 얻을 수 있다.40</p>
</li>
<li>
<p><strong>중력 벡터 정제:</strong> 중력 벡터의 크기는 약 9.8m/s2로 알려져 있으므로, 추정된 중력 벡터의 크기를 이 값에 맞추고 방향(2-DoF)만 다시 정밀하게 최적화하여 정확도를 높인다.40</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>이 초기화 과정은 VIO 문제 자체의 축소판과 같다. 즉, 상대적으로 제약이 적은 시각 정보(스케일 미정)로 먼저 기하학적 구조를 파악한 뒤, 이를 절대적인 물리량(중력)을 측정하는 관성 정보와 결합하여 전체 시스템의 상태를 완전하게 결정하는 과정을 거친다.</p>
<h3>센서 동기화 (Sensor Synchronization)</h3>
<p>VIO, 특히 측정값을 긴밀하게 결합하는 Tightly-coupled 방식에서는 카메라의 이미지 캡처 시점과 IMU의 측정 시점이 시간적으로 정확하게 일치해야 한다. 두 센서 간에 수 밀리초(ms)의 작은 시간 불일치(time offset)만 존재하더라도, 특히 회전이 빠르거나 움직임이 격렬한 동적인 상황에서는 이 오차가 큰 위치 추정 오차로 증폭될 수 있다.43</p>
<ul>
<li>
<p><strong>하드웨어 동기화:</strong> 가장 이상적이고 정확한 방법이다. 외부 트리거(trigger) 기능을 사용하여 IMU가 특정 시점에 카메라에 전기적 신호를 보내 이미지 캡처를 시작하도록 강제한다. 이를 통해 두 센서의 타임스탬프를 마이크로초(µs) 수준까지 정밀하게 일치시킬 수 있다.44 많은 상용 VIO 센서 모듈이 이 방식을 채택하고 있다.</p>
</li>
<li>
<p><strong>소프트웨어 동기화 및 온라인 보정:</strong> 하드웨어 동기화가 불가능한 저가형 시스템(예: 일반 스마트폰)에서는 소프트웨어적인 접근이 필요하다.</p>
<ul>
<li>
<p><strong>오프라인 보정:</strong> Kalibr와 같은 도구를 사용하여, VIO 시스템을 사용하기 전에 알려진 패턴(예: 체커보드)을 움직이면서 카메라와 IMU 데이터를 수집한다. 그 후, B-스플라인(B-Spline)과 같은 연속적인 궤적 보간 기법을 사용하여 두 센서 데이터 간의 상관관계가 최대가 되는 시간 차이를 찾아내어 보상한다.45</p>
</li>
<li>
<p><strong>온라인 보정:</strong> 시간 차이를 시스템의 상태 변수 중 하나로 포함시켜, VIO 동작 중에 다른 상태 변수들과 함께 실시간으로 추정하고 최적화하는 방법도 연구되고 있다.43 이는 시간에 따라 변할 수 있는 지연 시간(latency)까지도 처리할 수 있는 유연한 접근법이다.</p>
</li>
</ul>
</li>
</ul>
<h3>특징점 매칭과 이상치 제거 (Feature Matching and Outlier Rejection)</h3>
<p>특징점 기반 VIO에서는 프레임 간에 동일한 3D 지점에 해당하는 2D 이미지 특징점을 정확하게 찾아내는 매칭(matching) 과정이 필수적이다. 그러나 조명의 변화, 물체의 대칭성, 반복적인 패턴 등으로 인해 잘못된 매칭, 즉 ’이상치(outlier)’가 항상 발생한다. 이러한 이상치들을 효과적으로 제거하지 않으면, 단 하나의 잘못된 매칭만으로도 전체 상태 추정 결과가 크게 왜곡될 수 있으므로 강건한 이상치 제거 기법이 매우 중요하다.</p>
<ul>
<li>
<p><strong>매칭 기법:</strong> 주로 KLT 옵티컬 플로우(Kanade-Lucas-Tomasi Optical Flow)를 이용한 추적이나, ORB, SIFT와 같은 이진 또는 부동소수점 기술자(descriptor)의 유사도를 비교하는 방식이 사용된다.15</p>
</li>
<li>
<p><strong>이상치 제거 기법:</strong></p>
<ul>
<li>
<p><strong>RANSAC (RANdom SAmple Consensus):</strong> 가장 널리 사용되는 강력한 이상치 제거 기법이다.10 전체 매칭점 집합에서 무작위로 최소한의 샘플(예: 8개 점)을 추출하여 기하학적 모델(예: Fundamental Matrix 또는 Homography Matrix)을 추정한다. 그 후, 전체 데이터 중 이 모델을 지지하는 점(정상치, inlier)의 개수를 센다. 이 과정을 수차례 반복하여 가장 많은 정상치를 확보하는 모델을 최종 모델로 선택하고, 이 모델을 벗어나는 모든 점들을 이상치로 간주하여 제거한다.</p>
</li>
<li>
<p><strong>카이제곱 검정 (Chi-squared Test):</strong> 주로 필터 기반 VIO에서 사용되는 통계적 기법이다.50 새로운 시각적 관측이 들어왔을 때, 이 관측값과 현재 필터의 예측값 사이의 오차(잔차)를 계산한다. 이 잔차의 마할라노비스 거리(Mahalanobis distance)는 현재 필터의 불확실성(공분산)을 고려한 통계적 거리인데, 이 값이 특정 임계치(카이제곱 분포 테이블에서 결정)를 초과하면 해당 관측을 통계적으로 유의미하지 않은 이상치로 판단하고 업데이트에 사용하지 않는다.</p>
</li>
</ul>
</li>
</ul>
<h2>사례 연구: VINS-Mono 분석 (Case Study: Analysis of VINS-Mono)</h2>
<p>VINS-Mono는 홍콩과기대(HKUST)에서 개발하여 공개한 최적화 기반의 단안(monocular) VIO 시스템으로, 현재 학계와 산업계에서 가장 널리 참조되는 표준적인 VIO 프레임워크 중 하나이다.38 VINS-Mono의 아키텍처, 상태 벡터, 그리고 비용 함수를 분석하면 현대 최적화 기반 VIO의 핵심 원리를 깊이 있게 이해할 수 있다.</p>
<h3>시스템 아키텍처 개요</h3>
<p>VINS-Mono는 크게 네 가지 주요 모듈로 구성된 파이프라인을 가진다.38</p>
<ol>
<li>
<p><strong>측정 전처리 (Measurement Preprocessing):</strong> 프론트엔드에 해당하며, 카메라 이미지로부터 특징점을 추출하고 프레임 간에 추적한다. 동시에, IMU 측정값들을 사전적분(preintegration)하여 두 키프레임 사이의 상대적인 움직임 제약으로 변환한다.</p>
</li>
<li>
<p><strong>초기화 (Initialization):</strong> 앞서 설명한 바와 같이, 시각-관성 정렬을 통해 스케일, 중력, 속도, 바이어스 등의 초기값을 추정하여 시스템을 부트스트랩한다.</p>
</li>
<li>
<p><strong>지역 번들 조정 (Local Bundle Adjustment):</strong> 시스템의 핵심 백엔드로, 슬라이딩 윈도우 내의 상태 변수들을 대상으로 비선형 최적화를 수행하여 지역적으로 일관된 궤적을 추정한다.</p>
</li>
<li>
<p><strong>루프 폐쇄 (Loop Closure):</strong> (선택 사항) 이전에 방문했던 장소를 재인식하면, 시간적으로 멀리 떨어진 두 자세 사이에 제약을 추가하고 전역적인 포즈 그래프 최적화(pose graph optimization)를 수행하여 누적된 드리프트를 보정한다.</p>
</li>
</ol>
<h3>상태 벡터(State Vector)의 구성</h3>
<p>VINS-Mono의 슬라이딩 윈도우 최적화에서 동시에 추정되는 전체 상태 벡터 <span class="math math-inline">\mathcal{X}</span>는 다음과 같은 요소들로 구성된다.</p>
<p><span class="math math-display">
\mathcal{X} = [\mathbf{x}_0, \mathbf{x}_1,..., \mathbf{x}_n, \mathbf{x}_c^b, \lambda_0, \lambda_1,..., \lambda_m]
</span></p>
<ul>
<li>
<p><strong>IMU 상태 (<span class="math math-inline">\mathbf{x}_k</span>):</strong> 슬라이딩 윈도우 내에 포함된 각 카메라 키프레임 <span class="math math-inline">k</span> 시점에서의 IMU 상태를 나타낸다. 이는 월드 좌표계 기준의 위치 <span class="math math-inline">\mathbf{p}_{b_k}^w</span>, 속도 <span class="math math-inline">\mathbf{v}_{b_k}^w</span>, 자세(쿼터니언) <span class="math math-inline">\mathbf{q}_{b_k}^w</span>, 그리고 가속도계 바이어스 <span class="math math-inline">\mathbf{b}_a</span>와 자이로스코프 바이어스 <span class="math math-inline">\mathbf{b}_g</span>를 포함한다. 각 IMU 상태는 총 15 자유도(3+3+3+3+3, 쿼터니언은 4차원이지만 3자유도)를 가진다.35</p>
</li>
<li>
<p><strong>외부 파라미터 (<span class="math math-inline">\mathbf{x}_c^b</span>):</strong> 카메라와 IMU 센서 간의 상대적인 공간 변환 관계, 즉 위치 <span class="math math-inline">\mathbf{p}_c^b</span>와 자세 <span class="math math-inline">\mathbf{q}_c^b</span>를 나타낸다. 이 값들은 오프라인에서 미리 보정될 수도 있지만, VINS-Mono는 이를 상태 벡터에 포함시켜 온라인으로 미세 조정할 수 있는 기능을 제공한다 (총 6 자유도).38</p>
</li>
<li>
<p><strong>특징점의 역깊이 (<span class="math math-inline">\lambda_l</span>):</strong> 윈도우 내에서 관측되는 각 3D 특징점 <span class="math math-inline">l</span>의 깊이(depth)를 직접 최적화하는 대신, 수치적 안정성을 위해 그 역수(inverse depth)를 최적화한다. 이는 특징점이 처음 관측된 키프레임을 기준으로 정의된다 (특징점 당 1 자유도).38</p>
</li>
</ul>
<h3>비용 함수(Cost Function) 분석</h3>
<p>VINS-Mono는 베이즈 이론에 기반한 최대 사후 확률(Maximum A Posteriori, MAP) 추정 문제를 풀어 최적의 상태 벡터 <span class="math math-inline">\mathcal{X}</span>를 찾는다. 이는 결국 모든 측정값에 대한 확률적 오차(잔차)의 합을 최소화하는 비선형 최소 제곱 문제로 귀결된다. 전체 비용 함수는 크게 세 가지 종류의 잔차 항들의 마할라노비스 거리(Mahalanobis distance)의 합으로 구성된다.38</p>
<p><span class="math math-display">
\min_{\mathcal{X}} \left\{ \| \mathbf{r}_p - \mathbf{H}_p \mathcal{X} \|^2 + \sum_{k \in \mathcal{B}} \| \mathbf{r}_{\mathcal{B}}(\hat{\mathbf{z}}_{b_{k+1}}^{b_k}, \mathcal{X}) \|_{\mathbf{P}_{k+1}^k}^2 + \sum_{(l,j) \in \mathcal{C}} \rho \left( \| \mathbf{r}_{\mathcal{C}}(\hat{\mathbf{z}}_l^{c_j}, \mathcal{X}) \|_{\mathbf{P}_l^{c_j}}^2 \right) \right\}
</span></p>
<ol>
<li>
<p>사전 정보 항 (Prior Term): <span class="math math-inline">\| \mathbf{r}_p - \mathbf{H}_p \mathcal{X} \|^2</span></p>
<p>슬라이딩 윈도우가 앞으로 이동하면서 가장 오래된 상태를 제거할 때, 그냥 버리는 것이 아니라 마지널라이제이션을 통해 해당 상태가 가지고 있던 정보를 요약하여 사전 확률(prior)로 만든다. 이 항은 과거의 정보가 현재 최적화에 제약으로 작용하도록 하여 정보의 손실을 최소화하는 역할을 한다.38</p>
</li>
<li>
<p>IMU 사전적분 항 (IMU Preintegration Term): <span class="math math-inline">\sum_{k \in \mathcal{B}} \| \mathbf{r}_{\mathcal{B}}(\hat{\mathbf{z}}_{b_{k+1}}^{b_k}, \mathcal{X}) \|_{\mathbf{P}_{k+1}^k}^2</span></p>
<p>이 항은 연속된 두 키프레임 <span class="math math-inline">k</span>와 <span class="math math-inline">k+1</span>의 추정된 IMU 상태(자세, 속도, 위치, 바이어스)와, 두 시점 사이의 IMU 측정값을 사전적분하여 얻은 상대적인 움직임 측정값 사이의 오차를 나타낸다. 이 잔차항이 IMU의 동역학 모델을 시스템에 제약으로 부과하는 역할을 한다. 이 항의 크기는 사전적분 과정에서 함께 계산된 공분산 행렬 <span class="math math-inline">\mathbf{P}_{k+1}^k</span>에 의해 정규화된다.35</p>
</li>
<li>
<p>시각 재투영 항 (Visual Reprojection Term): <span class="math math-inline">\sum_{(l,j) \in \mathcal{C}} \rho \left( \| \mathbf{r}_{\mathcal{C}}(\hat{\mathbf{z}}_l^{c_j}, \mathcal{X}) \|_{\mathbf{P}_l^{c_j}}^2 \right)</span></p>
<p>이 항은 3D 특징점 <span class="math math-inline">l</span>을 현재 추정된 카메라 자세 <span class="math math-inline">j</span>를 이용하여 2D 이미지 평면에 투영했을 때의 예측된 픽셀 위치와, 실제 이미지에서 관측된 픽셀 위치 사이의 오차를 나타낸다. 이 잔차항이 카메라의 기하학적 관측 모델을 시스템에 제약으로 부과한다.35</p>
<p><span class="math math-inline">\rho(\cdot)</span>는 강건한 비용 함수(robust cost function, 예: Huber loss)로, 잘못된 특징점 매칭과 같은 이상치(outlier)가 전체 최적화에 미치는 과도한 영향을 줄여주는 역할을 한다.</p>
</li>
</ol>
<p>이러한 비용 함수의 구성은 VIO 문제를 ’요소 그래프(Factor Graph)’라는 관점에서 이해할 때 더욱 명확해진다. 상태 변수들은 그래프의 ’노드(node)’에 해당하고, 각 잔차 항은 특정 노드들 사이의 관계를 나타내는 ‘요소(factor)’ 또는 ’제약(constraint)’에 해당한다. IMU 항은 연속된 두 IMU 상태 노드를 연결하는 요소이고, 시각 항은 하나의 카메라 자세 노드와 하나의 특징점 노드를 연결하는 요소다. 최적화 과정은 이 그래프의 모든 제약들을 가장 잘 만족시키는 노드들의 상태를 찾는 과정으로 해석할 수 있다. 이러한 그래프 기반의 표현 방식은 시스템을 모듈화하고, GPS나 LiDAR와 같은 다른 센서 정보를 새로운 요소로 쉽게 추가할 수 있게 해주는 확장성을 제공한다.</p>
<h2>최신 동향 및 미래 전망</h2>
<p>VIO 기술은 지난 10년간 비약적인 발전을 이루었지만, 여전히 해결해야 할 과제들이 남아있다. 최근 연구 동향은 전통적인 기하학 기반 접근법의 한계를 극복하기 위해 딥러닝을 도입하거나, 더 많은 종류의 센서를 융합하여 강건성을 극대화하는 방향으로 나아가고 있다.</p>
<h3>딥러닝 기반 VIO (Deep Learning-based VIO)</h3>
<p>전통적인 VIO 시스템은 수작업으로 설계된 정밀한 수학적 모델에 크게 의존한다. 이는 모델의 가정이 잘 들어맞는 환경에서는 매우 높은 성능을 보이지만, 급격한 조명 변화, 심한 모션 블러, 예측하지 못한 센서 오차 등 모델이 고려하지 못한 비이상적인 상황에서는 취약점을 드러낸다. 딥러닝은 방대한 데이터로부터 복잡하고 비선형적인 관계를 직접 학습함으로써 이러한 한계를 극복할 새로운 가능성을 제시하고 있다.2</p>
<ul>
<li>
<p><strong>접근법:</strong></p>
<ol>
<li>
<p><strong>End-to-End 학습:</strong> 이미지 시퀀스와 IMU 데이터를 신경망의 입력으로 받아, 중간 과정 없이 직접 카메라의 자세를 출력하도록 전체 시스템을 학습시키는 방식이다.</p>
</li>
<li>
<p><strong>하이브리드 접근법:</strong> 현재 더 활발히 연구되는 방향으로, 전통적인 VIO 파이프라인의 특정 모듈을 딥러닝으로 대체하거나 성능을 향상시키는 방식이다. 예를 들어, 딥러닝을 사용하여 조명 변화에 더 강건한 특징점을 추출하거나, 단안 이미지로부터 직접 조밀한 깊이 맵(dense depth map)을 추정하여 초기화 과정을 돕거나 42, 온도나 진동과 같은 외부 요인에 따른 복잡한 IMU 바이어스 변화를 예측하는 모델을 학습시킬 수 있다.2</p>
</li>
</ol>
</li>
<li>
<p><strong>자기 지도 학습 (Self-supervised Learning):</strong> VIO 학습에 필요한 방대한 양의 정답 데이터(ground truth trajectory)를 구축하는 것은 매우 어렵고 비용이 많이 든다. 자기 지도 학습은 이러한 정답 데이터 없이, 데이터 자체의 내재적인 제약 조건을 손실 함수(loss function)로 활용하여 학습하는 기법이다.51 VIO 분야에서는 연속된 이미지 프레임 간의 ’광도 일관성(photometric consistency)’을 이용한다. 즉, 한 프레임의 이미지를 신경망이 예측한 깊이와 자세를 이용해 다른 프레임의 시점으로 변환(warping)했을 때, 원래 이미지와 최대한 유사해지도록 네트워크를 학습시킨다. 이 과정을 통해 네트워크는 정답 데이터 없이도 깊이와 카메라 모션을 동시에 추정하는 방법을 배우게 된다. SelfVIO는 이러한 접근법을 성공적으로 구현한 대표적인 연구 사례다.51</p>
</li>
</ul>
<h3>다중 센서 융합 (Multi-sensor Fusion)</h3>
<p>VIO의 강건성과 정확도를 한 단계 더 끌어올리기 위해, 카메라와 IMU 외에 다른 종류의 센서들을 추가적으로 융합하려는 연구가 활발히 진행되고 있다.</p>
<ul>
<li>
<p><strong>LiDAR-Inertial-Visual Odometry (LIVO):</strong> LiDAR는 레이저를 이용하여 주변 환경까지의 거리를 매우 정밀하게 측정할 수 있으며, 조명 변화에 전혀 영향을 받지 않는다. 이러한 LiDAR의 장점을 VIO의 저비용, 고주파수 모션 추정 능력과 결합하여, 동적인 객체가 많거나 환경의 구조가 복잡한 시나리오에서도 매우 높은 정확도와 강건성을 달성할 수 있다.52</p>
</li>
<li>
<p><strong>이벤트 카메라(Event Camera) 융합:</strong> 이벤트 카메라는 기존 카메라처럼 일정한 주기로 전체 이미지를 촬영하는 대신, 각 픽셀의 밝기 변화가 감지될 때만 비동기적으로 ’이벤트’를 발생시킨다. 이는 매우 빠른 움직임이나 명암 차이가 극심한 HDR(High Dynamic Range) 환경에서도 모션 블러 없이 시간 해상도가 매우 높은 정보를 제공하여, 전통적인 VIO가 실패하는 극한 상황에서의 성능을 크게 향상시킬 수 있다.</p>
</li>
<li>
<p><strong>ToF(Time-of-Flight) 카메라 융합:</strong> ToF 카메라는 빛이 물체에 반사되어 돌아오는 시간을 측정하여 각 픽셀의 깊이 정보를 직접 제공한다. 이를 VIO에 통합하면 단안 카메라의 스케일 모호성 문제를 해결하고, 어두운 환경에서의 성능을 개선할 수 있다.8</p>
</li>
</ul>
<h3>향후 연구 방향 및 결론</h3>
<p>VIO 기술은 로봇과 인간이 세상을 인식하고 상호작용하는 방식을 바꾸는 핵심 기술로 자리매김했다. 미래의 VIO 연구는 다음과 같은 방향으로 나아갈 것으로 전망된다.</p>
<ul>
<li>
<p><strong>궁극의 강건성 추구:</strong> 여전히 VIO 시스템의 성능을 저하시키는 극단적인 조명 변화, 심한 모션 블러, 특징이 거의 없는 환경, 수많은 동적 객체가 존재하는 혼잡한 공간 등 가장 어려운 시나리오에서의 안정적인 작동을 위한 연구가 계속될 것이다.</p>
</li>
<li>
<p><strong>의미론적(Semantic) VIO:</strong> 딥러닝 기반의 객체 탐지 및 분할 기술을 VIO에 통합하여, 주변 환경을 단순한 점과 선의 집합이 아닌, ‘사람’, ‘자동차’, ’문’과 같은 의미론적인 객체로 이해하는 단계로 나아갈 것이다. 이를 통해 동적인 객체의 움직임을 분리하여 추정하거나, 지도에 의미 정보를 부여하여 로봇이 더 높은 수준의 작업을 수행하도록 도울 수 있다.</p>
</li>
<li>
<p><strong>장기적 자율성 및 협업:</strong> 현재의 VIO는 시간에 따라 드리프트가 누적되는 한계를 가진다. 루프 폐쇄나 다른 전역 센서와의 융합을 통해 드리프트를 완전히 제거하고, 대규모 환경에서 평생(life-long) 동안 안정적으로 작동하며 지속적으로 지도를 갱신하고 다른 로봇과 지도를 공유하는 협업 SLAM 시스템으로의 발전이 기대된다.</p>
</li>
</ul>
<p>결론적으로, VIO는 지난 20년간 눈부신 발전을 거듭해왔다. 초기 필터 기반의 연구에서 출발하여, 최적화 기반 방법론의 등장과 IMU 사전적분과 같은 이론적 혁신을 통해 높은 정확도를 달성했다. 이제 VIO는 전통적인 기하학적 접근법의 정밀함과 딥러닝의 유연성을 결합하는 하이브리드 시대로 접어들고 있다. 이러한 융합은 VIO 시스템이 실제 세계의 예측 불가능하고 복잡한 환경에 더욱 강건하게 대처할 수 있도록 만들 것이며, 자율 로봇, 증강현실, 그리고 그 이상의 분야에서 무한한 가능성을 열어줄 것이다.</p>
<h4><strong>참고 자료</strong></h4>
<ol>
<li>Visual and Inertial Odometry | Department of Informatics | UZH, 9월 14, 2025에 액세스, https://www.ifi.uzh.ch/en/rpg/research/research_vo.html</li>
<li>What is Visual-Inertial Odometry? | Activeloop Glossary, 9월 14, 2025에 액세스, https://www.activeloop.ai/resources/glossary/visual-inertial-odometry-vio/</li>
<li>Visual Inertial Odometry (VIO) · PX4 User Guide, 9월 14, 2025에 액세스, https://docs.px4.io/v1.11/en/computer_vision/visual_inertial_odometry.html</li>
<li>VIO Revolution in Navigation and Positioning - Inertial Labs, 9월 14, 2025에 액세스, https://inertiallabs.com/vio-revolution-in-navigation-and-positioning/</li>
<li>Algorithm for Visual Odometry. - AGH, 9월 14, 2025에 액세스, https://home.agh.edu.pl/~kwant/wordpress/wp-content/uploads/Szczesny_Twardowski_MSc_final.pdf</li>
<li>Visual odometry - Wikipedia, 9월 14, 2025에 액세스, https://en.wikipedia.org/wiki/Visual_odometry</li>
<li>Review of visual odometry: types, approaches, challenges, and …, 9월 14, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC5084145/</li>
<li>Perception in the Dark; Development of a ToF Visual Inertial Odometry System - PMC, 9월 14, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC7085618/</li>
<li>Resolving scale ambiguity for monocular visual odometry | Request PDF - ResearchGate, 9월 14, 2025에 액세스, https://www.researchgate.net/publication/261382146_Resolving_scale_ambiguity_for_monocular_visual_odometry</li>
<li>Visual Odometry based on Stereo Image Sequences with RANSAC-based Outlier Rejection Scheme - Andreas Geiger, 9월 14, 2025에 액세스, https://www.cvlibs.net/publications/Kitt2010IV.pdf</li>
<li>Lecture 13 Visual Inertial Fusion, 9월 14, 2025에 액세스, https://rpg.ifi.uzh.ch/docs/teaching/2018/13_visual_inertial_fusion_advanced.pdf</li>
<li>Assessment of Noise of MEMS IMU Sensors of Different Grades for GNSS/IMU Navigation - PMC, 9월 14, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC10975684/</li>
<li>VINS Mono Body-Of-Knowledge | ai Werkstatt, 9월 14, 2025에 액세스, https://www.aiwerkstatt.com/wp-content/uploads/2020/08/VINS-Mono-Body-Of-Knowledge.pdf</li>
<li>SP-VIO: Robust and Efficient Filter-Based Visual Inertial Odometry with State Transformation Model and Pose-Only Visual Description - arXiv, 9월 14, 2025에 액세스, https://arxiv.org/html/2411.07551v1</li>
<li>XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization for XR Applications, 9월 14, 2025에 액세스, https://arxiv.org/html/2502.01297v1</li>
<li>Visual-Inertial SLAM Comparison - Bharat Joshi - GitHub Pages, 9월 14, 2025에 액세스, https://joshi-bharat.github.io/projects/visual_slam_comparison/</li>
<li>Overview of the current development in Visual-Iner- tial Systems - ZENTIME PUBLISHING CORPORATION LIMITED, 9월 14, 2025에 액세스, https://oss.zentimepublishing.com/upload/asset/20241231/615cc04601a4b3a1d8284907e11ea8e2f5a29b1d.pdf</li>
<li>Image formation and pinhole camera model - Notebook - Ontario Tech University, 9월 14, 2025에 액세스, https://csundergrad.science.uoit.ca/courses/cv-notes/notebooks/01-image-formation.html</li>
<li>Pinhole Camera Model | HediVision, 9월 14, 2025에 액세스, https://hedivision.github.io/Pinhole.html</li>
<li>1.2. The Pinhole Camera Matrix - Homepages of UvA/FNWI staff, 9월 14, 2025에 액세스, https://staff.fnwi.uva.nl/r.vandenboomgaard/IPCV20162017/LectureNotes/CV/PinholeCamera/PinholeCamera.html</li>
<li>Monocular Visual Odometry using a Planar Road Model to Solve Scale Ambiguity, 9월 14, 2025에 액세스, https://www.semanticscholar.org/paper/Monocular-Visual-Odometry-using-a-Planar-Road-Model-Kitt-Rehder/aa6e2c2e5f370601489d28834f2ca26ab2fa69f8</li>
<li>IMU Fundamentals, Part 1: Introduction to IMUs - Tangram Visions Blog, 9월 14, 2025에 액세스, https://www.tangramvision.com/blog/learn-imu-fundamentals-part-1-of-5</li>
<li>Mathematical Model of an IMU Kalman Filter Assumptions Mathematical Formulation References - Nitin J. Sanket, 9월 14, 2025에 액세스, https://nitinjsanket.github.io/tutorials/attitudeest/kf</li>
<li>IMU Propagation Derivations - OpenVINS, 9월 14, 2025에 액세스, https://docs.openvins.com/propagation.html</li>
<li>IMU Fundamentals, Part 5: Preintegration Basics - Tangram Visions …, 9월 14, 2025에 액세스, https://www.tangramvision.com/blog/imu-preintegration-basics-part-5-of-5</li>
<li>Learning IMU Bias with Diffusion Model - arXiv, 9월 14, 2025에 액세스, https://arxiv.org/html/2505.11763v1</li>
<li>Reading IMU specs for fun and profit - John Stechschulte, 9월 14, 2025에 액세스, https://stechschulte.net/2023/10/11/imu-specs.html</li>
<li>Development of Stochastic IMU Error Models for INS/GNSS Integration, 9월 14, 2025에 액세스, <a href="https://www.aoe.vt.edu/content/dam/aoe_vt_edu/people/faculty/joerger/publications/Development%20of%20Stochastic%20IMU%20Error%20Models%20for%20INS-GNSS%20Integration_2021.pdf">https://www.aoe.vt.edu/content/dam/aoe_vt_edu/people/faculty/joerger/publications/Development%20of%20Stochastic%20IMU%20Error%20Models%20for%20INS-GNSS%20Integration_2021.pdf</a></li>
<li>How to compare IMU - SBG Support Center, 9월 14, 2025에 액세스, https://support.sbg-systems.com/sc/kb/latest/technology-insights/how-to-compare-imu</li>
<li>3.1 IMU Specifications - VectorNav Technologies, 9월 14, 2025에 액세스, https://www.vectornav.com/resources/inertial-navigation-primer/specifications–and–error-budgets/specs-imuspecs</li>
<li>IMU Noise Model · ethz-asl/kalibr Wiki · GitHub, 9월 14, 2025에 액세스, https://github.com/ethz-asl/kalibr/wiki/IMU-Noise-Model</li>
<li>A Review of Visual-Inertial Simultaneous Localization and Mapping …, 9월 14, 2025에 액세스, https://www.mdpi.com/2218-6581/7/3/45</li>
<li>Qualitative comparison of the two algorithms (i): Visual – inertial odometry (VIO), and (ii) - ResearchGate, 9월 14, 2025에 액세스, https://www.researchgate.net/figure/Qualitative-comparison-of-the-two-algorithms-i-Visual-inertial-odometry-VIO-and_fig9_331786353</li>
<li>Visual-Inertial Odometry of Aerial Robots - Robotics and Perception …, 9월 14, 2025에 액세스, https://rpg.ifi.uzh.ch/docs/Encyclopedia19VIO_Scaramuzza.pdf</li>
<li>LRPL-VIO: A Lightweight and Robust Visual–Inertial Odometry with Point and Line Features, 9월 14, 2025에 액세스, https://www.mdpi.com/1424-8220/24/4/1322</li>
<li>IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation, 9월 14, 2025에 액세스, https://www.youtube.com/watch?v=CsJkci5lfco</li>
<li>IMU Preintegration Theory - Manohar Kuse’s Cyber, 9월 14, 2025에 액세스, https://kusemanohar.info/2021/10/26/imu-preintegration-theory/</li>
<li>Formula Derivation and Analysis of the VINS-Mono - arXiv, 9월 14, 2025에 액세스, https://arxiv.org/pdf/1912.11986</li>
<li>(PDF) Improving Monocular Visual-Inertial Initialization with Structureless Visual-Inertial Bundle Adjustment - ResearchGate, 9월 14, 2025에 액세스, https://www.researchgate.net/publication/389315517_Improving_Monocular_Visual-Inertial_Initialization_with_Structureless_Visual-Inertial_Bundle_Adjustment</li>
<li>VIO - IMU Initialization - Tong’s This Is It, 9월 14, 2025에 액세스, https://tongling916.github.io/2020/10/26/VIO-Initialization/</li>
<li>Monocular Visual-Inertial Odometry (VIO) Using Factor Graph - MATLAB &amp; Simulink, 9월 14, 2025에 액세스, https://www.mathworks.com/help/nav/ug/monocular-visual-inertial-odometry-using-factor-graph.html</li>
<li>Fast Monocular Visual-Inertial Initialization Leveraging Learned Single-View Depth - Patrick Geneva, 9월 14, 2025에 액세스, https://pgeneva.com/downloads/slides/Merrill2023RSS.pdf</li>
<li>Optimization-Based Online Initialization and Calibration of Monocular Visual-Inertial Odometry Considering Spatial-Temporal Constraints - MDPI, 9월 14, 2025에 액세스, https://www.mdpi.com/1424-8220/21/8/2673</li>
<li>A Tightly-Coupled Visual-Inertial System with Synchronized Time for Indoor Localization, 9월 14, 2025에 액세스, https://www.ijmerr.com/uploadfile/2021/0315/20210315020257170.pdf</li>
<li>Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO - arXiv, 9월 14, 2025에 액세스, https://arxiv.org/html/2403.12504v1</li>
<li>ROS camera and IMU synchronization | Work-is-Playing, 9월 14, 2025에 액세스, https://grauonline.de/wordpress/?page_id=1951</li>
<li>Online self-calibration and time synchronization of camera and IMU in substation scenarios, 9월 14, 2025에 액세스, https://www.researchgate.net/publication/373928539_Online_self-calibration_and_time_synchronization_of_camera_and_IMU_in_substation_scenarios</li>
<li>Modeling Varying Camera-IMU Time Offset in Optimization-Based Visual-Inertial Odometry - CVF Open Access, 9월 14, 2025에 액세스, https://openaccess.thecvf.com/content_ECCV_2018/papers/Yonggen_Ling_Modeling_Varying_Camera-IMU_ECCV_2018_paper.pdf</li>
<li>Robust Outliers Detection in Image Point Matching - Keck Institute for Space Studies, 9월 14, 2025에 액세스, https://www.kiss.caltech.edu/papers/surface/papers/robust.pdf</li>
<li>Robust Outlier-Adaptive Filtering for Vision-Aided Inertial Navigation - PMC, 9월 14, 2025에 액세스, https://pmc.ncbi.nlm.nih.gov/articles/PMC7181286/</li>
<li>arxiv.org, 9월 14, 2025에 액세스, https://arxiv.org/abs/1911.09968</li>
<li>Improved Multi-Sensor Fusion Dynamic Odometry Based on Neural Networks - MDPI, 9월 14, 2025에 액세스, https://www.mdpi.com/1424-8220/24/19/6193</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>