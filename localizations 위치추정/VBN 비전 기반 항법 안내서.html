<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:비전 기반 항법(Vision-Based Navigation) 안내서</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>비전 기반 항법(Vision-Based Navigation) 안내서</h1>
                    <nav class="breadcrumbs"><a href="../index.html">Home</a> / <a href="index.html">위치 추정 (Localization)</a> / <span>비전 기반 항법(Vision-Based Navigation) 안내서</span></nav>
                </div>
            </header>
            <article>
                <h1>비전 기반 항법(Vision-Based Navigation) 안내서</h1>
<h2>1.  비전 기반 항법의 이해</h2>
<h3>1.1  비전 기반 항법(VBN)의 정의 및 핵심 목표</h3>
<p>비전 기반 항법(Vision-Based Navigation, VBN)은 카메라와 같은 시각 센서로부터 획득한 이미지 정보를 활용하여 이동체의 위치, 자세, 경로 등을 추정하고 제어하는 포괄적인 기술 체계를 의미한다.1 이는 단순히 한 지점에서 다른 지점으로 이동하는 기술을 넘어, 주변 환경을 시각적으로 ’이해’하고 이를 바탕으로 자율적인 항법 결정을 내리는 전 과정을 포함한다.4 VBN의 핵심 목표는 위성항법시스템(GPS) 신호가 수신되지 않거나(GPS-denied) 신뢰할 수 없는 환경, 예를 들어 실내, 고층 빌딩이 밀집한 도심 협곡, 수중, 우주 공간 등에서 강인하고 정확한 항법 솔루션을 제공하는 것이다.3 이러한 맥락에서 VBN은 기존 항법 시스템을 대체하거나 보완하는 대체 위치·항법·시각(APNT, Alternative Position, Navigation, and Timing) 솔루션으로서 중요한 역할을 수행한다.6 기술의 발전에 따라 VBN은 항공 로봇(드론), 자율주행차, 도심 항공 모빌리티(UAM), 행성 탐사 로버 등 광범위한 분야에서 자율 시스템의 핵심 기술로 부상하고 있다.3</p>
<p>초기 VBN 연구는 주로 GPS가 없는 환경에서의 위치 추정 문제에 집중했으나, 기술이 성숙함에 따라 그 패러다임이 변화하고 있다. VBN은 더 이상 단순히 ’어디에 있는가’라는 위치 정보에만 국한되지 않고, ’무엇을 보고 있는가’라는 환경 인지의 문제로 확장되고 있다. 예를 들어, 지리 참조(Geo-Referencing) 기술은 현재 촬영된 이미지를 위성사진이나 3차원 건물 모델과 같은 기 구축된 지리 참조 공간정보와 대조하여 절대 위치를 추정한다.1 이는 단순한 상대 위치 추정을 넘어, 현재 위치의 지리적, 의미론적 정보를 능동적으로 활용하는 고차원적인 접근 방식이다. 또한, 딥러닝 기반의 VBN 시스템은 차선, 건물, 교통 표지판과 같은 의미론적 정보(semantic information)를 데이터로부터 직접 학습하여 항법 결정에 활용한다.11 이러한 경향은 VBN이 단순 측위 기술에서 벗어나, 주변 환경과의 복잡한 상호작용을 가능하게 하는 핵심적인 ‘인지(Perception)’ 기술로 자리매김하고 있음을 명확히 보여준다.</p>
<h3>1.2  항법 패러다임의 전환: GPS, INS와의 비교 분석</h3>
<p>전통적인 항법 시스템은 주로 GPS와 관성 항법 장치(INS)에 의존해왔다. 각 시스템은 명확한 장단점을 지닌다.</p>
<ul>
<li>
<p><strong>GPS (Global Positioning System):</strong> 위성 신호를 기반으로 작동하며, 전 지구적 범위에서 높은 정확도의 위치 정보를 제공한다. 하지만 위성 신호에 절대적으로 의존하기 때문에 실내, 터널, 도심 협곡과 같은 신호 음영 지역이나 전파 교란(jamming) 환경에서는 성능이 급격히 저하되거나 사용이 불가능하다.7</p>
</li>
<li>
<p><strong>INS (Inertial Navigation System):</strong> 가속도계와 자이로스코프를 이용해 이동체의 움직임을 측정하고, 이를 적분하여 위치와 자세를 계산한다. 외부 신호 없이 독립적으로 작동 가능하며 높은 데이터 갱신율을 보장하지만, 시간이 지남에 따라 측정 오차가 누적되는 ‘드리프트(drift)’ 현상이 치명적인 단점이다.7 미세한 가속도 및 각속도 오차는 적분 과정을 거치면서 위치 오차를 기하급수적으로 증가시킨다.</p>
</li>
<li>
<p><strong>VBN (Vision-Based Navigation):</strong> 저렴한 카메라 센서를 사용하여 주변 환경에 대한 풍부한 시각 정보(색상, 질감 등)를 획득하고, 이를 통해 위치를 추정한다.7 GPS 신호 간섭에 강인하며 GPS 사용이 불가능한 환경에서 독자적으로 작동할 수 있다.13 그러나 급격한 조명 변화나 특징이 없는(texture-less) 환경(예: 하얀 벽, 사막)에 취약하며, 이미지 처리 과정에서 높은 계산 복잡성을 요구한다는 단점이 있다.7</p>
</li>
</ul>
<p>이러한 각 시스템의 한계는 단일 센서만으로는 모든 환경에서 강인한 항법을 구현하기 어렵다는 공학적 현실을 보여준다. VBN의 단점(조명, 속도, 스케일 문제)을 극복하려는 노력은 자연스럽게 센서 융합(sensor fusion) 기술의 발전으로 이어졌으며, 특히 IMU와의 결합인 시각-관성 주행 거리 측정(Visual-Inertial Odometry, VIO)은 현대 VBN의 표준적인 접근법이 되었다.14 이는 ’하나의 완벽한 센서는 없다’는 철학 아래, 각 센서의 강점은 취하고 약점은 상호 보완하는 방향으로 시스템을 설계하는 패러다임의 변화를 반영한다. VIO 시스템에서 카메라는 장기적인 드리프트가 적은 위치 정보를 제공하고, IMU는 조명과 무관하게 단기적으로 빠르고 부드러운 움직임 정보를 제공하여 서로의 단점을 효과적으로 상쇄한다.16 이러한 융합 철학은 향후 라이다(LiDAR), 레이더(RADAR) 등 다른 센서들과의 결합으로 확장되며 VBN 시스템의 강인성과 신뢰성을 결정하는 핵심 요소로 작용할 것이다.</p>
<p><strong>표 1: 항법 시스템 비교 (VBN vs. GPS vs. INS)</strong></p>
<table><thead><tr><th>항법 시스템</th><th>작동 원리</th><th>장점</th><th>단점</th><th>주요 의존성</th><th>이상적 환경</th></tr></thead><tbody>
<tr><td><strong>VBN</strong></td><td>카메라 이미지 분석을 통한 상대/절대 위치 및 자세 추정</td><td>저비용, 풍부한 정보 획득, GPS 간섭에 강인함</td><td>조명/외형 변화에 민감, 특징 없는 환경에 취약, 높은 계산량</td><td>주변 환경의 시각적 특징</td><td>특징이 풍부한 실내/실외</td></tr>
<tr><td><strong>GPS</strong></td><td>위성 신호 수신을 통한 삼변측량으로 절대 위치 계산</td><td>전 지구적 커버리지, 높은 절대 위치 정확도, 누적 오차 없음</td><td>신호 음영/차단/간섭에 취약, 실내 사용 불가, 낮은 갱신율</td><td>4개 이상의 위성 신호</td><td>개활지, 하늘이 보이는 실외</td></tr>
<tr><td><strong>INS</strong></td><td>가속도계/자이로스코프 측정치 적분을 통한 상대 위치/자세 추정</td><td>외부 신호 불필요, 높은 갱신율(&gt;100Hz), 모든 환경에서 작동</td><td>시간 경과에 따른 오차 누적(드리프트), 초기 정렬 필요</td><td>초기 위치/자세 정보</td><td>모든 환경(단기적 사용)</td></tr>
</tbody></table>
<h3>1.3  VBN의 구성 요소: 위치 추정, 매핑, 장애물 회피, 경로 계획</h3>
<p>VBN 시스템은 일반적으로 네 가지 핵심 기능 모듈로 구성된다. 이 모듈들은 유기적으로 연동하여 자율 항법을 가능하게 한다.7</p>
<ol>
<li>
<p><strong>위치 추정(Localization):</strong> 현재 이동체의 3차원 공간상 위치(x,y,z)와 3차원 방향(roll, pitch, yaw)을 포함하는 6자유도(6-DOF) 자세(pose)를 추정하는 과정이다.</p>
</li>
<li>
<p><strong>지도 작성(Mapping):</strong> 카메라로 관측한 정보를 바탕으로 주변 환경의 지도를 생성하는 과정이다. 지도는 점유 격자 지도(occupancy grid map), 특징점 지도(feature map), 3D 점군(point cloud) 등 다양한 형태로 표현될 수 있으며, 항법, 장애물 회피, 경로 계획의 기준이 된다.</p>
</li>
<li>
<p><strong>장애물 회피(Obstacle Avoidance):</strong> 생성된 지도 정보와 실시간 센서 데이터를 기반으로 주변의 장애물을 감지하고 충돌을 피하는 경로를 생성하여 실행하는 기능이다.</p>
</li>
<li>
<p><strong>경로 계획(Path Planning):</strong> 현재 위치(시작점)에서 목표 지점까지 이동하기 위한 최적의 경로를 계산하는 과정으로, 전역 경로 계획(global path planning)과 지역 경로 계획(local path planning)으로 나뉜다.</p>
</li>
</ol>
<h2>2.  VBN의 수학적 및 알고리즘적 토대</h2>
<p>VBN의 핵심은 카메라를 통해 들어온 2D 이미지 정보로부터 3D 세계의 구조와 카메라의 움직임을 추론하는 것이다. 이는 기하학, 확률론, 최적화 이론에 깊이 뿌리를 둔 알고리즘들을 통해 구현된다.</p>
<h3>2.1  Visual Odometry (VO): 시각적 주행 거리 측정</h3>
<p>Visual Odometry(VO)는 연속적인 카메라 이미지로부터 카메라의 상대적인 움직임, 즉 자세 변화를 프레임 단위로 추정하는 기술이다.1 이는 자동차의 주행 거리계(Odometer)가 바퀴 회전 수를 기반으로 이동 거리를 측정하는 원리와 유사하여 ’시각적 주행 거리 측정’이라 불린다. VO의 일반적인 파이프라인은 다음과 같은 단계로 구성된다.10</p>
<ol>
<li>
<p><strong>이미지 수집:</strong> 연속적인 이미지 프레임을 획득한다.</p>
</li>
<li>
<p><strong>특징점 검출 및 기술(Feature Detection and Description):</strong> 각 이미지에서 추적하기 용이한 독특한 지점(코너, 블롭 등)을 찾고, 그 주변 영역의 패턴을 고유한 벡터(기술자)로 표현한다.</p>
</li>
<li>
<p><strong>특징점 매칭 또는 추적(Feature Matching or Tracking):</strong> 연속된 프레임 간에 동일한 특징점에 해당하는 기술자들을 찾아 대응 관계를 설정한다.</p>
</li>
<li>
<p><strong>움직임 추정(Motion Estimation):</strong> 설정된 대응 관계와 기하학적 제약을 이용하여 두 프레임 사이의 카메라 회전(Rotation)과 이동(Translation)을 계산한다.</p>
</li>
<li>
<p><strong>스케일 보정(Scale Correction):</strong> 단안 카메라의 경우, 추정된 이동량의 실제 크기(스케일)를 결정하기 위한 추가적인 과정이 필요하다.</p>
</li>
</ol>
<h4>2.1.1 수학적 심층 분석: 에피폴라 기하학(Epipolar Geometry)</h4>
<p>VO의 움직임 추정 단계의 수학적 근간은 에피폴라 기하학이다.22 이는 동일한 3D 지점을 촬영한 두 이미지 뷰 사이의 기하학적 관계를 설명하는 이론이다. 핵심 원리는 **에피폴라 제약(epipolar constraint)**으로, 한 이미지 상의 점</p>
<p><span class="math math-inline">p</span>에 대응하는 다른 이미지 상의 점 <span class="math math-inline">p&#39;</span>는 반드시 특정 직선, 즉 <strong>에피폴라 라인(epipolar line)</strong> 위에 존재한다는 것이다. 이 기하학적 관계는 행렬로 표현될 수 있다.</p>
<ul>
<li>
<p>카메라 내부 파라미터(초점 거리 등)를 모를 경우, 이 관계는 **기본 행렬(Fundamental Matrix) <span class="math math-inline">F</span>**에 의해 기술된다.</p>
<p><span class="math math-display">
p&#39;^{T} F p = 0
</span></p>
</li>
<li>
<p>카메라가 캘리브레이션되어 내부 파라미터를 알고 정규화된 이미지 좌표를 사용할 경우, 이 관계는 **본질 행렬(Essential Matrix) <span class="math math-inline">E</span>**로 표현된다.</p>
<p><span class="math math-display">
p&#39;^{T}_{norm} E p_{norm} = 0
</span></p>
</li>
</ul>
<p>본질 행렬 <span class="math math-inline">E</span>는 두 카메라 간의 상대적인 회전 행렬 <span class="math math-inline">R</span>과 이동 벡터 <span class="math math-inline">t</span> 정보만을 순수하게 담고 있으며, 다음과 같이 정의된다.23</p>
<p><span class="math math-display">
  E = [t]_{\times} R
</span><br />
여기서 <span class="math math-inline">[t]_{\times}</span>는 벡터 <span class="math math-inline">t</span>의 왜대칭 행렬(skew-symmetric matrix)이다. 따라서 이미지 간 특징점 대응 관계로부터 <span class="math math-inline">E</span>를 추정하고, 이를 특이값 분해(SVD)와 같은 방법으로 분해하면 카메라의 상대적인 움직임 <span class="math math-inline">R</span>과 <span class="math math-inline">t</span>를 복원할 수 있다.</p>
<h4>2.1.2 단안(Monocular) VO 대 스테레오(Stereo) VO</h4>
<ul>
<li>
<p><strong>단안 VO:</strong> 카메라 하나만을 사용하므로 시스템이 저렴하고 간단하다. 하지만 2D 이미지로부터 3D 포인트의 깊이(depth) 정보를 직접 얻을 수 없기 때문에, 추정된 이동 벡터 <span class="math math-inline">t</span>의 실제 크기, 즉 스케일(scale)을 알 수 없는 <strong>스케일 모호성(scale ambiguity)</strong> 문제가 근본적으로 존재한다.25 결과적으로 단안 VO가 추정한 궤적은 실제 궤적과 형태는 동일하지만 크기가 임의의 비율로 축소되거나 확대된 ‘닮음’ 관계에 놓이게 된다.</p>
</li>
<li>
<p><strong>스테레오 VO:</strong> 일정한 간격으로 고정된 두 개의 동기화된 카메라를 사용한다. 인간의 양안 시차와 같은 삼각측량(triangulation) 원리를 이용하여, 두 이미지에서의 특징점 위치 차이(disparity)로부터 3D 포인트의 깊이를 직접 계산할 수 있다.18 따라서 스케일 모호성 문제가 발생하지 않으며, 실제 단위의 움직임을 추정할 수 있다. 하지만 하드웨어 구성이 복잡하고 비용이 더 높다는 단점이 있다.</p>
</li>
</ul>
<h3>2.2  Simultaneous Localization and Mapping (SLAM): 동시적 위치 추정 및 지도 작성</h3>
<p>SLAM은 VO의 개념을 확장하여, 단순히 프레임 간의 상대적 움직임을 누적하는 것을 넘어 전역적으로 일관성 있는 지도와 궤적을 생성하는 것을 목표로 한다. 그 본질은 “지도가 있어야 현재 위치를 정확히 알 수 있고, 현재 위치를 정확히 알아야 올바른 지도를 그릴 수 있다“는 상호 의존적인, 소위 ’닭과 달걀’의 문제를 동시에 해결하려는 시도이다.29 이는 로봇이 미지의 환경을 탐험하면서 자신의 위치를 지속적으로 추정하고, 동시에 관측한 내용을 바탕으로 환경 지도를 점진적으로 구축해나가는 과정이다.31</p>
<p>VO가 ‘항해’ 그 자체에 비유된다면, SLAM은 ‘항해하며 해도를 그리고, 그려진 해도를 보며 자신의 항로를 수정하는’ 복합적인 행위에 가깝다. 이 둘은 별개의 기술이 아니라 ’기억’의 유무로 구분되는 연속체 상에 있다. VO는 주로 프레임 간의 ‘단기적인’ 상대적 움직임 추정에 집중하며, 이는 SLAM 시스템의 프론트엔드(frontend)에서 핵심적인 역할을 담당한다.10 반면, SLAM은 여기에 ‘장기적인’ 기억과 회고의 기능을 더한다. SLAM의 핵심 기능 중 하나인 **루프 폐쇄(loop closure)**는 과거에 방문했던 장소를 다시 인식하고, 이를 통해 그동안 누적된 오차(drift)를 전역적으로 보정하는 과정이다.30 이 ’기억’을 통한 보정 능력이야말로 VO를 완전한 SLAM으로 만드는 결정적인 차이점이다.35</p>
<h4>2.2.1 확률적 표현</h4>
<p>센서 측정과 로봇의 움직임에는 항상 노이즈와 불확실성이 내재되어 있으므로, SLAM은 본질적으로 확률적 추정 문제로 다루어진다.36 베이즈 필터링(Bayesian filtering) 프레임워크를 기반으로, 과거부터 현재까지의 모든 관측 정보 <span class="math math-inline">Z_{0:k}</span>와 제어 입력 <span class="math math-inline">U_{0:k}</span>가 주어졌을 때, 현재 로봇의 자세 <span class="math math-inline">x_k</span>와 지도 <span class="math math-inline">m</span>의 결합 사후 확률 분포(joint posterior probability distribution) <span class="math math-inline">P(x_k, m \vert Z_{0:k}, U_{0:k})</span>를 추정하는 것이 SLAM의 최종 목표이다.37</p>
<h4>2.2.2 필터 기반 SLAM</h4>
<p>필터 기반 SLAM은 새로운 관측이 들어올 때마다 상태 추정치를 순차적으로 갱신하는 온라인(on-line) 방식이다.</p>
<ul>
<li><strong>확장 칼만 필터 (EKF-SLAM):</strong> 로봇의 움직임과 센서 관측 모델이 비선형적이기 때문에, 이를 선형화하여 칼만 필터를 적용한 방식이다.30 상태(로봇 자세와 모든 랜드마크의 위치)와 그 불확실성을 하나의 거대한 평균 벡터와 공분산 행렬, 즉 단일 가우시안 분포로 모델링한다. 초기 SLAM 연구를 주도했으나, 랜드마크의 수가</li>
</ul>
<p><span class="math math-inline">N</span>개일 때 공분산 행렬의 크기가 <span class="math math-inline">(3+2N) \times (3+2N)</span>에 비례하여 갱신에 필요한 계산량이 <span class="math math-inline">O(N^2)</span>으로 증가하는 심각한 확장성 문제를 안고 있다.39</p>
<ul>
<li><strong>파티클 필터 (FastSLAM):</strong> 다수의 가중치를 가진 샘플, 즉 파티클(particle)을 사용하여 복잡한 비선형/비가우시안 확률 분포를 근사하는 방법이다.39 각 파티클은 로봇의 가능한 궤적 가설 하나를 나타내며, 각 파티클마다 독립적인 저차원 EKF들을 이용해 랜드마크의 위치를 개별적으로 추정한다. 이를 통해 SLAM 문제를 로봇 위치 추정 문제와 다수의 랜드마크 위치 추정 문제로 분리(Rao-Blackwellization)하여 EKF-SLAM의 계산 복잡도 문제를 완화했다.</li>
</ul>
<h4>2.2.3 최적화 기반 SLAM</h4>
<p>최적화 기반 SLAM은 일정 시간 동안 수집된 모든 데이터(관측 및 움직임)를 한 번에 고려하여 전체 궤적과 지도를 최적화하는 스무딩(smoothing) 또는 배치(batch) 방식이다.</p>
<ul>
<li>
<p><strong>그래프 최적화 (Graph Optimization):</strong> SLAM 문제를 하나의 거대한 그래프로 모델링한다.33 그래프의 **노드(node)**는 각 시간 스텝에서의 로봇 자세나 랜드마크의 위치를 나타낸다. **엣지(edge)**는 두 노드 간의 관계, 즉 로봇의 움직임(odometry)이나 센서 관측(measurement)으로 인한 공간적 제약(constraint)을 나타낸다. 로봇의 움직임과 센서 측정에는 오차가 있기 때문에, 이 제약들은 완벽하게 만족되지 않는다. 그래프 최적화의 목표는 모든 엣지(제약)에 의한 오차의 총합을 최소화하는 노드들의 상태(위치 및 자세)를 찾는 것이며, 이는 비선형 최소제곱법(non-linear least squares) 문제로 귀결된다.36</p>
</li>
<li>
<p><strong>번들 조정 (Bundle Adjustment):</strong> 시각 SLAM(vSLAM)에서 사용되는 그래프 최적화의 일종으로, 3차원 랜드마크의 위치와 모든 시간 스텝에서의 카메라 자세를 동시에 최적화하여, 3D 랜드마크를 각 카메라 이미지 평면에 다시 투영했을 때의 오차, 즉 **재투영 오차(reprojection error)**를 최소화하는 방법이다.30 가장 정확한 결과를 제공하지만 계산 비용이 매우 높다.</p>
</li>
</ul>
<p>SLAM 기술의 발전 과정은 ’정확성’과 ‘계산 효율성’ 사이의 트레이드오프 변화를 명확히 보여준다. 초기 EKF-SLAM은 실시간 온라인 처리에 적합했지만, 선형화 오차와 계산 복잡도로 인해 대규모 환경에서의 정확성과 확장성에 한계를 보였다.39 이후 등장한 그래프 기반 최적화는 모든 과거 정보를 활용하여 비선형 최적화를 수행하므로 훨씬 더 정확한 결과를 제공하지만, 본질적으로 배치 처리 방식이라 초기에는 실시간 적용이 어려웠다.35 그러나 컴퓨팅 파워의 발전과 더불어, SLAM 문제에서 발생하는 정보 행렬이 매우 희소(sparse)하다는 구조적 특성을 활용한 효율적인 최적화 기법(예: Cholesky 분해)이 개발되면서 그래프 SLAM도 실시간 처리가 가능해졌다.42 이로 인해, 과거에는 양립하기 어려웠던 ’높은 정확성’과 ’실시간성’을 동시에 달성하는 방향으로 SLAM 기술의 패러다임이 전환되었으며, 현재 대부분의 최신 SLAM 시스템(예: ORB-SLAM)은 그래프 최적화를 백엔드(backend)의 핵심 기술로 채택하고 있다.33</p>
<h2>3.  비전 기반 항법의 주요 방법론 분석</h2>
<p>VBN을 구현하는 접근 방식은 크게 특징점 기반, 외형 기반, 딥러닝 기반의 세 가지 패러다임으로 나눌 수 있다. 이 방법론들의 진화 과정은 이미지 정보를 ’어떻게 추상화하여 사용할 것인가’에 대한 관점의 변화로 요약될 수 있다. 초기에는 해석 가능하고 기하학적으로 명확한 ’점’으로 이미지를 추상화했고, 이후에는 장면의 전반적인 느낌을 담은 ’통계적 분포’로, 최근에는 데이터로부터 스스로 표현을 학습하는 ’종단간(End-to-End) 모델’로 발전해왔다.</p>
<h3>3.1  특징점 기반(Feature-Based) 항법</h3>
<p>이 방법은 이미지를 소수의 기하학적으로 안정적인 점(point)과 선(line)으로 추상화하여, 물리 세계의 3D 구조를 명시적으로 모델링하려는 시도이다.</p>
<ul>
<li>
<p><strong>핵심 원리:</strong> 이미지에서 조명, 시점, 스케일 변화에도 불구하고 안정적으로 검출되는 독특한 지점(특징점, keypoint)을 찾는다. 이후 각 특징점 주변의 픽셀 패턴을 고유한 수학적 벡터(기술자, descriptor)로 표현한다.43 연속된 이미지 프레임 간에 이 기술자들의 유사도를 비교하여 동일한 3D 지점에 해당하는 2D 특징점들의 대응 관계를 찾고, 이를 통해 카메라의 움직임을 추정한다.43</p>
</li>
<li>
<p><strong>주요 알고리즘:</strong> SIFT, SURF, ORB는 가장 대표적인 특징점 검출 및 기술 알고리즘이다. 이들의 성능과 특성은 VBN 시스템의 실시간성과 정확도를 결정하는 중요한 요소이다. (표 3 참조)</p>
</li>
<li>
<p><strong>강인성 확보 전략:</strong> 단순히 특정 알고리즘에 의존하는 것을 넘어, 시스템의 강인성을 높이기 위한 추가적인 전략이 사용된다. 예를 들어, RANSAC(Random Sample Consensus) 알고리즘은 다수의 특징점 대응 쌍 중에서 기하학적 제약(예: 에피폴라 제약)을 만족하는 정상적인 대응(inlier) 그룹을 찾아내고, 잘못된 매칭(outlier)을 효과적으로 제거하는 데 널리 사용된다.19 또한, 조명 변화에 본질적으로 강인한 Gabor 필터와 같은 도구를 기반으로 새로운 기술자를 설계하려는 연구도 진행되었다.47</p>
</li>
</ul>
<p><strong>표 3: 주요 특징점 검출기 성능 비교 (SIFT vs. SURF vs. ORB)</strong></p>
<table><thead><tr><th>알고리즘</th><th>불변성: 스케일</th><th>불변성: 회전</th><th>불변성: 조명</th><th>속도</th><th>정확도</th><th>라이선스</th></tr></thead><tbody>
<tr><td><strong>SIFT</strong></td><td>매우 높음</td><td>매우 높음</td><td>높음</td><td>느림</td><td>매우 높음</td><td>특허 만료</td></tr>
<tr><td><strong>SURF</strong></td><td>높음</td><td>높음</td><td>중간</td><td>중간</td><td>높음</td><td>특허 만료</td></tr>
<tr><td><strong>ORB</strong></td><td>제한적</td><td>높음</td><td>낮음</td><td>매우 빠름</td><td>중간</td><td>오픈 소스</td></tr>
</tbody></table>
<h3>3.2  외형 기반(Appearance-Based) 항법</h3>
<p>이 방법은 개별 특징점의 정확한 기하학적 위치보다는, 이미지 전체 또는 특정 영역의 ’외형(appearance)’이 담고 있는 전반적인 정보를 활용한다. 이는 이미지를 기하학적 구조 대신 ’시각적 단어’의 통계적 분포로 추상화하는 접근이다.</p>
<ul>
<li>
<p><strong>핵심 원리:</strong> 이미지를 하나의 고차원 벡터 공간에 있는 점으로 간주한다.49 두 이미지가 시각적으로 얼마나 유사한지를 이 고차원 공간에서의 거리로 측정하여 위치를 인식하거나(place recognition) 루프 폐쇄를 검출한다.50</p>
</li>
<li>
<p><strong>이미지 검색 기법:</strong></p>
</li>
<li>
<p><strong>Bag of Visual Words (BoVW):</strong> 대규모 이미지 데이터베이스에서 추출한 수많은 지역 기술자(local descriptor, 예: SIFT)들을 k-means와 같은 클러스터링 알고리즘을 사용하여 대표적인 기술자 그룹, 즉 ‘시각적 단어(visual word)’ 사전을 구축한다.53 이후, 각 이미지는 이 사전에 있는 시각적 단어들이 몇 번 나타나는지를 세어 만든 빈도수 히스토그램으로 표현된다.56 이 히스토그램 벡터 간의 유사도를 비교하여 두 이미지가 같은 장소를 촬영했는지 판단한다.</p>
</li>
<li>
<p><strong>VLAD (Vector of Locally Aggregated Descriptors):</strong> BoVW와 유사하게 클러스터링을 통해 시각적 단어(클러스터 중심)를 정의하지만, 단순히 빈도수를 세는 대신 더 풍부한 정보를 담는다. 이미지 내의 각 지역 기술자에 대해, 가장 가까운 시각적 단어와의 차이(residual) 벡터를 계산하고, 동일한 시각적 단어에 속하는 모든 차이 벡터들을 합산한다. 이 합산된 벡터들을 모두 이어 붙여 이미지 전체를 표현하는 하나의 고차원 벡터를 생성한다.57 NetVLAD는 이 전체 과정을 딥러닝 네트워크로 구현하여 종단간 학습이 가능하도록 만든 것이다.57</p>
</li>
</ul>
<h3>3.3  딥러닝 기반(Deep Learning-Based) 항법</h3>
<p>딥러닝 기반 항법은 인간이 설계한 명시적인 특징 추출 및 모델링 과정을 데이터 기반의 학습으로 대체한다. 이는 앞선 두 방법론의 중간 추상화 과정을 모두 생략하고, 원시 픽셀 데이터로부터 최종 목표(항법 명령)에 필요한 잠재 표현(latent representation)을 신경망이 스스로 학습하게 하는 종단간(End-to-End) 학습 패러다임을 지향한다.</p>
<ul>
<li>
<p><strong>End-to-End 학습:</strong> 전통적인 VBN 파이프라인의 여러 단계를(특징 추출, 매칭, 자세 추정, 제어) 하나의 거대한 신경망으로 통합한다. 이 네트워크는 원시 카메라 이미지를 입력으로 받아 조향각이나 속도와 같은 최종적인 제어 명령을 직접 출력하도록 학습된다.11 이는 각 모듈을 개별적으로 최적화할 때 발생하는 정보 손실이나 오차 누적 문제를 피하고, 전체 시스템의 성능을 한 번에 최적화하여 더 나은 결과를 기대할 수 있게 한다.60</p>
</li>
<li>
<p><strong>CNN-RNN 아키텍처:</strong> 이 구조는 시각 정보의 공간적 특성과 시간적 연속성을 모두 모델링하는 데 효과적이다. CNN(Convolutional Neural Network)은 각 이미지 프레임에서 계층적인 공간적 특징(예: 엣지, 질감, 객체 일부)을 추출하는 역할을 한다.61 이 추출된 특징 벡터는 순차적으로 RNN(Recurrent Neural Network) 또는 그 변형인 LSTM(Long Short-Term Memory)에 입력되어, 연속된 프레임 간의 시간적 관계나 움직임 패턴을 학습한다.62</p>
</li>
<li>
<p><strong>심층 강화 학습 (Deep Reinforcement Learning, DRL):</strong> 로봇이 명시적인 정답 데이터 없이 환경과의 상호작용(시행착오)을 통해 스스로 최적의 항법 정책(policy)을 학습하게 하는 방법이다.63 에이전트(로봇)는 현재 상태(카메라 이미지)를 관찰하고 행동을 선택한다. 그 행동의 결과로 환경으로부터 보상(reward) 신호(예: 목표 지점에 가까워지면 긍정적 보상, 장애물에 충돌하면 부정적 보상)를 받게 되며, 누적 보상을 최대화하는 방향으로 신경망으로 표현된 정책을 업데이트한다.67 이는 지도나 사전 경로 정보 없이도 복잡하고 동적인 환경에서 목표 지향적인 행동을 학습할 수 있는 강력한 프레임워크를 제공한다.</p>
</li>
</ul>
<p>현대의 VBN 시스템은 이 세 가지 방법론이 서로 배타적으로 사용되기보다는 계층적으로 결합되는 추세를 보인다. 예를 들어, 딥러닝으로 학습된 특징점 검출기(예: SuperPoint)는 전통적인 검출기보다 더 강인한 특징점을 제공하여 특징점 기반 SLAM의 성능을 향상시킨다. 딥러닝 기반의 전역 기술자인 NetVLAD는 외형 기반 장소 인식의 정확도를 극적으로 높였다.57 따라서 많은 최신 SLAM 시스템은 프론트엔드에서는 특징점 기반 VO를 사용하고, 루프 폐쇄 감지에서는 딥러닝 기반의 외형 기술자를 활용하며, 백엔드에서는 그래프 최적화를 수행하는 하이브리드 아키텍처를 채택한다. 이는 각 방법론이 서로 다른 문제(단기 추적 vs. 장기 인식)에 대해 고유한 강점을 가지며, 이들을 지능적으로 통합하는 것이 가장 강인하고 효율적인 시스템을 구축하는 길임을 시사한다.</p>
<p><strong>표 2: VBN 방법론 비교 (특징점 기반 vs. 외형 기반 vs. 딥러닝 기반)</strong></p>
<table><thead><tr><th>방법론</th><th>핵심 원리</th><th>주요 알고리즘/모델</th><th>강인성 프로파일</th><th>계산 비용</th><th>데이터 의존성</th></tr></thead><tbody>
<tr><td><strong>특징점 기반</strong></td><td>이미지 내 불변의 기하학적 특징점 검출 및 매칭</td><td>SIFT, SURF, ORB, FAST</td><td>시점/스케일 변화에 강함, 조명 변화에 다소 민감</td><td>중간 (ORB는 낮음)</td><td>낮음 (사전 학습 불필요)</td></tr>
<tr><td><strong>외형 기반</strong></td><td>이미지 전체/부분의 외형적 표현을 벡터화하여 비교</td><td>Bag of Visual Words, VLAD, NetVLAD</td><td>조명/계절 변화에 강함, 시점 변화에 다소 민감</td><td>높음 (사전 구축/학습)</td><td>중간 (단어 사전/모델 학습 필요)</td></tr>
<tr><td><strong>딥러닝 기반</strong></td><td>원시 픽셀에서 항법 명령까지 End-to-End 학습</td><td>CNN, RNN/LSTM, DRL (DQN, PPO)</td><td>학습 데이터에 따라 가변적, 일반화 성능이 중요</td><td>매우 높음 (학습), 낮음~높음 (추론)</td><td>매우 높음 (대규모 데이터셋 필수)</td></tr>
</tbody></table>
<h2>4.  VBN 시스템의 설계와 구현</h2>
<p>강인하고 신뢰성 있는 VBN 시스템을 구축하기 위해서는 알고리즘뿐만 아니라 하드웨어 구성요소, 센서 융합 전략, 그리고 전체 소프트웨어 아키텍처에 대한 깊은 이해가 필수적이다.</p>
<h3>4.1  핵심 하드웨어 구성요소</h3>
<p>VBN 시스템의 성능은 사용되는 하드웨어의 특성에 의해 근본적으로 결정된다. 이는 단순히 소프트웨어 알고리즘을 선택하는 문제를 넘어, 목표 애플리케이션(예: 저전력 소형 드론 vs. 고정밀 산업용 로봇)에 맞는 하드웨어 플랫폼을 전략적으로 선택하고 그에 최적화된 알고리즘을 개발하는 통합적인 과정임을 의미한다.</p>
<ul>
<li>
<p><strong>카메라 센서:</strong> VBN 시스템의 ‘눈’ 역할을 하는 핵심 부품으로, 종류에 따라 획득하는 정보와 시스템의 특성이 크게 달라진다.7</p>
</li>
<li>
<p><strong>단안(Monocular):</strong> 저렴하고 소형화에 유리하지만, 깊이 정보를 직접 얻을 수 없어 스케일 모호성 문제가 발생한다. 이 문제를 해결하기 위해 IMU 융합(VIO)이나 특정 기동(S-curve)과 같은 추가적인 기법이 요구된다.7</p>
</li>
<li>
<p><strong>스테레오(Stereo):</strong> 두 개의 카메라를 이용해 깊이 추정이 가능하므로 스케일 정확도 확보에 유리하다. 하지만 카메라 간 캘리브레이션이 중요하며, 시스템의 크기와 비용이 증가한다.8</p>
</li>
<li>
<p><strong>RGB-D:</strong> 컬러 이미지(RGB)와 함께 픽셀별 깊이(Depth) 정보를 직접 제공한다. 구조광이나 ToF(Time-of-Flight) 방식을 사용하며, 실내 환경에 적합하다. 그러나 측정 가능 거리가 제한적이고, 야외의 강한 햇빛(적외선) 아래에서는 성능이 저하될 수 있다.7</p>
</li>
<li>
<p><strong>어안(Fisheye):</strong> 180도 이상의 매우 넓은 화각(Field of View)을 제공하여, 적은 수의 카메라로도 주변 환경 전체를 파악하거나 장애물을 회피하는 데 유리하다. 다만, 이미지 왜곡이 심해 보정 과정이 필수적이다.13</p>
</li>
<li>
<p><strong>뉴로모픽(Neuromorphic)/이벤트(Event) 카메라:</strong> 기존 카메라와 달리, 각 픽셀이 독립적으로 밝기 변화를 감지할 때만 비동기적으로 신호(이벤트)를 발생시킨다. 이로 인해 마이크로초 단위의 시간 해상도, 매우 높은 동적 범위(HDR), 낮은 전력 소모, 모션 블러 없음 등의 장점을 가진다. 고속으로 움직이는 물체를 추적하거나 조명이 급격하게 변하는 환경에서 기존 카메라의 한계를 극복할 수 있다.72 이 센서의 등장은 기존의 프레임 기반 알고리즘을 근본적으로 바꿔야 하는, 하드웨어-소프트웨어 공동 설계(co-design)의 중요성을 부각시킨다.</p>
</li>
<li>
<p><strong>관성 측정 장치(IMU):</strong> 3축 가속도계와 3축 자이로스코프로 구성되어, 이동체의 선형 가속도와 각속도를 측정한다.14 VBN의 단점을 보완하고 스케일 모호성을 해결하는 데 핵심적인 역할을 한다.</p>
</li>
<li>
<p><strong>처리 장치(Processing Unit):</strong> VBN 알고리즘, 특히 딥러닝 기반 방법론은 상당한 계산량을 요구하므로 효율적인 처리를 위한 고성능 하드웨어가 필수적이다. GPU(Graphics Processing Unit)는 병렬 연산에 특화되어 있어 딥러닝 모델 추론과 이미지 처리를 가속하는 데 널리 사용된다.13 드론과 같은 소형 임베디드 시스템에서는 전력 효율이 중요한 ARM 기반 CPU나 특정 연산에 최적화된 ASIC(주문형 반도체)이 활용되기도 한다.76</p>
</li>
</ul>
<h3>4.2  센서 융합: Visual-Inertial Odometry (VIO)</h3>
<p>VIO는 시각 센서(카메라)와 관성 센서(IMU)의 측정값을 융합하여 이동체의 자세를 추정하는 기술이다. 카메라는 조명이 어둡거나, 텍스처가 없거나, 빠르게 움직일 때 특징점 추적에 실패하기 쉽다. 반면, IMU는 외부 환경과 무관하게 고주파수로 움직임을 측정할 수 있지만 시간이 지남에 따라 오차가 누적된다. VIO는 이 두 센서의 상호 보완적인 특성을 활용하여 강인성과 정확성을 극대화한다.14 VIO에서의 ‘결합(Coupling)’ 수준은 정확성과 시스템 복잡성 및 강인성 간의 근본적인 트레이드오프를 나타낸다.</p>
<ul>
<li>
<p><strong>느슨한 결합(Loosely Coupled):</strong> 비전 시스템(VO)과 IMU 전파 모듈이 각각 독립적으로 자세를 추정한 뒤, 그 ’결과물’을 칼만 필터와 같은 별도의 필터에서 융합하는 방식이다.15 구현이 비교적 간단하고 모듈화되어 있어 한쪽 센서에 문제가 생겨도 다른 쪽이 독립적으로 작동할 수 있다는 장점이 있다. 하지만 각 센서의 원시 정보에 담긴 상관관계를 완전히 활용하지 못해 최적의 정확도를 달성하기는 어렵다.</p>
</li>
<li>
<p><strong>강한 결합(Tightly Coupled):</strong> 카메라의 원시 측정값(예: 픽셀 좌표)과 IMU의 원시 측정값(가속도, 각속도)을 하나의 통합된 상태 추정 문제 안에서 함께 처리하는 방식이다.15 센서 간의 상호작용과 시간적 오차까지 모델링하여 최적화하므로 이론적으로 가장 정확한 해를 제공한다. 하지만 시스템이 고도로 통합되어 있어 수학적 모델링과 구현이 훨씬 복잡하며, 한 부분의 작은 오류가 전체 시스템에 영향을 미칠 수 있다.</p>
</li>
</ul>
<p>VIO 구현에는 칼만 필터 기반의 융합 기법이 널리 사용된다. 특히 비선형 시스템에 적용 가능한 EKF(확장 칼만 필터)나, 여러 시점의 카메라 제약을 동시에 활용하여 정확도를 높인 MSCKF(Multi-State Constraint Kalman Filter)가 대표적이다.78 이 필터들은 IMU 측정값을 이용해 상태를 예측(predict)하고, 새로운 카메라 이미지가 들어오면 시각적 관측을 통해 예측된 상태를 보정(update)하는 과정을 반복한다.</p>
<h3>4.3  소프트웨어 아키텍처와 데이터 흐름</h3>
<p>대부분의 현대 VBN/SLAM 시스템은 기능적으로 프론트엔드와 백엔드로 나뉘는 2단계 아키텍처를 채택한다.5</p>
<ul>
<li>
<p><strong>프론트엔드(Frontend):</strong> 센서로부터 들어오는 원시 데이터를 직접 처리하는 부분으로, 실시간성이 매우 중요하다. 주요 역할은 다음과 같다.</p>
</li>
<li>
<p>이미지에서 특징점을 추출하고 기술한다.</p>
</li>
<li>
<p>연속된 프레임 간 특징점을 추적하여 상대적인 카메라 움직임(VO)을 추정한다.</p>
</li>
<li>
<p>지도 작성을 위해 중요한 프레임, 즉 키프레임(keyframe)을 선별한다.</p>
</li>
<li>
<p><strong>백엔드(Backend):</strong> 프론트엔드에서 생성된 정보를 바탕으로 전역적인 일관성을 최적화하는 부분이다. 프론트엔드보다 낮은 빈도로 실행될 수 있으며, 더 많은 계산 자원을 사용한다.</p>
</li>
<li>
<p><strong>루프 폐쇄 검출(Loop Closure Detection):</strong> 현재 위치가 과거에 방문했던 곳인지를 판단한다.</p>
</li>
<li>
<p><strong>전역 최적화(Global Optimization):</strong> 루프 폐쇄가 감지되면, 누적된 오차를 전체 궤적과 지도에 걸쳐 분배하고 보정한다. 주로 그래프 최적화(번들 조정) 기법이 사용된다.</p>
</li>
<li>
<p><strong>지도 생성(Mapping):</strong> 최적화된 결과를 바탕으로 최종적인 지도를 생성한다.</p>
</li>
</ul>
<p>이러한 구조에서 데이터는 일반적으로 **[카메라/IMU 데이터 입력] → [프론트엔드: VO/상대 자세 추정, 키프레임 생성] → [백엔드: 루프 폐쇄 검출, 그래프 최적화] → [최종 지도 및 자세 출력]**의 흐름을 따른다.33</p>
<h2>5.  핵심 난제와 첨단 해결책</h2>
<p>VBN 기술은 상당한 발전을 이루었지만, 실제 환경에 강인하게 적용되기 위해서는 여전히 해결해야 할 핵심적인 난제들이 존재한다. 이러한 난제들은 대부분 VBN 시스템이 의존하는 핵심적인 가정들이 현실 세계의 복잡성 앞에서 붕괴될 때 발생한다. 문제 해결의 방향은 단순한 기하학적 계산을 넘어, 외부 지식(물리 법칙, 객체 의미, 시간적 변화 패턴)을 통합하여 더 높은 수준의 ’추론’을 수행하는 방향으로 진화하고 있다.</p>
<h3>5.1  단안 SLAM의 스케일 모호성(Scale Ambiguity) 문제 해결 방안</h3>
<ul>
<li>
<p><strong>문제 정의:</strong> ’단일 시점에서는 깊이를 알 수 없다’는 기하학적 한계에서 비롯된다. 단안 카메라는 3D 세계를 2D 이미지 평면에 투영하는 과정에서 절대적인 거리나 크기 정보를 상실한다. 이로 인해 단안 VO/SLAM이 재구성한 궤적과 지도는 실제 세계와 크기(스케일)가 다른 닮음 변환 관계에 놓이게 된다.26 또한, 작은 오차가 누적되면서 스케일이 시간에 따라 변하는 ‘스케일 드리프트’ 현상도 발생하여 궤적의 일관성을 해친다.82</p>
</li>
<li>
<p><strong>해결 방안:</strong></p>
</li>
<li>
<p><strong>센서 융합 (VIO):</strong> 가장 일반적이고 효과적인 해결책이다. IMU의 가속도계는 중력 가속도를 측정할 수 있는데, 이는 크기가 <span class="math math-inline">9.8 m/s^2</span>으로 알려진 절대적인 물리량이다. 시스템은 이 중력 벡터를 ’추론’하여 움직임의 절대 스케일을 복원할 수 있다.26</p>
</li>
<li>
<p><strong>알려진 크기의 객체 활용:</strong> 환경 내에 신용카드, A4 용지, 사람의 키 등 크기를 미리 아는 객체를 인식하고, 이미지상에서의 크기와 실제 크기를 비교하여 현재 프레임의 스케일을 결정한다.26</p>
</li>
<li>
<p><strong>3D 모델 활용:</strong> 3D 건물 모델이나 지리 정보 시스템(GIS) 데이터와 같이 절대 스케일을 가진 외부 지도와 현재 이미지를 매칭하여 스케일을 복원한다.1</p>
</li>
<li>
<p><strong>움직임 제약 활용:</strong> 지상을 이동하는 로봇의 경우, 카메라의 지면으로부터의 높이가 일정하다는 가정을 통해 스케일을 추정할 수 있다.</p>
</li>
</ul>
<h3>5.2  동적 환경에서의 강인성 확보</h3>
<ul>
<li>
<p><strong>문제 정의:</strong> ’세상은 정적이다’라는 대부분의 SLAM 시스템의 기본 가정이 붕괴되는 상황이다. 실제 환경에는 사람, 차량, 동물 등 움직이는 객체들이 존재한다. 이러한 동적 객체 위에 있는 특징점들은 정적인 배경과 달리 프레임 간에 독립적으로 움직이므로, 카메라의 움직임을 계산할 때 잘못된 정보를 제공하여 SLAM 시스템의 위치 추정 정확도를 심각하게 저하시키고 지도를 오염시킨다.83</p>
</li>
<li>
<p><strong>해결 방안:</strong></p>
</li>
<li>
<p><strong>기하학적 방법:</strong> 에피폴라 제약과 같은 기하학적 모델을 설정하고, RANSAC을 통해 이 모델을 따르지 않는 대다수의 특징점(outlier)들을 동적 객체에 속한 것으로 간주하고 제거한다. 이 방법은 별도의 사전 지식 없이 적용 가능하지만, 움직임이 크지 않거나 카메라 움직임과 유사한 방향으로 움직이는 객체를 걸러내지 못할 수 있다.83</p>
</li>
<li>
<p><strong>딥러닝 기반 객체 탐지:</strong> ’사람’이나 ’자동차’는 움직일 가능성이 높다는 의미론적 지식을 시스템에 부여하는 방식이다. YOLO, FastestDet과 같은 딥러닝 객체 탐지 모델을 사용하여 이미지 내에서 동적일 가능성이 높은 객체들의 경계 상자(bounding box)를 식별하고, 해당 영역 내의 모든 특징점을 계산에서 제외한다.83</p>
</li>
<li>
<p><strong>의미론적 분할(Semantic Segmentation):</strong> 객체 탐지보다 더 정교한 접근법으로, 이미지의 모든 픽셀을 ‘사람’, ‘자동차’, ‘건물’, ‘도로’ 등 미리 정의된 클래스로 분류한다. 이를 통해 동적 객체의 정확한 픽셀 단위 마스크를 생성하고, 해당 영역을 SLAM 계산에서 완전히 배제할 수 있다.83</p>
</li>
</ul>
<h3>5.3  장기 운용을 위한 외형 변화 대응 기술</h3>
<ul>
<li>
<p><strong>문제 정의:</strong> ’장소의 외형은 변하지 않는다’는 가정이 깨지는 상황이다. 로봇이 장기간 또는 반복적으로 동일한 장소를 운용할 때, 계절의 변화(여름의 녹음 vs. 겨울의 눈), 날씨, 시간대(낮 vs. 밤)에 따른 조명 조건의 극심한 변화, 그리고 가구 배치 변경과 같은 환경의 구조적 변화로 인해 장소의 외형(appearance)이 크게 달라질 수 있다.86 이는 과거에 저장된 지도 데이터와 현재의 관측 데이터 간의 매칭을 어렵게 만들어, 루프 폐쇄나 재위치 추정(re-localization)을 실패하게 만드는 주요 원인이 된다.</p>
</li>
<li>
<p><strong>해결 방안:</strong></p>
</li>
<li>
<p><strong>조명 변화에 강인한 특징 기술자:</strong> 전통적인 컴퓨터 비전 접근법에서는 조명 변화에 덜 민감하도록 수학적으로 설계된 특징 기술자(예: Gabor 필터 기반 기술자)를 사용하여 외형 변화의 영향을 줄이려 시도한다.47</p>
</li>
<li>
<p><strong>딥러닝 기반 불변 표현 학습:</strong> 딥러닝 모델을 사용하여 다양한 조명, 계절, 시점 변화에도 불구하고 장소의 본질적인 구조적 특징을 나타내는 불변 표현(invariant representation)을 학습한다. 이는 이미지 검색 기반의 위치 인식(Visual Place Recognition, VPR) 기술의 핵심으로, 외형이 크게 변하더라도 같은 장소임을 인식할 수 있게 해준다.57</p>
</li>
<li>
<p><strong>다중 경험 지도(Multi-experience Maps):</strong> 같은 장소에 대해 여러 다른 시간대나 조건(예: 맑은 날, 비 오는 날, 밤)에서 얻은 정보를 모두 지도에 저장한다. 이후 위치를 추정할 때, 현재의 조건과 가장 유사한 과거의 경험 데이터를 선택적으로 사용하여 매칭 성공률을 높인다.</p>
</li>
</ul>
<h2>6.  비전 기반 항법의 미래와 전망</h2>
<p>VBN 기술은 하드웨어, 알고리즘, 인공지능 모델의 융합을 통해 빠르게 발전하고 있으며, 미래 자율 시스템의 핵심 역량을 정의할 몇 가지 중요한 기술 동향이 부상하고 있다. VBN의 미래는 단순히 수집된 데이터를 학습하는 ‘데이터 중심(Data-centric)’ 접근을 넘어, 현실 세계를 이해하고 생성할 수 있는 정교한 ’월드 모델(World Model)’을 구축하고, 이 모델과 실제 센서 데이터를 융합하여 추론하고 계획하는 새로운 패러다임으로 나아갈 것이다.</p>
<h3>6.1  뉴로모픽 비전: 이벤트 기반 카메라가 가져올 혁신</h3>
<p>이벤트 카메라는 인간의 시신경을 모방한 뉴로모픽 센서로, 기존 프레임 기반 카메라의 근본적인 한계를 극복할 잠재력을 가지고 있다. 프레임 단위로 모든 픽셀 정보를 전송하는 대신, 각 픽셀의 밝기 변화가 감지될 때만 비동기적으로 데이터를 생성한다.74 이 방식은 다음과 같은 혁신적인 장점을 제공한다.</p>
<ul>
<li>
<p><strong>초고속 감지:</strong> 마이크로초 단위의 시간 해상도로 매우 빠른 움직임도 모션 블러 없이 포착할 수 있다.</p>
</li>
<li>
<p><strong>높은 동적 범위(HDR):</strong> 매우 어두운 곳과 밝은 곳이 공존하는 장면에서도 정보 손실 없이 데이터를 수집할 수 있다.</p>
</li>
<li>
<p><strong>저전력, 저대역폭:</strong> 변화가 있는 부분의 정보만 전송하므로 데이터 양이 적고 전력 소모가 매우 낮다.</p>
</li>
</ul>
<p>이러한 특성 덕분에 이벤트 카메라는 고속으로 비행하는 드론, 급격한 조명 변화가 발생하는 환경에서의 자율주행차 등 기존 VBN/SLAM 기술이 어려움을 겪는 시나리오에서 획기적인 성능 개선을 가져올 수 있다.72 현재는 이벤트 데이터를 효율적으로 처리하기 위한 새로운 알고리즘, 즉 이벤트 기반 SLAM(Event-based SLAM) 개발이 활발히 연구 중이며, 이는 VBN의 새로운 패러다임을 열 것으로 기대된다.75</p>
<h3>6.2  생성형 AI와 시뮬레이션을 통한 데이터 증강 및 훈련</h3>
<p>딥러닝 기반 VBN 모델의 성능은 학습 데이터의 양과 질에 크게 좌우된다. 그러나 현실 세계에서 모든 가능한 시나리오, 특히 사고나 위험 상황에 대한 데이터를 수집하는 것은 거의 불가능하며 막대한 비용과 시간이 소요된다.</p>
<ul>
<li>
<p><strong>생성형 AI 기반 데이터 증강:</strong> 생성형 AI(Generative AI) 기술을 활용하여 실제와 같은 합성 훈련 데이터를 대량으로 생성할 수 있다.92 이를 통해 다양한 날씨(비, 눈, 안개), 조명(낮, 밤, 황혼), 환경 조건을 시뮬레이션하여 모델이 어떠한 상황에서도 강인하게 작동하도록 훈련시킬 수 있다.</p>
</li>
<li>
<p><strong>Sim-to-Real 전이 학습:</strong> CARLA, AirSim, Isaac Sim과 같은 사실적인 시뮬레이터 내에서 강화학습 에이전트를 수백만 번의 시행착오를 통해 안전하고 효율적으로 훈련시킨 후, 학습된 정책을 실제 로봇에 이전(transfer)하여 미세 조정하는 Sim-to-Real 접근 방식이 더욱 중요해질 것이다.65</p>
</li>
</ul>
<h3>6.3  차세대 자율 시스템을 위한 기술 동향</h3>
<ul>
<li>
<p><strong>3D 내비게이션 및 고밀도 3D 인식:</strong> 전통적인 2D 점유 격자 지도나 희소한 특징점 지도를 넘어, 현실 세계를 사실적으로 표현하는 고밀도 3D 지도를 활용한 항법 기술이 부상하고 있다.94 특히 NeRF(Neural Radiance Fields)와 같은 신경망 기반의 암시적 3D 표현(implicit representation) 기술은 소수의 이미지만으로도 사진과 같은 품질의 3D 환경을 모델링하고 새로운 시점의 이미지를 렌더링할 수 있어, VBN을 위한 강력한 지도 생성 및 위치 추정 도구로 활용될 잠재력이 크다.92</p>
</li>
<li>
<p><strong>딥러닝과 센서 융합의 고도화:</strong> 미래의 자율 시스템은 카메라, 라이다, 레이더, IMU 등 다양한 센서로부터의 데이터를 딥러닝 모델을 통해 초기 단계에서부터 지능적으로 융합(early fusion)할 것이다. 이는 각 센서 데이터의 특징을 개별적으로 추출하여 나중에 결합하는 방식(late fusion)보다 더 풍부하고 강인한 통합 환경 표현을 학습하게 하여, 전반적인 인지 및 항법 성능을 향상시킬 수 있다.96</p>
</li>
<li>
<p><strong>대규모 언어 모델(LLM) 및 비전-언어 모델(VLM)의 통합:</strong> “저기 보이는 빨간색 교회 건물 앞에서 좌회전해“와 같은 인간의 자연어 명령을 로봇이 이해하고, 시각적 환경 정보와 연결하여 항법을 수행하는 비전-언어 항법(Vision-Language Navigation, VLN)이 차세대 인간-로봇 상호작용의 핵심 기술로 주목받고 있다.63 이는 로봇이 단순히 기하학적 경로를 따르는 것을 넘어, 인간과 같이 의미론적이고 맥락적인 이해를 바탕으로 작업을 수행할 수 있게 함을 의미한다.</p>
</li>
</ul>
<h2>7.  결론</h2>
<p>비전 기반 항법(VBN)은 지난 수십 년간 눈부신 발전을 거듭하며, GPS와 같은 전통적인 항법 시스템의 보조 수단을 넘어 자율 시스템의 독립적이고 핵심적인 ‘인지’ 능력으로 진화했다. 특징점 기반의 기하학적 접근에서 출발하여, 외형 기반의 통계적 인식, 그리고 딥러닝 기반의 종단간 학습에 이르기까지, VBN은 이미지로부터 의미 있는 정보를 추출하고 활용하는 방식에 있어 패러다임의 전환을 겪어왔다.</p>
<p>현재 VBN 기술은 Visual Odometry와 SLAM을 통해 미지의 환경에서도 자신의 위치를 추정하고 지도를 작성할 수 있는 수준에 도달했다. 그러나 동적 환경, 극심한 외형 변화, 단안 카메라의 스케일 모호성과 같은 현실 세계의 복잡성은 여전히 중요한 도전 과제로 남아있다. 이러한 문제들을 해결하기 위한 노력은 딥러닝 기반의 의미론적 이해, 강인한 표현 학습, 그리고 다중 센서 융합 기술의 발전을 촉진하고 있다.</p>
<p>미래의 VBN은 이벤트 카메라와 같은 차세대 센서, 생성형 AI를 통한 무한한 데이터 생성, 그리고 NeRF와 같은 새로운 3D 표현 모델과 결합하여 그 한계를 더욱 확장할 것이다. 기술의 궁극적인 지향점은 단순히 ’어디에 있는가’를 아는 것을 넘어, ’무엇을 보고, 어떻게 행동해야 하는가’를 인간과 같이 이해하고 추론하는 고도의 지능을 구현하는 것이다. 하드웨어, 알고리즘, 그리고 AI 모델의 심층적인 융합은 이러한 비전을 현실로 만들고, 자율주행차, 서비스 로봇, UAM 등 미래 사회를 이끌어갈 자율 시스템의 보편화를 앞당기는 핵심 동력이 될 것이다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>Advanced Air Mobility를 위한 영상 기반 위치 추정 및 Geo-Referencing 기술 동향, <a href="https://ettrends.etri.re.kr/ettrends/209/0905209001/001-009.%20%EC%B5%9C%EC%9D%98%ED%99%98_209%ED%98%B8%20%EC%B5%9C%EC%A2%85.pdf">https://ettrends.etri.re.kr/ettrends/209/0905209001/001-009.%20%EC%B5%9C%EC%9D%98%ED%99%98_209%ED%98%B8%20%EC%B5%9C%EC%A2%85.pdf</a></li>
<li>영상기반 항법시스템 개발 - CHOSUN, <a href="https://oak.chosun.ac.kr/bitstream/2020.oak/13148/2/%EC%98%81%EC%83%81%EA%B8%B0%EB%B0%98%20%ED%95%AD%EB%B2%95%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EA%B0%9C%EB%B0%9C.pdf">https://oak.chosun.ac.kr/bitstream/2020.oak/13148/2/%EC%98%81%EC%83%81%EA%B8%B0%EB%B0%98%20%ED%95%AD%EB%B2%95%EC%8B%9C%EC%8A%A4%ED%85%9C%20%EA%B0%9C%EB%B0%9C.pdf</a></li>
<li>[ 메릭 웨비나 ] 항공 로봇(드론)의 자율 비행을 위한 비전 기반 항법 시스템 - YouTube, https://www.youtube.com/watch?v=0nRIWRs8FqY</li>
<li>공중항법 - 아이파일럿, <a href="https://ipilot.co.kr/wp-content/uploads/2018/08/PILOT02_%E1%84%8C%E1%85%A9%E1%84%8C%E1%85%A9%E1%86%BC%E1%84%89%E1%85%A1_%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%92%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8_2017%E1%84%82%E1%85%A7%E1%86%AB.pdf">https://ipilot.co.kr/wp-content/uploads/2018/08/PILOT02_%E1%84%8C%E1%85%A9%E1%84%8C%E1%85%A9%E1%86%BC%E1%84%89%E1%85%A1_%E1%84%80%E1%85%A9%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%BC%E1%84%92%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8_2017%E1%84%82%E1%85%A7%E1%86%AB.pdf</a></li>
<li>Vision Navigation Part 1: Overview of Vision Navigation - AlphaPixel Software Development, https://alphapixeldev.com/vision-navigation-part-1-overview-of-vision-navigation/</li>
<li>Vision-based Approach and Landing System (VALS) - NASA Technology Transfer Program, https://technology.nasa.gov/patent/TOP2-322</li>
<li>Full article: A survey on vision-based UAV navigation - Taylor &amp; Francis Online, https://www.tandfonline.com/doi/full/10.1080/10095020.2017.1420509</li>
<li>Vision-based Localization: A Guide to VBL Techniques for GPS-denied Environments, https://encord.com/blog/vision-based-localization-a-guide-to-vbl-technique/</li>
<li>Vision-Based Navigation for Urban Air Mobility: A Survey - ResearchGate, https://www.researchgate.net/publication/376618069_Vision-Based_Navigation_for_Urban_Air_Mobility_A_Survey</li>
<li>Visual odometry - Wikipedia, https://en.wikipedia.org/wiki/Visual_odometry</li>
<li>[1604.07316] End to End Learning for Self-Driving Cars - arXiv, https://arxiv.org/abs/1604.07316</li>
<li>End-to-End Deep Learning for Self-Driving Cars | NVIDIA Technical Blog, https://developer.nvidia.com/blog/deep-learning-self-driving-cars/</li>
<li>Vision-Based Navigation Techniques for Unmanned Aerial Vehicles …, https://www.mdpi.com/2504-446X/7/2/89</li>
<li>[1708.06652] Build your own visual-inertial odometry aided cost-effective and open-source autonomous drone - ar5iv, https://ar5iv.labs.arxiv.org/html/1708.06652</li>
<li>Modeling Varying Camera-IMU Time Offset in Optimization-Based Visual-Inertial Odometry - CVF Open Access, https://openaccess.thecvf.com/content_ECCV_2018/papers/Yonggen_Ling_Modeling_Varying_Camera-IMU_ECCV_2018_paper.pdf</li>
<li>Lecture 13 Visual Inertial Fusion, https://rpg.ifi.uzh.ch/docs/teaching/2018/13_visual_inertial_fusion_advanced.pdf</li>
<li>Visual-Inertial Odometry Using Synthetic Data - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/fusion/ug/visual-inertial-odometry-using-synthetic-data.html</li>
<li>Stereo-visual-odometry - GitHub Pages, https://cgarg92.github.io/Stereo-visual-odometry/</li>
<li>Visual Odometry: A practical guide | by Rushideshmukh - Medium, https://medium.com/@rushideshmukh23/visual-odometry-a-practical-guide-71ddff1687cd</li>
<li>Mini-project: A visual odometry pipeline! - Robotics and Perception Group, https://rpg.ifi.uzh.ch/docs/teaching/2017/vo_project_statement.pdf</li>
<li>Visual Odometry Using the KITTI Vision Dataset - Abhishek Sankar, https://abhishek-sankar.com/visual-odometry/</li>
<li>Epipolar geometry in practice: 3D runtime odometry with a web-camera - PEARL, https://pearl.plymouth.ac.uk/cgi/viewcontent.cgi?article=3150&amp;context=secam-research</li>
<li>From Epipolar Geometry to PnP: A Practical Guide to Visual Odometry | by Ko - Medium, https://medium.com/@koko0915/from-epipolar-geometry-to-pnp-a-practical-guide-to-visual-odometry-d918b732025c</li>
<li>Introduction to Epipolar Geometry and Stereo Vision - LearnOpenCV, https://learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/</li>
<li>Monocular visual odometry 구현 (OpenCV) - 감자보이로그 - 티스토리, https://voilier-bsc.tistory.com/7</li>
<li>Solving the scale ambiguity in monocular SLAM : r/computervision - Reddit, https://www.reddit.com/r/computervision/comments/3zovt9/solving_the_scale_ambiguity_in_monocular_slam/</li>
<li>Scale Drift-Aware Large Scale Monocular SLAM - Robotics, https://www.roboticsproceedings.org/rss06/p10.pdf</li>
<li>Visual Odmetry from scratch - A tutorial for beginners - Avi Singh’s, https://avisingh599.github.io/vision/visual-odometry-full/</li>
<li>The definitive guide to SLAM &amp; mobile mapping - NavVis, https://www.navvis.com/technology/slam</li>
<li>SLAM의 이해와 구현 PART 01 요약, https://roytravel.tistory.com/405</li>
<li>A Review of Simultaneous Localization and Mapping Algorithms Based on Lidar - MDPI, https://www.mdpi.com/2032-6653/16/2/56</li>
<li>Simultaneous localization and mapping - Wikipedia, https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping</li>
<li>What is SLAM? A Beginner to Expert Guide - Kodifly, https://kodifly.com/what-is-slam-a-beginner-to-expert-guide</li>
<li>What Is SLAM (Simultaneous Localization and Mapping)? - MATLAB &amp; Simulink, https://www.mathworks.com/discovery/slam.html</li>
<li>SLAM-past-present-future.pdf - cs.wisc.edu, https://pages.cs.wisc.edu/~jphanna/teaching/25spring_cs639/resources/SLAM-past-present-future.pdf</li>
<li>(PDF) A tutorial on graph-based SLAM - ResearchGate, https://www.researchgate.net/publication/231575337_A_tutorial_on_graph-based_SLAM</li>
<li>Simultaneous Localisation and Mapping (SLAM): Part I The Essential Algorithms - People @EECS, https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/Durrant-Whyte_Bailey_SLAM-tutorial-I.pdf</li>
<li>A Random Finite Set Approach to Bayesian SLAM - Ba-Ngu Vo, https://ba-ngu.vo-au.com/vo/MVAV_SLAM11.pdf</li>
<li>EKF SLAM vs. FastSLAM – A Comparison - Infoscience, https://infoscience.epfl.ch/bitstreams/d5871034-a345-4220-9e46-293f9e66e02a/download</li>
<li>Enhanced Unscented Kalman Filter-Based SLAM in Dynamic Environments: Euclidean Approach - arXiv, https://arxiv.org/html/2312.12204v1</li>
<li>Graph SLAM: From Theory to Implementation - Federico Sarrocco, https://federicosarrocco.com/blog/graph-slam-tutorial</li>
<li>Graph SLAM - Washington, <a href="https://courses.cs.washington.edu/courses/cse571/23sp/slides/L09/Lecture09_Modern%20SLAM.pdf">https://courses.cs.washington.edu/courses/cse571/23sp/slides/L09/Lecture09_Modern%20SLAM.pdf</a></li>
<li>Performance and Trade-off Evaluation of SIFT, SURF, FAST, STAR and ORB feature detection algorithms in Visual Odometry - DergiPark, https://dergipark.org.tr/tr/download/article-file/1375257</li>
<li>ORB Feature Detection Boosts Visual SLAM Performance - Ceva, https://www.ceva-ip.com/blog/oriented-fast-and-rotated-brief-orb-feature-detection-speeds-up-visual-slam/</li>
<li>(PDF) A comparative analysis of SIFT, SURF, KAZE, AKAZE, ORB, and BRISK, https://www.researchgate.net/publication/323561586_A_comparative_analysis_of_SIFT_SURF_KAZE_AKAZE_ORB_and_BRISK</li>
<li>Hybrid Visual Odometry Algorithm Using a Downward-Facing Monocular Camera - MDPI, https://www.mdpi.com/2076-3417/14/17/7732</li>
<li>A Local Image Descriptor Robust to Illumination Changes - ResearchGate, https://www.researchgate.net/publication/278713836_A_Local_Image_Descriptor_Robust_to_Illumination_Changes</li>
<li>A Local Image Descriptor Robust to Illumination Changes - Computer Vision Lab, https://cvl.tuwien.ac.at/wp-content/uploads/2014/12/scia13.pdf</li>
<li>Appearance-based Eye Gaze Estimation - Computer Vision and Robotics Laboratory, <a href="https://vision.ai.illinois.edu/html-files-to-import/publications/Appearance-based%20Eye%20Gaze%20Estimation.pdf">https://vision.ai.illinois.edu/html-files-to-import/publications/Appearance-based%20Eye%20Gaze%20Estimation.pdf</a></li>
<li>A Computer Vision-Based Navigation and Localization Method for Station-Moving Aircraft Transport Platform with Dual Cameras - PubMed Central, https://pmc.ncbi.nlm.nih.gov/articles/PMC6982710/</li>
<li>A Low-cost Vision Based Navigation System for Small Size Unmanned Aerial Vehicle Applications - Longdom Publishing, https://www.longdom.org/open-access/a-lowcost-vision-based-navigation-system-for-small-size-unmanned-aerial-vehicle-applications-7588.html</li>
<li>Image Appearance Exploration by Model-Based Navigation - TAU, https://www.cs.tau.ac.il/~dcor/articles/2009/Image-Appearance.pdf</li>
<li>Bag of Visual Words | Pinecone, https://www.pinecone.io/learn/series/image-search/bag-of-visual-words/</li>
<li>Bag of visual words | Computer Vision and Image Processing Class Notes - Fiveable, https://library.fiveable.me/computer-vision-and-image-processing/unit-5/bag-visual-words/study-guide/gKqlaeA7CzYBBncs</li>
<li>The bag of (visual) words model – PyImageSearch, https://customers.pyimagesearch.com/the-bag-of-visual-words-model/</li>
<li>Image Classification with Bag of Visual Words - MATLAB &amp; Simulink - MathWorks, https://www.mathworks.com/help/vision/ug/image-classification-with-bag-of-visual-words.html</li>
<li>Contextual Patch-NetVLAD: Context-Aware Patch Feature Descriptor and Patch Matching Mechanism for Visual Place Recognition - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC10857504/</li>
<li>SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition, https://openreview.net/forum?id=bZpZMdY1sj</li>
<li>End-to-end Learning for Autonomous Driving - NYU Computer Science, https://cs.nyu.edu/media/publications/zhang_jiakai.pdf</li>
<li>End to End Learning for Self-Driving Cars - NVIDIA, https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf</li>
<li>Deep learning architectures - IBM Developer, https://developer.ibm.com/articles/cc-machine-learning-deep-learning-architectures/</li>
<li>Figures depicting CNN and RNN schematic | Download Scientific Diagram - ResearchGate, https://www.researchgate.net/figure/Figures-depicting-CNN-and-RNN-schematic_fig2_343686910</li>
<li>Deep Learning for Embodied Vision Navigation: A Survey - ResearchGate, https://www.researchgate.net/publication/353790409_Deep_Learning_for_Embodied_Vision_Navigation_A_Survey</li>
<li>Deep Learning for Embodied Vision Navigation: A Survey - Semantic Scholar, https://www.semanticscholar.org/paper/Deep-Learning-for-Embodied-Vision-Navigation%3A-A-Zhu-Zhu/3ed715e6d0cda5897b71d03691cfeae2201bccc5</li>
<li>Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning - Robert Babuska, http://www.robertbabuska.com/pdf/Kulhanek2021VisualNavigationReal.pdf</li>
<li>Deep Reinforcement Learning of Mobile Robot Navigation in Dynamic Environment: A Review - MDPI, https://www.mdpi.com/1424-8220/25/11/3394</li>
<li>Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning, https://jkulhanek.com/robot-visual-navigation/</li>
<li>(PDF) Growing Robot Navigation Based on Deep Reinforcement Learning - ResearchGate, https://www.researchgate.net/publication/371816230_Growing_Robot_Navigation_Based_on_Deep_Reinforcement_Learning</li>
<li>Deep reinforcement learning-aided autonomous navigation with landmark generators - Frontiers, https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2023.1200214/full</li>
<li>The TinyV3RSE Hardware-in-the-Loop Vision-Based Navigation Facility - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC9740262/</li>
<li>NVIDIA Isaac ROS In-Depth: cuVSLAM and the DP3.1 Release - Intermodalics, https://www.intermodalics.ai/blog/nvidia-isaac-ros-in-depth-cuvslam-and-the-dp3-1-release</li>
<li>Real-Time Neuromorphic Navigation: Guiding Physical Robots with Event-Based Sensing and Task-Specific Reconfigurable Autonomy Stack - arXiv, https://arxiv.org/html/2503.09636v1</li>
<li>Our R&amp;D team is delving into Neuromorphic Cameras integration - FlySight, https://www.flysight.it/exploring-neuromorphic-cameras/</li>
<li>A Dataset for Visual Navigation with Neuromorphic Methods - Frontiers, https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2016.00049/full</li>
<li>Event-Based Visual Simultaneous Localization and Mapping (EVSLAM) Techniques: State of the Art and Future Directions - MDPI, https://www.mdpi.com/2224-2708/14/1/7</li>
<li>Low Latency Visual Inertial Odometry with On-Sensor Accelerated Optical Flow for Resource-Constrained UAVs - Research Collection, https://www.research-collection.ethz.ch/bitstreams/44b4e013-4feb-4523-939c-5e11c8c8a801/download</li>
<li>A Robust Multi-Stereo Visual-Inertial Odometry Pipeline, https://robots.et.byu.edu/jmangelson/pubs/2020/jaekel20iros.pdf</li>
<li>Visual-Inertial Odometry of Aerial Robots, https://rpg.ifi.uzh.ch/docs/Encyclopedia19VIO_Scaramuzza.pdf</li>
<li>Visual-Inertial Odometry for Autonomous Ground Vehicles - DiVA portal, http://www.diva-portal.org/smash/get/diva2:1155019/FULLTEXT01.pdf</li>
<li>Integrating GRU with a Kalman Filter to Enhance Visual Inertial Odometry Performance in Complex Environments - MDPI, https://www.mdpi.com/2226-4310/10/11/923</li>
<li>Understanding SLAM in Robotics and Autonomous Vehicles - Flyability, https://www.flyability.com/blog/simultaneous-localization-and-mapping</li>
<li>Solving Monocular Visual Odometry Scale Factor with Adaptive Step Length Estimates for Pedestrians Using Handheld Devices - MDPI, https://www.mdpi.com/1424-8220/19/4/953</li>
<li>Optimization of visual SLAM in dynamic environments using object detection - SPIE Digital Library, https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13486/134860J/Optimization-of-visual-SLAM-in-dynamic-environments-using-object-detection/10.1117/12.3055716.full</li>
<li>Optimization method to improve visual SLAM in dynamic environment, https://alife-robotics.co.jp/members2024/icarob/data/html/data/OS/OS20-3.pdf</li>
<li>A Multi-Strategy Visual SLAM System for Motion Blur Handling in Indoor Dynamic Environments - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11944682/</li>
<li>Appearance-based landmark selection for efficient long-term visual localization - YouTube, https://www.youtube.com/watch?v=JL_5zMEQKYc</li>
<li>Where Is Your Place, Visual Place Recognition? - IJCAI, https://www.ijcai.org/proceedings/2021/0603.pdf</li>
<li>A Review on Deep Learning for UAV Absolute Visual Localization - MDPI, https://www.mdpi.com/2504-446X/8/11/622</li>
<li>CVPR 2021 tutorial on Cross-view and Cross-modal Visual Geo-Localization - SRI, https://www.sri.com/research/information-computing-sciences/computer-vision/cvpr-2021-tutorial-on-cross-view-and-cross-modal-visual-geo-localization/</li>
<li>Event-based Simultaneous Localization and Mapping: A Comprehensive Survey, https://www.researchgate.net/publication/370126912_Event-based_Simultaneous_Localization_and_Mapping_A_Comprehensive_Survey</li>
<li>Event-based Simultaneous Localization and Mapping: A Comprehensive Survey - arXiv, https://arxiv.org/html/2304.09793v2</li>
<li>Vision-Based Navigation and Perception for Autonomous Robots: Sensors, SLAM, Control Strategies, and Cross-Domain Applications—A Review - MDPI, https://www.mdpi.com/2673-4117/6/7/153</li>
<li>OpenDriveLab/End-to-end-Autonomous-Driving - GitHub, https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving</li>
<li>Explore the Top 10 Navigation Technology Trends in 2024 - StartUs Insights, https://www.startus-insights.com/innovators-guide/navigation-technology-trends/</li>
<li>Renderable Neural Radiance Map for Visual Navigation - CVF Open Access, https://openaccess.thecvf.com/content/CVPR2023/papers/Kwon_Renderable_Neural_Radiance_Map_for_Visual_Navigation_CVPR_2023_paper.pdf</li>
<li>Sensor-Fusion Based Navigation for Autonomous Mobile Robot - PMC, https://pmc.ncbi.nlm.nih.gov/articles/PMC11861736/</li>
<li>Autonomous Navigation by Mobile Robot with Sensor Fusion Based on Deep Reinforcement Learning - MDPI, https://www.mdpi.com/1424-8220/24/12/3895</li>
<li>Sensor Fusion Techniques in Autonomous Vehicle Navigation: Delving into various methodologies and their effectiveness - Chaklader Asfak Arefe, https://chaklader.medium.com/sensor-fusion-techniques-in-autonomous-vehicle-navigation-delving-into-various-methodologies-and-c95acc67e3af</li>
<li>Designing an Autonomous Vehicle Using Sensor Fusion Based on Path Planning and Deep Learning Algorithms - SciELO South Africa, https://scielo.org.za/scielo.php?script=sci_arttext&amp;pid=S1991-16962024000300002</li>
<li>Deep Reinforcement Learning for Instruction Following Visual Navigation in 3D Maze-Like Environments - ResearchGate, https://www.researchgate.net/publication/338593177_Deep_Reinforcement_Learning_for_Instruction_Following_Visual_Navigation_in_3D_Maze-Like_Environments</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>