<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BIJUNG:FoundationPose (6D 객체 자세 추정을 위한 통합 파운데이션 모델)</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117607984-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-117607984-2');
    </script>

    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        },
        startup: {
          ready: () => {
            // pulldown-cmark 출력을 MathJax 형식으로 변환
            document.querySelectorAll('.math-inline').forEach(node => {
              node.outerHTML = '$' + node.innerText + '$';
            });
            document.querySelectorAll('.math-display').forEach(node => {
              node.outerHTML = '$$' + node.innerText + '$$';
            });
            MathJax.startup.defaultReady();
          }
        }
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.esm.mjs';
        let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'default';
        mermaid.initialize({ startOnLoad: false, theme: theme });
        
        document.addEventListener("DOMContentLoaded", function() {
            var mermaidBlocks = document.querySelectorAll("pre > code.language-mermaid");
            mermaidBlocks.forEach(function(block) {
                var pre = block.parentElement;
                var div = document.createElement("div");
                div.className = "mermaid";
                div.textContent = block.textContent.trim();
                pre.replaceWith(div);
            });
            mermaid.run({
                querySelector: '.mermaid'
            });
        });
    </script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            var toggler = document.getElementsByClassName("caret");
            for (var i = 0; i < toggler.length; i++) {
                toggler[i].addEventListener("click", function() {
                    this.parentElement.querySelector(".nested").classList.toggle("active");
                    this.classList.toggle("caret-down");
                });
            }
            
            // 활성 경로 자동 확장
            var activeLink = document.querySelector(".sidebar .active");
            if (activeLink) {
                var parents = [];
                var el = activeLink;
                while (el) {
                    if (el.classList && el.classList.contains("nested")) {
                        el.classList.add("active");
                        // 이 중첩된 목록의 캐럿 찾기
                        // 중첩된 목록은 캐럿이 있는 li 안에 있습니다
                        var li = el.parentElement;
                        if (li) {
                            var caret = li.querySelector(".caret");
                            if (caret) {
                                caret.classList.add("caret-down");
                            }
                        }
                    }
                    el = el.parentElement;
                    if (el && el.classList.contains("sidebar")) break;
                }
            }


        });
    </script>
    <link rel="stylesheet" href="../../style.css">
</head>
<body>
    <div class="container">
        <main class="content">
            <header>
                <div class="header-content">
                    <h1>FoundationPose (6D 객체 자세 추정을 위한 통합 파운데이션 모델)</h1>
                    <nav class="breadcrumbs"><a href="../../index.html">Home</a> / <a href="../index.html">위치 추정 (Localization)</a> / <a href="index.html">피두셜 마커 및 객체 인식에 의한 위치 추정</a> / <span>FoundationPose (6D 객체 자세 추정을 위한 통합 파운데이션 모델)</span></nav>
                </div>
            </header>
            <article>
                <h1>FoundationPose (6D 객체 자세 추정을 위한 통합 파운데이션 모델)</h1>
<h2>1.  6D 객체 자세 추정의 발전과 파운데이션 모델의 등장</h2>
<h3>1.1  6D 객체 자세 추정 문제의 정의</h3>
<p>6D 객체 자세 추정(6D Object Pose Estimation)은 컴퓨터 비전 및 로보틱스 분야의 핵심 문제로, 3차원 공간상에 존재하는 특정 강체(rigid object)의 위치와 방향을 정밀하게 파악하는 작업을 의미한다.1 여기서 ‘6D’ 또는 ‘6DoF’(Six Degrees of Freedom, 6자유도)는 객체의 자세를 완벽하게 기술하는 여섯 가지 요소를 지칭한다. 이는 3차원 위치를 나타내는 이동(translation) 벡터 (<span class="math math-inline">x,y,z</span> 좌표)와 3차원 방향을 나타내는 회전(rotation) 행렬(롤, 피치, 요 각도)로 구성된다.3 이 6D 자세 정보는 로봇이 객체를 물리적으로 조작하거나, 증강현실(AR) 환경에서 가상 객체를 실제 세계에 정합시키는 등, 객체와의 모든 물리적 또는 가상적 상호작용의 근간이 된다.</p>
<p>이 기술의 중요성은 다양한 첨단 응용 분야에서 명확하게 드러난다. 로보틱스 분야에서는 로봇이 부품을 정확하게 집어 옮기거나(bin-picking) 정밀 조립 작업을 수행하기 위해 필수적이다.3 증강현실(AR) 분야에서는 디지털 콘텐츠를 실제 객체 위에 흔들림 없이 정확하게 겹쳐 보이게 함으로써 몰입감 높은 경험을 제공하는 데 핵심적인 역할을 한다.3 또한, 자율주행 자동차는 주변 차량이나 장애물의 정확한 위치와 방향을 파악하여 안전한 주행 경로를 계획하는 데 이 기술을 활용한다.2 이처럼 6D 자세 추정은 가상과 현실의 상호작용을 매개하는 근본적인 기술이라 할 수 있다.6</p>
<h3>1.2  역사적 방법론과 내재적 한계</h3>
<p>6D 자세 추정 기술의 발전 과정은 크게 고전적 접근법과 딥러닝 기반 접근법으로 나눌 수 있다. 초기 연구들은 별도의 훈련 과정이 없는 고전적 기법에 의존했다. 대표적으로 SIFT(Scale-Invariant Feature Transform)와 같은 수작업 특징(hand-crafted feature)을 이미지에서 추출하고, 이를 3D 모델의 특징점과 매칭하여 2D-3D 대응 관계를 설정한 뒤 자세를 계산하는 방식이 있었다.5 다른 한편으로는, 3D 모델을 다양한 각도에서 렌더링한 템플릿 이미지들과 입력 이미지를 직접 비교하여 가장 유사한 템플릿의 자세를 추정하는 템플릿 매칭(template matching) 방식도 널리 사용되었다.5 이러한 고전적 방법들은 특정 조건에서는 효과적이었으나, 질감이 없는(texture-less) 객체, 다른 객체에 의해 일부가 가려지는 폐색(occlusion) 상황, 그리고 조명 변화가 심한 환경에서는 특징점 추출이나 템플릿 매칭의 정확도가 급격히 저하되는 명백한 한계를 보였다.5</p>
<p>이러한 한계를 극복하기 위해 딥러닝, 특히 합성곱 신경망(CNN, Convolutional Neural Networks)이 등장하면서 패러다임의 전환이 일어났다. PoseCNN과 같은 초기 딥러닝 모델들은 이미지로부터 자세를 직접 회귀(regression)하거나, 3D 모델의 특정 지점에 해당하는 2D 이미지상의 키포인트(keypoint)를 예측하는 방식을 채택했다.3 예측된 2D-3D 키포인트 대응 관계가 주어지면, PnP(Perspective-n-Point) 알고리즘을 통해 최종적인 6D 자세를 계산할 수 있었다.7 CNN 기반 방법들은 고전적 기법보다 훨씬 강인하고 정확한 성능을 보였지만, 새로운 문제를 야기했다. 대부분의 모델은 특정 객체 인스턴스(instance-level)나 특정 카테고리(category-level)에 대해 대규모의 라벨링된 데이터셋으로 훈련되어야만 했다. 이는 훈련 과정에서 보지 못한 새로운(novel) 객체에 대해서는 자세를 추정할 수 없는, 즉 일반화 성능이 부족한 근본적인 한계를 낳았다.</p>
<h3>1.3  일반화 문제의 대두: ‘새로운 객체’ 자세 추정</h3>
<p>딥러닝 모델의 일반화 한계는 실용적인 관점에서 심각한 병목 현상을 초래했다. 예를 들어, 수만 가지의 상품을 다루는 물류창고 자동화 시스템이나 일상생활의 모든 사물을 대상으로 하는 AR 애플리케이션에서 모든 객체에 대해 개별적인 모델을 훈련시키는 것은 현실적으로 불가능하다.7 이러한 배경에서 “훈련 데이터에 없었던 새로운 객체의 자세를 어떻게 추정할 것인가?“라는 ’새로운 객체 자세 추정(Novel Object Pose Estimation)’이라는 하위 연구 분야가 부상하게 되었다.</p>
<p>이 문제는 다시 테스트 시점에 어떤 정보가 주어지는지에 따라 두 가지 주요 설정으로 분화되었다 9:</p>
<ol>
<li><strong>모델 기반(Model-Based) 설정</strong>: 새로운 객체의 3D CAD 모델이 제공되는 경우. 이 모델은 객체의 정확한 기하학적 정보를 담고 있다.6</li>
<li><strong>모델 프리(Model-Free) 설정</strong>: 3D CAD 모델 대신, 객체를 여러 각도에서 촬영한 소수의 참조 이미지나 비디오가 제공되는 경우. 이 이미지들로부터 객체의 형태와 외형을 유추해야 한다.9</li>
</ol>
<p>이러한 문제 설정의 분화는 각각의 하위 작업에 특화된 전문화된 방법론들의 개발로 이어졌다. 모델 기반 접근법과 모델 프리 접근법은 서로 다른 파이프라인과 기술을 요구했으며, 이는 전체 6D 자세 추정 연구 분야와 실제 구현의 복잡성을 가중시키는 결과를 낳았다.9</p>
<h3>1.4  파운데이션 모델 패러다임과 FoundationPose의 핵심 명제</h3>
<p>이러한 파편화된 연구 지형에 새로운 해법을 제시한 것이 바로 ‘파운데이션 모델(Foundation Model)’ 패러다임이다. DINOv2, CLIP, ALIGN과 같은 거대 모델들은 방대하고 다양한 데이터셋으로 사전 훈련된 후, 별도의 미세 조정(fine-tuning) 없이도 다양한 하위 비전 작업에서 놀라운 일반화 성능을 보여주었다.6</p>
<p>FoundationPose는 이러한 파운데이션 모델의 개념을 6D 객체 자세 추정 문제에 직접적으로 적용한 시도이다. FoundationPose의 핵심 명제는, 테스트 시점에 어떠한 미세 조정이나 재훈련 없이도 새로운 객체에 대해 <strong>모델 기반 설정과 모델 프리 설정을 모두 지원</strong>하는 단일화된(unified) 파운데이션 모델을 구축하는 것이다.4 이는 단순히 성능을 조금 개선하는 것을 넘어, 기존에 분리되어 있던 새로운 객체 자세 추정의 여러 하위 문제들을 하나의 일관된 프레임워크 아래로 통합하려는 야심 찬 목표를 제시한다.</p>
<p>이러한 접근 방식은 6D 자세 추정 분야의 근본적인 질문을 “이 <em>특정</em> 객체의 자세를 어떻게 정확히 추정할까?“에서 “이전에 본 적 없는 <em>어떤</em> 객체든 그 자세를 정확히 추정할 수 있는 단일 시스템을 어떻게 만들까?“로 전환시킨다. 이처럼 특정성에 대한 해결에서 보편적 일반성으로의 패러다임 전환은 FoundationPose를 단순한 점진적 개선이 아닌, 개념적 도약으로 평가하게 만드는 핵심적인 배경이다. 또한, FoundationPose가 모델 기반과 모델 프리, 그리고 자세 추정과 추적(tracking)까지 통합하려는 설계 철학은, 최종 사용자가 입력 데이터의 형태(CAD 모델 또는 이미지)에 구애받지 않고 하나의 일관된 도구를 사용할 수 있도록 공학적 복잡성을 줄이는 데 우선순위를 두었음을 시사한다. 이는 기술의 실용적 채택에 있어 매우 중요한 고려사항이다.</p>
<hr />
<h2>2.  FoundationPose의 아키텍처 청사진</h2>
<p>FoundationPose의 강력한 성능과 유연성은 네 가지 핵심 기술 기둥의 시너지 효과에 기반한다. 이들 기술은 통일된 ‘렌더링 및 비교(render-and-compare)’ 파이프라인 내에서 유기적으로 작동하여, 전례 없는 일반화 능력과 통합적 기능을 구현한다.</p>
<h3>2.1  통합된 ‘렌더링 및 비교’ 파이프라인 개요</h3>
<p>FoundationPose의 전체적인 작동 방식은 ’가설 생성 - 정제 - 선택’의 3단계 전략을 따른다. 먼저, 객체 주변 공간에 균일하게 분포된 다수의 초기 포즈 가설(coarse global pose hypotheses)을 샘플링한다. 이 가설들은 ’포즈 정제 네트워크(pose refinement network)’로 전달되어 더 정확한 위치로 조정된다. 마지막으로, ’포즈 선택 네트워크(pose selection module)’가 정제된 모든 포즈 후보들의 점수를 매기고, 가장 높은 점수를 받은 포즈를 최종 결과물로 출력한다.4 이 파이프라인 자체는 기존 연구에서도 볼 수 있지만, FoundationPose의 혁신은 파이프라인의 입력, 즉 비교 대상을 생성하는 방식에 있다.</p>
<h3>2.2  신경망의 다리: 암시적 표현을 통한 모달리티 통합</h3>
<p>가장 큰 기술적 난제 중 하나는 근본적으로 다른 두 데이터 유형, 즉 기하학적 정보를 담은 3D CAD 모델(모델 기반)과 희소한 2D 참조 이미지 세트(모델 프리)를 어떻게 하나의 파이프라인에서 처리할 것인가이다. 전통적인 방식이라면 각 설정에 맞는 별도의 처리 로직이 필요했을 것이다.</p>
<p>FoundationPose는 이 간극을 ’객체 중심 신경장(object-centric neural field)’이라는 독창적인 방식으로 해결한다. 구체적으로, 이 신경장은 객체의 표면을 부호화된 거리 함수(SDF, Signed Distance Function)로 표현하는 신경 암시적 표현(neural implicit representation) 기술을 사용한다.10</p>
<ul>
<li><strong>메커니즘</strong>: 이 신경장은 두 개의 함수로 객체를 표현한다. 첫째, 기하학 함수 Ω:x–&gt;&gt;s는 3D 공간상의 한 점 x∈R3을 입력받아 그 점이 객체 표면으로부터 얼마나 떨어져 있는지를 나타내는 부호화된 거리 값 <span class="math math-inline">s \in \mathbb{R}</span>를 출력한다. 둘째, 외형 함수 Φ:(fΩ(x),n,d)–&gt;&gt;c는 기하학 네트워크의 중간 피처 벡터 fΩ(x), 표면의 법선 벡터 n∈R3, 그리고 바라보는 방향 d∈R3을 입력받아 해당 지점의 색상 c∈R3를 출력한다.10</li>
<li><strong>통합의 원리</strong>:</li>
<li><strong>모델 기반 설정</strong>에서는 제공된 CAD 모델로부터 이 신경장을 직접적으로 유도할 수 있다.</li>
<li><strong>모델 프리 설정</strong>에서는 약 16개 정도의 소수 참조 이미지를 사용하여 이 신경망을 최적화함으로써, 이미지들로부터 객체의 암시적 3D 표현을 학습한다.9</li>
</ul>
<p>이 과정의 가장 우아한 지점은, 입력 소스(CAD 또는 이미지)에 관계없이 결과적으로 생성된 신경장은 어떤 가상의 포즈에서든 객체의 새로운 RGBD 뷰를 효율적으로 렌더링할 수 있다는 점이다.16 이렇게 렌더링된 뷰는 후속 단계인 정제 및 선택 네트워크에 일관된 형식의 입력을 제공한다. 결과적으로, 하위 모듈들은 초기 입력이 CAD 모델이었는지 참조 이미지였는지에 대해 전혀 신경 쓸 필요가 없게 되어(invariant), 전체 프레임워크가 완벽하게 통합된다.4 이 신경장은 서로 다른 데이터 모달리티를 하나의 표준화된, 렌더링 가능한 표현으로 변환하는 ‘범용 어댑터’ 역할을 수행하며, 이는 유연한 AI 시스템을 구축하는 강력한 설계 패턴을 보여준다.</p>
<h3>2.3  설계에 의한 일반화: LLM 기반 합성 데이터 엔진</h3>
<p>미세 조정 없이 새로운 객체에 대한 강력한 일반화 성능을 달성하기 위해서는, 모델이 극도로 방대하고 다양한 데이터셋으로 훈련되어야 한다. 이러한 규모의 실제 데이터를 수집하는 것은 현실적으로 불가능하며, 개인정보 보호와 같은 윤리적 문제를 야기할 수 있다.15 FoundationPose는 이 문제를 정면으로 돌파하기 위해 독자적인 합성 데이터 생성 파이프라인을 구축했다.</p>
<p>이 파이프라인은 NVIDIA Isaac Sim 시뮬레이터 내에서 구축되었으며 15, 그 핵심은 다음과 같다.</p>
<ul>
<li><strong>기하학적 다양성 확보</strong>: Objaverse 및 Google Scanned Objects(GSO)와 같은 대규모 3D 모델 데이터베이스로부터 42,000개 이상의 다양한 3D 에셋을 활용하여 기하학적 형태의 다양성을 극대화했다.15</li>
<li><strong>LLM을 통한 외형 다양성 창출</strong>: 이것이 FoundationPose의 핵심 혁신 중 하나이다. 수작업으로 수백만 개의 다양하고 사실적인 텍스처를 제작하는 것은 엄청난 병목이다. FoundationPose는 이 ‘창의적’ 과정을 자동화하기 위해 거대 언어 모델(LLM)을 도입했다. LLM은 “긁힌 페인트”, “헤어라인 마감된 알루미늄”, “꽃무늬가 있는 유광 세라믹“과 같이 무수히 많은 다양한 텍스처를 묘사하는 텍스트 프롬프트를 생성한다. 이 프롬프트들은 다시 확산 모델(diffusion model)에 입력되어 사실적인 텍스처 이미지를 생성하고, 이 텍스처들이 3D 에셋에 적용된다.9 이 파이프라인은 현실 세계의 시각적 복잡성에 대응할 수 있는 끝없는 외형 변주를 자동으로 만들어내며, 이는 모델의 최종 일반화 성능에 직접적인 영향을 미친다.</li>
<li><strong>사실적 렌더링</strong>: Isaac Sim은 다양한 조명, 배경, 폐색 환경 등을 포함하는 대규모 영역 무작위화(domain randomization)와 함께 고품질의 사실적인 장면을 렌더링한다.14</li>
</ul>
<p>최종적으로 678,000개의 기본 이미지로 구성된 데이터셋이 생성되었으며, 증강을 통해 총 540만 개의 훈련 샘플로 확장되었다.15 모든 데이터가 순수하게 합성되었기 때문에 개인 식별 정보를 포함하지 않아 윤리적 문제로부터 자유롭다.15</p>
<h3>2.4  계층적 포즈 평가: 트랜스포머 기반 정제 및 선택</h3>
<p>초기 포즈 가설들을 평가하고 최적의 포즈를 선택하는 과정은 트랜스포머 아키텍처를 통해 계층적으로 수행된다.</p>
<ul>
<li><strong>포즈 정제 네트워크</strong>: 이 모듈은 초기 포즈 가설들을 입력받아 반복적으로 정제하여 더 정확한 포즈로 업데이트한다. 회전과 이동 업데이트를 분리된 표현으로 다루어 안정성을 높였다.16</li>
<li><strong>포즈 선택 네트워크 (Scorer)</strong>: 정제된 포즈 후보 리스트 중에서 단 하나의 최적 포즈를 선택하는 것이 이 네트워크의 역할이다. 이 네트워크는 포즈 평가가 개별적인 절대 평가가 아닌, 후보군 전체의 맥락 속에서 이루어지는 상대 평가라는 깊은 이해를 바탕으로 설계되었다.</li>
<li><strong>아키텍처</strong>: 새로운 트랜스포머 기반 아키텍처를 채택했다.9</li>
<li><strong>프로세스</strong>: 각 정제된 포즈 가설에 대해, 관찰된 이미지와의 정렬 상태를 인코딩하는 피처 임베딩이 추출된다. 이 임베딩들은 하나의 시퀀스 <span class="math math-inline">F = [F_0,..., F_{K-1}]</span>로 취급되어 트랜스포머에 입력된다.10</li>
<li><strong>계층적 비교</strong>: 트랜스포머의 셀프 어텐션(self-attention) 메커니즘은 각 포즈 임베딩이 시퀀스 내의 다른 모든 포즈들과 상호 비교되도록 한다. 이를 통해 네트워크는 각 포즈를 독립적으로 평가하는 대신, 전체 후보군 내에서의 상대적인 우수성을 평가하는 ’계층적 비교’를 수행할 수 있다.10 예를 들어, 시각적으로 유사한 여러 개의 좋은 포즈 후보들이 있을 때, 그 그룹 내에서 가장 뛰어난 하나를 선택하는, 보다 강인한 전략을 학습하게 된다.</li>
<li><strong>순열 불변성</strong>: 중요한 설계 결정으로, 입력 시퀀스에 위치 인코딩(positional encoding)을 적용하지 않았다. 이로 인해 스코어링 네트워크는 포즈 가설들의 초기 입력 순서에 영향을 받지 않게 되어(permutation-agnostic), 보다 일반적이고 강인한 평가가 가능해진다.10</li>
</ul>
<h3>2.5  차이를 학습하기: 대조 검증</h3>
<p>포즈 선택 네트워크가 좋은 포즈와 나쁜 포즈를 효과적으로 구별하도록 훈련시키기 위해, 대조 학습(contrastive learning) 기반의 손실 함수가 제안되었다.</p>
<ul>
<li><strong>포즈 조건부 삼중항 손실</strong>: FoundationPose는 다음과 같은 형태의 삼중항 손실(triplet loss)을 사용한다: <span class="math math-inline">L(i+,i−)=\max(S(i−)−S(i+)+\alpha,0)</span>.10 이 손실 함수의 목표는 ‘긍정적’(정답에 가까운) 포즈 샘플 <span class="math math-inline">i_+</span>의 점수 <span class="math math-inline">S(i_+)</span>가 ‘부정적’(정답에서 먼) 포즈 샘플 <span class="math math-inline">i_-</span>의 점수 <span class="math math-inline">S(i_-)</span>보다 최소한 여유(margin) <span class="math math-inline">\alpha</span>만큼 더 높도록 만드는 것이다.</li>
<li><strong>핵심 뉘앙스</strong>: 이는 표준적인 삼중항 손실과는 미묘한 차이가 있다. 각 포즈 가설의 이동(translation) 값에 따라 입력 이미지의 크롭(crop) 영역이 달라지기 때문에, 긍정적 샘플과 부정적 샘플이 공유하는 ‘앵커(anchor)’ 샘플이 존재하지 않는다. 또한, 두 포즈가 모두 정답과 매우 멀리 떨어져 있는 경우처럼 모호한 비교를 통해 네트워크가 혼란을 겪는 것을 방지하기 위해 훈련 쌍을 필터링한다. 오직 긍정적 샘플 <span class="math math-inline">i_+</span>이 정답에 충분히 가까운 쌍들만 훈련에 사용하여, 학습 신호가 의미 있도록 보장한다.10</li>
</ul>
<hr />
<h2>3.  실증적 검증 및 성능 분석</h2>
<p>FoundationPose의 우수성은 표준화된 벤치마크에서의 정량적, 정성적 평가를 통해 입증된다. 특히, 이 모델은 기존의 전문화된 방법론들을 압도하는 성능을 보여주며 새로운 기술 표준을 제시했다.</p>
<h3>3.1  평가의 기준: BOP 벤치마크</h3>
<p>BOP(Benchmark for 6D Object Pose Estimation)는 6D 객체 자세 추정 방법론의 성능을 평가하기 위한 학계 및 산업계의 표준 벤치마크이다.18 BOP는 폐색, 질감 없는 객체, 다양한 조명 조건 등 현실적인 시나리오를 다루는 여러 데이터셋, 통일된 데이터 형식, 그리고 표준화된 평가 지표와 툴킷을 제공한다.18</p>
<p>FoundationPose는 이 BOP 벤치마크 리더보드에서 <strong>모델 기반 새로운 객체 자세 추정 부문 1위</strong>를 차지하며, 그 기술적 우월성을 공식적으로 인정받았다.4 이는 FoundationPose가 현시점 최첨단(SOTA, State-Of-The-Art) 기술임을 명백히 보여주는 핵심 지표이다.</p>
<h3>3.2  정량적 성능 분석</h3>
<p>FoundationPose는 여러 핵심 BOP 데이터셋에서 RGBD(컬러 및 깊이) 이미지를 사용하여 평가되었다. 주된 평가 지표는 올바르게 추정된 포즈의 비율을 나타내는 평균 재현율(AR, Average Recall)이다.</p>
<p>주요 정량적 결과는 다음과 같다 10:</p>
<ul>
<li><strong>YCB-Video</strong>: 88.0% AR</li>
<li><strong>Occluded-LINEMOD (LM-O)</strong>: 83.0% AR</li>
<li><strong>T-LESS</strong>: 78.8% AR</li>
<li><strong>위 3개 데이터셋 평균</strong>: 83.3% AR</li>
</ul>
<p>이러한 수치는 이전에 각 작업에 특화되어 개발된 기존의 방법론들을 큰 차이로 능가하는 결과이다.4 이는 통합된 단일 모델이 여러 전문화된 모델들의 성능을 뛰어넘을 수 있음을 보여주는 강력한 증거이다. 범용 모델이 유연성을 위해 성능을 희생해야 한다는 일반적인 통념을 뒤집는 결과로, 방대한 합성 데이터로 학습된 일반화 능력과 강인한 통합 아키텍처가 기존 모델들의 수작업 튜닝이나 전문화보다 더 강력한 성능을 이끌어낼 수 있음을 시사한다.</p>
<h4>3.2.1 표 1: BOP 벤치마크에서의 FoundationPose 성능 비교 (RGBD)</h4>
<p>다음 표는 FoundationPose와 주요 경쟁 모델들의 성능을 표준 데이터셋에서 정량적으로 비교하여, SOTA 성능 주장을 뒷받침한다.</p>
<table><thead><tr><th>방법론</th><th>YCB-Video (AR)</th><th>Occluded-LINEMOD (AR)</th><th>T-LESS (AR)</th><th>평균 (AR)</th><th>출처</th><th></th><th></th></tr></thead><tbody>
<tr><td><strong>FoundationPose</strong></td><td><strong>88.0%</strong></td><td><strong>83.0%</strong></td><td><strong>78.8%</strong></td><td><strong>83.3%</strong></td><td></td><td>10</td><td></td></tr>
<tr><td>SurfEmb + ICP</td><td>-</td><td>-</td><td>-</td><td>79.7%</td><td>10</td><td></td><td></td></tr>
<tr><td>MegaPose-RGBD</td><td>-</td><td>-</td><td>-</td><td>58.6%</td><td>10</td><td></td><td></td></tr>
<tr><td>GCPose</td><td>-</td><td>65.2%</td><td>-</td><td>-</td><td>10</td><td></td><td></td></tr>
<tr><td>OVE6D</td><td>-</td><td>49.6%</td><td>-</td><td>-</td><td>10</td><td></td><td></td></tr>
<tr><td>OSOP + ICP</td><td>57.2%</td><td>-</td><td>-</td><td>-</td><td>10</td><td></td><td></td></tr>
</tbody></table>
<h3>3.3  정성적 평가</h3>
<p>수치적 성능을 넘어, YCB-Video와 같이 복잡하고 어수선한 장면을 포함하는 데이터셋에서의 시각적 결과는 FoundationPose가 생성하는 포즈 추정의 높은 품질을 보여준다. 추정된 포즈는 실제 정답(ground truth)과 시각적으로 거의 구별하기 어려울 정도로 정확하다.4</p>
<p>또한, FoundationPose가 “감소된 가정에도 불구하고 인스턴스 수준 방법론과 비교할 만한 결과“를 달성했다는 점은 매우 중요한 성과이다.4 인스턴스 수준 방법론은 테스트할 특정 객체에 대해 광범위하게 사전 훈련되므로 상당한 이점을 가진다. 이러한 조건 없이, 처음 보는 객체에 대해 즉시 작동하는 제로샷(zero-shot) 추정기가 인스턴스 수준의 성능에 근접했다는 것은, 일반화된 모델이 인스턴스별 훈련이 불가능한 실제 환경에 실용적으로 배포될 수 있는 가능성을 크게 높였음을 의미한다. 즉, 일반화를 위해 치러야 했던 성능 저하의 비용을 극적으로 낮춘 것이다.</p>
<hr />
<h2>4.  새로운 객체 자세 추정 기술 지형에서의 비교 분석</h2>
<p>FoundationPose의 혁신성을 더 깊이 이해하기 위해서는, 이를 새로운 객체 자세 추정 분야의 다른 주요 접근법들과 비교하여 그 철학과 방법론의 차이를 명확히 할 필요가 있다.</p>
<h3>4.1  FoundationPose 대 SfM 기반 재구성 (예: OnePose)</h3>
<ul>
<li><strong>OnePose의 접근법</strong>: 명시적인 ‘재구성 후 매칭(reconstruct-then-match)’ 파이프라인을 따른다. 새로운 객체에 대해 비디오 스캔을 요구하며, 고전적인 SfM(Structure-from-Motion) 기술을 사용하여 객체의 희소 3D 포인트 클라우드를 명시적으로 구축한다. 추론 시에는 학습된 그래프 어텐션 네트워크(GAT)를 사용하여 쿼리 이미지의 2D 특징점과 이 3D 모델 간의 2D-3D 대응 관계를 찾는다.13</li>
<li><strong>FoundationPose의 접근법</strong>: 암시적인 ‘표현 학습 후 렌더링 및 비교(learn-representation-then-render-and-compare)’ 전략을 사용한다. 소수의 참조 이미지로부터 연속적인 암시적 신경장을 학습하고, 이 신경장을 사용하여 비교를 위한 새로운 뷰를 합성한다. 명시적인 포인트 클라우드를 구축하는 과정이 전혀 없다.10</li>
<li><strong>핵심 차이점</strong>: 근본적인 차이는 3D 객체 표현의 본질에 있다. OnePose는 <strong>명시적이고 기하학적인 포인트 클라우드</strong>를 사용하는 반면, FoundationPose는 <strong>암시적이고 학습된 신경장</strong>을 사용한다. 이 차이는 중요한 트레이드오프를 시사한다. OnePose의 SfM 모델은 기하학적으로 명확하고 해석 가능하지만, 질감이 없는 객체에 취약하고 절차적인 파이프라인에 의존하는 단점이 있다. 반면, FoundationPose의 신경장은 일종의 ’블랙박스’이지만, 방대한 데이터로 훈련되었기 때문에 모호함이나 질감 부족과 같은 어려운 상황을 더 유연하게 처리할 수 있는 강인한 표현을 학습했을 가능성이 높다. 즉, FoundationPose는 기하학적 추론 과정을 네트워크 가중치 안에 내재화시킨 것이다.9</li>
</ul>
<h3>4.2  FoundationPose 대 이미지 매칭 파이프라인 (예: Gen6D)</h3>
<ul>
<li><strong>Gen6D의 접근법</strong>: ’탐지-검색-정제(detect-retrieve-refine)’의 순차적 단계를 따른다. 객체를 탐지한 후, 데이터베이스에 있는 참조 이미지들 중에서 쿼리 이미지와 시각적으로 가장 유사한 이미지를 ’검색’하여 초기 포즈를 얻고, 이를 정제한다.12 이는 본질적으로 검색 기반(retrieval-based) 방법론이다.</li>
<li><strong>FoundationPose의 접근법</strong>: 검색 기반이 아닌 생성 기반(generative)이다. FoundationPose는 단순히 ‘가장 좋은’ 참조 이미지를 찾는 것이 아니라, 학습된 암시적 모델로부터 비교에 가장 <em>이상적인</em> 뷰를 직접 ’생성(합성)’한다. 이를 통해 희소한 참조 이미지 세트에는 존재하지 않는 중간 각도의 뷰도 만들어낼 수 있다.</li>
<li><strong>핵심 차이점</strong>: <strong>검색 기반 매칭(Gen6D) 대 생성적 렌더링 기반 비교(FoundationPose)</strong>. 이 차이는 유연성과 정밀도에서 큰 차이를 만든다. Gen6D는 참조 이미지 세트의 이산적인 시점에 의해 성능이 제한될 수 있다. 만약 쿼리 뷰가 두 참조 뷰의 정확히 중간에 있다면, Gen6D는 둘 중 하나를 선택하고 거기서부터 정제를 시작해야 한다. 반면, FoundationPose는 이론적으로 완벽한 비교를 위해 정확한 중간 뷰를 생성할 수 있다. 이러한 연속적이고 생성적인 능력은 이산적인 검색보다 본질적으로 더 강력하다. FoundationPose 논문은 Gen6D가 분포를 벗어난(out-of-distribution) 테스트 케이스를 처리하기 위해 미세 조정이 필요할 수 있다고 지적하며, 이는 FoundationPose가 피하고자 하는 제약 조건이다.9</li>
</ul>
<h3>4.3  FoundationPose 대 FoundPose: 파운데이션 피처에 대한 다른 철학</h3>
<ul>
<li><strong>FoundPose의 접근법</strong>: 범용으로 사전 훈련된 비전 파운데이션 모델(DINOv2)을 <strong>별도의 훈련 없이(training-free)</strong> 활용한다. 쿼리 이미지와 CAD 모델로부터 렌더링된 템플릿 간에 DINOv2 패치 디스크립터를 매칭하여 2D-3D 대응 관계를 설정한다.6 이 방식은 자세 추정 작업에 특화된 훈련을 전혀 요구하지 않는다.</li>
<li><strong>FoundationPose의 접근법</strong>: 대규모의 <strong>작업 특화적(task-specific)</strong> 훈련 모델이다. 자체적으로 구축한 방대한 합성 데이터셋 위에서 오직 6D 자세 추정 작업만을 위해 최적화된 고유의 피처를 학습한다.15</li>
<li><strong>핵심 차이점</strong>: <strong>범용 피처의 활용(FoundPose) 대 작업 특화 피처의 학습(FoundationPose)</strong>. 이는 파운데이션 모델 응용에 있어 핵심적인 철학적 논쟁을 반영한다. 즉, 거대한 범용 모델을 그대로 가져와 그 위에 간단한 로직을 구축하는 것이 나은가, 아니면 특정 작업을 위해 처음부터 거대한 전문 모델을 구축하고 훈련하는 것이 나은가? FoundPose의 단순성은 매력적이지만, FoundationPose가 BOP 벤치마크에서 달성한 SOTA 성능은 6D 자세 추정과 같이 복잡하고 정밀도가 요구되는 작업에서는 현재로서는 작업 특화적 훈련이 더 우수한 결과를 낳는다는 것을 시사한다.</li>
</ul>
<hr />
<h2>5.  연구실에서 현실 세계로: 생태계와 응용</h2>
<p>FoundationPose는 단순한 학술 연구 결과물에 그치지 않고, NVIDIA의 강력한 생태계 안에서 적극적으로 제품화 및 지원되고 있다. 이는 연구와 실제 산업 적용 사이의 간극을 메우는 중요한 역할을 한다.</p>
<h3>5.1  배포 및 접근성: NVIDIA Isaac 생태계</h3>
<ul>
<li><strong>제품화 및 배포</strong>: FoundationPose는 NVIDIA의 NGC 카탈로그에서 사전 훈련된 모델로 제공되어 누구나 쉽게 접근하고 다운로드할 수 있다.15 이는 모델이 상업적 응용을 위해 추가 훈련 없이 바로 사용될 수 있음을 의미한다.15</li>
<li><strong>Isaac ROS 통합</strong>: 이 모델은 NVIDIA의 로보틱스 운영체제(ROS) 프레임워크인 Isaac ROS 내에서 <code>isaac_ros_foundationpose</code>라는 최적화된 패키지로 제공된다. 이 패키지는 필요한 전처리, 모델 추론, 후처리 단계를 완벽하게 통합하여 개발자가 손쉽게 자신의 로봇 시스템에 적용할 수 있도록 지원한다.15</li>
<li><strong>사용 편의성</strong>: Docker 컨테이너를 제공하여 다양한 시스템에서 동일한 환경을 재현하고 설치 과정을 대폭 간소화했다.14 이는 개발자가 복잡한 의존성 문제 없이 신속하게 모델을 테스트하고 배포할 수 있게 해준다.</li>
<li><strong>하드웨어 최적화</strong>: FoundationPose는 NVIDIA 하드웨어에 맞춰 최적화되어 있다. 상대적으로 계산량이 많은 초기 자세 추정은 낮은 빈도로 실행하고, 가벼운 추적(tracking) 모드는 엣지 디바이스인 Jetson Orin에서도 120 FPS 이상의 빠른 속도로 작동한다.27 이러한 하이브리드 접근 방식은 제한된 자원을 가진 로봇에서도 실시간 성능을 보장한다.</li>
</ul>
<p>이러한 요소들을 종합해 보면, FoundationPose는 NVIDIA의 수직 통합 AI 파이프라인 전략을 명확하게 보여준다. NVIDIA의 시뮬레이션 도구(Isaac Sim)를 사용하여 데이터를 생성하고, NVIDIA GPU로 훈련하며, NVIDIA의 로보틱스 플랫폼(Isaac ROS)을 통해 NVIDIA의 엣지 하드웨어(Jetson)에 최적화된 패키지로 배포된다. 이는 개발을 가속화하는 강력한 생태계를 형성하는 동시에, NVIDIA의 소프트웨어 및 하드웨어 스택에 대한 의존성을 높이는 양면성을 가진다.</p>
<h3>5.2  상호작용의 미래를 열다: 응용 분야</h3>
<p>FoundationPose의 실질적인 가치는 로보틱스와 증강현실이라는 두 가지 핵심 응용 분야에서 발현된다.</p>
<ul>
<li><strong>로보틱스</strong>: 가장 주요한 응용 분야는 로봇이 주변 환경의 객체를 정확하게 인식하고 상호작용할 수 있도록 만드는 것이다. 이는 자동화된 부품 피킹, 복잡한 조립 라인, 인간-로봇 협업과 같은 고도화된 작업을 가능하게 하는 핵심 기술이다.3 실제로 공개된 데모 영상에서는 로봇 팔이 FoundationPose를 사용하여 머스터드 병을 성공적으로 조작하는 모습을 보여주며, 그 실용성을 구체적으로 입증했다.17</li>
<li><strong>증강현실 (AR)</strong>: 제로샷으로 고품질의 자세 추정이 가능해짐에 따라, 가상 콘텐츠를 실제 객체 위에 안정적이고 정확하게 겹쳐 놓을 수 있게 되었다. 이는 사용자에게 훨씬 더 몰입감 있고 상호작용적인 AR 경험을 제공하는 데 필수적이다.3</li>
</ul>
<p>FoundationPose의 가장 중요한 실용적 함의는 학술 연구와 산업 응용 사이의 간극을 효과적으로 메운다는 점이다. NVIDIA는 사전 훈련되고 최적화되었으며 배포하기 쉬운 SOTA 모델을 제공함으로써, 로보틱스 기업이나 개발자들이 고급 인식 기능을 자사 제품에 통합하는 데 필요한 진입 장벽을 극적으로 낮추었다. 이는 단순한 연구 논문 발표를 넘어, 첨단 기술의 실질적인 현장 채택을 촉진하는 핵심적인 촉매제 역할을 한다.</p>
<hr />
<h2>6.  비판적 평가와 미래 연구 방향</h2>
<p>FoundationPose는 6D 자세 추정 분야에서 기념비적인 성과를 이루었지만, 모든 기술과 마찬가지로 한계점을 가지고 있으며, 이는 미래 연구의 방향을 제시한다.</p>
<h3>6.1  인지된 한계점</h3>
<p>NVIDIA가 공식적으로 배포한 모델 카드에는 FoundationPose의 명확한 한계점이 기술되어 있다.15</p>
<ul>
<li><strong>까다로운 재질</strong>: 모델은 <strong>반사율이 높거나 투명한 객체</strong>에 대해 부정확한 포즈를 예측하는 등 성능 저하를 보일 수 있다. 이는 RGB-D 센서에 의존하는 방법론들의 공통적인 실패 모드인데, 이러한 재질의 표면에서는 깊이(depth) 측정이 매우 부정확하거나 노이즈가 많기 때문이다.</li>
<li><strong>심각한 이미지 품질 저하</strong>: 어느 정도의 흐림(blur)에는 강인하지만, “극심하게 흐릿한” 이미지 역시 부정확한 포즈 및 추적 성능으로 이어질 수 있다.</li>
</ul>
<p>이러한 한계점들은 FoundationPose가 90%의 일반적인 상황에서는 탁월하게 작동하지만, 나머지 10%의 예외적인 경우, 즉 비정형적인 인간 환경에서 흔히 발견되는 까다로운 객체들을 처리하는 데에는 여전히 어려움이 있음을 보여준다. 이는 인식 기술의 ‘라스트 마일(last mile)’ 문제로, 이러한 엣지 케이스를 해결하는 것이 전천후 환경에서의 강인한 배포를 위한 다음 주요 과제임을 시사한다.</p>
<h3>6.2  추론된 미래 연구 방향</h3>
<p>논문 자체에서는 명시적인 ‘향후 연구(Future Work)’ 섹션을 제공하지 않지만 4, 언급된 한계점과 관련 분야의 최신 연구 동향을 바탕으로 다음과 같은 몇 가지 핵심적인 미래 연구 방향을 추론할 수 있다.</p>
<ol>
<li><strong>센서 오류 및 까다로운 재질에 대한 강인성 확보</strong>: 주된 한계점이 깊이 데이터의 품질에 대한 의존성을 지적하므로, 향후 연구는 깊이 센서의 노이즈나 실패에 더 강인한 표현을 만드는 데 집중될 것이다. 이는 다중 모달리티 융합 기술을 발전시키거나, 깊이 정보가 신뢰할 수 없을 때 RGB 정보로부터 기하학적 구조를 더 강력하게 추론하도록 학습하는 아키텍처 개발을 포함할 수 있다.</li>
<li><strong>비강체(Non-Rigid) 객체로의 확장</strong>: FoundationPose는 대부분의 SOTA 방법론과 마찬가지로 형태가 변하지 않는 강체(rigid object)를 대상으로 설계되었다. 자세 추정 분야의 다음 개척지는 가위, 노트북, 인체와 같은 관절형 객체(articulated object)나 천, 케이블과 같은 변형 가능 객체(deformable object)를 처리할 수 있도록 이러한 강력한 일반화 프레임워크를 확장하는 것이다.</li>
<li><strong>계산 및 데이터 효율성 향상</strong>: 추적 기능은 빠르지만 초기 추정은 “계산 집약적“이다.27 양자화(quantization)나 증류(distillation)와 같은 모델 압축 기술에 대한 연구는 전체 파이프라인을 더 많은 자원 제약적 로봇에서 접근 가능하게 만들 수 있다. 마찬가지로, 모델 프리 설정이 약 16개의 이미지만을 요구하지만, SinRef-6D와 같은 최신 연구들은 단일 참조 뷰(single-reference-view) 추정을 목표로 하고 있다.28 이는 데이터 효율성의 궁극적인 목표를 나타낸다.</li>
<li><strong>오픈 보캐뷸러리(Open-Vocabulary) 자세 추정</strong>: CAD 모델이나 참조 이미지로 객체를 지정하는 것을 넘어, 자연어(natural language)로 객체를 지정하는 것이 다음 단계이다. Oryon과 같이 텍스트 프롬프트를 사용하여 관심 객체를 정의하는 접근법은 30, FoundationPose와 같은 파운데이션 모델과 통합될 수 있는 매우 흥미로운 미래 방향을 제시한다.</li>
</ol>
<p>이러한 미래 연구 방향들은 두 가지 명확한 발전 벡터를 가리킨다. 첫 번째는 <strong>더 높은 수준의 추상화</strong>를 향한 움직임이다(강체에서 관절형 객체로, 시각적 참조에서 언어적 참조로). 두 번째는 <strong>더 높은 효율성</strong>을 향한 움직임이다(더 적은 계산량, 더 적은 참조 이미지). FoundationPose는 이 여정에서 중요한 이정표를 세웠지만, 동시에 진정으로 보편적이고 손쉬운 객체 인식 시스템으로 가는 길에 놓인 다음 단계들을 명확하게 조명하고 있다.</p>
<h2>7. 결론</h2>
<p>FoundationPose는 6D 객체 자세 추정 분야에서 단순한 점진적 개선을 넘어선 패러다임의 전환을 대표하는 모델이다. 본 고찰을 통해 분석한 바와 같이, FoundationPose의 핵심적인 기여는 세 가지 차원에서 요약될 수 있다.</p>
<p>첫째, <strong>기술적 통합의 실현</strong>이다. LLM을 활용한 대규모 합성 데이터 생성, 모델 기반과 모델 프리 설정을 잇는 신경 암시적 표현, 그리고 트랜스포머 기반의 계층적 평가 네트워크라는 네 가지 기술 기둥을 유기적으로 결합함으로써, 이전까지 파편화되어 있던 새로운 객체 자세 추정 문제를 하나의 일관된 프레임워크로 통합했다. 특히, 신경장을 ’범용 어댑터’로 활용하여 하위 모듈을 입력 모달리티로부터 독립시킨 설계는 공학적 우아함의 정수를 보여준다.</p>
<p>둘째, <strong>성능의 새로운 기준 제시</strong>이다. BOP 벤치마크에서의 압도적인 1위 성능은 FoundationPose가 단지 유연하기만 한 것이 아니라, 각 작업에 특화된 기존의 SOTA 모델들을 능가하는 강력한 성능을 갖추었음을 실증적으로 입증했다. 이는 잘 설계된 범용 모델이 방대한 데이터와 강인한 아키텍처를 통해 전문화된 모델을 뛰어넘을 수 있다는 중요한 선례를 남겼다.</p>
<p>셋째, <strong>연구와 산업의 가교 역할</strong>이다. NVIDIA Isaac 생태계와의 긴밀한 통합을 통해, FoundationPose는 학술적 성과를 넘어 개발자들이 즉시 활용할 수 있는 실용적인 산업 구성 요소로 자리매김했다. 이는 첨단 AI 기술이 로보틱스 및 AR과 같은 실제 응용 분야로 확산되는 속도를 가속화하는 결정적인 역할을 한다.</p>
<p>물론, 반사율이 높거나 투명한 객체에 대한 취약점과 같은 명백한 한계는 이 기술이 아직 가야 할 길이 남아있음을 보여준다. 그러나 이러한 한계점들은 오히려 비강체 객체로의 확장, 데이터 및 계산 효율성 증대, 오픈 보캐뷸러리 인식과 같은 차세대 연구 주제를 명확히 제시하는 나침반이 된다.</p>
<p>결론적으로, FoundationPose는 6D 객체 자세 추정의 ‘파운데이션 모델’ 시대를 본격적으로 연 기념비적인 작업이다. 이는 현재의 기술적 정점을 보여주는 동시에, 미래의 연구자들이 도전해야 할 새로운 지평을 열어주었다는 점에서 그 의의가 매우 크다고 평가할 수 있다.</p>
<h2>8. 참고 자료</h2>
<ol>
<li>www.cmu.edu, accessed July 19, 2025, <a href="https://www.cmu.edu/xrtc/news/2024/jan/repose.html#:~:text=To%20explain%20what%206D%20object,object%20in%20three-dimensional%20space.">https://www.cmu.edu/xrtc/news/2024/jan/repose.html#:~:text=To%20explain%20what%206D%20object,object%20in%20three%2Ddimensional%20space.</a></li>
<li>6D Pose Estimation of Objects: Recent Technologies and Challenges - MDPI, accessed July 19, 2025, https://www.mdpi.com/2076-3417/11/1/228</li>
<li>RePOSE: 6D Refinement via Deep Texture Rendering - Carnegie Mellon University, accessed July 19, 2025, https://www.cmu.edu/xrtc/news/2024/jan/repose.html</li>
<li>FoundationPose: Unified 6D Pose Estimation and Tracking … - NVlabs, accessed July 19, 2025, https://nvlabs.github.io/FoundationPose/</li>
<li>PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes - Robotics, accessed July 19, 2025, https://www.roboticsproceedings.org/rss14/p19.pdf</li>
<li>FoundPose: Unseen Object Pose Estimation with Foundation Features - arXiv, accessed July 19, 2025, https://arxiv.org/html/2311.18809v2</li>
<li>6D Pose Estimation using RGB - Papers With Code, accessed July 19, 2025, https://paperswithcode.com/task/6d-pose-estimation</li>
<li>Omni6DPose: A Benchmark and Model for Universal 6D Object Pose Estimation and Tracking - arXiv, accessed July 19, 2025, https://arxiv.org/html/2406.04316v1</li>
<li>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects - arXiv, accessed July 19, 2025, https://arxiv.org/html/2312.08344v1</li>
<li>FoundationPose: Unified 6D Pose Estimation … - CVF Open Access, accessed July 19, 2025, https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.pdf</li>
<li>A Survey of 6D Object Detection Based on 3D Models for Industrial Applications - PMC, accessed July 19, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC8952329/</li>
<li>Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images - ar5iv, accessed July 19, 2025, https://ar5iv.labs.arxiv.org/html/2204.10776</li>
<li>OnePose: One-Shot Object Pose Estimation without CAD … - arXiv, accessed July 19, 2025, http://arxiv.org/pdf/2205.12257</li>
<li>[CVPR 2024 Highlight] FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects - GitHub, accessed July 19, 2025, https://github.com/NVlabs/FoundationPose</li>
<li>FoundationPose | NVIDIA NGC, accessed July 19, 2025, https://catalog.ngc.nvidia.com/orgs/nvidia/teams/isaac/models/foundationpose</li>
<li>FoundationPose: Unified 6D Pose Estimation and … - Jan Kautz, accessed July 19, 2025, https://www.jankautz.com/publications/FoundationPose_CVPR24.pdf</li>
<li>tusharsangam/FoundationPoseForSpotSim2Real: FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects for Spot-sim2real - GitHub, accessed July 19, 2025, https://github.com/tusharsangam/FoundationPoseForSpotSim2Real</li>
<li>BOP: Benchmark for 6D Object Pose Estimation - CVF Open Access, accessed July 19, 2025, https://openaccess.thecvf.com/content_ECCV_2018/papers/Tomas_Hodan_PESTO_6D_Object_ECCV_2018_paper.pdf</li>
<li>Benchmark for 6D Object Pose Estimation: BOP, accessed July 19, 2025, https://bop.felk.cvut.cz/</li>
<li>BOP: Benchmark for 6D Object Pose Estimation - Hugging Face, accessed July 19, 2025, https://huggingface.co/bop-benchmark</li>
<li>Datasets - BOP: Benchmark for 6D Object Pose Estimation, accessed July 19, 2025, https://bop.felk.cvut.cz/datasets/</li>
<li>thodan/bop_toolkit: A Python toolkit of the BOP benchmark for 6D object pose estimation., accessed July 19, 2025, https://github.com/thodan/bop_toolkit</li>
<li>OnePose: One-Shot Object Pose Estimation without CAD Models - ResearchGate, accessed July 19, 2025, https://www.researchgate.net/publication/363910110_OnePose_One-Shot_Object_Pose_Estimation_without_CAD_Models</li>
<li>Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images | Request PDF - ResearchGate, accessed July 19, 2025, https://www.researchgate.net/publication/365312569_Gen6D_Generalizable_Model-Free_6-DoF_Object_Pose_Estimation_from_RGB_Images</li>
<li>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects - arXiv, accessed July 19, 2025, https://arxiv.org/html/2312.08344v2</li>
<li>Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation …, accessed July 19, 2025, https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920297.pdf</li>
<li>FoundationPose - isaac_ros_docs documentation - NVIDIA Isaac ROS, accessed July 19, 2025, https://nvidia-isaac-ros.github.io/concepts/pose_estimation/foundationpose/index.html</li>
<li>Novel Object 6D Pose Estimation with a Single Reference View - arXiv, accessed July 19, 2025, https://arxiv.org/html/2503.05578v1</li>
<li>One2Any: One-Reference 6D Pose Estimation for Any Object - arXiv, accessed July 19, 2025, https://arxiv.org/html/2505.04109v1</li>
<li>Open-vocabulary object 6D pose estimation - arXiv, accessed July 19, 2025, https://arxiv.org/html/2312.00690v2</li>
</ol>

            </article>
            <footer>
                <p>Generated by Rust Site Gen</p>
            </footer>
        </main>
    </div>
</body>
</html>