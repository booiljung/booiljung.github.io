# 비선형 최소제곱 문제

## 서론: 최소제곱 문제의 본질과 비선형성의 도전

최소제곱법(Least Squares Method)은 관측된 데이터 집합에 가장 잘 부합하는 모델을 찾는 데 사용되는 가장 근본적이고 강력한 최적화 기법 중 하나이다.1 이 방법의 근원적인 목표는 모델이 예측하는 값과 실제 관측된 값 사이의 차이, 즉 잔차(residual)의 제곱합(Sum of Squared Residuals, SSR 또는 Sum of Squared Errors, SSE)을 최소화하는 모델의 파라미터(parameter)를 결정하는 것이다.2 수학적으로, $m$개의 데이터 포인트 $(x_i, y_i)$에 대해 파라미터 벡터 $\beta$로 정의되는 모델 함수 $f(x, \beta)$가 주어졌을 때, 최소제곱법의 목적 함수 $S(\beta)$는 다음과 같이 정의된다.4
$$
S(\beta) = \sum_{i=1}^{m} r_i(\beta)^2 = \sum_{i=1}^{m} (y_i - f(x_i, \beta))^2
$$
여기서 $r_i(\beta)$는 $i$번째 관측에 대한 잔차를 나타낸다. 이 목적 함수 $S(\beta)$를 최소화하는 파라미터 벡터 $\beta^*$를 찾는 것이 바로 최소제곱 문제의 해를 구하는 과정이다.7

최소제곱 문제는 모델 함수 $f(x, \beta)$가 파라미터 $\beta$에 대해 선형(linear)인지 비선형(non-linear)인지에 따라 두 가지 유형으로 명확하게 구분된다. 이 구분은 변수 $x$의 형태가 아니라, 오직 파라미터와의 관계에 의해 결정된다는 점을 이해하는 것이 매우 중요하다.9 예를 들어, 다항 회귀 모델 $y = \beta_1 x^2 + \beta_2 x + \beta_3$는 독립 변수 $x$에 대해서는 2차 비선형 함수이지만, 파라미터 $\beta_1, \beta_2, \beta_3$에 대해서는 선형 결합으로 표현되므로 선형 최소제곱(Linear Least Squares, LLS) 문제에 해당한다. 반면, 지수 붕괴 모델 $y = \beta_1 e^{-\beta_2 x}$와 같이 파라미터가 함수 내부에 곱셈, 나눗셈, 지수, 로그 등 비선형적으로 결합된 경우, 이는 비선형 최소제곱(Non-linear Least Squares, NLLS) 문제로 분류된다.8

이러한 '파라미터의 비선형성'은 문제의 해결 패러다임을 근본적으로 변화시킨다. 선형 최소제곱 문제의 목적 함수 $S(\beta)$는 파라미터 $\beta$에 대한 볼록(convex) 2차 함수(quadratic function) 형태를 띤다.11 이러한 함수의 최솟값은 그레디언트(gradient)를 0으로 설정함으로써 유일하게 결정될 수 있으며, 이는 정규방정식(Normal Equations)이라 불리는 닫힌 형태(closed-form)의 선형 대수 방정식으로 귀결된다.2 따라서 선형 문제의 해는 해석적으로, 단 한 번의 계산으로 정확하게 구할 수 있다.11

그러나 비선형 최소제곱 문제에서는 모델 함수 $f(x, \beta)$가 파라미터 $\beta$에 대해 비선형이므로, 목적 함수 $S(\beta)$는 더 이상 단순한 2차 함수가 아니다. 일반적으로 다수의 극소점(local minima)을 갖는 복잡한 형태의 비볼록(non-convex) 함수가 된다.15 이로 인해 그레디언트 $\nabla S(\beta) = 0$을 만족하는 해를 찾는 것은 더 이상 간단한 선형 방정식으로 해결할 수 없는 비선형 방정식 시스템을 푸는 문제가 된다.11 결과적으로, 비선형 문제에 대해서는 닫힌 형태의 해가 존재하지 않으며, 해를 찾기 위해 반드시 초기 추정치에서 시작하여 점진적으로 해를 개선해 나가는 반복적(iterative) 수치 해법에 의존해야만 한다.11 이처럼 '파라미터의 비선형성'이라는 단 하나의 특성이 해법을 해석적 접근에서 수치적 반복 접근으로 전환시키며, 이는 비선형 최소제곱 문제 해결 과정에서 마주하게 되는 초깃값 의존성, 지역 최적해, 수렴 실패 가능성과 같은 모든 복잡성의 근원이 된다.

본 보고서는 이러한 비선형 최소제곱 문제의 수학적 구조를 깊이 있게 탐구하고, 가우스-뉴턴(Gauss-Newton) 방법과 레벤버그-마르쿼트(Levenberg-Marquardt) 알고리즘과 같은 핵심적인 반복 해법들을 상세히 분석한다. 나아가 알고리즘의 수치적 안정성, 초깃값 설정의 중요성, 그리고 지역 최적해 문제와 같은 실용적인 난제들을 고찰하고, 구체적인 수치 예제를 통해 이론적 개념이 실제 문제 해결 과정에 어떻게 적용되는지를 명확히 보이고자 한다.

## 1.  비선형 최소제곱 문제의 핵심 해법: 가우스-뉴턴 방법 (Gauss-Newton Method)

비선형 최소제곱 문제를 해결하기 위한 가장 대표적인 알고리즘 중 하나인 가우스-뉴턴 방법은, 계산 비용이 많이 드는 뉴턴 최적화 방법을 문제의 특성에 맞게 변형하여 효율성을 높인 기법이다. 이 방법의 핵심은 복잡한 비선형 문제를 각 반복 단계에서 다루기 쉬운 선형 최소제곱 문제로 근사하여 푸는 데 있다.

### 1.1  뉴턴 최적화 방법으로부터의 유도: 헤시안(Hessian) 행렬의 근사

일반적인 비선형 함수 $S(\beta)$의 최솟값을 찾는 뉴턴 최적화 방법은 현재 지점 $\beta_k$에서 함수를 2차 테일러 급수로 근사하고, 이 근사 함수의 최솟값을 다음 지점 $\beta_{k+1}$로 삼는 방식이다. 뉴턴 스텝 $\Delta\beta = \beta_{k+1} - \beta_k$는 다음의 선형 시스템을 통해 결정된다.17
$$
H(\beta_k) \Delta\beta = -g(\beta_k)
$$
여기서 $g(\beta_k) = \nabla S(\beta_k)$는 $S(\beta)$의 그레디언트 벡터이고, $H(\beta_k) = \nabla^2 S(\beta_k)$는 헤시안(Hessian) 행렬, 즉 2차 편도함수 행렬이다.

비선형 최소제곱 문제의 목적 함수 $S(\beta) = \sum_{i=1}^{m} r_i(\beta)^2 = r(\beta)^T r(\beta)$에 대해 그레디언트와 헤시안을 구체적으로 계산하면 다음과 같다. 먼저, 잔차 벡터 $r(\beta)$의 자코비안(Jacobian) 행렬을 $J(\beta)$라 정의하자. $J(\beta)$는 $m \times n$ 행렬이며, 그 원소는 $(J)_{ij} = \frac{\partial r_i}{\partial \beta_j}$이다. 이를 이용하면 그레디언트 $g(\beta)$는 다음과 같이 표현된다.19
$$
g(\beta) = \nabla S(\beta) = 2 \sum_{i=1}^{m} r_i(\beta) \nabla r_i(\beta) = 2 J(\beta)^T r(\beta)
$$
헤시안 $H(\beta)$는 그레디언트를 한 번 더 미분하여 얻을 수 있다.20
$$
H(\beta) = \nabla^2 S(\beta) = 2 J(\beta)^T J(\beta) + 2 \sum_{i=1}^{m} r_i(\beta) \nabla^2 r_i(\beta)
$$
뉴턴 방법은 이 헤시안 행렬을 직접 계산하고 그 역행렬을 구해야 하므로 계산 비용이 매우 높다는 단점이 있다. 가우스-뉴턴 방법의 핵심 아이디어는 이 헤시안 행렬의 두 번째 항, 즉 $2 \sum r_i \nabla^2 r_i$를 무시하고 헤시안을 근사하는 것이다.22 이러한 근사는 두 가지 합리적인 가정 하에 타당성을 갖는다. 첫째, 해에 가까워질수록 잔차 $r_i(\beta)$의 값이 0에 가까워지므로 두 번째 항의 영향력이 미미해진다. 둘째, 모델의 비선형성이 크지 않은 경우, 2차 미분항 $\nabla^2 r_i(\beta)$ 자체가 작을 수 있다.23 이 근사를 통해 헤시안은 다음과 같이 간단하게 표현된다.
$$
H(\beta) \approx 2 J(\beta)^T J(\beta)
$$
이 근사된 헤시안을 뉴턴 방법의 업데이트 식에 대입하면 가우스-뉴턴 업데이트 규칙이 유도된다. 이 방법의 가장 큰 장점은 계산이 복잡하고 비용이 많이 드는 2차 미분을 전혀 사용하지 않고, 오직 1차 미분 정보인 자코비안 행렬만으로 헤시안을 근사하여 2차 수렴에 가까운 빠른 속도를 달성할 수 있다는 점이다.17

### 1.2  자코비안 행렬을 이용한 국소적 선형화(Local Linearization)

가우스-뉴턴 방법은 다른 관점에서 더 직관적으로 이해할 수 있다. 이는 비선형 잔차 함수 $r(\beta)$를 현재 파라미터 추정치 $\beta_k$ 근방에서 1차 테일러 급수를 이용하여 국소적으로 선형화하는 과정으로 해석할 수 있다.4 파라미터의 작은 변화량 $\Delta\beta$에 대해 잔차 벡터는 다음과 같이 근사된다.
$$
r(\beta_k + \Delta\beta) \approx r(\beta_k) + J(\beta_k) \Delta\beta
$$
원래의 비선형 최소제곱 문제는 목적 함수 $S(\beta_{k+1}) = \|r(\beta_k + \Delta\beta)\|_2^2$를 최소화하는 것이었다. 위 선형 근사식을 목적 함수에 대입하면, 각 반복 단계에서 풀어야 할 문제는 다음과 같은 선형 최소제곱 문제로 변환된다.17
$$
\min_{\Delta\beta} \|r(\beta_k) + J(\beta_k) \Delta\beta\|_2^2
$$
이 접근법은 복잡한 비선형 최적화 문제를, 각 단계에서 현재 지점을 가장 잘 근사하는 '선형 모델'을 만들고 그 모델에 대한 최소제곱 해를 구하는, 훨씬 간단한 선형 대수 문제들의 연속으로 치환하는 영리한 전략을 보여준다. 즉, 복잡한 곡면 위에서 최저점을 찾는 대신, 현재 위치에서의 접평면을 만들고 그 평면의 최저점을 향해 한 걸음 나아가는 과정을 반복하는 것이다.

### 1.3  가우스-뉴턴 스텝(Gauss-Newton Step)의 수학적 유도 및 반복 규칙

위에서 정의된 선형 최소제곱 문제의 해 $\Delta\beta$는 정규방정식을 통해 구할 수 있다. 목적 함수 $\|r + J\Delta\beta\|_2^2$를 $\Delta\beta$에 대해 미분하여 0으로 두면 다음과 같은 정규방정식을 얻는다.4
$$
(J(\beta_k)^T J(\beta_k)) \Delta\beta = -J(\beta_k)^T r(\beta_k)
$$
만약 행렬 $J^T J$가 가역(invertible)이라면, 파라미터 업데이트를 위한 가우스-뉴턴 스텝 $\Delta\beta$는 다음과 같이 명시적으로 계산된다.
$$
\Delta\beta = -(J(\beta_k)^T J(\beta_k))^{-1} J(\beta_k)^T r(\beta_k)
$$
이를 통해 가우스-뉴턴 알고리즘의 전체 반복 규칙은 다음과 같이 정의된다 6:
$$
\beta_{k+1} = \beta_k + \Delta\beta = \beta_k - (J(\beta_k)^T J(\beta_k))^{-1} J(\beta_k)^T r(\beta_k)
$$
이 알고리즘은 초기 추정치 $\beta_0$에서 시작하여, 위 업데이트 규칙을 통해 새로운 파라미터 $\beta_{k+1}$을 계산하는 과정을 수렴 조건이 만족될 때까지 반복한다.

### 1.4  알고리즘의 수렴 조건과 기하학적 해석

가우스-뉴턴 방법의 수렴 속도는 문제의 특성, 특히 최종 해에서의 잔차 크기에 크게 의존한다. 만약 잔차가 매우 작은 문제(small residual problem)라면, 헤시안 근사 $H \approx 2J^T J$의 정확도가 높아져 뉴턴 방법과 유사하게 2차 수렴(quadratic convergence)에 가까운 매우 빠른 수렴 속도를 보인다.23 그러나 잔차가 큰 문제(large residual problem)의 경우, 무시했던 헤시안의 두 번째 항이 중요해지면서 근사 오차가 커진다. 이로 인해 수렴 속도는 선형(linear)으로 저하되거나, 심한 경우 알고리즘이 수렴하지 않고 발산할 수도 있다.6

알고리즘의 안정적인 수렴을 위해서는 몇 가지 조건이 요구된다. 첫째, 초기 추정치 $\beta_0$가 해의 수렴 영역(basin of attraction) 내에, 즉 해에 충분히 가까이 있어야 한다.23 둘째, 매 반복 과정에서 자코비안 행렬 $J(\beta_k)$가 완전 열 랭크(full column rank)를 가져야 한다. 이는 행렬 $J^T J$가 가역 행렬이 되어 유일한 스텝 $\Delta\beta$를 계산할 수 있도록 보장하는 조건이다.20 이 조건이 깨지면 알고리즘은 수치적으로 불안정해지거나 실패하게 된다. 이러한 근사 기반 접근법의 본질적인 취약점은 가우스-뉴턴 방법의 한계로 작용하며, 이를 극복하기 위한 보다 강인한 알고리즘의 필요성을 제기한다.

## 2.  강인성(Robustness) 확보를 위한 진화: 레벤버그-마르쿼트 알고리즘 (Levenberg-Marquardt Algorithm)

가우스-뉴턴 방법은 특정 조건 하에서 매우 효율적이지만, 실용적인 문제에서 종종 불안정성을 드러낸다. 레벤버그-마르쿼트(Levenberg-Marquardt, LM) 알고리즘은 이러한 가우스-뉴턴 방법의 단점을 보완하고 강인성을 획기적으로 개선하여, 오늘날 비선형 최소제곱 문제 해결을 위한 사실상의 표준(de facto standard)으로 자리 잡았다.

### 2.1  가우스-뉴턴 방법의 한계와 경사 하강법(Gradient Descent)의 필요성

가우스-뉴턴 방법의 주된 한계는 두 가지로 요약할 수 있다. 첫째, 해에서 멀리 떨어진 초기 추정치에서 시작할 경우, 국소적 선형 근사가 실제 비선형 함수를 제대로 표현하지 못하여 발산하기 쉽다.26 둘째, 파라미터 간의 강한 상관관계나 데이터 부족으로 인해 자코비안 행렬 $J$의 열들이 거의 선형 종속에 가까워지면, 행렬 $J^T J$는 특이(singular) 행렬에 가깝거나 불량 조건(ill-conditioned)이 된다. 이 경우 $(J^T J)^{-1}$의 계산이 수치적으로 매우 불안정해져 알고리즘이 실패할 수 있다.26

이와 대조적으로, 가장 기본적인 최적화 기법인 경사 하강법(Gradient Descent)은 목적 함수 $S(\beta)$를 가장 가파르게 감소시키는 방향, 즉 음의 그레디언트 방향으로 파라미터를 업데이트한다.27 경사 하강법의 업데이트 스텝은 다음과 같다.
$$
\Delta\beta = -\alpha g(\beta) = -2\alpha J(\beta)^T r(\beta)
$$
여기서 $\alpha$는 학습률(learning rate) 또는 스텝 크기(step size)를 조절하는 스칼라 값이다. 경사 하강법은 수렴 속도가 선형적으로 느리다는 단점이 있지만, 목적 함수 값을 감소시키는 방향으로의 이동을 보장하므로 전역적으로 매우 안정적이고 강인하다. 가우스-뉴턴 방법이 '속도'를 추구한다면, 경사 하강법은 '안정성'을 보장한다.

### 2.2  두 방법론의 융합: 신뢰 영역(Trust Region) 접근법의 도입

레벤버그-마르쿼트 알고리즘의 핵심 철학은 가우스-뉴턴의 빠른 수렴 속도와 경사 하강법의 안정성을 동적으로 결합하는 것이다.17 이는 신뢰 영역(Trust Region) 또는 감쇠(Damping)라는 개념을 통해 구현된다. LM 알고리즘은 가우스-뉴턴의 정규방정식을 다음과 같이 수정한다.27
$$
(J(\beta_k)^T J(\beta_k) + \lambda I) \Delta\beta = -J(\beta_k)^T r(\beta_k)
$$
여기서 $I$는 단위 행렬(identity matrix)이며, $\lambda \ge 0$는 감쇠 인자(damping parameter)라 불리는 비음수 스칼라 값이다. 이 $\lambda I$ 항의 추가는 단순해 보이지만 알고리즘의 동작 방식을 근본적으로 변화시킨다. 이는 국소적 선형 근사 모델이 유효하다고 '신뢰'할 수 있는 반경을 제한하는 신뢰 영역 접근법의 한 형태로 해석될 수 있다.29

이 수정된 정규방정식은 현재의 국소적 선형 모델에 대한 '신뢰도'를 수학적으로 관리하는 정교한 위험 관리 전략으로 볼 수 있다. 가우스-뉴턴 방법의 실패는 국소 모델이 실제 함수를 얼마나 잘 표현하는지에 대한 불확실성에서 비롯된다. LM 알고리즘은 감쇠 인자 $\lambda$를 이 신뢰도의 대리 지표로 활용한다. $\lambda$ 값이 크다는 것은 현재 모델에 대한 신뢰도가 낮음을 의미하며, 이 경우 알고리즘은 모델이 제시하는 최적점으로 과감히 이동하는 대신, 가장 확실한 정보인 국소적 기울기 방향으로 보수적으로 움직여 위험을 회피한다. 반대로 $\lambda$ 값이 작다는 것은 모델에 대한 신뢰도가 높음을 의미하므로, 모델이 제시하는 방향으로 공격적으로 이동하여 빠른 수렴을 추구한다.

### 2.3  감쇠 인자 $\lambda$의 역할과 동적 제어 전략

감쇠 인자 $\lambda$는 LM 알고리즘이 가우스-뉴턴 방법과 경사 하강법 사이를 부드럽게 전환하도록 만드는 조절 장치 역할을 한다.27

- **$\lambda \to 0$ (가우스-뉴턴에 가까워짐):** $\lambda$가 0에 가까워지면, $\lambda I$ 항은 무시할 수 있게 되어 $(J^T J)\Delta\beta = -J^T r$라는 가우스-뉴턴의 정규방정식과 거의 동일해진다. 이 경우 알고리즘은 해 근방에서 2차 수렴에 가까운 빠른 속도를 보인다. 이는 현재 파라미터가 최적해에 가까워져 국소적 선형 근사가 매우 정확할 때의 동작 방식이다.
- **$\lambda \to \infty$ (경사 하강법에 가까워짐):** $\lambda$가 매우 큰 값을 가지면, $J^T J$에 비해 $\lambda I$ 항이 지배적이게 된다. 따라서 수정된 정규방정식은 $\lambda I \Delta\beta \approx -J^T r$로 근사할 수 있으며, 이는 $\Delta\beta \approx -\frac{1}{\lambda} (J^T r)$를 의미한다. 이 업데이트 방향은 음의 그레디언트 방향과 동일하며, 스텝 크기는 $\frac{1}{\lambda}$에 비례하여 매우 작아진다. 이는 현재 파라미터가 해에서 멀리 떨어져 있거나, 가우스-뉴턴 스텝이 목적 함수를 증가시키는 실패한 상황에서 안정성을 확보하기 위한 보수적인 동작 방식이다.

LM 알고리즘의 진정한 힘은 이 $\lambda$ 값을 매 반복마다 동적으로 조절하는 전략에 있다. 일반적인 제어 규칙은 다음과 같다.17

1. 현재 $\lambda_k$ 값을 사용하여 $\Delta\beta$를 계산한다.
2. 새로운 파라미터 $\beta_k + \Delta\beta$에서 실제 목적 함수 값 $S(\beta_k + \Delta\beta)$를 계산한다.
3. 만약 $S(\beta_k + \Delta\beta) < S(\beta_k)$이면, 스텝이 성공적인 것으로 간주한다. 파라미터를 $\beta_{k+1} = \beta_k + \Delta\beta$로 업데이트하고, 다음 반복을 위해 $\lambda$ 값을 줄인다 (예: $\lambda_{k+1} = \lambda_k / \nu$, $\nu > 1$). 이는 가우스-뉴턴 방식에 더 가깝게 만들어 수렴을 가속화하려는 시도이다.
4. 만약 $S(\beta_k + \Delta\beta) \ge S(\beta_k)$이면, 스텝이 실패한 것으로 간주한다. 파라미터를 업데이트하지 않고($\beta_{k+1} = \beta_k$), $\lambda$ 값을 크게 증가시킨 후(예: $\lambda_{k+1} = \lambda_k \cdot \nu$), 현재 단계($k$)를 다시 시도한다. 이는 경사 하강법 방식에 더 가깝게 만들어 스텝 크기를 줄이고 안정적인 감소 방향을 찾으려는 시도이다.

### 2.4. 알고리즘의 동작 방식 요약

요약하자면, 레벤버그-마르쿼트 알고리즘은 해에서 멀리 떨어져 있을 때는 큰 $\lambda$ 값을 사용하여 경사 하강법처럼 안정적으로 전역 탐색을 수행하고, 해에 가까워짐에 따라 $\lambda$ 값을 줄여 가우스-뉴턴 방법처럼 빠르게 수렴하는 지능적인 적응형 전략을 사용한다.17 이러한 하이브리드 접근 방식은 가우스-뉴턴의 수치적 불안정성 문제를 효과적으로 해결하며, 초기 추정치가 좋지 않은 경우에도 수렴할 가능성을 크게 높여준다. 아래 표는 본 보고서에서 논의된 주요 최적화 알고리즘들의 특성을 비교 분석한 것이다.

| 특성 (Feature)                 | 경사 하강법 (Gradient Descent)     | 뉴턴 방법 (Newton's Method)    | 가우스-뉴턴 방법 (Gauss-Newton)    | 레벤버그-마르쿼트 (Levenberg-Marquardt)      |
| ------------------------------ | ---------------------------------- | ------------------------------ | ---------------------------------- | -------------------------------------------- |
| **핵심 원리**                  | 1차 미분(그레디언트) 기반 최급강하 | 2차 미분(헤시안) 기반 2차 근사 | 헤시안의 1차 미분 근사             | 경사 하강법과 가우스-뉴턴의 적응형 결합      |
| **수렴 속도**                  | 선형 (Linear)                      | 2차 (Quadratic)                | 2차에 가까움 (잔차가 작을 시)      | 2차에 가까움 (해 근방에서)                   |
| **2차 미분(헤시안) 필요 여부** | 불필요                             | 필수                           | 불필요                             | 불필요                                       |
| **계산 비용 (1회 반복)**       | 낮음 ($O(mn)$)                     | 매우 높음 ($O(mn + n^3)$)      | 높음 ($O(mn^2 + n^3)$)             | 높음 ($O(mn^2 + n^3)$)                       |
| **안정성 및 강인성**           | 매우 높음                          | 낮음 (초깃값에 민감)           | 낮음 (발산 가능성)                 | 높음 (안정적)                                |
| **주요 적용 분야**             | 일반적인 최적화, 딥러닝            | 일반적인 최적화 (이론적)       | 비선형 최소제곱 (잔차가 작은 문제) | **사실상 표준 (De facto standard)** for NLLS |

## 3.  알고리즘의 안정성과 성능에 대한 심층 고찰

비선형 최소제곱 문제를 성공적으로 해결하기 위해서는 알고리즘의 수학적 원리를 이해하는 것뿐만 아니라, 실제 적용 과정에서 발생하는 근본적인 난제들을 깊이 있게 고찰해야 한다. 수치적 불안정성, 초깃값 설정의 민감성, 그리고 지역 최적해의 함정은 NLLS 문제의 본질적인 특성에서 비롯되며, 이들은 서로 긴밀하게 연결되어 있다.

### 3.1  수치적 불안정성 분석: $J^T J$ 행렬의 조건수(Condition Number)와 특이성 문제

가우스-뉴턴과 레벤버그-마르쿼트 알고리즘의 핵심 연산은 $(J^T J)$ 또는 $(J^T J + \lambda I)$의 역행렬을 포함하는 선형 시스템을 푸는 것이다. 따라서 이 행렬의 상태는 알고리즘의 안정성과 성능에 결정적인 영향을 미친다.6

- **랭크 부족(Rank Deficiency)과 특이성(Singularity):** 자코비안 행렬 $J$의 열 벡터들이 선형 종속(linearly dependent) 관계에 있을 경우, $J$는 완전 열 랭크(full column rank)를 갖지 못한다. 이 경우, 행렬 $J^T J$는 특이 행렬(singular matrix)이 되어 역행렬이 존재하지 않게 된다. 이는 가우스-뉴턴 스텝을 계산할 수 없음을 의미하며, 알고리즘의 완전한 실패로 이어진다.20 물리적으로 이는 모델의 파라미터들이 서로 구분되지 않음(non-identifiable)을 시사한다. 예를 들어, 모델이 

  $y = c \cdot e^{ax} \cdot e^{bx} = c \cdot e^{(a+b)x}$와 같이 파라미터 $a$와 $b$의 합으로만 표현된다면, $a$와 $b$ 각각의 값을 데이터로부터 유일하게 결정하는 것은 불가능하다.

- **불량 조건(Ill-conditioning):** $J^T J$가 수학적으로는 특이 행렬이 아니더라도, 행렬의 조건수(condition number)가 매우 큰 경우를 불량 조건이라 한다. 조건수는 행렬의 입력값 변화에 대한 출력값 변화의 민감도를 나타내는 척도이다.26

  $J^T J$가 불량 조건일 경우, 데이터의 작은 노이즈나 컴퓨터의 부동소수점 연산 오차가 역행렬 계산 과정에서 증폭되어, 계산된 파라미터 업데이트 스텝 $\Delta\beta$가 매우 부정확해지거나 비정상적으로 커질 수 있다. 이는 최적화 과정의 불안정성을 초래하고, 결국 해가 발산하는 원인이 된다.26 이러한 현상은 보통 파라미터 간의 강한 상관관계가 존재할 때 발생한다.

레벤버그-마르쿼트 알고리즘에서 감쇠 항 $\lambda I$를 더하는 것은 이러한 수치적 불안정성을 완화하는 효과적인 정규화(regularization) 기법으로 작용한다. $\lambda > 0$인 한, 행렬 $(J^T J + \lambda I)$는 항상 양의 정부호(positive definite)가 되어 가역 행렬임이 보장된다. 즉, $\lambda I$ 항은 $J^T J$의 대각 요소를 강화하여 조건수를 개선하고 특이 행렬이 되는 것을 방지함으로써 알고리즘의 수치적 안정성을 크게 향상시킨다.17

### 3.2  초깃값 설정의 중요성: 수렴 영역과 발산의 갈림길

모든 반복적 최적화 알고리즘과 마찬가지로, 비선형 최소제곱 해법은 탐색을 시작할 초기 파라미터 추정치 $\beta_0$를 반드시 필요로 한다.11 이 초깃값의 선택은 알고리즘의 성공 여부에 지대한 영향을 미친다. 목적 함수 $S(\beta)$의 복잡한 지형 위에서, 초깃값이 최종 해의 수렴 영역(basin of attraction) 내에 위치하지 않으면 알고리즘은 엉뚱한 방향으로 나아가거나, 진동하며, 결국 수렴에 실패할 수 있다.33 설령 수렴하더라도, 원하는 전역 최적해가 아닌 다른 지역 최적해로 수렴할 위험이 존재한다.16

따라서 좋은 초깃값을 설정하는 것은 NLLS 문제 해결의 매우 중요한 단계이다. 이를 위한 몇 가지 실용적인 전략은 다음과 같다.

- **사전 지식 활용:** 모델 파라미터의 물리적 의미나 이론적 배경에 대한 지식을 활용하여 합리적인 범위 내에서 초깃값을 추정한다.35
- **모델 선형화:** 가능하다면, 비선형 모델을 변수 변환 등을 통해 선형 모델로 근사한 뒤, 선형 최소제곱법으로 파라미터의 근사치를 구해 이를 초깃값으로 사용한다. 예를 들어, $y = a e^{bx}$ 모델은 $\ln(y) = \ln(a) + bx$로 선형화할 수 있다.12
- **데이터 시각화:** 데이터를 그래프로 그려보고, 모델 함수의 형태를 고려하여 파라미터 값을 직관적으로 추정한다. 예를 들어, 로지스틱 곡선 모델에서는 데이터의 최대/최소값, 변곡점 위치 등을 통해 파라미터의 대략적인 값을 유추할 수 있다.35
- **전역 최적화 기법 활용:** 그리드 탐색(grid search)이나 무작위 탐색(random search)과 같은 간단한 전역 최적화 기법을 사용하여 광범위한 파라미터 공간을 탐색하고, 가장 좋은 성능을 보인 지점을 NLLS 알고리즘의 초깃값으로 사용한다.35

### 3.3  지역 최적해 문제: 비볼록(Non-convex) 최적화의 함정

NLLS 문제의 목적 함수 $S(\beta)$는 일반적으로 여러 개의 골짜기, 즉 지역 최적해(local minima)를 갖는 비볼록 함수이다.15 가우스-뉴턴이나 레벤버그-마르쿼트와 같은 알고리즘은 본질적으로 국소 최적화(local optimization) 기법이다. 이는 현재 위치에서 가장 가파른 내리막길을 따라 내려가는 방식으로 작동하기 때문에, 일단 한 골짜기에 빠지면 다른 더 깊은 골짜기(전역 최적해, global minimum)가 존재하더라도 그곳에서 벗어나지 못한다.38 따라서 알고리즘이 수렴하여 찾은 해가 단지 여러 지역 최적해 중 하나일 뿐, 전체 문제에 대한 최상의 해라는 보장은 없다.31

이러한 지역 최적해 문제를 완화하고 전역 최적해를 찾을 가능성을 높이기 위한 몇 가지 접근법은 다음과 같다.

- **다중 시작점(Multi-start) 전략:** 서로 다른 여러 초기 추정치에서 알고리즘을 독립적으로 여러 번 실행한다. 그 후, 각 실행에서 얻은 최종 목적 함수 값들을 비교하여 가장 작은 값을 갖는 해를 최종 해로 선택한다. 이는 지역 최적해에 갇힐 위험을 줄이는 가장 간단하고 널리 사용되는 방법이다.40
- **전역 최적화 알고리즘 활용:** 시뮬레이티드 어닐링(Simulated Annealing), 유전 알고리즘(Genetic Algorithms), 입자 군집 최적화(Particle Swarm Optimization)와 같은 전역 최적화 알고리즘을 사용한다. 이러한 알고리즘들은 탐색 과정에서 확률적으로 좋지 않은 해를 선택하기도 함으로써 지역 최적해에서 탈출할 수 있는 메커니즘을 가지고 있다.40 이들을 단독으로 사용하거나, 전역 최적화 알고리즘으로 좋은 후보 영역을 찾은 뒤 그 지점에서 LM 알고리즘을 시작하여 해를 정밀하게 다듬는 하이브리드 방식을 사용할 수 있다.

결론적으로, NLLS 문제의 세 가지 주요 난제인 수치적 불안정성, 초깃값 의존성, 지역 최적해 문제는 서로 분리된 것이 아니라 하나의 인과 관계 사슬로 연결되어 있다. 문제의 근본 원인인 '비볼록성'은 '지역 최적해'의 존재를 야기한다. 복잡한 비볼록 함수에 대한 효율적인 전역 탐색의 어려움 때문에 우리는 '국소 탐색 알고리즘'에 의존하게 되고, 이는 필연적으로 '초깃값 의존성' 문제를 발생시킨다. 마지막으로, 국소 탐색 과정에서 목적 함수의 복잡한 지형을 탐색하다 보면 자코비안 행렬이 불량 조건이 되어 '수치적 불안정성' 문제에 직면하게 된다. 이처럼 NLLS의 난제들은 문제의 근본적인 성질로부터 파생된 필연적인 결과들의 연쇄 반응으로 이해해야 한다.

## 4.  종합 실습: 미카엘리스-멘텐(Michaelis-Menten) 모델 파라미터 추정

이론적 개념을 실제 계산 과정과 연결하여 이해를 돕기 위해, 효소 반응 속도론에서 널리 사용되는 미카엘리스-멘텐 모델의 파라미터를 가우스-뉴턴 방법을 사용하여 추정하는 과정을 단계별로 시연한다.

### 4.1  문제 설정: 데이터 및 비선형 모델 정의

효소 반응 속도 실험을 통해 얻은 7개의 기질 농도($x_i$ =)와 그에 해당하는 반응 속도($y_i$ = Rate) 데이터가 다음과 같이 주어졌다.23

| i                 | 1     | 2     | 3     | 4      | 5      | 6      | 7      |
| ----------------- | ----- | ----- | ----- | ------ | ------ | ------ | ------ |
| 기질 농도 ($x_i$) | 0.038 | 0.194 | 0.425 | 0.626  | 1.253  | 2.500  | 3.740  |
| 반응 속도 ($y_i$) | 0.050 | 0.127 | 0.094 | 0.2122 | 0.2729 | 0.2665 | 0.3317 |

이 데이터에 피팅할 비선형 모델은 미카엘리스-멘텐 방정식이다.23
$$
f(x, \beta) = \frac{\beta_1 x}{\beta_2 + x}
$$
여기서 추정해야 할 파라미터는 최대 반응 속도 $V_{\max}$에 해당하는 $\beta_1$과 미카엘리스 상수 $K_m$에 해당하는 $\beta_2$이다. 파라미터 벡터는 $\beta = [\beta_1, \beta_2]^T$로 정의한다. 가우스-뉴턴 알고리즘을 시작하기 위한 초기 추정치는 $\beta_0 = [0.9, 0.2]^T$로 설정한다.23

### 4.2  가우스-뉴턴 방법의 단계별 수치 계산 (Iteration 1)

1단계: 초기 잔차 벡터 $r(\beta_0)$ 계산

$i$번째 잔차는 $r_i(\beta) = y_i - \frac{\beta_1 x_i}{\beta_2 + x_i}$로 계산된다. 초기 추정치 $\beta_0 = [0.9, 0.2]^T$를 사용하여 각 데이터 포인트에 대한 잔차를 계산하면 다음과 같다.
$$
r(\beta_0) = \begin{bmatrix}
y_1 - f(x_1, \beta_0) \\
y_2 - f(x_2, \beta_0) \\
\vdots \\
y_7 - f(x_7, \beta_0)
\end{bmatrix} = \begin{bmatrix}
0.050 - \frac{0.9 \cdot 0.038}{0.2 + 0.038} \\
0.127 - \frac{0.9 \cdot 0.194}{0.2 + 0.194} \\
\vdots \\
0.3317 - \frac{0.9 \cdot 3.740}{0.2 + 3.740}
\end{bmatrix} = \begin{bmatrix}
-0.0936 \\
-0.3160 \\
-0.5432 \\
-0.6722 \\
-0.5048 \\
-0.5648 \\
-0.5223
\end{bmatrix}
$$
초기 잔차 제곱합은 $S(\beta_0) = \|r(\beta_0)\|_2^2 = 1.445$이다.

2단계: 자코비안 행렬 $J(\beta_0)$ 계산

자코비안 행렬 $J$의 원소는 $(J)_{ij} = \frac{\partial r_i}{\partial \beta_j}$이다. 잔차 함수를 각 파라미터로 편미분하면 다음과 같다.
$$
\frac{\partial r_i}{\partial \beta_1} = -\frac{x_i}{\beta_2 + x_i} \quad , \quad \frac{\partial r_i}{\partial \beta_2} = \frac{\beta_1 x_i}{(\beta_2 + x_i)^2}
$$
$\beta_0 = [0.9, 0.2]^T$에서 자코비안 행렬 $J(\beta_0)$를 계산하면 $7 \times 2$ 행렬이 된다.
$$
J(\beta_0) = \begin{bmatrix}
-0.1597 & 0.5833 \\
-0.4924 & 0.9983 \\
-0.6800 & 0.9168 \\
-0.7580 & 0.7968 \\
-0.8623 & 0.5346 \\
-0.9259 & 0.3142 \\
-0.9492 & 0.2173
\end{bmatrix}
$$
3단계: 정규방정식 구성

$J^T J$와 $J^T r$을 계산한다.
$$
J(\beta_0)^T J(\beta_0) = \begin{bmatrix}
3.533 & -2.480 \\
-2.480 & 3.514
\end{bmatrix}
J(\beta_0)^T r(\beta_0) = \begin{bmatrix}
2.813 \\
-1.344
\end{bmatrix}
$$
4단계: 파라미터 업데이트 스텝 $\Delta\beta$ 계산

선형 시스템 $(J^T J)\Delta\beta = -J^T r$을 푼다.
$$
\begin{bmatrix}
3.533 & -2.480 \\
-2.480 & 3.514
\end{bmatrix} \begin{bmatrix}
\Delta\beta_1 \\
\Delta\beta_2
\end{bmatrix} = -\begin{bmatrix}
2.813 \\
-1.344
\end{bmatrix} = \begin{bmatrix}
-2.813 \\
1.344
\end{bmatrix}
$$
이 선형 방정식을 풀면 $\Delta\beta_0 = [-0.545, 0.328]^T$를 얻는다.

5단계: 파라미터 업데이트

새로운 파라미터 추정치 $\beta_1$을 계산한다.
$$
\beta_1 = \beta_0 + \Delta\beta_0 = \begin{bmatrix}
0.9 \\
0.2
\end{bmatrix} + \begin{bmatrix}
-0.545 \\
0.328
\end{bmatrix} = \begin{bmatrix}
0.355 \\
0.528
\end{bmatrix}
$$
첫 번째 반복만으로 파라미터 추정치는 초깃값 $[0.9, 0.2]^T$에서 $[0.355, 0.528]^T$로 크게 개선되었다. 이 새로운 $\beta_1$을 사용하여 잔차 제곱합을 다시 계산하면 $S(\beta_1) = 0.00843$으로, 초기값 1.445에 비해 극적으로 감소했음을 확인할 수 있다.

### 4.3  반복 과정 및 최종 결과 분석

이러한 계산 과정을 수렴할 때까지 반복하면 파라미터는 점차 최적값으로 수렴한다. 아래 표는 5회 반복 동안의 파라미터 값과 잔차 제곱합의 변화를 요약한 것이다.23

| 반복 (k)   | $\beta_1 (V_{\max})$ | $\beta_2 (K_m)$ | 잔차 제곱합 $S(\beta)$ | $|\Delta\beta|_2$ |
| ---------- | -------------------- | --------------- | ---------------------- | ----------------- |
| 0 (초깃값) | 0.900                | 0.200           | 1.445                  | -                 |
| 1          | 0.355                | 0.528           | 0.00843                | 0.633             |
| 2          | 0.362                | 0.554           | 0.00784                | 0.027             |
| 3          | 0.362                | 0.556           | 0.00784                | 0.002             |
| 4          | 0.362                | 0.556           | 0.00784                | $< 10^{-5}$       |
| 5 (최종)   | **0.362**            | **0.556**       | **0.00784**            | -                 |

표에서 볼 수 있듯이, 알고리즘은 단 몇 번의 반복만으로 최적해 $\beta^* \approx [0.362, 0.556]^T$에 매우 빠르게 수렴한다. 잔차 제곱합은 첫 번째 반복에서 가장 크게 감소하며, 이후 파라미터 변화량 $\|\Delta\beta\|_2$가 급격히 줄어드는 것은 가우스-뉴턴 방법의 2차 수렴에 가까운 특성을 잘 보여준다. 최종적으로 얻은 모델 $f(x) = \frac{0.362x}{0.556+x}$는 주어진 데이터를 매우 잘 설명하는 최적의 곡선이라 할 수 있다. 이 예제는 가우스-뉴턴 방법이 적절한 조건 하에서 얼마나 효율적이고 강력한지를 명확하게 보여준다.

## 5. 결론: 비선형 최소제곱 문제 해결을 위한 종합적 시각

본 보고서는 비선형 최소제곱 문제의 수학적 기초부터 시작하여, 가우스-뉴턴과 레벤버그-마르쿼트라는 두 가지 핵심 해결 알고리즘을 심도 있게 분석하고, 실제 적용 시 발생하는 주요 난제들을 고찰하였다. 분석을 통해 다음과 같은 종합적인 결론을 도출할 수 있다.

첫째, 비선형 최소제곱 문제의 핵심은 모델의 '파라미터에 대한 비선형성'에 있으며, 이는 닫힌 형태의 해석적 해법을 불가능하게 만들고 반복적 수치 해법을 필수적으로 요구하는 근본적인 원인이다. 이로 인해 발생하는 비볼록 목적 함수는 지역 최적해, 초깃값 의존성, 수치적 불안정성이라는 세 가지 상호 연결된 난제를 파생시킨다.

둘째, 가우스-뉴턴 방법은 2차 미분(헤시안) 계산을 피하면서도 2차 수렴에 가까운 빠른 속도를 제공하는 영리한 알고리즘이다. 이는 복잡한 비선형 최적화 문제를 각 단계에서 더 간단한 선형 최소제곱 문제로 근사하여 푸는 전략에 기반한다. 그러나 이 방법은 잔차가 크거나 자코비안 행렬이 불량 조건일 때 수치적으로 불안정하고 발산하기 쉬운 명백한 한계를 가진다.

셋째, 레벤버그-마르쿼트 알고리즘은 가우스-뉴턴의 속도와 경사 하강법의 안정성을 감쇠 인자 $\lambda$를 통해 동적으로 결합함으로써 비선형 최소제곱 문제에 대한 가장 강인하고 신뢰성 높은 해법을 제공한다. 이 알고리즘은 국소 선형 모델의 신뢰도를 수학적으로 관리하는 정교한 적응형 전략을 통해, 해에서 멀리 있을 때는 안정적으로 탐색하고 해에 가까워지면 빠르게 수렴한다. 이러한 특성 덕분에 LM 알고리즘은 다양한 공학 및 과학 분야에서 사실상의 표준으로 널리 사용되고 있다.

결론적으로, 성공적인 비선형 최소제곱 문제 해결은 단순히 우수한 알고리즘을 선택하는 것 이상의 종합적인 접근을 요구한다. 문제에 대한 깊은 이해를 바탕으로 한 신중한 모델 정식화, 파라미터의 물리적 의미를 고려한 합리적인 초깃값 설정, 그리고 다중 시작점 전략 등을 통한 지역 최적해 함정 회피 노력이 병행되어야 한다. 향후 연구는 희소(sparse) 자코비안 구조를 활용하여 대규모(large-scale) 문제의 계산 효율성을 높이는 기법, 그리고 전역 최적화 알고리즘과의 체계적인 결합을 통해 보다 신뢰성 높은 해를 찾는 방향으로 지속될 것이다. 비선형 최소제곱 문제는 앞으로도 데이터 기반 모델링과 최적화 분야에서 핵심적인 역할을 수행할 것이며, 그 이론과 실제에 대한 깊이 있는 이해는 모든 관련 분야 전문가에게 필수적인 역량으로 남을 것이다.

#### **참고 자료**

1. 최적화: 최소 제곱법을 사용한 모델 성능 최적화 - FasterCapital, 8월 17, 2025에 액세스, [https://fastercapital.com/ko/content/%EC%B5%9C%EC%A0%81%ED%99%94--%EC%B5%9C%EC%86%8C-%EC%A0%9C%EA%B3%B1%EB%B2%95%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-%EB%AA%A8%EB%8D%B8-%EC%84%B1%EB%8A%A5-%EC%B5%9C%EC%A0%81%ED%99%94.html](https://fastercapital.com/ko/content/최적화--최소-제곱법을-사용한-모델-성능-최적화.html)
2. [선형대수학] 최소제곱법 (Method of Least Squares) - SUBORATORY - 티스토리, 8월 17, 2025에 액세스, https://subprofessor.tistory.com/104
3. 선형, 비선형 회귀모형 적합방법과 Bias-variance Trade off 관계, 8월 17, 2025에 액세스, https://mustlearning.tistory.com/37
4. [개념 정리] 가우스-뉴턴 (Gauss-Newton) 최적화 기법 - xoft - 티스토리, 8월 17, 2025에 액세스, https://xoft.tistory.com/90
5. 최소제곱법(Least Squares Method)의 개념, 8월 17, 2025에 액세스, [https://healthy119.tistory.com/entry/%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95Least-Squares-Method%EC%9D%98-%EA%B0%9C%EB%85%90](https://healthy119.tistory.com/entry/최소제곱법Least-Squares-Method의-개념)
6. Gauss-Newton Method Explained - Number Analytics, 8월 17, 2025에 액세스, https://www.numberanalytics.com/blog/gauss-newton-method-explained
7. 최소제곱법, 최소자승법(Least Squares Method), 정사영 행렬(Projection Matrix), Normal Equation에 대한 개념 정리 - AI와 데이터의 모든 것, 8월 17, 2025에 액세스, https://kyull-it.tistory.com/165
8. 오류 최소화: 최소 제곱법을 사용한 불일치 감소 - FasterCapital, 8월 17, 2025에 액세스, [https://fastercapital.com/ko/content/%EC%98%A4%EB%A5%98-%EC%B5%9C%EC%86%8C%ED%99%94--%EC%B5%9C%EC%86%8C-%EC%A0%9C%EA%B3%B1%EB%B2%95%EC%9D%84-%EC%82%AC%EC%9A%A9%ED%95%9C-%EB%B6%88%EC%9D%BC%EC%B9%98-%EA%B0%90%EC%86%8C.html](https://fastercapital.com/ko/content/오류-최소화--최소-제곱법을-사용한-불일치-감소.html)
9. The Difference between Linear and Nonlinear Regression Models - Statistics By Jim, 8월 17, 2025에 액세스, https://statisticsbyjim.com/regression/difference-between-linear-nonlinear-regression-models/
10. Linear versus nonlinear regression? Linear regressions with a curved line of best fit? Different equations? Confused. - Reddit, 8월 17, 2025에 액세스, https://www.reddit.com/r/statistics/comments/7bo2ig/linear_versus_nonlinear_regression_linear/
11. Non-linear least squares - Wikipedia, 8월 17, 2025에 액세스, https://en.wikipedia.org/wiki/Non-linear_least_squares
12. Nonlinear Least Squares Regression (Curve Fitter) - Interactive Statistical Calculation Pages, 8월 17, 2025에 액세스, https://statpages.info/nonlin.html
13. 13. Nonlinear least squares, 8월 17, 2025에 액세스, http://www.seas.ucla.edu/~vandenbe/133A/lectures/nlls.pdf
14. Introduction to Least-Squares Fitting - MATLAB & Simulink - MathWorks, 8월 17, 2025에 액세스, https://www.mathworks.com/help/curvefit/least-squares-fitting.html
15. When does non-convex nonlinear least squares have local minima? - Math Stack Exchange, 8월 17, 2025에 액세스, https://math.stackexchange.com/questions/2556738/when-does-non-convex-nonlinear-least-squares-have-local-minima
16. Lecture 17-18: Least Squares Optimization - VNAV, 8월 17, 2025에 액세스, https://vnav.mit.edu/material/17-18-NonLinearLeastSquares-notes.pdf
17. 함수최적화 기법 정리 (Levenberg–Marquardt 방법 ... - 다크 프로그래머, 8월 17, 2025에 액세스, https://darkpgmr.tistory.com/142
18. 4.5.3 Variants of Newton's Method 4.5.4 Gauss Newton Approximation - CSE, IIT Bombay, 8월 17, 2025에 액세스, https://www.cse.iitb.ac.in/~cs709/notes/enotes/lecture21b.pdf
19. Solving Non-linear Least Squares - Ceres Solver, 8월 17, 2025에 액세스, http://ceres-solver.org/nnls_solving.html
20. Approximate Gauss-Newton methods for nonlinear least squares problems - University of Reading, 8월 17, 2025에 액세스, https://www.reading.ac.uk/maths-and-stats/-/media/project/uor-main/schools-departments/maths/documents/0904pdf.pdf?la=en&hash=45BCE1B5038E631F3DCAF7450E46B114
21. solving nonlinear least-squares problems with the gauss-newton and levenberg-marquardt methods - Math@LSU, 8월 17, 2025에 액세스, [https://www.math.lsu.edu/system/files/MunozGroup1%20-%20Paper.pdf](https://www.math.lsu.edu/system/files/MunozGroup1 - Paper.pdf)
22. Unit I: Data Fitting Chapter I.4: Nonlinear Least Squares, 8월 17, 2025에 액세스, https://courses.seas.harvard.edu/courses/am205/fall13/AM205_unit_1_chapter_4.pdf
23. Gauss–Newton algorithm - Wikipedia, 8월 17, 2025에 액세스, [https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm](https://en.wikipedia.org/wiki/Gauss–Newton_algorithm)
24. 복소 구배 최소자승법에의한 파형 역산을 이용한 지하 구조의 영상화 장치 및 방법, 8월 17, 2025에 액세스, https://patents.google.com/patent/KR20110121402A/ko
25. 16. Gauss–Newton method, 8월 17, 2025에 액세스, http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf
26. 최적화 이론 기초 정리 (Gradient Descent, Newton Method, Gauss ..., 8월 17, 2025에 액세스, https://gaussian37.github.io/math-calculus-basic_optimization/
27. [개념 정리] Levenberg-Marquardt 알고리즘 : 최적화 기법, 8월 17, 2025에 액세스, https://xoft.tistory.com/82
28. Optimzation - Haehwan Lee, 8월 17, 2025에 액세스, https://haehwan.github.io/posts/DS-optimization/
29. Nonlinear Least-Squares Fitting - GSL 2.8 documentation - GNU, 8월 17, 2025에 액세스, https://www.gnu.org/software/gsl/doc/html/nls.html
30. The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems - Duke People, 8월 17, 2025에 액세스, https://people.duke.edu/~hpgavin/lm.pdf
31. Levenberg–Marquardt algorithm - Wikipedia, 8월 17, 2025에 액세스, [https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm](https://en.wikipedia.org/wiki/Levenberg–Marquardt_algorithm)
32. en.wikipedia.org, 8월 17, 2025에 액세스, [https://en.wikipedia.org/wiki/Least_squares#:~:text=Differences%20between%20linear%20and%20nonlinear%20least%20squares,-The%20model%20function&text=are%20either%20constant%20or%20depend,LLSQ%20does%20not%20require%20them.](https://en.wikipedia.org/wiki/Least_squares#:~:text=Differences between linear and nonlinear least squares,-The model function&text=are either constant or depend,LLSQ does not require them.)
33. 뉴턴 방법 - 위키백과, 우리 모두의 백과사전, 8월 17, 2025에 액세스, [https://ko.wikipedia.org/wiki/%EB%89%B4%ED%84%B4_%EB%B0%A9%EB%B2%95](https://ko.wikipedia.org/wiki/뉴턴_방법)
34. An Initial Guess Free Method for Least Squares Parameter Estimation in Nonlinear Models, 8월 17, 2025에 액세스, https://www.researchgate.net/publication/345437767_An_Initial_Guess_Free_Method_for_Least_Squares_Parameter_Estimation_in_Nonlinear_Models
35. How to guess initialization parameters for non-linear curve fitting to get the best fit? - MATLAB Answers - MathWorks, 8월 17, 2025에 액세스, https://www.mathworks.com/matlabcentral/answers/1963849-how-to-guess-initialization-parameters-for-non-linear-curve-fitting-to-get-the-best-fit
36. How to choose initial values for nonlinear least squares fit - Cross Validated, 8월 17, 2025에 액세스, https://stats.stackexchange.com/questions/62995/how-to-choose-initial-values-for-nonlinear-least-squares-fit
37. Introduction to Non-Linear Least Squares - VNAV, 8월 17, 2025에 액세스, https://vnav.mit.edu/material/17-IntroNonlinearLeastSquares-slides.pdf
38. Beware the Local Minima - Learning Tree, 8월 17, 2025에 액세스, https://www.learningtree.com/blog/beware-local-minima/
39. lsqnonlin - Solve nonlinear least-squares (nonlinear data-fitting) problems - MathWorks, 8월 17, 2025에 액세스, https://www.mathworks.com/help/optim/ug/lsqnonlin.html
40. The Curse of Local Minima: How to Escape and Find the Global Minimum | by Mohit Mishra, 8월 17, 2025에 액세스, https://mohitmishra786687.medium.com/the-curse-of-local-minima-how-to-escape-and-find-the-global-minimum-fdabceb2cd6a

